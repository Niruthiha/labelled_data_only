{
  "repository": "BerriAI/litellm",
  "repository_info": {
    "repo": "BerriAI/litellm",
    "stars": 24386,
    "language": "Python",
    "description": "Python SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, Groq]",
    "url": "https://github.com/BerriAI/litellm",
    "topics": [
      "ai-gateway",
      "anthropic",
      "azure-openai",
      "bedrock",
      "gateway",
      "langchain",
      "litellm",
      "llm",
      "llm-gateway",
      "llmops",
      "mcp-gateway",
      "openai",
      "openai-proxy",
      "vertex-ai"
    ],
    "created_at": "2023-07-27T00:09:52Z",
    "updated_at": "2025-06-22T01:51:19Z",
    "search_query": "openai anthropic language:python stars:>2",
    "total_issues_estimate": 66,
    "labeled_issues_estimate": 63,
    "labeling_rate": 95.5,
    "sample_labeled": 21,
    "sample_total": 22,
    "has_issues": true,
    "repo_id": 671269505,
    "default_branch": "main",
    "size": 454547
  },
  "extraction_date": "2025-06-21T23:32:03.412523",
  "extraction_type": "LABELED_ISSUES_ONLY",
  "total_labeled_issues": 500,
  "issues": [
    {
      "issue_number": 11956,
      "title": "[Feature]: expand vertex passthrough credential check to be region independent",
      "body": "### The Feature\n\nRemove location check when searching for credentials.\n\n<img width=\"1133\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/675c3c65-e4bc-4fbc-b37d-f9b3486a0071\" />\n\n### Motivation, pitch\n\nIt will allow me to use `global` / any other location with the vertex passthrough \n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "krrishdholakia",
      "author_type": "User",
      "created_at": "2025-06-22T00:43:04Z",
      "updated_at": "2025-06-22T00:43:04Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11956/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11956",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11956",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:08.313236",
      "comments": []
    },
    {
      "issue_number": 11920,
      "title": "[Bug]: Improper handling of `null` token usages for Anthropic chats when streaming",
      "body": "### What happened?\n\n## Bug Summary\nTL;DR: `null` token usage values aren't handled properly at https://github.com/BerriAI/litellm/blob/main/litellm/llms/anthropic/chat/transformation.py#L805, resulting in \"cannot add None\" errors.\n\n## Background & Bug Description\nAt our workplace we use an internal proxy for LLMs we've made ourselves, which uses the anthropic SDK underneath. We have some tools (namely Aider) that use LiteLLM as a library, pointing at this proxy.\n\nIn `v0.51.0` of the Anthropic SDK, Anthropic added some fields to the message_delta event's data on token usage information which had None as a default - see [https://github.com/anthropics/anthropic-sdk-python/compare/v0.50.0...v0.51.0](https://github.com/anthropics/anthropic-sdk-python/compare/v0.50.0...v0.51.0), look for `class MessageDeltaUsage(BaseModel)`.\n\nThe result is that sometimes our proxy sends back `null` for message streams, since it directly forwards what the Anthropic SDK outputs. Example chunk:\n```\n{\n  \"delta\": {\n    \"stop_reason\": \"end_turn\",\n    \"stop_sequence\": null\n  },\n  \"type\": \"message_delta\",\n  \"usage\": {\n    \"cache_creation_input_tokens\": null,\n    \"cache_read_input_tokens\": null,\n    \"input_tokens\": null,\n    \"output_tokens\": 43,\n    \"server_tool_use\": null\n  }\n}\n```\n\nThis results in code in the function at https://github.com/BerriAI/litellm/blob/main/litellm/llms/anthropic/chat/transformation.py#L805 crashing as it doesn't have a proper null guard - it only checks for field presence, not if the fields are actually ints (see crash logs).\n\n## Proposed fix\nIf the values extracted from the usage object are `None`, default to 0. I am happy to implement such a fix myself, just want to check if it would be useful to you (it certainly would be useful for us as it is blocking us upgrading our anthropic SDK version!)\n\n\n\n### Relevant log output\n\n```shell\nlitellm.APIConnectionError: unsupported operand type(s) for +=: 'NoneType' and 'NoneType'\nTraceback (most recent call last):\n  File \"/home/ksambhi/.local/lib/python3.11/site-packages/litellm/litellm_core_utils/streaming_handler.py\", line 1508, in __next__\n    chunk = next(self.completion_stream)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ksambhi/.local/lib/python3.11/site-packages/litellm/llms/anthropic/chat/handler.py\", line 773, in __next__\n    return self.chunk_parser(chunk=data_json)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ksambhi/.local/lib/python3.11/site-packages/litellm/llms/anthropic/chat/handler.py\", line 656, in chunk_parser\n    usage = self._handle_usage(anthropic_usage_chunk=message_delta[\"usage\"])\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ksambhi/.local/lib/python3.11/site-packages/litellm/llms/anthropic/chat/handler.py\", line 496, in _handle_usage\n    return AnthropicConfig().calculate_usage(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ksambhi/.local/lib/python3.11/site-packages/litellm/llms/anthropic/chat/transformation.py\", line 651, in calculate_usage\n    prompt_tokens += cache_read_input_tokens\nTypeError: unsupported operand type(s) for +=: 'NoneType' and 'NoneType'\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.68.1\n\n### Twitter / LinkedIn details\n\n[https://uk.linkedin.com/in/kishan-sambhi](https://uk.linkedin.com/in/kishan-sambhi)",
      "state": "open",
      "author": "Gum-Joe",
      "author_type": "User",
      "created_at": "2025-06-20T10:34:34Z",
      "updated_at": "2025-06-22T00:41:09Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11920/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11920",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11920",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:08.313264",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "PR here is welcome @Gum-Joe ",
          "created_at": "2025-06-22T00:40:51Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Just bump me once you have something live. And please add a unit test for your change inside `test_litellm/` ",
          "created_at": "2025-06-22T00:41:09Z"
        }
      ]
    },
    {
      "issue_number": 11935,
      "title": "[Bug]: Vertex AI Claude Prompt Caching costs are not tracked",
      "body": "### What happened?\n\nUsing LiteLLM GCP Vertex Passthrough with Claude Code.\nGot a `5.94$` spent alert from Claude Code but LiteLLM shows the spending as `0.54$`.\n\nClaude Code excessively used \"cache read\" and \"cache write\" in my session. This seems to be missing from the LiteLLM calculations.\n\n```\n> /cost \n  ‚éø  Total cost:            $5.94\n...\n     Token usage by model:\n         claude-3-5-haiku:  105.4k input, 3.6k output, 0 cache read, 0 cache write\n         claude-sonnet:  450 input, 36.1k output, 14.0m cache read, 293.7k cache write\n```\n\nFrom the numbers and model pricing, it seems like LiteLLM is only considering input / output tokens. However the cache cost is not included.\n\n### Configuration\n\n```\nmodel_list:\n  - model_name: vertex-passthrough-us-east5\n    litellm_params:\n      model: vertex_ai/*\n      use_in_pass_through: true\n      vertex_project: project-name\n      vertex_location: us-east5\n      vertex_credentials: /some-path\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.6.post1-nightly\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "yigitcan-dh",
      "author_type": "User",
      "created_at": "2025-06-20T21:46:50Z",
      "updated_at": "2025-06-22T00:36:36Z",
      "closed_at": null,
      "labels": [
        "bug",
        "ai tools"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11935/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11935",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11935",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:08.505458",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Hey @yigitcan-dh can you check the UI for a spend log called via claude code - it'll help confirm if prompt caching costs are not being tracked. \n\nHere's what i'm looking for: \n\n<img width=\"1498\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/a1177b48-c8be-4b5d-b656-cdaca52e7a3a\" />",
          "created_at": "2025-06-22T00:36:36Z"
        }
      ]
    },
    {
      "issue_number": 11940,
      "title": "[Bug]: Cannot pass dimensions to openai compatible embedding model",
      "body": "### What happened?\n\nI was trying to pass the `dimensions` param to an openai compatible embedding model with the below code, but failed with the error message `litellm.UnsupportedParamsError: Setting dimensions is not supported for OpenAI `text-embedding-3` and later models. `\n```python\nimport os\nfrom openai import OpenAI\n#%%\nclient = OpenAI(\n    api_key=\"\", \n    base_url=\"\"  \n)\n\ncompletion = client.embeddings.create(\n    model=\"openai/text-embedding-v4\",\n    input=\"test\",\n    dimensions=1024,\n    encoding_format=\"float\",\n)\n```\nThe error was raised from this piece of code. This check limits the `dimensions` param to `text-embedding-3` model only.  But the custom embedding api I'm using support the `dimensions` param, are there any workaround for this issue?\n```python\n## raise exception if non-default value passed for non-openai/azure embedding calls\n    elif custom_llm_provider == \"openai\":\n        # 'dimensions` is only supported in `text-embedding-3` and later models\n\n        if (\n            model is not None\n            and \"text-embedding-3\" not in model\n            and \"dimensions\" in non_default_params.keys()\n        ):\n            raise UnsupportedParamsError(\n                status_code=500,\n                message=\"Setting dimensions is not supported for OpenAI `text-embedding-3` and later models. To drop it from the call, set `litellm.drop_params = True`.\",\n            )\n        else:\n            optional_params = non_default_params\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "sheldonxxxx",
      "author_type": "User",
      "created_at": "2025-06-21T06:05:29Z",
      "updated_at": "2025-06-22T00:34:07Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11940/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11940",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11940",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:08.670692",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Hey @sheldonxxxx i think we just need to support it as an allowed_openai_param - https://docs.litellm.ai/docs/completion/drop_params#specify-allowed-openai-params-in-a-request\n\n",
          "created_at": "2025-06-22T00:34:07Z"
        }
      ]
    },
    {
      "issue_number": 11944,
      "title": "[Bug]: aiohttp not respecting HTTP_PROXY settings ",
      "body": "### What happened?\n\nWhen we upgraded to newer version with performance improvements, we are not able to connect to openai or gemini from the dev servers. \n‚úÖ  curl from inside the container to OpenAI works\n‚ùå But Litellm (running inside the same container) hangs when calling OpenAI‚Äôs API\n\n\nwe can see litellm is running\n```\n/app # curl http://localhost:8443/health/liveliness\n\"I'm alive!\"/app #\n```\n\n\nMy guess is\n```\nrequests respects HTTP_PROXY, HTTPS_PROXY, and NO_PROXY by default.\naiohttp does not automatically use system environment proxy variables unless explicitly configured.\n```\n\n### Relevant log output\n\n```shell\nInside the container I can see the call to openai works\n/app # ./test_script.sh https://api.openai.com\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"object\": \"embedding\",\n      \"index\": 0,\n      \"embedding\": [\n        0.005132983,\n        0.017242905,\n        -0.018698474,\n        -0.018558515,\n        -0.047250036,\n9:54\nI can also verify, when we use requests from python it works\n>>> requests.post(url, headers=headers, json=payload, timeout=30)\n<Response [200]>\n>>> requests.post(url, headers=headers, json=payload, timeout=30).json()\n{'id': 'chatcmpl-BkWLOmV044CDux0FSOQMFomTORjY6', 'object': 'chat.completion', 'created': 1750427118, 'model': 'gpt-4o-2024-08-06', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"Hello! I'm here and ready to help you with anything you need. How can I assist you today?\", 'refusal': None, 'annotations': []}, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 14, 'completion_tokens': 21, 'total_tokens': 35, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'service_tier': 'default', 'system_fingerprint': 'fp_07871e2ad8'}\nbut when we use async io http, it doesn't\nasync def call_openai():\n    api_key = os.getenv(\"OPENAI_KEY\")\n    if not api_key:\n        raise ValueError(\"‚ùå OPENAI_KEY environment variable is not set.\")\n\n    url = \"https://api.openai.com/v1/chat/completions\"\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\"\n    }\n    payload = {\n        \"model\": \"gpt-4o\",\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"Hey, how's it going?\"\n            }\n        ]\n    }\n\n    timeout = aiohttp.ClientTimeout(total=30)\n\n    async with aiohttp.ClientSession(timeout=timeout) as session:\n        try:\n            async with session.post(url, headers=headers, json=payload) as response:\n                if response.status == 200:\n                    data = await response.json()\n                    print(\"‚úÖ Response:\")\n                    print(data[\"choices\"][0][\"message\"][\"content\"])\n                else:\n                    text = await response.text()\n                    print(f\"‚ùå Error {response.status}: {text}\")\n        except asyncio.TimeoutError:\n            print(\"‚ùå Request timed out.\")\n        except Exception as e:\n            print(f\"‚ùå Exception: {e}\")\n\n# Run the async function\nasyncio.run(call_openai())\n9:55\nHowever when we call it from litellm\n/app # curl -v --connect-timeout 5 --max-time 20 \\\n>   --output /dev/null \\\n>   --header \"Authorization: Bearer sk-BUISoMyez5ncKVAK-z06-Q\" \\\n>   --header \"Content-Type: application/json\" \\\n>   --url http://localhost:8443/v1/chat/completions \\\n>   --data '{\n>     \"model\": \"gpt-4o-mini\",\n>     \"messages\": [\n>       {\n>         \"role\": \"user\",\n>         \"content\": \"Assume you have these Invoice numbers INV_01, INV_02, IN\nV_03, INV_04, INV_05. Could you predict what the next invoice number could be.\n Then also give an explanation.\"\n>       }\n>     ],\n>     \"stream\": false\n>   }' \\\n>   -w \"\\nDNS: %{time_namelookup}s\\nConnect: %{time_connect}s\\nTLS: %{time_app\nconnect}s\\nStart Transfer: %{time_starttransfer}s\\nTotal: %{time_total}s\\n\"\n* Uses proxy env variable NO_PROXY == '169.254.169.254,169.254.170.2,/var/run/docker.sock,*.amazonaws.com,*.cashview.com,localhost,*.githubusercontent.com,*.billdot.io'\n* Host localhost:8443 was resolved.\n* IPv6: ::1\n* IPv4: 127.0.0.1\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0*   Trying [::1]:8443...\n* connect to ::1 port 8443 from ::1 port 60674 failed: Connection refused\n*   Trying 127.0.0.1:8443...\n* Connected to localhost (127.0.0.1) port 8443\n* using HTTP/1.x\n> POST /v1/chat/completions HTTP/1.1\n> Host: localhost:8443\n> User-Agent: curl/8.11.0\n> Accept: */*\n> Authorization: Bearer sk-BUISoMyez5ncKVAK-z06-Q\n> Content-Type: application/json\n> Content-Length: 305\n>\n} [305 bytes data]\n* upload completely sent off: 305 bytes\n100   305    0     0  100   305      0     16  0:00:19  0:00:19 --:--:--     0* Operation timed out after 20002 milliseconds with 0 bytes received\n100   305    0     0  100   305      0     15  0:00:20  0:00:20 --:--:--     0\n* closing connection #0\ncurl: (28) Operation timed out after 20002 milliseconds with 0 bytes received\n\nDNS: 0.000064s\nConnect: 0.000551s\nTLS: 0.000000s\nStart Transfer: 0.000000s\nTotal: 20.002198s\n\n\n\n\nI also verified when we specify proxy variables it works. The fix would look something like this\n\nasync def call_openai_via_proxy():\n    api_key = os.getenv(\"OPENAI_KEY\")\n    if not api_key:\n        raise ValueError(\"‚ùå OPENAI_KEY environment variable is not set.\")\n\n    url = \"https://api.openai.com/v1/chat/completions\"\n    proxy = \"http://proxy.local:3128\"  # üîÅ your proxy URL\n\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\"\n    }\n\n    payload = {\n        \"model\": \"gpt-4o\",\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"Hey, how's it going?\"\n            }\n        ]\n    }\n\n    timeout = aiohttp.ClientTimeout(total=60)\n\n    # Optional: restrict to IPv4 to avoid IPv6 fallback issues\n    connector = aiohttp.TCPConnector(family=socket.AF_INET)\n\n    async with aiohttp.ClientSession(connector=connector, timeout=timeout, trust_env=True) as session:\n        try:\n            async with session.post(url, headers=headers, json=payload, proxy=proxy) as response:\n                if response.status == 200:\n                    data = await response.json()\n                    print(\"‚úÖ Response:\", data[\"choices\"][0][\"message\"][\"content\"])\n                else:\n                    print(f\"‚ùå Status {response.status}: {await response.text()}\")\n        except asyncio.TimeoutError:\n            print(\"‚ùå Request timed out.\")\n        except Exception as e:\n            print(f\"‚ùå Error: {e}\")\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.72.6\n\n### Twitter / LinkedIn details\n\nhttps://www.linkedin.com/in/deepanshululla/",
      "state": "open",
      "author": "deepanshululla",
      "author_type": "User",
      "created_at": "2025-06-21T16:00:31Z",
      "updated_at": "2025-06-22T00:32:10Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11944/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11944",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11944",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:08.859615",
      "comments": []
    },
    {
      "issue_number": 11951,
      "title": "[Bug]: Custom Root Path - Admin UI login failing with http status code- 303 see other",
      "body": "### What happened?\n\nDoes not matter how many times I try to login on litellm admin ui, it keeps on redirecting to same page.\nThe login endpoint keeps sending status code `303 see other`\n\nI am using latest litellm 1.72.9 with custom SERVER_ROOT_PATH=/bpa-litellm\n\n<img width=\"1728\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/2359e607-876d-4fe5-831f-bd45b3f00202\" />\n\n### Relevant log output\n\n```shell\nINFO:     172.16.0.23:36516 - \"GET /bpa-litellm/.well-known/litellm-ui-config HTTP/1.1\" 200 OK\nINFO:     172.16.0.23:36516 - \"GET /bpa-litellm/sso/key/generate HTTP/1.1\" 200 OK\nINFO:     172.16.0.23:51622 - \"POST /bpa-litellm/login HTTP/1.1\" 303 See Other\nINFO:     172.16.0.23:33994 - \"POST /bpa-litellm/login HTTP/1.1\" 303 See Other\nINFO:     172.16.0.23:33994 - \"POST /bpa-litellm/login HTTP/1.1\" 303 See Other\nINFO:     172.16.0.23:33994 - \"POST /bpa-litellm/login HTTP/1.1\" 303 See Other\nINFO:     172.16.0.23:33994 - \"POST /bpa-litellm/login HTTP/1.1\" 303 See Other\nINFO:     172.16.0.23:33994 - \"POST /bpa-litellm/login HTTP/1.1\" 303 See Other\nINFO:     172.16.0.23:33994 - \"POST /bpa-litellm/login HTTP/1.1\" 303 See Other\nINFO:     172.16.0.23:33994 - \"POST /bpa-litellm/login HTTP/1.1\" 303 See Other\nINFO:     172.16.0.23:33994 - \"POST /bpa-litellm/login HTTP/1.1\" 303 See Other\nINFO:     172.16.0.23:33994 - \"POST /bpa-litellm/login HTTP/1.1\" 303 See Other\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.9\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "thedevd",
      "author_type": "User",
      "created_at": "2025-06-21T18:38:55Z",
      "updated_at": "2025-06-22T00:31:02Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11951/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11951",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11951",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:08.859637",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "hi @thedevd can you please share the complete log output",
          "created_at": "2025-06-22T00:30:30Z"
        }
      ]
    },
    {
      "issue_number": 4001,
      "title": "[Feature]: Allow Redis Semantic caching with custom Embedding models",
      "body": "### The Feature\n\nCurrently, I notice that the schema for the redis semantic cache enforces a dimension size of **1536**. This works well with OpenAI's text-embedding-ada-002 models but fail for any other model. \r\n\r\n```python\r\nschema = {\r\n            \"index\": {\r\n                \"name\": \"litellm_semantic_cache_index\",\r\n                \"prefix\": \"litellm\",\r\n                \"storage_type\": \"hash\",\r\n            },\r\n            \"fields\": {\r\n                \"text\": [{\"name\": \"response\"}],\r\n                \"text\": [{\"name\": \"prompt\"}],\r\n                \"vector\": [\r\n                    {\r\n                        \"name\": \"litellm_embedding\",\r\n                        \"dims\": 1536,\r\n                        \"distance_metric\": \"cosine\",\r\n                        \"algorithm\": \"flat\",\r\n                        \"datatype\": \"float32\",\r\n                    }\r\n                ],\r\n            },\r\n        }\r\n```\r\nPlease make the embedding dims configurable from the model-config.yaml file so that a larger range of embedding models deployed with LiteLLM can be used for the cache.\n\n### Motivation, pitch\n\nFor caching, much smaller embedding models might be preferred due to their cost and speed. If the dims becomes a configurable item, then this opens up much more interesting modes of caching and cache optimization. It should also reduce costs significantly as this would avoid calls to OpenAI every time a new user query is recieved. \n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "anandsriraman",
      "author_type": "User",
      "created_at": "2024-06-04T10:26:26Z",
      "updated_at": "2025-06-22T00:02:19Z",
      "closed_at": "2025-06-22T00:02:19Z",
      "labels": [
        "enhancement",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/4001/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/4001",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/4001",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:09.045548",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "hi @anandsriraman I followed up over Linkedin to better understand what you need. This is my linkedin: https://www.linkedin.com/in/reffajnaahsi/ ",
          "created_at": "2024-06-05T20:29:42Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-01-28T03:00:06Z"
        },
        {
          "author": "qdrddr",
          "body": "+1",
          "created_at": "2025-03-16T02:48:58Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-15T00:02:14Z"
        }
      ]
    },
    {
      "issue_number": 8954,
      "title": "[Feature]: Enhance Helm Chart",
      "body": "### The Feature\n\nImprove the Helm Chart with some features to make it more extensible. In my opinion, these features are necessary:\n\n- [x] Avoid forcing environment variables to follow `key: value` convention (see [this](https://github.com/BerriAI/litellm/blob/db83cbe5c04875032e2fc754800172ceb46baed0/deploy/charts/litellm-helm/templates/deployment.yaml#L94-L98)). This issue prevents the user to configure environment variables for the deployment in this way:\n```yaml\n- name: DD_ENV\n  valueFrom:\n    fieldRef:\n      fieldPath: metadata.labels['tags.datadoghq.com/env']\n- name: DD_SERVICE\n  valueFrom:\n    fieldRef:\n      fieldPath: metadata.labels['tags.datadoghq.com/service']\n- name: DD_VERSION\n  valueFrom:\n    fieldRef:\n      fieldPath: metadata.labels['tags.datadoghq.com/version']\n- name: DD_AGENT_HOST\n  valueFrom:\n    fieldRef:\n      fieldPath: status.hostIP\n```\n\n- [x] Avoid forcing the user to create the masterkey in the `values.yaml` file to allow people use other secret mechanisms like [Secret Store CSI Driver](https://secrets-store-csi-driver.sigs.k8s.io/) where secrets are created directly from external secrets storage like Hashicorp Vault.\n- [ ] Publish the Chart with the proper version (now the version of the chart is [0.4.1](https://github.com/BerriAI/litellm/blob/db83cbe5c04875032e2fc754800172ceb46baed0/deploy/charts/litellm-helm/Chart.yaml#L21) but we are releasing `0.1.626` and it has to be used in this way being harder to follow the changes:\n```yaml\n  - name: litellm-helm\n    version: 0.1.604\n    repository: oci://ghcr.io/berriai\n    condition: litellm-helm.enabled\n```\n\nAlso, as other optional improvements that I think could be good:\n- [x] Sometimes integration tests are not possible or are not necessary. Unit tests are better in many cases and we can configure them with [helm-unittest](https://github.com/helm-unittest/helm-unittest).\n- [ ] Create JSON Schema  (see [Backstage example](https://github.com/backstage/charts/blob/main/charts/backstage/values.schema.json))\n- [ ] Create a separated repository for charts like is done in other open source / CNCF projects (e.g. [backstage](https://github.com/backstage/charts), [open-webui](https://github.com/open-webui/helm-charts/tree/main), [keda](https://github.com/kedacore/charts))\n\nOf course, I can collaborate but I wanted to open the issue first to discuss if you have some concerns @ishaan-jaff @krrishdholakia \n\n### Motivation, pitch\n\nImprove the helm chart to make it more extensible and follow community conventions.\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\nhttps://www.linkedin.com/in/mcaneteubeda/",
      "state": "closed",
      "author": "mknet3",
      "author_type": "User",
      "created_at": "2025-03-03T12:01:34Z",
      "updated_at": "2025-06-22T00:02:12Z",
      "closed_at": "2025-06-22T00:02:12Z",
      "labels": [
        "enhancement",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8954/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8954",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8954",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:09.297392",
      "comments": [
        {
          "author": "mknet3",
          "body": "hey @ishaan-jaff @krrishdholakia , I've added Helm Unit Tests to start improving the Chart with other PRs\nThe configuration is in this PR -> https://github.com/BerriAI/litellm/pull/9068\n\nPlease, review when you have time üôè . Thank you!",
          "created_at": "2025-03-10T19:42:29Z"
        },
        {
          "author": "JakobStadlhuber",
          "body": "> Avoid forcing the user to create the masterkey in the values.yaml file to allow people use other secret mechanisms like [Secret Store CSI Driver](https://secrets-store-csi-driver.sigs.k8s.io/) where secrets are created directly from external secrets storage like Hashicorp Vault.\n\nthis would be rea",
          "created_at": "2025-03-16T10:32:14Z"
        },
        {
          "author": "mknet3",
          "body": "@JakobStadlhuber let me prioritize and fix it",
          "created_at": "2025-03-16T11:01:47Z"
        },
        {
          "author": "mknet3",
          "body": "@JakobStadlhuber #9288 fixes that",
          "created_at": "2025-03-16T11:52:17Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-15T00:02:04Z"
        }
      ]
    },
    {
      "issue_number": 9245,
      "title": "[Feature]: Multiple callbacks with configuration for redaction",
      "body": "### The Feature\n\nIt would be great to add and configure multiple callback settings to redact the request and response body across those.\n\nIn our case, we are looking to redact the body and only show traces via otel/langfuse and retain the body within s3. \n\nThis is an example of how otel is configured, and it should be easy to keep this same pattern.\n\n```yaml\nlitellm_settings:\n  callbacks: [\"otel\", \"s3\"]\n\ncallback_settings:\n  otel:\n    message_logging: False\n```\n\nThere was a topic with the rearchitected for redaction in #8489. \n\n### Motivation, pitch\n\nConfiguring the redaction callbacks keeps separation on traces and more hardened locations like s3 for containing private information \n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "wagnerjt",
      "author_type": "User",
      "created_at": "2025-03-14T13:21:58Z",
      "updated_at": "2025-06-22T00:02:09Z",
      "closed_at": "2025-06-22T00:02:09Z",
      "labels": [
        "enhancement",
        "stale",
        "march 2025"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9245/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9245",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9245",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:09.495762",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-15T00:01:52Z"
        }
      ]
    },
    {
      "issue_number": 9278,
      "title": "[Bug]: XAI vision model example should be updated",
      "body": "### What happened?\n\nThe example here: https://docs.litellm.ai/docs/providers/xai no longer works since the latest model now does not support vision. It should say `grok-2-vision-latest` instead of  `grok-2-latest`\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.63.11\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "yhenon",
      "author_type": "User",
      "created_at": "2025-03-15T17:16:38Z",
      "updated_at": "2025-06-22T00:02:07Z",
      "closed_at": null,
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9278/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9278",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9278",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:09.685922",
      "comments": [
        {
          "author": "colesmcintosh",
          "body": "@yhenon good catch! https://github.com/BerriAI/litellm/pull/9286",
          "created_at": "2025-03-16T00:31:40Z"
        },
        {
          "author": "a-rbsn",
          "body": "@colesmcintosh I'm not sure if this is related, but when I'm putting the xAI API key in to add grok-2-latest, it fails and it looks like it's trying to use openai to test the key:\n\n```\nlitellm.AuthenticationError: AuthenticationError: XaiException - Incorrect API key provided: xai-bs3B**************",
          "created_at": "2025-03-16T13:59:37Z"
        },
        {
          "author": "colesmcintosh",
          "body": "@a-rbsn yes this is a separate issue, can you open an issue with this alongside a snippet of the code that led to this?",
          "created_at": "2025-03-16T14:08:12Z"
        },
        {
          "author": "jamie-dit",
          "body": "Could be related to https://github.com/BerriAI/litellm/issues/9291",
          "created_at": "2025-03-18T03:01:49Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-22T00:02:07Z"
        }
      ]
    },
    {
      "issue_number": 9293,
      "title": "[Bug]:  Bedrock Rerank Testing does not work without creds",
      "body": "### What happened?\n\nHey Devs,\n\nFrom what I understand the test written in`tests\\litellm\\rerank_api\\test_rerank_main.py`, tries to mock a call bedrock by patching the `post` method of the `HttpHandler`. This should've worked, but I think that the only reason it doesn't is because whether its an actual call or a mocked one the `rerank` function in `BedrockRerankHandler`, calls `_prepare_request` which tries to setup `Boto3CredentialsInfo`. \n\nI am trying to contribute to the rerank API, and wanted to help out by writing one for HuggingFace with a test, which led me to this error. Should I add in something that checks for a `mock_request`\n\nRelevant Code Sections:\n1. `tests\\litellm\\rerank_api\\test_rerank_main.py`\n```python\ndef test_rerank_infer_region_from_model_arn(monkeypatch):\n    mock_response = MagicMock()\n\n    monkeypatch.setenv(\"AWS_REGION_NAME\", \"us-east-1\")\n    args = {\n        \"model\": \"bedrock/arn:aws:bedrock:us-west-2::foundation-model/amazon.rerank-v1:0\",\n        \"query\": \"hello\",\n        \"documents\": [\"hello\", \"world\"],\n    }\n\n    def return_val():\n        return {\n            \"results\": [\n                {\"index\": 0, \"relevanceScore\": 0.6716859340667725},\n                {\"index\": 1, \"relevanceScore\": 0.0004994205664843321},\n            ]\n        }\n\n    mock_response.json = return_val\n    mock_response.headers = {\"key\": \"value\"}\n    mock_response.status_code = 200\n\n    client = HTTPHandler()\n    with patch.object(client, \"post\", return_value=mock_response) as mock_post:\n        print(mock_post.post(\n            \"https://0.0.0.0/rerank\"\n        ))\n        rerank(\n            model=args[\"model\"],\n            query=args[\"query\"],\n            documents=args[\"documents\"],\n            client=client, # No optional params passed here\n        ) # Client not being patched, its actually trying to call boto3.\n        mock_post.assert_called_once()\n        print(f\"mock_post.call_args: {mock_post.call_args.kwargs}\")\n        assert \"us-west-2\" in mock_post.call_args.kwargs[\"url\"]\n        assert \"us-east-1\" not in mock_post.call_args.kwargs[\"url\"]\n```\n`litellm\\llms\\bedrock\\rerank\\handler.py`\n\n```python\nrequest_data = RerankRequest(\n            model=model,\n            query=query,\n            documents=documents,\n            top_n=top_n,\n            rank_fields=rank_fields,\n            return_documents=return_documents,\n        )\n        data = BedrockRerankConfig()._transform_request(request_data)\n\n        prepared_request = self._prepare_request(\n            model=model,\n            optional_params=optional_params,\n            api_base=api_base,\n            extra_headers=extra_headers,\n            data=cast(dict, data),\n        )\n\n        logging_obj.pre_call(\n            input=data,\n            api_key=\"\",\n            additional_args={\n                \"complete_input_dict\": data,\n                \"api_base\": prepared_request[\"endpoint_url\"],\n                \"headers\": prepared_request[\"prepped\"].headers,\n            },\n        )\n\n        if _is_async:\n            return self.arerank(prepared_request, client=client if client is not None and isinstance(client, AsyncHTTPHandler) else None)  # type: ignore\n\n        if client is None or not isinstance(client, HTTPHandler):\n            client = _get_httpx_client()\n        try:\n            response = client.post(url=prepared_request[\"endpoint_url\"], headers=prepared_request[\"prepped\"].headers, data=prepared_request[\"body\"])  # type: ignore\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:\n            error_code = err.response.status_code\n            raise BedrockError(status_code=error_code, message=err.response.text)\n        except httpx.TimeoutException:\n            raise BedrockError(status_code=408, message=\"Timeout error occurred.\")\n\n```\n\nüí°Possible Solutions:\n1. Mocking the `_prepare_request` method \n2. Setting up Boto3 creds that don't really work (I tried passing in fake ones as optional_params, still got the same error, probably got the setup wrong)\n3. (Least Ideal, bad practice) Modifying original function `rerank` to bypass during mocking. \n\n### Relevant log output\n\n```shell\nE                   litellm.exceptions.APIConnectionError: litellm.APIConnectionError: Unable to locate credentials\nE                   Traceback (most recent call last):\nE                     File \"/mnt/c/Users/athak/Desktop/Documents/CUB/Spring 25/LiteLLM/litellm/litellm/rerank_api/main.py\", line 310, in rerank\nE                       response = bedrock_rerank.rerank(\nE                     File \"/mnt/c/Users/athak/Desktop/Documents/CUB/Spring 25/LiteLLM/litellm/litellm/llms/bedrock/rerank/handler.py\", line 74, in rerank\nE                       prepared_request = self._prepare_request(\nE                     File \"/mnt/c/Users/athak/Desktop/Documents/CUB/Spring 25/LiteLLM/litellm/litellm/llms/bedrock/rerank/handler.py\", line 156, in _prepare_request     \nE                       sigv4.add_auth(request)\nE                     File \"/mnt/c/Users/athak/Desktop/Documents/CUB/Spring 25/LiteLLM/litellm/.venv/lib/python3.10/site-packages/botocore/auth.py\", line 424, in add_auth\nE                       raise NoCredentialsError()\nE                   botocore.exceptions.NoCredentialsError: Unable to locate credentials\n\nlitellm/litellm_core_utils/exception_mapping_utils.py:2190: APIConnectionError\n-------------------------------------------------------------------------- Captured stdout call --------------------------------------------------------------------------\n<MagicMock name='post.post()' id='139887050651872'>\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\n\nProvider List: https://docs.litellm.ai/docs/providers\n\n-------------------------------------------------------------------------- Captured stderr call --------------------------------------------------------------------------\n11:01:27 - LiteLLM:ERROR: main.py:333 - Error in rerank: Unable to locate credentials\n--------------------------------------------------------------------------- Captured log call ----------------------------------------------------------------------------\nERROR    LiteLLM:main.py:333 Error in rerank: Unable to locate credentials\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.63.1\n\n### Twitter / LinkedIn details\n\nhttps://www.linkedin.com/in/aditya-thaker2811/",
      "state": "closed",
      "author": "ADIthaker",
      "author_type": "User",
      "created_at": "2025-03-16T17:52:14Z",
      "updated_at": "2025-06-22T00:02:05Z",
      "closed_at": "2025-06-22T00:02:05Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9293/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9293",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9293",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:09.952563",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-15T00:01:49Z"
        }
      ]
    },
    {
      "issue_number": 9351,
      "title": "[Bug]: JSON Schema properties are not being sent to AWS Bedrock Tools API",
      "body": "### What happened?\n\nA bug happened!\n\nJSON schema properties are not being reflected in request to bedrock.\n\nFull Log\n```\n16:28:56 - LiteLLM:DEBUG: utils.py:311 - Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'type': 'object', 'additionalProperties': True, 'properties': {}}}}], 'tool_choice': {'tool': {'name': 'json_tool_call'}}, 'json_mode': True, 'aws_region_name': 'us-east-1', 'aws_access_key_id': '***********', 'aws_secret_access_key': '*********', 'aws_session_token': '*****'}\n```\nRelevant part:\n```javascript\n{'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'type': 'object', 'additionalProperties': True, 'properties': {}}}}], 'tool_choice': {'tool': {'name': 'json_tool_call'}}\n```\n\nNotice lack of properties key/value pairs in above JSON object.\n\n\nFull Log:\n```\n{\"asctime\": \"2025-03-18 16:28:56,434\", \"levelname\": \"DEBUG\", \"name\": \"LiteLLM\", \"filename\": \"utils.py\", \"lineno\": 3002, \"dd.service\": null, \"dd.env\": null, \"dd.version\": null, \"dd.trace_id\": null, \"dd.span_id\": null, \"message\": \"\\nLiteLLM: Params passed to completion() {'model': 'us.anthropic.claude-3-haiku-20240307-v1:0', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'bedrock', 'response_format': {'type': 'json_schema', 'schema': {'type': 'object', 'properties': {'summary': {'type': 'string', 'description': \\\"A concise description of the document's content including key information and metadata\\\"}, 'purpose': {'type': 'string', 'description': 'Assessment of the likely intended purpose or function of this document'}}, 'required': ['summary', 'purpose']}, 'strict': True}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '**************'}], 'thinking': None, 'aws_region_name': 'us-east-1', 'aws_access_key_id': '***************', 'aws_secret_access_key': '**************', 'aws_session_token': '***'}\", \"taskName\": \"Task-5\"}\n```\n\nRelevant part:\n```\n'response_format': {\n'type': 'json_schema', \n'schema': {\n'type': 'object', \n'properties': \n{\n'summary': {'type': 'string', 'description': \\\"A concise description of the document's content including key information and metadata\\\"}, \n'purpose': {'type': 'string', 'description': 'Assessment of the likely intended purpose or function of this document'}}, \n'required': ['summary', 'purpose']}, \n'strict': True\n}\n```\n\n\n### Relevant log output\n\n```shell\n16:28:56 - LiteLLM:DEBUG: utils.py:311 - Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'type': 'object', 'additionalProperties': True, 'properties': {}}}}], 'tool_choice': {'tool': {'name': 'json_tool_call'}}, 'json_mode': True, 'aws_region_name': 'us-east-1', 'aws_access_key_id': '***********', 'aws_secret_access_key': '*********', 'aws_session_token': '*****'}\n\n{\"asctime\": \"2025-03-18 16:28:56,434\", \"levelname\": \"DEBUG\", \"name\": \"LiteLLM\", \"filename\": \"utils.py\", \"lineno\": 3002, \"dd.service\": null, \"dd.env\": null, \"dd.version\": null, \"dd.trace_id\": null, \"dd.span_id\": null, \"message\": \"\\nLiteLLM: Params passed to completion() {'model': 'us.anthropic.claude-3-haiku-20240307-v1:0', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'bedrock', 'response_format': {'type': 'json_schema', 'schema': {'type': 'object', 'properties': {'summary': {'type': 'string', 'description': \\\"A concise description of the document's content including key information and metadata\\\"}, 'purpose': {'type': 'string', 'description': 'Assessment of the likely intended purpose or function of this document'}}, 'required': ['summary', 'purpose']}, 'strict': True}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '**************'}], 'thinking': None, 'aws_region_name': 'us-east-1', 'aws_access_key_id': '***************', 'aws_secret_access_key': '**************', 'aws_session_token': '***'}\", \"taskName\": \"Task-5\"}\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\n1.63.8\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "Dmarcotrigiano",
      "author_type": "User",
      "created_at": "2025-03-18T20:48:51Z",
      "updated_at": "2025-06-22T00:01:59Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request",
        "stale"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9351/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9351",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9351",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:10.136854",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "What's your complete call to liteLLM?\n\nClear steps to repro would be helpful. ",
          "created_at": "2025-03-23T16:52:00Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-22T00:01:59Z"
        }
      ]
    },
    {
      "issue_number": 9418,
      "title": "[Bug]:",
      "body": "### What happened?\n\nAfter installing docker-based Litellm and rpm-based postgresql on my linux(Amazon linux 2023) successfully, I can‚Äôt login litellm ui(http://ip:4000/ui or http://ip:4000/sso/key/generate) through default user name admin and password MASTER_KEY, I have clearly set environment parmaters MASTER_KEY and LITELLM_MASTER_KEY isolately. How to solve it, please give me details steps.\n\nOnce I input admin and password(MASTER_KEY),I got following error.\n{\"error\":{\"message\":\"Invalid credentials used to access UI.\\nCheck 'UI_USERNAME', 'UI_PASSWORD' in .env file\",\"type\":\"auth_error\",\"param\":\"invalid_credentials\",\"code\":\"401\"}}\n\nso I input the other 2 environment parameters(UI_USERNAME & UI_PASSWORD), I got following error again.\n{\"error\":{\"message\":\"Authentication Error, User not found, passed user_id=default_user_id\",\"type\":\"auth_error\",\"param\":\"None\",\"code\":\"400\"}}\n\nI have tried so many times, how to solve this type of issue?\n\n### Relevant log output\n\n```shell\n{\"error\":{\"message\":\"Invalid credentials used to access UI.\\nCheck 'UI_USERNAME', 'UI_PASSWORD' in .env file\",\"type\":\"auth_error\",\"param\":\"invalid_credentials\",\"code\":\"401\"}}\n\n{\"error\":{\"message\":\"Authentication Error, User not found, passed user_id=default_user_id\",\"type\":\"auth_error\",\"param\":\"None\",\"code\":\"400\"}}\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nthe latest.\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "jerryjin2018",
      "author_type": "User",
      "created_at": "2025-03-20T19:59:49Z",
      "updated_at": "2025-06-22T00:01:54Z",
      "closed_at": null,
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9418/reactions",
        "total_count": 4,
        "+1": 4,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9418",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9418",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:10.353244",
      "comments": [
        {
          "author": "yoswa",
          "body": "I ran the following command after running docker, to manually create the admin user. Use the key from your .env for sk-****\n\ncurl -X 'POST'\n'http://localhost:4000/user/new'\n-H 'accept: application/json'\n-H 'Ocp-Apim-Subscription-Key: sk-*****'\n-H 'Content-Type: application/json'\n-d '{\n\"user_id\": \"ad",
          "created_at": "2025-03-20T22:09:10Z"
        },
        {
          "author": "mcbain07",
          "body": "this one had me stumped for two days.   turns out the username box isn't sending the username \"admin\" its sending \"default_user_id\" regardless of what I type. \n\nSo I used the api call from Yoswa to create a user called default_user_id and I'm finally in. \n\n curl -X POST \"http://localhost:4000/user/n",
          "created_at": "2025-03-23T03:26:44Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-22T00:01:53Z"
        }
      ]
    },
    {
      "issue_number": 9477,
      "title": "unsupported protocol",
      "body": "$ uv run main.py\nINFO     [browser_use] BrowserUse logging setup complete with level info\nEnter your query: hi\nINFO     [src.workflow] Starting workflow with user input: hi\nINFO     [src.graph.nodes] Coordinator talking.\n14:21:03 - LiteLLM:INFO: utils.py:2999 -\nLiteLLM completion() model= qwen2.5:0.5b; provider = ollama\nINFO     [LiteLLM]\nLiteLLM completion() model= qwen2.5:0.5b; provider = ollama\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nWARNING  [langchain_core.language_models.llms] Retrying langchain_community.chat_models.litellm.ChatLiteLLM.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: litellm.APIConnectionError: OllamaException - Request URL has an unsupported protocol 'ttp://'..",
      "state": "open",
      "author": "renhongkai",
      "author_type": "User",
      "created_at": "2025-03-23T06:26:42Z",
      "updated_at": "2025-06-22T00:01:50Z",
      "closed_at": null,
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9477/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9477",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9477",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:10.540311",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-22T00:01:50Z"
        }
      ]
    },
    {
      "issue_number": 9480,
      "title": "[Bug]: cannot delete fallback by API",
      "body": "### What happened?\n\ni ran into an issue which i cannot delete a fallback by API\n\nmy litellm proxy server has one postgres connected and don't have any fallbacks configured  in my config.yaml\n\n```bash\nmy config.yaml\nrouter_settings:\n  routing_strategy: simple-shuffle\n```\n\nset fallback\n\n```\nPOST /config/update\n{\n  \"router_settings\": {\n                \"qwq-qwq-32b\": [\n                    \"qwen-qwq-32b-backup\"\n                ]\n      }\n}\n```\n\n\nthen i tried to unset current fallback\n\n```\nPOST /config/update\n{\n  \"router_settings\": {\n      \"fallbacks\": []\n  }\n}\n```\n\n```\nGET /get/config/callbacks\n.....\n\"router_settings\": {\n        \"routing_strategy_args\": {},\n        \"routing_strategy\": \"simple-shuffle\",\n        \"allowed_fails\": 3,\n        \"cooldown_time\": 5,\n        \"num_retries\": 2,\n        \"timeout\": 6000,\n        \"retry_after\": 0,\n        \"fallbacks\": [\n            {\n                \"qwq-qwq-32b\": [   # this fallback still exists\n                    \"qwen-qwq-32b-backup\"\n                ]\n            }\n        ],\n        \"context_window_fallbacks\": null,\n        \"model_group_retry_policy\": {}\n    },\n......\n```\n\nif i initialize the fallbacks in my config.yaml like\n```\nrouter_settings:\n  routing_strategy: simple-shuffle\n  fallbacks: []  ## initialize fallbacks with []\n```\n\ni can set and unset fallback normally \n\nbut after got config at line 7393,\nthe config related to fallbacks disappeared\nbecause inside proxy_config.get_config()\n\nthe return value of _update_config_from_db() also doesn't contain fallbacks settings \n\n\n![Image](https://github.com/user-attachments/assets/e2f3abd7-7319-480c-a242-e08a4d9010c8)\n\n![Image](https://github.com/user-attachments/assets/cd8e415c-2131-4aa9-b855-c236581396b0)\n\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.63.14-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "to-jiawen",
      "author_type": "User",
      "created_at": "2025-03-23T07:46:37Z",
      "updated_at": "2025-06-22T00:01:49Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request",
        "stale"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9480/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9480",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9480",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:10.778652",
      "comments": [
        {
          "author": "to-jiawen",
          "body": "i might got the reason?\n\nat `ProxyConfig._update_config_fields` when executing `run_coroutine_job`\n\n```python\nnon_empty_values = {k: v for k, v in b.items() if v}\n```\nmay exclude `fallbacks` field with `[]`\n\n<img width=\"879\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e6219474-252a-4",
          "created_at": "2025-03-23T17:53:54Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-22T00:01:48Z"
        }
      ]
    },
    {
      "issue_number": 9483,
      "title": "[Bug]: inconsistent stop symbol handling for Nova vs other LLMs",
      "body": "### What happened?\n\nThe \"stop\" symbol handling for Nova seems incorrect as the stop symbol is *not* removed from the response message content. In other words, for Nova, the generation stops *after* the stop symbol, whereas for other LLMs the generation stops just *before* the stop symbol.\n\nThis little script illustrates the behavior with Nova, as compared to other LLMs:\n\n```\nfrom litellm import completion\nimport os\n\nmodel_ids = [\"gpt-4o-mini\",\n             \"bedrock/anthropic.claude-3-sonnet-20240229-v1:0\",\n             \"ollama/gemma3:4b\",\n             \"bedrock/us.amazon.nova-micro-v1:0\",\n             \"bedrock/us.amazon.nova-pro-v1:0\",\n             ]\n\nfor m in model_ids:\n    response = completion(\n        model=m,\n        messages=[\n            {\"role\": \"system\", \"content\": \"Wrap the response in <final><answer>...</answer></final> tags.\"},\n            {\"role\": \"user\",   \"content\": \"Just say one word: \\\"Hi\\\"\"},\n        ],\n        stop=[\"</answer>\"]\n    )\n    content = response.choices[0].message.content\n    finish_reason=response.choices[0].finish_reason\n    print(f'{m}\\nfinish_reason: {finish_reason}; content: {content}\\n')\n```\n\nOutput:\n```\ngpt-4o-mini\nfinish_reason: stop; content: <final><answer>Hi\n\nbedrock/anthropic.claude-3-sonnet-20240229-v1:0\nfinish_reason: stop; content: <final><answer>Hi\n\nollama/gemma3:4b\nfinish_reason: stop; content: <final><answer>Hi\n\nbedrock/us.amazon.nova-micro-v1:0\nfinish_reason: stop; content: <final><answer>Hi</answer>\n\nbedrock/us.amazon.nova-pro-v1:0\nfinish_reason: stop; content: <final><answer>Hi</answer>\n```\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n 1.61.8\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "markusdr",
      "author_type": "User",
      "created_at": "2025-03-23T18:37:41Z",
      "updated_at": "2025-06-22T00:01:47Z",
      "closed_at": null,
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9483/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9483",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9483",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:10.992939",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-22T00:01:47Z"
        }
      ]
    },
    {
      "issue_number": 9486,
      "title": "[Bug]: Batches for VertexAI is not fully supported (GCS download option)",
      "body": "### What happened?\n\nI'm unable to download batches from vertex ai. \n\nHere is my runner script:\n\n\n```python\nretrieved_batch = litellm.retrieve_batch(\n    batch_id=batch_id,\n    custom_llm_provider=llm_provider,\n)\nif retrieved_batch.status == 'completed':\n    # download the file\n    file = litellm.file_content(\n        file_id=retrieved_batch.output_file_id,\n        custom_llm_provider=llm_provider,\n    )\n    \n    with open(write_dest, 'wb') as f:\n        f.write(file.content)\n        print(f\"Downloaded {namespace}_{version}.jsonl\")\n    \n    err_file_id = retrieved_batch.error_file_id\n    if err_file_id:\n        err_file = litellm.file_content(\n            file_id=err_file_id,\n            custom_llm_provider=llm_provider,\n        )\n        err_dest = f\"{err_folder}/{namespace}_{version}.jsonl\"\n        with open(err_dest, 'wb') as f:\n            f.write(err_file.content)\n```\n\nThe script works fine with openai, but fails with VertexAI.\n\nOne issue seems to be `litellm.llms.vertex_ai.batches.transformation.VertexAIBatchTransformation._get_output_file_id_from_vertex_ai_batch_response`. The function returns `response['outputConfig']['gcsDestination']['outputUriPrefix'], but it seems that the desired value is `response['outputInfo']['gcsOutputDirectory']\n\nFor instance, this is outputUriPrefix:\n'gs://MyProject/litellm-vertex-files/publishers/google/models/gemini-2.0-flash-001'\n\nMeanwhile, this is gcsOutputDirectory:\n'gs://MyProject/litellm-vertex-files/publishers/google/models/gemini-2.0-flash-001/prediction-model-2025-03-22T22:14:01.174950Z'\n\n### Relevant log output\n\n```shell\n16:32:39 - LiteLLM:WARNING: model_param_helper.py:147 - Error getting transcription kwargs cannot import\n name 'TranscriptionCreateParamsNonStreaming' from 'openai.types.audio.transcription_create_params' (c:\\\nUsers\\conjunct\\.virtualenvs\\enzy\\Lib\\site-packages\\openai\\types\\audio\\transcription_create_params.py)   \nTraceback (most recent call last):\n  File \"C:\\Program Files\\Python311\\Lib\\runpy.py\", line 198, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Python311\\Lib\\runpy.py\", line 88, in _run_code\n    exec(code, run_globals)\n  File \"c:\\Users\\conjunct\\.vscode\\extensions\\ms-python.debugpy-2025.0.1-win32-x64\\bundled\\libs\\debugpy\\launcher/../..\\debugpy\\__main__.py\", line 71, in <module>\n    cli.main()\n  File \"c:\\Users\\conjunct\\.vscode\\extensions\\ms-python.debugpy-2025.0.1-win32-x64\\bundled\\libs\\debugpy\\launcher/../..\\debugpy/..\\debugpy\\server\\cli.py\", line 501, in main\n    run()\n  File \"c:\\Users\\conjunct\\.vscode\\extensions\\ms-python.debugpy-2025.0.1-win32-x64\\bundled\\libs\\debugpy\\launcher/../..\\debugpy/..\\debugpy\\server\\cli.py\", line 351, in run_file\n    runpy.run_path(target, run_name=\"__main__\")\n  File \"c:\\Users\\conjunct\\.vscode\\extensions\\ms-python.debugpy-2025.0.1-win32-x64\\bundled\\libs\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_runpy.py\", line 310, in run_path\n    return _run_module_code(code, init_globals, run_name, pkg_name=pkg_name, script_name=fname)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\conjunct\\.vscode\\extensions\\ms-python.debugpy-2025.0.1-win32-x64\\bundled\\libs\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_runpy.py\", line 127, in _run_module_code\n    _run_code(code, mod_globals, init_globals, mod_name, mod_spec, pkg_name, script_name)\n  File \"c:\\Users\\conjunct\\.vscode\\extensions\\ms-python.debugpy-2025.0.1-win32-x64\\bundled\\libs\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_runpy.py\", line 118, in _run_code\n    exec(code, run_globals)\n  File \"C:\\conjunct\\Inference\\pkg\\experiments\\pipeline\\exp2_download.py\", line 11, in <module>        \n    download(\n  File \"C:\\conjunct\\Inference\\pkg\\Inference\\pipeline\\step2_download.py\", line 42, in download       \n    file = litellm.file_content(\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\conjunct\\dev\\litellm\\litellm\\files\\main.py\", line 834, in file_content\n    raise e\n  File \"C:\\conjunct\\dev\\litellm\\litellm\\files\\main.py\", line 820, in file_content\n    raise litellm.exceptions.BadRequestError(\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: LiteLLM doesn't support vertex_ai for 'custom_llm_provider'. Supported providers are 'openai', 'azure', 'vertex_ai'.\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.63.12\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "conjuncts",
      "author_type": "User",
      "created_at": "2025-03-23T21:50:37Z",
      "updated_at": "2025-06-22T00:01:46Z",
      "closed_at": null,
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9486/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9486",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9486",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:11.192357",
      "comments": [
        {
          "author": "conjuncts",
          "body": "Another issue that the implementation for loading file content for VertexAI is missing, although the error message implies support for Vertex AI. Using the GCS sdk would involve adding another dependency, however. For potential readers, here is the script that worked for me:\n\n```python\nfrom google.c",
          "created_at": "2025-03-23T22:00:35Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-22T00:01:45Z"
        }
      ]
    },
    {
      "issue_number": 11657,
      "title": "[Bug]: aiohttp \"Unclosed client session\" warning when using Ollama embeddings",
      "body": "### What happened?\n\nI am getting `aiohttp` warnings if I try to retrieve embeddings with `Ollama` like this:\n\n```\nfrom litellm import embedding\n\nprompts = [\n    'what is the capital for Germany',\n    'research data',\n    'collection of data'\n    ]\n\nfor prompt in prompts:\n    response = embedding(\n        input=prompt,\n        model='ollama/nomic-embed-text:latest'\n    )\n```\n\nEach `litellm` call results in an \"Unclosed client session\" warning:\n\n```\nUnclosed client session\nclient_session: <aiohttp.client.ClientSession object at 0x118dc46b0>\nUnclosed connector\nconnections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x119383dd0>, 565026.275626625)])']\nconnector: <aiohttp.connector.TCPConnector object at 0x1192266c0>\nUnclosed client session\nclient_session: <aiohttp.client.ClientSession object at 0x1193c57f0>\nUnclosed connector\nconnections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x119383ef0>, 565026.314000833)])']\nconnector: <aiohttp.connector.TCPConnector object at 0x1193c5580>\nUnclosed client session\nclient_session: <aiohttp.client.ClientSession object at 0x1193c49b0>\nUnclosed connector\nconnections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1193e83b0>, 565026.348865708)])']\nconnector: <aiohttp.connector.TCPConnector object at 0x1193c61e0>\n```\n\nIf I run the same code with a different embedding provider (e.g. `OpenAI`) I do not get these warnings.\n\nDisabling the `aiohttp` transport has no effect either:\n\n```\nimport litellm\nfrom litellm import embedding\n\nlitellm.disable_aiohttp_transport = True\n\nprompts = [\n    'what is the capital for Germany',\n    'research data',\n    'collection of data'\n    ]\n\nfor prompt in prompts:\n    response = embedding(\n        input=prompt,\n        model='ollama/nomic-embed-text:latest'\n    )\n```\n\nOutput:\n```\nUnclosed client session\nclient_session: <aiohttp.client.ClientSession object at 0x1060a26c0>\nUnclosed connector\nconnections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10d103e90>, 565512.439544125)])']\nconnector: <aiohttp.connector.TCPConnector object at 0x1060a19d0>\nUnclosed client session\nclient_session: <aiohttp.client.ClientSession object at 0x10d14d4f0>\nUnclosed connector\nconnections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10d103fb0>, 565512.492847208)])']\nconnector: <aiohttp.connector.TCPConnector object at 0x10d14d2b0>\nUnclosed client session\nclient_session: <aiohttp.client.ClientSession object at 0x10d14c050>\nUnclosed connector\nconnections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10d154470>, 565512.536474791)])']\nconnector: <aiohttp.connector.TCPConnector object at 0x10d14c7d0>\n```\n\nRunning `completion` with `Ollama` does not result in any `aiohttp` warnings. Only `embedding` seems to be the problem.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.72.3\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "tsmdt",
      "author_type": "User",
      "created_at": "2025-06-12T07:24:41Z",
      "updated_at": "2025-06-21T16:17:18Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11657/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11657",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11657",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:11.437047",
      "comments": [
        {
          "author": "jerry153fish",
          "body": "The `acompletion` does not work as well\n\n```\nfrom litellm import acompletion\nimport asyncio\nimport os\n\nasync def test_get_response():\n    user_message = \"Hello, how are you?\"\n    messages = [{\"content\": user_message, \"role\": \"user\"}]\n    response = await acompletion(model=\"ollama/qwen3:latest\", mess",
          "created_at": "2025-06-15T03:32:35Z"
        },
        {
          "author": "anuar12",
          "body": "Got the same issue with `litellm=1.72.4`",
          "created_at": "2025-06-19T12:14:14Z"
        },
        {
          "author": "jeromeroussin",
          "body": "Same issue here seen in the logs of the proxy\n```\nUnclosed client session\nclient_session: <aiohttp.client.ClientSession object at 0x7fea0fb47750>\nUnclosed client session\nclient_session: <aiohttp.client.ClientSession object at 0x7fea0fb45590>\nUnclosed client session\nclient_session: <aiohttp.client.Cl",
          "created_at": "2025-06-19T12:16:31Z"
        },
        {
          "author": "medihack",
          "body": "I also see this with `acompletion` (using `hosted_vllm` endpoint) and `litellm v1.72.9`.",
          "created_at": "2025-06-21T16:16:04Z"
        }
      ]
    },
    {
      "issue_number": 11929,
      "title": "[Bug]: Usage Dashboard: Two Issues with Spend Reporting and Failed Request Attribution",
      "body": "### What happened?\n\nWe've identified two separate bugs in the LiteLLM usage dashboard that cause incorrect data display:\n\n1. **Frontend Pagination Bug**: Total spend significantly underreported when data spans multiple pages\n2. **Backend Provider Attribution Bug**: Failed requests show as 0 for all providers despite actual failures occurring\n\n üîç Bug Details\n\n### Bug 1: Frontend Pagination Aggregation Issue\n\n**Problem**: The usage dashboard only displays totals from the first page of API results instead of aggregating across all pages.\n\n**API Calls Made**:\n```\n/user/daily/activity?page=1  ‚Üí Returns metadata with totals\n/user/daily/activity?page=2  ‚Üí Returns metadata with totals  \n/user/daily/activity?page=3  ‚Üí Returns metadata with totals\n/user/daily/activity?page=4  ‚Üí Returns metadata with totals\n```\n\n**Expected**: Dashboard should sum all metadata totals across pages\n**Actual**: Dashboard only shows totals from page 1\n\n**Affected Metrics**: \n- Total spend\n- Total API requests\n- Successful/failed requests\n- Token usage\n- Cache statistics\n\n### Bug 2: Backend Provider Attribution Issue\n\n**Problem**: \"Spend by Provider\" section shows `failed_requests: 0` for all providers despite failed requests being tracked in overall metadata.\n\n**API Response Structure**:\n```json\n{\n  \"metadata\": {\n    \"total_failed_requests\": 162  // ‚úÖ Correct count\n  },\n  \"results\": [\n    {\n      \"breakdown\": {\n        \"providers\": {\n          \"anthropic\": {\n            \"metrics\": {\n              \"failed_requests\": 0  // ‚ùå Always 0\n            }\n          },\n          \"vertex_ai\": {\n            \"metrics\": {\n              \"failed_requests\": 0  // ‚ùå Always 0  \n            }\n          }\n        }\n      }\n    }\n  ]\n}\n```\n\n### Frontend Issue Location\n- **File**: `ui/litellm-dashboard/src/components/new_usage.tsx`\n- **Root Cause**: Pagination logic correctly fetches all pages but only retains metadata from first API response\n- **Impact**: Affects all paginated usage reports (>1000 records)\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\n1.72.2-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "HarpreetBaathTR",
      "author_type": "User",
      "created_at": "2025-06-20T20:15:22Z",
      "updated_at": "2025-06-21T16:06:09Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11929/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11929",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11929",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:11.678384",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": ">  \"metadata\": {\n    \"total_failed_requests\": 162  // ‚úÖ Correct count\n  },\n\nwhat does a row in the daily user activity table for a failed request look like? @HarpreetBaathTR ",
          "created_at": "2025-06-21T16:05:55Z"
        },
        {
          "author": "krrishdholakia",
          "body": "cc: @NANDINI-star can you look at the UI issue outlined? ",
          "created_at": "2025-06-21T16:06:09Z"
        }
      ]
    },
    {
      "issue_number": 11836,
      "title": "[Bug]: No module named 'proxy_server' After updating to 1.72.7 or 1.72.6",
      "body": "### What happened?\n\nNo module named 'proxy_server' error on startup and LiteLLM crashes\n\nHow to fix it?\nFix it by\npip install litellm[proxy]\nafter that, restart LiteLLM\nbut LiteLLM should install and update dependencies automatically, I should not have to do it.\n\n### Relevant log output\n\n```shell\nFile \"/opt/litellm/venv/lib/python3.11/site-packages/litellm/proxy/proxy_cli.py\", line 538, in run_server\nJun 18 09:17:55 litellm[511221]:     from proxy_server import (\nJun 18 09:17:55 litellm[511221]: ModuleNotFoundError: No module named 'proxy_server'\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv.1.72.7\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "Classic298",
      "author_type": "User",
      "created_at": "2025-06-18T07:21:58Z",
      "updated_at": "2025-06-21T15:50:09Z",
      "closed_at": "2025-06-21T14:09:28Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11836/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11836",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11836",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:11.968755",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "what's the full stacktrace? ",
          "created_at": "2025-06-20T05:42:01Z"
        },
        {
          "author": "Classic298",
          "body": " @krrishdholakia \n\n```\n(venv) root@srv-openwebui:/opt/litellm# pip install -U litellm\nRequirement already satisfied: litellm in ./venv/lib/python3.11/site-packages (1.72.0)\nCollecting litellm\n  Downloading litellm-1.72.9-py3-none-any.whl (8.4 MB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8.4/8.4",
          "created_at": "2025-06-21T11:21:39Z"
        },
        {
          "author": "krrishdholakia",
          "body": " > ModuleNotFoundError: No module named 'mcp.client.streamable_http'\n\nThis is what you were missing ",
          "created_at": "2025-06-21T14:09:18Z"
        },
        {
          "author": "Classic298",
          "body": "@krrishdholakia if i were missing it, and it is a dependency, why is it not being installed when updating litellm? Is it not in your requirements file?",
          "created_at": "2025-06-21T15:18:04Z"
        },
        {
          "author": "krrishdholakia",
          "body": "it is in our requirements.txt - https://github.com/BerriAI/litellm/blob/c4c8d9a250138e582b0079753199af9b4d85b6e0/requirements.txt#L18\n\nit is not a core sdk requirement, but it is a proxy requirement - https://github.com/BerriAI/litellm/blob/c4c8d9a250138e582b0079753199af9b4d85b6e0/pyproject.toml#L82",
          "created_at": "2025-06-21T15:46:41Z"
        }
      ]
    },
    {
      "issue_number": 11787,
      "title": "[Bug]: Cost tracking and logging via the /v1/messages API are not working when using Claude Code.",
      "body": "### What happened?\n\nAccording to the Claude Code [documentation](https://docs.anthropic.com/en/docs/claude-code/llm-gateway), we have completed the integration of Claude Code with Litellm Proxy and have attempted to test using claude-3.5 and claude-3.7 as models.\n\nCurrently, requests successfully receive responses; however, we noticed that the cost and log information do not appear in the returned results.\n\nThe Litellm Proxy configuration follows the official [documentation](https://docs.litellm.ai/docs/anthropic_unified) and has not been customized in any way.\n\n![Image](https://github.com/user-attachments/assets/305a5b58-186c-452f-83ec-27bf1bd2864c)\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.6-rc\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "kkc-tonywu",
      "author_type": "User",
      "created_at": "2025-06-17T03:16:26Z",
      "updated_at": "2025-06-21T15:47:21Z",
      "closed_at": "2025-06-21T01:08:36Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11787/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11787",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11787",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:12.230279",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "Are you calling anthropic models or bedrock models @kkc-tonywu ? ",
          "created_at": "2025-06-17T15:12:24Z"
        },
        {
          "author": "kkc-tonywu",
          "body": "> Are you calling anthropic models or bedrock models [@kkc-tonywu](https://github.com/kkc-tonywu) ?\n\nbedrock model",
          "created_at": "2025-06-17T15:45:36Z"
        },
        {
          "author": "deanbear",
          "body": "We've encountered the same issue. Any progress on this?\n\nlitellm + aws bedrock\n\n",
          "created_at": "2025-06-20T05:19:55Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "Picking this up tomorrow\r\n\r\nIshaan Jaffer\r\nCo-Founder https://github.com/BerriAI/litellm\r\n\r\n\r\nOn Thu, Jun 19, 2025 at 10:20‚ÄØPM bear ***@***.***> wrote:\r\n\r\n> *deanbear* left a comment (BerriAI/litellm#11787)\r\n> <https://github.com/BerriAI/litellm/issues/11787#issuecomment-2989827201>\r\n>\r\n> We've enco",
          "created_at": "2025-06-20T05:29:00Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "done here: https://github.com/BerriAI/litellm/pull/11928 ",
          "created_at": "2025-06-20T18:50:30Z"
        }
      ]
    },
    {
      "issue_number": 8333,
      "title": "[Bug]: ollama_chat/ provider does not honor timeout",
      "body": "### What happened?\n\nI can pass `timeout` to `completion()` and most models seem to honor it. Models from the `ollama_chat/` provider do not.\n\n### Relevant log output\n\n```shell\nimport litellm\n\ndef doit(model):\n    messages=[{\"role\": \"user\", \"content\": \"hi\"}]\n    try:\n        comp = litellm.completion(model, messages, timeout=0.1)\n        print(model, comp.choices[0].message.content)\n    except Exception as e:\n        print(model, type(e))\n\ndoit(\"gpt-4o\") \n# outputs: gpt-4o <class 'litellm.exceptions.Timeout'>\n\ndoit(\"ollama/llama3.2:3b-instruct-q5_K_S\")\n# outputs: ollama/llama3.2:3b-instruct-q5_K_S <class 'litellm.exceptions.APIConnectionError'>\n\ndoit(\"ollama_chat/llama3.2:3b-instruct-q5_K_S\")\n# outputs: ollama_chat/llama3.2:3b-instruct-q5_K_S Hello! How can I assist you today?\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.60.5\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "paul-gauthier",
      "author_type": "User",
      "created_at": "2025-02-06T19:33:57Z",
      "updated_at": "2025-06-21T00:02:00Z",
      "closed_at": null,
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8333/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "krrishdholakia"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8333",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8333",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:12.404115",
      "comments": [
        {
          "author": "vmajor",
          "body": "Since I was copied into this I am guessing at least one of my reports made it even though I cannot see them anywhere.\n\nI do not us ollama, I use the OG llama-server and aider also ignores any and all --timeout settings that I tried and times out the session mid response.\n\nI have yet to see any advic",
          "created_at": "2025-02-06T21:41:22Z"
        },
        {
          "author": "paul-gauthier",
          "body": "@vmajor Please follow up back in the aider issue:\nhttps://github.com/Aider-AI/aider/issues/276",
          "created_at": "2025-02-06T22:00:17Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Thanks for the issue @paul-gauthier. I believe we need to just refactor ollama_chat to also use the base_llm_http_handler \n\nshould fix this ",
          "created_at": "2025-02-07T02:24:16Z"
        },
        {
          "author": "vmajor",
          "body": "I am getting hit by the same(?) timeout issue, but now with QwQ-32B and HF smolagents \"Open Deep Research\". I will open a new bug. From appearances LiteLLM is not honoring timeout settings for \"openai\" models.",
          "created_at": "2025-03-21T23:52:09Z"
        },
        {
          "author": "krrishdholakia",
          "body": "> From appearances LiteLLM is not honoring timeout settings for \"openai\" models\n\nthis is not correct @vmajor \n\nI can see it being passed in - https://github.com/BerriAI/litellm/blob/1255775acfc9fc2b345209fbc31d88770b924835/litellm/llms/openai/openai.py#L420\n\nIf you have a litellm code snippet to rep",
          "created_at": "2025-03-22T00:49:02Z"
        }
      ]
    },
    {
      "issue_number": 9007,
      "title": "[Bug]: Model Overloaded exception",
      "body": "### What happened?\n\nWould be good if you handle this scenario and pause and retry a few minutes later rather than throwing an exception as in this case the application will bomb out.\n\nAlso the api key in the exception message should really be obfuscated but i manually removed it.\n\n\n\n\n### Relevant log output\n\n```shell\n15:36:34 - LiteLLM:WARNING: utils.py:428 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\nWARNING:LiteLLM:`litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\nInitialized litellm callbacks, Async Success Callbacks: [<litellm.integrations.langfuse.langfuse_prompt_management.LangfusePromptManagement object at 0x000002CD95369730>, 'cache', <crewai.utilities.token_counter_callback.TokenCalcHandler object at 0x000002CD99E37B90>]\nSYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\nFinal returned optional params: {'stop_sequences': ['\\nObservation:']}\n\n\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\n\nProvider List: https://docs.litellm.ai/docs/providers\n\nERROR:root:LiteLLM call failed: litellm.InternalServerError: litellm.InternalServerError: VertexAIException - {\n  \"error\": {\n    \"code\": 503,\n    \"message\": \"The model is overloaded. Please try again later.\",\n    \"status\": \"UNAVAILABLE\"\n  }\n}\n\n Error during LLM call: litellm.InternalServerError: litellm.InternalServerError: VertexAIException - {\n  \"error\": {\n    \"code\": 503,\n    \"message\": \"The model is overloaded. Please try again later.\",\n    \"status\": \"UNAVAILABLE\"\n  }\n}\n\n[Flow._execute_single_listener] Error in method PDFExtractor: litellm.InternalServerError: litellm.InternalServerError: VertexAIException - {\n  \"error\": {\n    \"code\": 503,\n    \"message\": \"The model is overloaded. Please try again later.\",\n    \"status\": \"UNAVAILABLE\"\n  }\n}\n\nTraceback (most recent call last):\n  File \"D:\\Users\\newla\\crewai\\financial_analyst1\\productmanager\\productmanagementflow\\.venv\\Lib\\site-packages\\litellm\\llms\\vertex_ai\\gemini\\vertex_and_google_ai_studio_gemini.py\", line 1282, in completion\n    response = client.post(url=url, headers=headers, json=data)  # type: ignore\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\newla\\crewai\\financial_analyst1\\productmanager\\productmanagementflow\\.venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\http_handler.py\", line 553, in post\n    raise e\n  File \"D:\\Users\\newla\\crewai\\financial_analyst1\\productmanager\\productmanagementflow\\.venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\http_handler.py\", line 534, in post\n    response.raise_for_status()\n  File \"D:\\Users\\newla\\crewai\\financial_analyst1\\productmanager\\productmanagementflow\\.venv\\Lib\\site-packages\\httpx\\_models.py\", line 763, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Server error '503 Service Unavailable' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=<manually removed>'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/503\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"D:\\Users\\newla\\crewai\\financial_analyst1\\productmanager\\productmanagementflow\\.venv\\Lib\\site-packages\\litellm\\main.py\", line 2299, in completion\n    response = vertex_chat_completion.completion(  # type: ignore\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\newla\\crewai\\financial_analyst1\\productmanager\\productmanagementflow\\.venv\\Lib\\site-packages\\litellm\\llms\\vertex_ai\\gemini\\vertex_and_google_ai_studio_gemini.py\", line 1286, in completion\n    raise VertexAIError(\nlitellm.llms.vertex_ai.common_utils.VertexAIError: {\n  \"error\": {\n    \"code\": 503,\n    \"message\": \"The model is overloaded. Please try again later.\",\n    \"status\": \"UNAVAILABLE\"\n  }\n}\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"D:\\Users\\newla\\crewai\\financial_analyst1\\productmanager\\productmanagementflow\\.venv\\Lib\\site-packages\\crewai\\flow\\flow.py\", line 990, in _execute_single_listener\n    listener_result = await self._execute_method(listener_name, method)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\newla\\crewai\\financial_analyst1\\productmanager\\productmanagementflow\\.venv\\Lib\\site-packages\\crewai\\flow\\flow.py\", line 825, in _execute_method\n    else method(*args, **kwargs)\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\newla\\crewai\\financial_analyst1\\productmanager\\productmanagementflow\\src\\productmanagementflow\\main.py\", line 120, in PDFExtractor\n    .kickoff(inputs={\"pdf_filename\": self.state.pdffilename,\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\newla\\crewai\\financial_analyst1\\productmanager\\productmanagementflow\\.venv\\Lib\\site-packages\\crewai\\crew.py\", line 578, in kickoff\n    result = self._run_hierarchical_process()\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\newla\\crewai\\financial_analyst1\\productmanager\\productmanagementflow\\.venv\\Lib\\site-packages\\crewai\\crew.py\", line 688, in _run_hierarchical_process\n    return self._execute_tasks(self.tasks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\newla\\crewai\\financial_analyst1\\productmanager\\productmanagementflow\\.venv\\Lib\\site-packages\\crewai\\crew.py\", line 781, in _execute_tasks\n    task_output = task.execute_sync(\n                  ^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\newla\\crewai\\financial_analyst1\\productmanager\\productmanagementflow\\.venv\\Lib\\site-packages\\crewai\\task.py\", line 302, in execute_sync\n    return self._execute_core(agent, context, tools)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\newla\\crewai\\financial_analyst1\\productmanager\\productmanagementflow\\.venv\\Lib\\site-packages\\crewai\\task.py\", line 366, in _execute_core\n    result = agent.execute_task(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\newla\\crewai\\financial_analyst1\\productmanager\\productmanagementflow\\.venv\\Lib\\site-packages\\crewai\\agent.py\", line 254, in execute_task\n    raise e\n  File \"D:\\Users\\newla\\crewai\\financial_analyst1\\productmanager\\productmanagementflow\\.venv\\Lib\\site-packages\\crewai\\agent.py\", line 243, in execute_task\n    result = self.agent_executor.invoke(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\newla\\crewai\\financial_analyst1\\productmanager\\productmanagementflow\\.venv\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py\", line 112, in invoke\n    raise e\n  File \"D:\\Users\\newla\\crewai\\financial_analyst1\\productmanager\\productmanagementflow\\.venv\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py\", line 102, in invoke\n    formatted_answer = self._invoke_loop()\n                       ^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\newla\\crewai\\financial_analyst1\\productmanager\\productmanagementflow\\.venv\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py\", line 160, in _invoke_loop\n    raise e\n  File \"D:\\Users\\newla\\crewai\\financial_analyst1\\productmanager\\productmanagementflow\\.venv\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py\", line 140, in _invoke_loop\n    answer = self._get_llm_response()\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\newla\\crewai\\financial_analyst1\\productmanager\\productmanagementflow\\.venv\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py\", line 210, in _get_llm_response\n    raise e\n  File \"D:\\Users\\newla\\crewai\\financial_analyst1\\productmanager\\productmanagementflow\\.venv\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py\", line 201, in _get_llm_response\n    answer = self.llm.call(\n             ^^^^^^^^^^^^^^\n  File \"D:\\Users\\newla\\crewai\\financial_analyst1\\productmanager\\productmanagementflow\\.venv\\Lib\\site-packages\\crewai\\llm.py\", line 291, in call\n    response = litellm.completion(**params)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\newla\\crewai\\financial_analyst1\\productmanager\\productmanagementflow\\.venv\\Lib\\site-packages\\litellm\\utils.py\", line 1154, in wrapper\n    raise e\n  File \"D:\\Users\\newla\\crewai\\financial_analyst1\\productmanager\\productmanagementflow\\.venv\\Lib\\site-packages\\litellm\\utils.py\", line 1032, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\newla\\crewai\\financial_analyst1\\productmanager\\productmanagementflow\\.venv\\Lib\\site-packages\\litellm\\main.py\", line 3068, in completion\n    raise exception_type(\n          ^^^^^^^^^^^^^^^\n  File \"D:\\Users\\newla\\crewai\\financial_analyst1\\productmanager\\productmanagementflow\\.venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2201, in exception_type\n    raise e\n  File \"D:\\Users\\newla\\crewai\\financial_analyst1\\productmanager\\productmanagementflow\\.venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 1197, in exception_type\n    raise litellm.InternalServerError(\nlitellm.exceptions.InternalServerError: litellm.InternalServerError: litellm.InternalServerError: VertexAIException - {\n  \"error\": {\n    \"code\": 503,\n    \"message\": \"The model is overloaded. Please try again later.\",\n    \"status\": \"UNAVAILABLE\"\n  }\n}\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.60.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "andrewn3",
      "author_type": "User",
      "created_at": "2025-03-05T15:51:20Z",
      "updated_at": "2025-06-21T00:01:57Z",
      "closed_at": "2025-06-21T00:01:57Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9007/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9007",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9007",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:12.661222",
      "comments": [
        {
          "author": "colesmcintosh",
          "body": "@andrewn3 working on a fix for this",
          "created_at": "2025-03-10T23:43:04Z"
        },
        {
          "author": "colesmcintosh",
          "body": "@andrewn3 there is built-in support for retries and fallbacks to manage such transient errors, here are the docs --> https://docs.litellm.ai/docs/proxy/reliability \n\nI'll open a PR to cover the api key masking üëç ",
          "created_at": "2025-03-15T22:53:21Z"
        },
        {
          "author": "colesmcintosh",
          "body": "@andrewn3 opened this PR for api key masking https://github.com/BerriAI/litellm/pull/9285",
          "created_at": "2025-03-15T23:06:49Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-14T00:01:55Z"
        }
      ]
    },
    {
      "issue_number": 11759,
      "title": "[Bug]: UserWarning: Pydantic serializer warnings:   PydanticSerializationUnexpectedValue(Expected 9 fields but got 6: ...",
      "body": "### What happened?\n\nWhen upgrading from LiteLLM 1.72.4 to 1.72.6, I now get a pydantic Serialization warning about unexpected values. \n\nIt's triggered by this code:\n\n<img width=\"1084\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/695b315b-1058-4485-8547-2d7e8c4b68bb\" />\n\nI'm using `model_id=\"gpt-4.1-mini\"` and the acompletion API\n\nFound while working on https://github.com/mozilla-ai/any-agent/issues/370\n\n### Relevant log output\n\n```shell\n/Users/nbrake/scm/any-agent/.venv/lib/python3.13/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected 9 fields but got 6: Expected `Message` - serialized value may not be as expected [input_value=Message(content=None, rol...: None}, annotations=[]), input_type=Message])\n  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='to...ider_specific_fields={}), input_type=Choices])\n  return self.__pydantic_serializer__.to_python(\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.72.6\n\n### Twitter / LinkedIn details\n\nhttps://www.linkedin.com/in/njbrake/",
      "state": "open",
      "author": "njbrake",
      "author_type": "User",
      "created_at": "2025-06-16T12:50:34Z",
      "updated_at": "2025-06-20T21:25:09Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11759/reactions",
        "total_count": 15,
        "+1": 15,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11759",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11759",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:12.878796",
      "comments": [
        {
          "author": "kumarss20",
          "body": "I'm getting similar error when i tried to use GCP gemini models via litellm on AWS Strands Agents, \n\nI have tested with litellm version 1.72.2 , 1.72.4 and 1.72.6\n\nhere is my code \n\n```\n  from litellm import completion\n  tools = [{\"googleSearch\": {}}]\n  response = completion(\n        model=\"gemini/g",
          "created_at": "2025-06-19T17:47:46Z"
        },
        {
          "author": "ShivanshJ",
          "body": "I'm getting a similar error.\nUsing model = claude-3.7-sonnet\n```\nmessages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are an expert in code analysis, regex pattern matching, and static analysis. You also decide whether it's needed or not.\",\n            },\n    ",
          "created_at": "2025-06-20T21:25:09Z"
        }
      ]
    },
    {
      "issue_number": 9348,
      "title": "[Bug]: Logging and Alert UI : No Delete option",
      "body": "### What happened?\n\nOnce an active logging callback item is created from the UI, cannot see any option to delete the callback.\nAlso, the edit button brings up a new entry record and the existing values not populated\n\n![Image](https://github.com/user-attachments/assets/52ca4d37-4d63-4f3a-9728-2b2af934f831)\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.63.11 - patch1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "metalshanked",
      "author_type": "User",
      "created_at": "2025-03-18T18:56:22Z",
      "updated_at": "2025-06-20T20:50:39Z",
      "closed_at": "2025-06-20T20:50:39Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9348/reactions",
        "total_count": 4,
        "+1": 4,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9348",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9348",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:13.159871",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-17T00:01:49Z"
        }
      ]
    },
    {
      "issue_number": 11921,
      "title": "[Bug]: reasoning_effort works for gemini-2.5-flash but not gemini-2.5-pro",
      "body": "### What happened?\n\n```python\nfrom litellm import completion\n\nresponse = completion(\n  model=\"vertex_ai/gemini-2.5-flash\",\n  messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n  reasoning_effort=\"low\"\n)\n\nprint(response)\n```\n‚¨ÜÔ∏è Works as desired. \n\n```python\nfrom litellm import completion\n\nresponse = completion(\n  model=\"vertex_ai/gemini-2.5-pro\",\n  messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n  reasoning_effort=\"low\" # or \"medium\", or \"high\"\n)\n\nprint(response)\n```\n\nraises:\n```bash\nlitellm.exceptions.UnsupportedParamsError: litellm.UnsupportedParamsError: vertex_ai does not support parameters: ['reasoning_effort'], for model=gemini-2.5-pro. To drop these, set `litellm.drop_params=True` or for proxy:\n\n`litellm_settings:\n drop_params: true`\n. \n If you want to use these params dynamically send allowed_openai_params=['reasoning_effort'] in your request.\n```\n\nWould it be possible to support reasoning_effort for gemini-2.5-pro as well? Thank you in advance for looking into this!\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.6.post1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "zhiyuan-1iu",
      "author_type": "User",
      "created_at": "2025-06-20T10:37:47Z",
      "updated_at": "2025-06-20T20:11:38Z",
      "closed_at": "2025-06-20T20:11:38Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11921/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11921",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11921",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:13.347768",
      "comments": [
        {
          "author": "zhiyuan-1iu",
          "body": "OpenAI Codex suggests adding following lines into the `litellm/model_prices_and_context_window.json`\n\n```json\n \"gemini-2.5-pro\": {\n        \"max_tokens\": 65535,\n        \"max_input_tokens\": 1048576,\n        \"max_output_tokens\": 65535,\n        \"max_images_per_prompt\": 3000,\n        \"max_videos_per_prom",
          "created_at": "2025-06-20T11:30:02Z"
        },
        {
          "author": "pauljz",
          "body": "Looks like ~#11840~ #11927 fixes this once merged.",
          "created_at": "2025-06-20T14:10:23Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "fixed",
          "created_at": "2025-06-20T20:11:38Z"
        }
      ]
    },
    {
      "issue_number": 11404,
      "title": "[Bug]: Knowledge Base Call returning error",
      "body": "### What happened?\n\nWe are running LiteLLM in AWS.\nUsing other functionality successfully.\nSet up Bedrock Knowledge Base and added it.\nI am able to run it through the **\"Test Key\"** with the model and the Vector Store without issue.\n\nWhen I run it from an application I get an error.\n\nUsing Curl:\n\n```shell\ncurl https://myurl/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer sk-mykey\" \\\n  -d '{\n    \"model\": \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Who is on the Platform Engineering team?\"}],\n    \"tools\": [\n        {\n            \"type\": \"file_search\",\n            \"vector_store_ids\": [\"QZTOGKY5HL\"]\n        }\n    ]\n  }'\n```\n\nERROR:\n```json\n{\n    \"error\": {\n        \"code\": \"400\",\n        \"message\": \"litellm.BadRequestError: BedrockException - {\\\"message\\\":\\\"3 validation errors detected: Value '' at 'toolConfig.tools.1.member.toolSpec.name' failed to satisfy constraint: Member must have length greater than or equal to 1; Value '' at 'toolConfig.tools.1.member.toolSpec.name' failed to satisfy constraint: Member must satisfy regular expression pattern: [a-zA-Z0-9_-]+; Value '' at 'toolConfig.tools.1.member.toolSpec.description' failed to satisfy constraint: Member must have length greater than or equal to 1\\\"}. Received Model Group=us.anthropic.claude-3-5-sonnet-20241022-v2:0Available Model Group Fallbacks=None\",\n        \"param\": null,\n        \"type\": null\n    }\n}\n```\n\nI get the same error when using OpenAI lib\n\n```python\n# Get env variables\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nOPENAI_API_BASE = os.getenv(\"OPENAI_API_BASE\")\nMODEL = os.getenv(\"MODEL\")\nKB_ID=\"QZTOGKY5HL\"\n\n# set up OpenAI client\nclient = openai.OpenAI(\n    api_key=OPENAI_API_KEY,\n    base_url=OPENAI_API_BASE,\n)\n\n# send a chat message to the model\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Who is on the Platform Engineering Team?\"\n    }\n]\n\ntools = [\n    {\n        \"type\": \"file_search\",\n        \"vector_store_ids\": [KB_ID]\n    }\n]\n\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages = messages,\n    tools=tools,\n)\n```\n\nIf I remove the tools the call runs successfully (of course without KB) so I know settings are correct.\n\nIf I do a curl to get a list of Vector Stores I see the one I am using.\n\n\n### Relevant log output\n\n```shell\n{\n  \"status\": \"failure\",\n  \"batch_models\": null,\n  \"usage_object\": null,\n  \"user_api_key\": \"3f1c7c212629bacf3fc183b9dc1dd5b9f569d2105dc2ded4f2bb15ef06cc1dbd\",\n  \"error_information\": {\n    \"traceback\": \"  File \\\"/usr/lib/python3.13/site-packages/litellm/proxy/proxy_server.py\\\", line 3537, in chat_completion\\n    return await base_llm_response_processor.base_process_llm_request(\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n    ...<16 lines>...\\n    )\\n    ^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/proxy/common_request_processing.py\\\", line 391, in base_process_llm_request\\n    responses = await llm_responses\\n                ^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/router.py\\\", line 990, in acompletion\\n    raise e\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/router.py\\\", line 966, in acompletion\\n    response = await self.async_function_with_fallbacks(**kwargs)\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/router.py\\\", line 3536, in async_function_with_fallbacks\\n    raise original_exception\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/router.py\\\", line 3350, in async_function_with_fallbacks\\n    response = await self.async_function_with_retries(*args, **kwargs)\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/router.py\\\", line 3728, in async_function_with_retries\\n    raise original_exception\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/router.py\\\", line 3619, in async_function_with_retries\\n    response = await self.make_call(original_function, *args, **kwargs)\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/router.py\\\", line 3737, in make_call\\n    response = await response\\n               ^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/router.py\\\", line 1129, in _acompletion\\n    raise e\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/router.py\\\", line 1088, in _acompletion\\n    response = await _response\\n               ^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/utils.py\\\", line 1492, in wrapper_async\\n    raise e\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/utils.py\\\", line 1353, in wrapper_async\\n    result = await original_function(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/main.py\\\", line 531, in acompletion\\n    raise exception_type(\\n          ~~~~~~~~~~~~~~^\\n        model=model,\\n        ^^^^^^^^^^^^\\n    ...<3 lines>...\\n        extra_kwargs=kwargs,\\n        ^^^^^^^^^^^^^^^^^^^^\\n    )\\n    ^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\\\", line 2239, in exception_type\\n    raise e\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\\\", line 939, in exception_type\\n    raise BadRequestError(\\n    ...<4 lines>...\\n    )\\n\",\n    \"error_code\": \"400\",\n    \"error_class\": \"BadRequestError\",\n    \"llm_provider\": \"bedrock\",\n    \"error_message\": \"litellm.BadRequestError: BedrockException - {\\\"message\\\":\\\"3 validation errors detected: Value '' at 'toolConfig.tools.1.member.toolSpec.name' failed to satisfy constraint: Member must have length greater than or equal to 1; Value '' at 'toolConfig.tools.1.member.toolSpec.name' failed to satisfy constraint: Member must satisfy regular expression pattern: [a-zA-Z0-9_-]+; Value '' at 'toolConfig.tools.1.member.toolSpec.description' failed to satisfy constraint: Member must have length greater than or equal to 1\\\"}. Received Model Group=us.amazon.nova-lite-v1:0\\nAvailable Model Group Fallbacks=None LiteLLM Retried: 1 times, LiteLLM Max Retries: 2\"\n  },\n  \"applied_guardrails\": null,\n  \"user_api_key_alias\": \"ai-en\",\n  \"user_api_key_org_id\": null,\n  \"requester_ip_address\": \"\",\n  \"user_api_key_team_id\": \"2fee7bea-c133-46df-9979-ede094b333aa\",\n  \"user_api_key_user_id\": \"default_user_id\",\n  \"guardrail_information\": null,\n  \"model_map_information\": null,\n  \"mcp_tool_call_metadata\": null,\n  \"additional_usage_values\": {},\n  \"user_api_key_team_alias\": \"Platform Engineering\",\n  \"vector_store_request_metadata\": null\n}\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.71.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "apanagos",
      "author_type": "User",
      "created_at": "2025-06-04T14:10:24Z",
      "updated_at": "2025-06-20T19:11:51Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11404/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "ishaan-jaff"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11404",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11404",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:13.577093",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "Fixed here @apanagos \n\nhttps://github.com/BerriAI/litellm/pull/11467\n\ncan you test again and confirm it's fixed ",
          "created_at": "2025-06-06T01:23:52Z"
        },
        {
          "author": "apanagos",
          "body": "ok. I will test it out today and report back.",
          "created_at": "2025-06-06T13:01:32Z"
        },
        {
          "author": "germanilia",
          "body": "I don't think the issue is fixed with this version, it fails to even with the regular completion with the lates docker version",
          "created_at": "2025-06-06T16:51:05Z"
        },
        {
          "author": "apanagos",
          "body": "I tested locally but had issues. I just updated with 1.72.2 (ghcr.io/berriai/litellm:main-stable) but I don't think the fix is in there. Once I deploy this to prod for some of my users to use I will circle back to test v1.72.1.dev8 in dev.",
          "created_at": "2025-06-09T14:42:57Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "Please try this image: `ghcr.io/berriai/litellm:main-v1.72.2.rc`",
          "created_at": "2025-06-09T14:47:54Z"
        }
      ]
    },
    {
      "issue_number": 11797,
      "title": "[Bug]: MCP error Team doesn't exist in cache",
      "body": "### What happened?\n\nHi,\n\nI've set up an MCP server. The unusual thing is that the \"admin\" user key works fine, but not for the other virtual keys, even though \"admin\" is a member of the same team as the other virtual keys. \n\nThe (MCP client) error is: `Team doesn't exist in cache + check_cache_only=True. Team=1106dfa3-5f96-4f52-b8b7-c766e306509f.`\n\nI've tried:\n  - to create a new virtual key\n  - to change the virtual key team\n\nBut always the same error.\n\nTested with this code:\n\n```\nimport asyncio\nimport json\n\nfrom fastmcp import Client\nfrom fastmcp.client.transports import StreamableHttpTransport\n\n# Create the transport with your LiteLLM MCP server URL\nserver_url = \"https://mylitellm.com/mcp\"\ntransport = StreamableHttpTransport(\n    server_url,\n    headers={\n        \"Authorization\": \"Bearer sk-1234\"\n    }\n)\n\n# Initialize the client with the transport\nclient = Client(transport=transport)\n\n\nasync def main():\n    # Connection is established here\n    print(\"Connecting to LiteLLM MCP server...\")\n    async with client:\n        print(f\"Client connected: {client.is_connected()}\")\n\n        # Make MCP calls within the context\n        print(\"Fetching available tools...\")\n        tools = await client.list_tools()\n\n        print(f\"Available tools: {json.dumps([t.__dict__ for t in tools], indent=2)}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\nDetails error message:\n\n```\nConnecting to LiteLLM MCP server...\nClient connected: True\nFetching available tools...\nTraceback (most recent call last):\n  File \"/home/matt/Projects/ai/playground/MCP/test_client_fastmcp.py\", line 44, in <module>\n    asyncio.run(main())\n  File \"/home/matt/.local/share/mise/installs/python/3.12.4/lib/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/home/matt/.local/share/mise/installs/python/3.12.4/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/matt/.local/share/mise/installs/python/3.12.4/lib/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/home/matt/Projects/ai/playground/MCP/test_client_fastmcp.py\", line 28, in main\n    tools = await client.list_tools()\n            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/matt/Projects/ai/playground/.venv/lib/python3.12/site-packages/fastmcp/client/client.py\", line 615, in list_tools\n    result = await self.list_tools_mcp()\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/matt/Projects/ai/playground/.venv/lib/python3.12/site-packages/fastmcp/client/client.py\", line 603, in list_tools_mcp\n    result = await self.session.list_tools()\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/matt/Projects/ai/playground/.venv/lib/python3.12/site-packages/mcp/client/session.py\", line 324, in list_tools\n    return await self.send_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/matt/Projects/ai/playground/.venv/lib/python3.12/site-packages/mcp/shared/session.py\", line 286, in send_request\n    raise McpError(response_or_error.error)\nmcp.shared.exceptions.McpError: Team doesn't exist in cache + check_cache_only=True. Team=1106dfa3-5f96-4f52-b8b7-c766e306509f.\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.6.rc\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "mboret",
      "author_type": "User",
      "created_at": "2025-06-17T13:12:45Z",
      "updated_at": "2025-06-20T19:11:02Z",
      "closed_at": "2025-06-17T23:25:13Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11797/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "ishaan-jaff"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11797",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11797",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:13.779596",
      "comments": [
        {
          "author": "zbloss",
          "body": "I am still seeing this same issue on 1.72.1 using the same code above",
          "created_at": "2025-06-20T18:57:44Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "It's fixed on the latest nightly, please try @zbloss ",
          "created_at": "2025-06-20T19:11:02Z"
        }
      ]
    },
    {
      "issue_number": 11879,
      "title": "[Feature]: Supports the volcengine new thinking mode for doubao-seed-1.6-250615",
      "body": "### The Feature\n\nvolcengine new model doubao-seed-1.6 add a new params \"thinking\": {\"type\": \"xxx\"}.\nLinks:\n- https://www.volcengine.com/docs/82379/1593702#%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3\n\n![Image](https://github.com/user-attachments/assets/d014e5f9-d5c4-4e8f-8c3a-39197137bf76)\n\n\n\n### Motivation, pitch\n\nI am working on AGI company as a Devoloper\n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nYes\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "gumutianqi",
      "author_type": "User",
      "created_at": "2025-06-19T07:48:25Z",
      "updated_at": "2025-06-20T16:40:34Z",
      "closed_at": "2025-06-20T16:40:34Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11879/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11879",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11879",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:13.945939",
      "comments": []
    },
    {
      "issue_number": 11834,
      "title": "[Bug]: Azure gpt-4-1 doesn't support response_schema",
      "body": "### What happened?\n\nI have an azure gpt4.1 deployment named 'gpt-4-1'.\nBut when I use json_schema such as\n```\n{\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"<My Prompt>\"\n        }\n    ],\n    \"model\": \"gpt-4-1\",\n    \"response_format\": {\n        \"type\": \"json_schema\",\n        \"json_schema\": {\n            \"schema\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"category\": {\n                        \"type\": \"string\"\n                    },\n                    \"type\": {\n                        \"type\": \"string\"\n                    },\n                    \"labelReason\": {\n                        \"type\": \"string\"\n                    }\n                },\n                \"required\": [\n                    \"category\",\n                    \"type\",\n                    \"labelReason\"\n                ],\n                \"additionalProperties\": false\n            },\n            \"name\": \"llm_response\"\n        }\n    },\n    \"stream\": true,\n    \"user\": \"1a033388-5213-3d08-8218-48817f6ebc4b\"\n}\n```\nLitellm will convert it to json_tool_call\n\n### Relevant log output\n\n```shell\ndata: {\"id\":\"chatcmpl-63474ca9-8ea2-445d-8d0c-d95612be82d7\",\"created\":1750218751,\"model\":\"gpt-4.1-2025-04-14\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_07e970ab25\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"tool_calls\":[{\"id\":\"call_HuI5VcZXOJbqorpRlWne0Qe5\",\"function\":{\"arguments\":\"\",\"name\":\"json_tool_call\"},\"type\":\"function\",\"index\":0}]}}]}\n\ndata: {\"id\":\"chatcmpl-63474ca9-8ea2-445d-8d0c-d95612be82d7\",\"created\":1750218751,\"model\":\"gpt-4.1-2025-04-14\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_07e970ab25\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"tool_calls\":[{\"function\":{\"arguments\":\"{\\\"\"},\"type\":\"function\",\"index\":0}]}}]}\n\ndata: {\"id\":\"chatcmpl-63474ca9-8ea2-445d-8d0c-d95612be82d7\",\"created\":1750218751,\"model\":\"gpt-4.1-2025-04-14\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_07e970ab25\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"tool_calls\":[{\"function\":{\"arguments\":\"category\"},\"type\":\"function\",\"index\":0}]}}]}\n\ndata: {\"id\":\"chatcmpl-63474ca9-8ea2-445d-8d0c-d95612be82d7\",\"created\":1750218751,\"model\":\"gpt-4.1-2025-04-14\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_07e970ab25\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"tool_calls\":[{\"function\":{\"arguments\":\"\\\":\\\"\"},\"type\":\"function\",\"index\":0}]}}]}\n\ndata: {\"id\":\"chatcmpl-63474ca9-8ea2-445d-8d0c-d95612be82d7\",\"created\":1750218751,\"model\":\"gpt-4.1-2025-04-14\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_07e970ab25\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"tool_calls\":[{\"function\":{\"arguments\":\"Áâ©ÊµÅ\"},\"type\":\"function\",\"index\":0}]}}]}\n...\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.70.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "KyleZhang0536",
      "author_type": "User",
      "created_at": "2025-06-18T05:45:22Z",
      "updated_at": "2025-06-20T16:40:34Z",
      "closed_at": "2025-06-20T16:40:34Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11834/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11834",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11834",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:13.945960",
      "comments": [
        {
          "author": "KyleZhang0536",
          "body": "I think the possible reason is that my deployment name 'gpt-4-1' doesn't match the key 'gpt-4.1' in model_prices_and_context_window.json. Then supports_response_schema return false\nhttps://github.com/BerriAI/litellm/blob/01af7fe1a848569c56741df99d4f76dde58a5af2/litellm/llms/azure/chat/gpt_transforma",
          "created_at": "2025-06-18T06:19:28Z"
        },
        {
          "author": "krrishdholakia",
          "body": "yes it is because of the gpt-4-1 naming convention\n",
          "created_at": "2025-06-20T05:55:11Z"
        }
      ]
    },
    {
      "issue_number": 11919,
      "title": "[Bug]: LiteLLM resposnes api does not support mcp tool call",
      "body": "### What happened?\n\nTrying out couple of mcp as a tool call in litellm responses api, but that is not supported.\n\n```\nimport litellm\n\nresp = litellm.responses(\n    model=\"openai/gpt-4.1\",\n    tools=[{\n        \"type\": \"mcp\",\n        \"server_label\": \"\",\n        \"server_url\": f\"\",\n        \"require_approval\": \"never\",\n        \"allowed_tools\": [],\n    }],\n    input=\"Use the tool calling and tell me all the meetings on June 19th 2025 from Google calendar, I have already connected my calednar, first list all the calendars then selcted my (john) calendar, after than use the as calendar_id to get all the events. then at the end give me a summary of the day\",\n)\n```\n\n### Relevant log output\n\n```shell\nAPIConnectionError: litellm.APIConnectionError: APIConnectionError: OpenAIException - 15 validation errors for ResponsesAPIResponse\ntools.list[union[FileSearchTool,FunctionTool,ComputerTool,WebSearchTool]].0.FileSearchTool.type\n  Input should be 'file_search' [type=literal_error, input_value='mcp', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/literal_error\ntools.list[union[FileSearchTool,FunctionTool,ComputerTool,WebSearchTool]].0.FileSearchTool.vector_store_ids\n  Field required [type=missing, input_value={'type': 'mcp', 'allowed_....unified.to/<redacted>'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\ntools.list[union[FileSearchTool,FunctionTool,ComputerTool,WebSearchTool]].0.FunctionTool.name\n  Field required [type=missing, input_value={'type': 'mcp', 'allowed_....unified.to/<redacted>'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\ntools.list[union[FileSearchTool,FunctionTool,ComputerTool,WebSearchTool]].0.FunctionTool.parameters\n  Field required [type=missing, input_value={'type': 'mcp', 'allowed_....unified.to/<redacted>'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\ntools.list[union[FileSearchTool,FunctionTool,ComputerTool,WebSearchTool]].0.FunctionTool.strict\n  Field required [type=missing, input_value={'type': 'mcp', 'allowed_....unified.to/<redacted>'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\ntools.list[union[FileSearchTool,FunctionTool,ComputerTool,WebSearchTool]].0.FunctionTool.type\n  Input should be 'function' [type=literal_error, input_value='mcp', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/literal_error\ntools.list[union[FileSearchTool,FunctionTool,ComputerTool,WebSearchTool]].0.ComputerTool.display_height\n  Field required [type=missing, input_value={'type': 'mcp', 'allowed_....unified.to/<redacted>'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\ntools.list[union[FileSearchTool,FunctionTool,ComputerTool,WebSearchTool]].0.ComputerTool.display_width\n  Field required [type=missing, input_value={'type': 'mcp', 'allowed_....unified.to/<redacted>'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\ntools.list[union[FileSearchTool,FunctionTool,ComputerTool,WebSearchTool]].0.ComputerTool.environment\n  Field required [type=missing, input_value={'type': 'mcp', 'allowed_....unified.to/<redacted>'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\ntools.list[union[FileSearchTool,FunctionTool,ComputerTool,WebSearchTool]].0.ComputerTool.type\n  Input should be 'computer_use_preview' [type=literal_error, input_value='mcp', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/literal_error\ntools.list[union[FileSearchTool,FunctionTool,ComputerTool,WebSearchTool]].0.WebSearchTool.type\n  Input should be 'web_search_preview' or 'web_search_preview_2025_03_11' [type=literal_error, input_value='mcp', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/literal_error\ntools.list[ResponseFunctionToolCall].0.arguments\n  Field required [type=missing, input_value={'type': 'mcp', 'allowed_....unified.to/<redacted>'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\ntools.list[ResponseFunctionToolCall].0.call_id\n  Field required [type=missing, input_value={'type': 'mcp', 'allowed_....unified.to/<redacted>'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\ntools.list[ResponseFunctionToolCall].0.name\n  Field required [type=missing, input_value={'type': 'mcp', 'allowed_....unified.to/<redacted>'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\ntools.list[ResponseFunctionToolCall].0.type\n  Input should be 'function_call' [type=literal_error, input_value='mcp', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/literal_error\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.7\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "AakarSharma",
      "author_type": "User",
      "created_at": "2025-06-20T10:21:02Z",
      "updated_at": "2025-06-20T16:29:21Z",
      "closed_at": "2025-06-20T15:50:05Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11919/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11919",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11919",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:14.142109",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "@AakarSharma please try upgrading the version of your OpenAI python SDK and retry. It should work ",
          "created_at": "2025-06-20T15:50:05Z"
        },
        {
          "author": "AakarSharma",
          "body": "This worked, thanks a lot",
          "created_at": "2025-06-20T16:29:21Z"
        }
      ]
    },
    {
      "issue_number": 11925,
      "title": "[Bug]: Passthrough should not modify model base_uri",
      "body": "### What happened?\n\nLet us consider the following model:\n```\n  - model_name: xxxx\n    litellm_params:\n      model: hosted_vllm/xxxx\n      api_base: \"https://xxxx/server/2/v1/\"\n    model_info:\n      mode: embedding\n```\n\n**Expected behaviour:**\nCalling `/vllm/endpoint` should forward to `https://xxxx/server/2/v1/endpoint`\n\n**Actual behaviour:**\nUser defined base_uri get stripped from any pre-existing route and forward to: `https://xxxx/endpoint` instead.\n\nError happens [here](https://github.com/BerriAI/litellm/blob/7abece4ad8d0911d62fc920f5f88cb4b5b5f9e46/litellm/passthrough/main.py#L136) in the code.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.72.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "MightyGoldenOctopus",
      "author_type": "User",
      "created_at": "2025-06-20T15:26:13Z",
      "updated_at": "2025-06-20T15:26:22Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11925/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11925",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11925",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:14.321818",
      "comments": []
    },
    {
      "issue_number": 11922,
      "title": "[Feature]:  Request: Support Anthropic SDK to OpenAI-Compatible Service Reverse Translation",
      "body": "### The Feature\n\n## Summary\nRequest support for reverse translation: allowing clients to use **Anthropic SDK** while the backend routes to **OpenAI-compatible services** through LiteLLM proxy.\n\n## Current Behavior\nLiteLLM currently supports:\n- **Forward translation**: OpenAI SDK ‚Üí LiteLLM Proxy ‚Üí Various providers (Anthropic, Azure, etc.)\n- **Pass-through**: Anthropic SDK ‚Üí LiteLLM Proxy ‚Üí Anthropic API (direct)\n\n## Requested Feature\nSupport **reverse translation**:\n- **Client**: Anthropic SDK format\n- **Backend**: OpenAI-compatible services (e.g., vLLM, local models, custom APIs)\n- **Flow**: `Anthropic SDK` ‚Üí `LiteLLM Proxy` ‚Üí `OpenAI-compatible service`\n\n## Use Case\nWe have OpenAI-compatible services (like vLLM, local models) but want to:\n1. Maintain existing client code that uses Anthropic SDK\n2. Leverage LiteLLM's excellent proxy features (logging, rate limiting, cost tracking)\n3. Route Anthropic-formatted requests to our OpenAI-compatible backends\n\n## Proposed Configuration\n```yaml\nmodel_list:\n  - model_name: claude-3-sonnet-20240229  # Client uses this model name\n    litellm_params:\n      model: openai/my-local-model        # Route to OpenAI-compatible service\n      api_base: http://my-vllm-server:8000/v1\n      api_key: dummy-key\n    input_format: anthropic               # NEW: Accept Anthropic format\n    output_format: anthropic              # NEW: Return Anthropic format\n```\n\n## Example Client Code\n```python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    base_url=\"http://litellm-proxy:4000/anthropic\",  # LiteLLM proxy\n    api_key=\"sk-proxy-key\"\n)\n\n# This should work and route to OpenAI-compatible backend\nresponse = client.messages.create(\n    model=\"claude-3-sonnet-20240229\",\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n)\n```\n\n### Motivation, pitch\n\n## Current Workaround\nCurrently, we have to:\n1. Rewrite client code to use OpenAI SDK, or\n2. Build a custom translation layer\n\n## Benefits\n- Maintain API compatibility for existing Anthropic SDK users\n- Leverage LiteLLM's proxy features with any backend\n- Enable gradual migration from Anthropic to local/custom models\n- Support hybrid deployments (some models on Anthropic, others local)\n\n## Related Documentation\n- [Anthropic SDK Pass-through](https://docs.litellm.ai/docs/pass_through/anthropic_completion)\n- [OpenAI-Compatible Providers](https://docs.litellm.ai/docs/providers/openai_compatible)\n\n## Additional Context\nThis would be the reverse of LiteLLM's current design philosophy, but would enable powerful hybrid deployment scenarios where teams want to:\n- Keep familiar Anthropic SDK interfaces\n- Route to cost-effective local models\n- Maintain centralized logging and control through LiteLLM proxy\n\nThank you for considering this feature request! \n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "lacia-hIE",
      "author_type": "User",
      "created_at": "2025-06-20T12:08:26Z",
      "updated_at": "2025-06-20T12:39:54Z",
      "closed_at": "2025-06-20T12:39:54Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11922/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11922",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11922",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:14.321834",
      "comments": []
    },
    {
      "issue_number": 11389,
      "title": "[Bug]: Proxy Server env HTTP__PROXY is not taking effect",
      "body": "### What happened?\n\nA bug happened!\nuse k8s deploy litellm proxy serverÔºåthe env HTTP_PROXY not taking effect\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.0\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "Ccheers",
      "author_type": "User",
      "created_at": "2025-06-04T04:40:53Z",
      "updated_at": "2025-06-20T10:38:52Z",
      "closed_at": "2025-06-12T04:13:38Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11389/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11389",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11389",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:14.321838",
      "comments": [
        {
          "author": "Ccheers",
          "body": "![Image](https://github.com/user-attachments/assets/e1f15600-4a85-4a40-9bf6-dc4c39e1f712)\n\n![Image](https://github.com/user-attachments/assets/a1fc28ed-ac2a-483e-ac3a-8a7a90367f94)\n\n![Image](https://github.com/user-attachments/assets/3b595bbf-2ef7-437e-bcf8-e82704ff0789)\n\nthe provider is Openrouter\n",
          "created_at": "2025-06-04T05:34:56Z"
        },
        {
          "author": "X4tar",
          "body": "Same to you!  Version: **v1.72.2.rc**\nALL below was set:\n- HTTP_PROXY \n- HTTPS_PROXY \n- http_proxy \n- https_proxy \n\nEverything works well in **main-v1.70.4-nightly**",
          "created_at": "2025-06-08T09:50:02Z"
        },
        {
          "author": "idootop",
          "body": "### Root Cause:‚Äã‚Äã\nIssue introduced in commit https://github.com/BerriAI/litellm/commit/86cdb8382b15a470d06f9b3c43a91c96611977a5#r159732232\n\n### Status:‚Äã‚Äã\nFix submitted in PR https://github.com/BerriAI/litellm/pull/11616\n\n![Image](https://github.com/user-attachments/assets/9f7f11fe-7c7d-484f-b071-3c0",
          "created_at": "2025-06-11T11:45:09Z"
        },
        {
          "author": "xmcp",
          "body": "I think this issue should be re-opened since the PR is reverted? Not supporting http proxy env variables is currently preventing me from upgrading litellm.",
          "created_at": "2025-06-16T16:22:36Z"
        },
        {
          "author": "YoshichikaAAA",
          "body": "> I think this issue should be re-opened since the PR is reverted? Not supporting http proxy env variables is currently preventing me from upgrading litellm.\n\nI completely agree. I believe it should be reopened.\nPR #11616 was reverted due to concerns about blocking caused by synchronous file I/O.\nLi",
          "created_at": "2025-06-17T01:30:45Z"
        }
      ]
    },
    {
      "issue_number": 11835,
      "title": "[Bug]: LiteLLM 1.72.6 claims to \"Enable System Proxy Support for aiohttp transport\" while it doesn't",
      "body": "### What happened?\n\nBoth the [1.72.6 release note in docs](https://github.com/BerriAI/litellm/blob/main/docs/my-website/release_notes/v1.72.6-stable/index.md) and [the GitHub release page](https://github.com/BerriAI/litellm/releases/tag/v1.72.6.rc) mention that:\n\n> - Enable System Proxy Support for aiohttp transport - [PR](https://github.com/BerriAI/litellm/pull/11616) (s/o [idootop](https://github.com/idootop))\n\nHowever this PR is reverted by https://github.com/BerriAI/litellm/commit/098fb0307a7d0c97297f465e05821e0934e332ec which is also included in 1.72.6-rc. Therefore, this version actually doesn't have support for system proxy and the release note is misleading.\n\nAccording to the discussion under https://github.com/BerriAI/litellm/issues/11389, this is affecting multiple users. So I would suggest to:\n\n- Remove the \"Enable System Proxy Support for aiohttp transport\" line in the docs and the release page\n- Add a note to tell users about this breaking change, so users requiring http proxy should opt-out with `USE_AIOHTTP_TRANSPORT=False`\n- Bring back the http proxy support (possibly following comments at https://github.com/BerriAI/litellm/pull/11616#issuecomment-2973465102) in the next release\n\n### Relevant log output\n\n```shell\nN/A\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\n1.72.6.rc\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "xmcp",
      "author_type": "User",
      "created_at": "2025-06-18T06:45:41Z",
      "updated_at": "2025-06-20T10:27:17Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11835/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11835",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11835",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:14.512487",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "cc: @ishaan-jaff ",
          "created_at": "2025-06-20T05:50:18Z"
        },
        {
          "author": "fbzhong",
          "body": "Yeah, the commit was reverted in 098fb0307a7d0c97297f465e05821e0934e332ec . I am also wondering why revert it?",
          "created_at": "2025-06-20T10:27:17Z"
        }
      ]
    },
    {
      "issue_number": 11918,
      "title": "[Bug]: Bulk Invite Users is not working for \"internal_user_view_only\" role",
      "body": "### What happened?\n\nGo to Internal Users\nClick on Bulk Invite Users\nDownload the csv template\nUpdate user details and upload the file\nExpected: csv must be uploaded and the users must get invite to create API key\nActual: cant upload with internal_user_viewer\n\n```\nuser_email,user_role,teams,max_budget,budget_duration,models\nu01@hive.gov.sg,internal_user_view_only,\"c7c7d157-5044-48eb-9b1f-086247ccd070\",,,\n```\n\n\nError message\n```\n{\n  \"detail\": [\n    {\n      \"type\": \"literal_error\",\n      \"loc\": [\"body\", \"user_role\"],\n      \"msg\": \"Input should be <LitellmUserRoles.PROXY_ADMIN: 'proxy_admin'>, <LitellmUserRoles.PROXY_ADMIN_VIEW_ONLY: 'proxy_admin_viewer'>, <LitellmUserRoles.INTERNAL_USER: 'internal_user'> or <LitellmUserRoles.INTERNAL_USER_VIEW_ONLY: 'internal_user_viewer'>\",\n      \"input\": \"internal_user_view_only\",\n      \"ctx\": {\n        \"expected\": \"<LitellmUserRoles.PROXY_ADMIN: 'proxy_admin'>, <LitellmUserRoles.PROXY_ADMIN_VIEW_ONLY: 'proxy_admin_viewer'>, <LitellmUserRoles.INTERNAL_USER: 'internal_user'> or <LitellmUserRoles.INTERNAL_USER_VIEW_ONLY: 'internal_user_viewer'>\"\n      }\n    }\n  ]\n}\n```\n\n![Image](https://github.com/user-attachments/assets/c8a98f1b-3c50-42ab-8036-342603014a07)\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.6-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "limxf",
      "author_type": "User",
      "created_at": "2025-06-20T09:47:15Z",
      "updated_at": "2025-06-20T09:58:51Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11918/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11918",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11918",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:14.703312",
      "comments": []
    },
    {
      "issue_number": 7254,
      "title": "[Bug]: LiteLLM Azure TTS Streaming Issue",
      "body": "### What happened?\n\n**Describe the bug**\r\n\r\nLiteLLM's TTS proxy functions correctly for standard text-to-speech conversion but fails to operate in streaming mode. While audio is generated successfully for normal TTS requests, attempting to use the streaming functionality with Azure OpenAI results in unexpected behavior.\r\n\r\n**Expected behavior**\r\n\r\nTTS should function seamlessly in both normal and streaming modes, mirroring the behavior observed when interacting directly with Azure OpenAI APIs. \r\n\r\n**Current behavior**\r\n\r\nCurrently, streaming TTS requests through LiteLLM do not function as expected, while direct requests to Azure or OpenAI provide the desired streaming capability.\r\n\r\n**Impact**\r\n\r\nThis issue prevents us from effectively utilizing the streaming features of Azure OpenAI TTS through LiteLLM, hindering real-time audio generation and processing.\r\n\r\n**Request**\r\n\r\nWe request that this issue be investigated and addressed to ensure LiteLLM's TTS proxy fully supports streaming functionality, aligning its performance with direct API interactions. \r\n\n\n### Relevant log output\n\n_No response_\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nlast\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "yigitkonur",
      "author_type": "User",
      "created_at": "2024-12-16T15:26:35Z",
      "updated_at": "2025-06-20T09:43:39Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/7254/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/7254",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/7254",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:14.703332",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "@yigitkonur can you share the request you're making client side ?",
          "created_at": "2024-12-26T01:18:23Z"
        },
        {
          "author": "gad2103",
          "body": "Did this get resolved or just closed because of staleness?",
          "created_at": "2025-04-17T17:03:42Z"
        },
        {
          "author": "krrishdholakia",
          "body": "the PR referenced was trying to close another issue, reopening now ",
          "created_at": "2025-04-17T17:20:31Z"
        },
        {
          "author": "prd-tuong-nguyen",
          "body": "Any update?",
          "created_at": "2025-06-20T09:43:39Z"
        }
      ]
    },
    {
      "issue_number": 11917,
      "title": "[Bug]: o3-pro does not work with Azure endpoint",
      "body": "### What happened?\n\nIt seems that the completions > Response bridge that has been implemented for OpenAI's o3-pro model is not applied to Azure models. \n\nWe are getting the following log when trying a completion call on the litellm deployment (litellm v.1.72.7-nightly): \n```\n400: litellm.BadRequestError: AzureException - The chatCompletion operation does not work with the specified model, o3-pro. Please choose different model and try again. You can learn more about which models can be used with each operation here: https://go.microsoft.com/fwlink/?linkid=2197993.. Received Model Group=o3-pro\nAvailable Model Group Fallbacks=None\n```\n\n### Relevant log output\n\n```shell\n400: litellm.BadRequestError: AzureException - The chatCompletion operation does not work with the specified model, o3-pro. Please choose different model and try again. You can learn more about which models can be used with each operation here: https://go.microsoft.com/fwlink/?linkid=2197993.. Received Model Group=o3-pro\nAvailable Model Group Fallbacks=None\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.72.7-nightly\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "valentin-pringault",
      "author_type": "User",
      "created_at": "2025-06-20T09:07:26Z",
      "updated_at": "2025-06-20T09:07:26Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11917/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11917",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11917",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:14.906112",
      "comments": []
    },
    {
      "issue_number": 10994,
      "title": "[Feature]: Does litellm support redis sentinel for router_settings?",
      "body": "### The Feature\n\nI notice that litellm support reids sentinel for caching.\n[redis-sentinel](https://docs.litellm.ai/docs/proxy/caching#redis-sentinel)\nDoes it also support redis sentinel for router_settings?\n[config_settings](https://docs.litellm.ai/docs/proxy/config_settings)\nBesides, if I want to use caching and router_settings at the same time, should I config redis twice?\n\n\n### Motivation, pitch\n\nabove\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "KyleZhang0536",
      "author_type": "User",
      "created_at": "2025-05-21T01:12:39Z",
      "updated_at": "2025-06-20T07:49:37Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10994/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10994",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10994",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:14.906133",
      "comments": [
        {
          "author": "zhoufengen",
          "body": "Yes, I am also confused about this",
          "created_at": "2025-06-20T07:49:37Z"
        }
      ]
    },
    {
      "issue_number": 11915,
      "title": "[Bug]: LiteLLM in streaming mode does not return usage details",
      "body": "### What happened?\n\nWhen using streaming mode, the completion API does not return the token usage in the chunk responses.\n\nFollowing the [doc](https://docs.litellm.ai/docs/completion/usage):\n```py\nimport os\n\nfrom litellm import completion\n\nos.environ[\"OPENAI_API_KEY\"] = \"API_KEY\"\n\nresponse = completion(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"},\n    ],\n    stream=True,\n    stream_options={\"include_usage\": True},\n)\n\nfor chunk in response:\n    print(chunk.choices[0].delta)\n```\n\nBelow the logs\n\n### Relevant log output\n\n```shell\nDelta(provider_specific_fields=None, refusal=None, content='Hello', role='assistant', function_call=None, tool_calls=None, audio=None)\nDelta(provider_specific_fields=None, refusal=None, content='!', role=None, function_call=None, tool_calls=None, audio=None)\nDelta(provider_specific_fields=None, refusal=None, content=' How', role=None, function_call=None, tool_calls=None, audio=None)\nDelta(provider_specific_fields=None, refusal=None, content=' can', role=None, function_call=None, tool_calls=None, audio=None)\nDelta(provider_specific_fields=None, refusal=None, content=' I', role=None, function_call=None, tool_calls=None, audio=None)\nDelta(provider_specific_fields=None, refusal=None, content=' assist', role=None, function_call=None, tool_calls=None, audio=None)\nDelta(provider_specific_fields=None, refusal=None, content=' you', role=None, function_call=None, tool_calls=None, audio=None)\nDelta(provider_specific_fields=None, refusal=None, content=' today', role=None, function_call=None, tool_calls=None, audio=None)\nDelta(provider_specific_fields=None, refusal=None, content='?', role=None, function_call=None, tool_calls=None, audio=None)\nDelta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None)\nDelta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None)\nDelta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None)\n....venv/lib/python3.11/site-packages/pydantic/main.py:426: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue: Expected 9 fields but got 5 for type `Message` with value `Message(content='Hello! How can I assist you today...unction_call=None, provider_specific_fields=None)` - serialized value may not be as expected.\n  PydanticSerializationUnexpectedValue: Expected `StreamingChoices` but got `Choices` with value `Choices(finish_reason='st...r_specific_fields=None))` - serialized value may not be as expected\n  return self.__pydantic_serializer__.to_python(\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv.1.72.6\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "ootkin",
      "author_type": "User",
      "created_at": "2025-06-20T07:05:55Z",
      "updated_at": "2025-06-20T07:05:55Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11915/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11915",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11915",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:15.076679",
      "comments": []
    },
    {
      "issue_number": 11913,
      "title": "[Bug]: LiteLLLM Router doesn't  compute the cost for self-hosted model",
      "body": "### What happened?\n\nI am running self-hosted model and using litellm Router to integrate servers for calling models in the prodution.\n\n```\nimport os\n\nimport litellm\n\nlitellm.success_callback = [\"langfuse\"]\nlitellm.failure_callback = [\"langfuse\"]\n\nmodel_list = [\n    {\n        \"model_name\": \"abel-math-2506\",\n        \"litellm_params\": {\n            \"model\": \"vllm_hosted/abel-math-2506\",\n            \"api_key\": os.getenv(\"ABEL_MATH_API_KEY\", \"EMPTY\"),\n            \"api_base\": os.getenv(\"ABEL_MATH_API_BASE\", \"http://localhost:8000\")\n        },\n    },\n    {\n        \"model_name\": \"gpt-4o-mini\",\n        \"litellm_params\": {\n            \"model\": \"gpt-4o-mini\",\n            \"api_key\": os.environ[\"OPENAI_API_KEY\"],\n            \"api_base\": os.environ[\"OPENAI_API_BASE\"]\n        },\n    },\n]\n\nllm = litellm.Router(model_list=model_list)\n```\n\nI am trying to logs the generation to langfuse, but Langfuse isn't compute the input cost/output cost.\n\n![Image](https://github.com/user-attachments/assets/1cd97771-9315-4c15-920b-95f39cd45685)\n\nInput/output tokens is logged and the model object is linked with the configed cost model, the cost wasn't computed. But when i call the model using litellm (not Router), the cost is computed.\n\nHow can i fix it. Thanks!\n\nEnvironment:\n- LiteLLM Python SDK: v1.72.7\n- Langfuse SDK: v2.60.8\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.7\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "duynvher",
      "author_type": "User",
      "created_at": "2025-06-20T05:40:32Z",
      "updated_at": "2025-06-20T05:42:21Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11913/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11913",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11913",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:16.863664",
      "comments": []
    },
    {
      "issue_number": 11885,
      "title": "azure/mistral-large-2411 unable to send a completion with a response format",
      "body": "I am systematically getting the error: `litellm.exceptions.BadRequestError: litellm.BadRequestError: AzureException BadRequestError - invalid input error`\nwhen adding a response_format to the completion call\nhere is a simplifies version of the code\n\n```\nimport litellm\n\nclass Response(BaseModel):\n    greeting: str\n\nresponse = await litellm.completion(\n            model=\"azure/Mistral-Large-2411\",\n            api_base=API_BASE,\n            api_key=API_KEY,\n            api_version=None,\n            messages = [{\"role\": \"user\", \"content\": \"Hello world!\"}],\n            response_format=Response,\n        )\n```\n\nDo you have any idea why ?\nthanks !",
      "state": "open",
      "author": "sandragjacinto",
      "author_type": "User",
      "created_at": "2025-06-19T13:41:38Z",
      "updated_at": "2025-06-20T05:41:36Z",
      "closed_at": null,
      "labels": [
        "awaiting: user response"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11885/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11885",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11885",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:16.863688",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "hey @sandragjacinto can you run the call with `litellm._turn_on_debug()` \n\nIt'll confirm how litellm is sending the request. It could be that the model just doesn't support response_format. ",
          "created_at": "2025-06-19T19:07:54Z"
        }
      ]
    },
    {
      "issue_number": 11911,
      "title": "[Bug]:",
      "body": "### What happened?\n\ngetting an error that the resouce is not found after i initialised the resources\n\n### Relevant log output\n\n```shell\nNotFoundError: litellm.NotFoundError: NotFoundError: OpenAIException - Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\n1.65.8\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "blacckbeard4",
      "author_type": "User",
      "created_at": "2025-06-20T04:23:25Z",
      "updated_at": "2025-06-20T05:39:15Z",
      "closed_at": "2025-06-20T05:39:15Z",
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11911/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11911",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11911",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:17.074520",
      "comments": []
    },
    {
      "issue_number": 11839,
      "title": "[Bug]: Unable to handle PDF file URLs",
      "body": "### What happened?\n\nI am trying this sample and get the error log for mime type\n\n```python\n\nimport litellm\nimport asyncio\nimport os\nimport traceback\nfrom litellm import acompletion\nfrom litellm.utils import supports_pdf_input\n\n# Initialize the client\nlitellm.use_litellm_proxy = True\n\n# Define the model and PDF URL\nmodel = \"bedrock/us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\npdf_url = \"https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf\"\n\n\n# Construct the message content with text and PDF file\nfile_content = [\n    {\"type\": \"text\", \"text\": \"What is this document about?\"},\n    {\"type\": \"file\", \"file\": {\"file_id\": pdf_url, \"format\": \"application/pdf\"}}\n]\n\nasync def completion_call():\n    try:\n        print(\"Starting acompletion with PDF input and streaming\")\n        response = await acompletion(\n            model=model,\n            messages=[{\"role\": \"user\", \"content\": file_content}],\n            stream=False\n        )\n        print(response)\n    except Exception:\n        print(f\"An error occurred: {traceback.format_exc()}\")\n\nasyncio.run(completion_call())\n```\n\n### Relevant log output\n\n```shell\n'message': 'litellm.APIConnectionError: No supported extensions for MIME type: application/pdf; qs=0.001. Supported formats: [\\'pdf\\', \\'csv\\', \\'doc\\', \\'docx\\', \\'xls\\', \\'xlsx\\', \\'html\\', \\'txt\\', \\'md\\']\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n 1.65.4 \n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "dhruv-wrk",
      "author_type": "User",
      "created_at": "2025-06-18T07:58:30Z",
      "updated_at": "2025-06-20T05:34:19Z",
      "closed_at": "2025-06-20T05:34:19Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11839/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "krrishdholakia"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11839",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11839",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:17.074543",
      "comments": []
    },
    {
      "issue_number": 11843,
      "title": "[Bug]: /ui is not accessible when server root path set (recurring issue)",
      "body": "### What happened?\n\nLitellm is installed with pip and `SERVER_ROOT_PATH` environment variable is set. Landing page is accessible, but admin page (`<ip>/<SERVER_ROOT_PATH>/ui`) cannot be accessed.\n\n`SERVER_ROOT_PATH` is set like:\n```\ncat /etc/systemd/system/litellm.service\n[Unit]\nDescription=LiteLLM Service\nAfter=network.target\n\n[Service]\nEnvironment=SERVER_ROOT_PATH=/api2\nExecStart=/usr/local/bin/litellm --port 80 --num_workers 64 --config /etc/litellm/config.yaml\nRestart=always\nUser=root\nGroup=root\n\nSyslogIdentifier=litellm\nStandardOutput=syslog\nStandardError=syslog\n\n[Install]\nWantedBy=multi-user.target\n```\n\nI use v1.61.20 before and this setting works well. Let me know if you need me to add more details.\n\n### Relevant log output\n\n```shell\nJun 18 16:42:54 [litellm] INFO:     ... - \"GET /api2/ui/ HTTP/1.1\" 200 OK\nJun 18 16:42:54 [litellm] INFO:     ... - \"GET /_next/static/chunks/webpack-a426aae3231a8df1.js HTTP/1.1\" 404 Not Found\nJun 18 16:42:54 [litellm] INFO:     ... - \"GET /_next/static/chunks/fd9d1056-205af899b895cbac.js HTTP/1.1\" 404 Not Found\nJun 18 16:42:54 [litellm] INFO:     ... - \"GET /_next/static/chunks/117-c4922b1dd81b62ce.js HTTP/1.1\" 404 Not Found\nJun 18 16:42:54 [litellm] INFO:     ... - \"GET /_next/static/chunks/main-app-475d6efe4080647d.js HTTP/1.1\" 404 Not Found\nJun 18 16:42:54 [litellm] INFO:     ... - \"GET /api2/ui/favicon.ico HTTP/1.1\" 200 OK\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.72.6.post1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "Uuq114",
      "author_type": "User",
      "created_at": "2025-06-18T08:53:59Z",
      "updated_at": "2025-06-20T05:30:47Z",
      "closed_at": "2025-06-20T05:30:47Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11843/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "krrishdholakia"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11843",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11843",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:17.074549",
      "comments": [
        {
          "author": "Uuq114",
          "body": "I see similar issues like #11592 and #11531, and upgrade to a newer version doesn't help though...",
          "created_at": "2025-06-18T08:58:08Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Fixed with - https://github.com/BerriAI/litellm/commit/0e6f1c089d1280b7874a158306b2db2731bb4bb1#diff-cb47ff448a23857ca19cd50f8baa9aea47ee5cdefb475803fdc022449dc10d7e\n\n@Uuq114 if you can test the commit, it would help! ",
          "created_at": "2025-06-20T05:30:40Z"
        }
      ]
    },
    {
      "issue_number": 11865,
      "title": "[Bug]: serverRootPath Configuration Not Working for LiteLLM UI Assets in restricted environment",
      "body": "### What happened?\n\n## Summary\n\nI'm experiencing issues with the `serverRootPath` configuration in LiteLLM. The LiteLLM application starts successfully and the API endpoints work correctly, but the UI assets (JavaScript files) are returning 404 errors because they're not being served from the correct path.\n\nThe tests are done with port forwarding\n\n## Environment Details\n\n- **Kubernetes Environment**: Google Kubernetes Engine (GKE)\n- **LiteLLM Image**: `ghcr.io/berriai/litellm-non_root:main-latest`\n- **Deployment Method**: Helm chart deployment\n- **Base Path**: `/litellm` (configured via `serverRootPath`)\n\n## Current Configuration\n\n### Environment Variables\n```yaml\ndata:\n  SERVER_ROOT_PATH: \"/litellm\"\n  PORT: \"4000\"\n  # ... other configuration variables\n```\n\n### Deployment Configuration\n```yaml\n# Container Configuration\nimage: ghcr.io/berriai/litellm-non_root:main-latest\ncommand: [\"/bin/sh\"]\nargs: [\"-c\", \"exec litellm --host 0.0.0.0 --port 4000\"]\n\n# Health Checks\nlivenessProbe:\n  httpGet:\n    path: /health/liveliness\n    port: http\nreadinessProbe:\n  httpGet:\n    path: /health/readiness\n    port: http\n```\n## Issue Details\n\n### Working Endpoints\nThe following endpoints work correctly with the `serverRootPath` configuration:\n- `GET /litellm/` - Main UI page loads successfully\n- `GET /litellm/openapi.json` - OpenAPI specification\n- `GET /health/liveliness` - Health check endpoint\n- `GET /health/readiness` - Readiness check endpoint\n\n### Failing Asset Requests\nThe following UI asset requests are returning 404 errors:\n```\nGET /litellm-asset-prefix/_next/static/chunks/webpack-a426aae3231a8df1.js HTTP/1.1\" 404 Not Found\nGET /litellm-asset-prefix/_next/static/chunks/fd9d1056-205af899b895cbac.js HTTP/1.1\" 404 Not Found\nGET /litellm-asset-prefix/_next/static/chunks/117-c4922b1dd81b62ce.js HTTP/1.1\" 404 Not Found\nGET /litellm-asset-prefix/_next/static/chunks/main-app-475d6efe4080647d.js HTTP/1.1\" 404 Not Found\n```\n\n### Expected vs Actual Behavior\n\n**Expected**: UI assets should be served from `/litellm/_next/static/...`\n**Actual**: UI assets are being requested from `/litellm-asset-prefix/_next/static/...`\n\n## Application Logs\n\n```\nINFO: Application startup complete.\nINFO: Uvicorn running on http://0.0.0.0:4000 (Press CTRL+C to quit)\nINFO: 127.0.0.1:38074 - \"GET /litellm/ui/ HTTP/1.1\" 200 OK\nINFO: 127.0.0.1:38074 - \"GET /litellm-asset-prefix/_next/static/chunks/webpack-a426aae3231a8df1.js HTTP/1.1\" 404 Not Found\nINFO: 127.0.0.1:38074 - \"GET /litellm-asset-prefix/_next/static/chunks/fd9d1056-205af899b895cbac.js HTTP/1.1\" 404 Not Found\nINFO: 127.0.0.1:38074 - \"GET /litellm-asset-prefix/_next/static/chunks/117-c4922b1dd81b62ce.js HTTP/1.1\" 404 Not Found\nINFO: 127.0.0.1:38074 - \"GET /litellm-asset-prefix/_next/static/chunks/main-app-475d6efe4080647d.js HTTP/1.1\" 404 Not Found\n```\n\n## Root Cause Analysis\n\nThe issue appears to be that:\n\n1. **API Endpoints**: The `serverRootPath` configuration correctly affects API endpoints (`/litellm/openapi.json`, `/litellm/ui/`)\n2. **Static Assets**: The static assets (JavaScript files) are being requested with a different path prefix (`/litellm-asset-prefix/`) instead of using the configured `serverRootPath`\n3. **Asset Path Mismatch**: The frontend is generating asset URLs that don't match the configured base path\n\n## Testing and Validation\n\n### Working Requests\n```bash\n# These work correctly\ncurl http://localhost:4000/litellm/\ncurl http://localhost:4000/litellm/openapi.json\ncurl http://localhost:4000/health/liveliness\n```\n\n### Failing Requests\n```bash\n# These return 404\ncurl http://localhost:4000/litellm-asset-prefix/_next/static/chunks/webpack-a426aae3231a8df1.js\ncurl http://localhost:4000/litellm-asset-prefix/_next/static/chunks/fd9d1056-205af899b895cbac.js\n```\n\n## Potential Solutions\n\n1. **Frontend Asset Path Configuration**: The frontend needs to be configured to use the correct asset path prefix\n2. **Static File Serving**: Ensure static files are served from the correct path\n3. **Asset Prefix Environment Variable**: There might be a separate environment variable for asset prefix configuration\n4. **Build-time Configuration**: The asset paths might need to be configured at build time\n\n## Request for Assistance\n\nI would appreciate guidance on:\n\n1. **Correct Configuration**: What is the proper way to configure `serverRootPath` for both API endpoints and static assets?\n2. **Asset Prefix**: Is there a separate configuration for static asset paths?\n3. **Environment Variables**: Are there additional environment variables needed for proper asset serving?\n4. **Documentation**: Is there documentation for deploying LiteLLM with custom base path?\n\n## Additional Context\n\n- The application is deployed in a Kubernetes environment with ingress routing\n- The API functionality works correctly with the current configuration\n- The issue is specifically with the UI static assets\n- The same configuration pattern works for other applications in the same environment\n\n## Environment Information\n\n- **Kubernetes Version**: GKE 1.28+\n- **Base Path**: `/litellm`\n- **Port**: 4000 (internal)\n- **External Access**: Through ingress at `https://domain.com/litellm/`\n\n## Expected Behavior\n\nWhen accessing `https://domain.com/litellm/`, the UI should load completely with all assets (JavaScript, CSS) being served from the correct paths under `/litellm/`.\n\n---\n\n**Note**: This issue is separate from the previous Prisma connection issue and occurs after the application has successfully started and is running. \n![Image](https://github.com/user-attachments/assets/d0d22ca6-5200-4366-ac6d-e2426712af8b)\n\n![Image](https://github.com/user-attachments/assets/c4445ecb-344f-40b9-a39b-8e9f993127be)\n\n![Image](https://github.com/user-attachments/assets/ea0cab0f-0c94-40da-b547-e7145e397d9c)\n\nFull -f logs: \n```bash\n2025-06-18 18:12:26,460 - litellm_proxy_extras - INFO - Running prisma migrate deploy\n2025-06-18 18:12:39,692 - litellm_proxy_extras - INFO - prisma migrate deploy stdout: Installing Prisma CLI\nPrisma schema loaded from schema.prisma\nDatasource \"client\": PostgreSQL database \"litellm\", schema \"public\" at \"<hidden>:5432\"\n\n24 migrations found in prisma/migrations\n\n\nNo pending migrations to apply.\n\n2025-06-18 18:12:39,692 - litellm_proxy_extras - INFO - prisma migrate deploy completed\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:4000 (Press CTRL+C to quit)\n\n#------------------------------------------------------------#\n#                                                            #\n#              'I don't like how this works...'               #\n#        https://github.com/BerriAI/litellm/issues/new        #\n#                                                            #\n#------------------------------------------------------------#\n\n Thank you for using LiteLLM! - Krrish & Ishaan\n\n\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n\n\nINFO:     10.2.0.49:41234 - \"GET /health/liveliness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:41236 - \"GET /health/readiness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:55852 - \"GET /health/liveliness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:55854 - \"GET /health/readiness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:46420 - \"GET /health/liveliness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:46430 - \"GET /health/readiness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:54730 - \"GET /health/liveliness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:54722 - \"GET /health/readiness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:45852 - \"GET /health/liveliness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:45844 - \"GET /health/readiness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:50428 - \"GET /health/liveliness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:50430 - \"GET /health/readiness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:56868 - \"GET /health/liveliness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:56854 - \"GET /health/readiness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:41098 - \"GET /health/liveliness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:41092 - \"GET /health/readiness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:45580 - \"GET /health/liveliness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:45596 - \"GET /health/readiness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:60706 - \"GET /health/liveliness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:60712 - \"GET /health/readiness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:36524 - \"GET /health/liveliness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:36522 - \"GET /health/readiness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:32770 - \"GET /health/liveliness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:32778 - \"GET /health/readiness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:33458 - \"GET /health/liveliness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:33460 - \"GET /health/readiness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:53654 - \"GET /health/liveliness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:53666 - \"GET /health/readiness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:51804 - \"GET /health/liveliness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:51802 - \"GET /health/readiness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:52070 - \"GET /health/liveliness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:52068 - \"GET /health/readiness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:38962 - \"GET /health/liveliness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:38960 - \"GET /health/readiness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:53822 - \"GET /health/liveliness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:53830 - \"GET /health/readiness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:40408 - \"GET /health/liveliness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:40410 - \"GET /health/readiness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:48486 - \"GET /health/liveliness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:48484 - \"GET /health/readiness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:41488 - \"GET /health/liveliness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:41498 - \"GET /health/readiness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:44312 - \"GET /health/liveliness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:44302 - \"GET /health/readiness HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:56220 - \"GET / HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:56220 - \"GET /litellm/openapi.json HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:38074 - \"GET /litellm/ui/ HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:38074 - \"GET /litellm-asset-prefix/_next/static/chunks/webpack-a426aae3231a8df1.js HTTP/1.1\" 404 Not Found\nINFO:     127.0.0.1:38084 - \"GET /litellm-asset-prefix/_next/static/chunks/fd9d1056-205af899b895cbac.js HTTP/1.1\" 404 Not Found\nINFO:     127.0.0.1:38108 - \"GET /litellm-asset-prefix/_next/static/chunks/117-c4922b1dd81b62ce.js HTTP/1.1\" 404 Not Found\nINFO:     127.0.0.1:38098 - \"GET /litellm-asset-prefix/_next/static/chunks/main-app-475d6efe4080647d.js HTTP/1.1\" 404 Not Found\nINFO:     127.0.0.1:38098 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\nINFO:     10.2.0.49:52536 - \"GET /health/liveliness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:52546 - \"GET /health/readiness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:57880 - \"GET /health/liveliness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:57872 - \"GET /health/readiness HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:45330 - \"GET /.well-known/appspecific/com.chrome.devtools.json HTTP/1.1\" 404 Not Found\nINFO:     127.0.0.1:45330 - \"GET /litellm/ui/ HTTP/1.1\" 304 Not Modified\nINFO:     127.0.0.1:45330 - \"GET /litellm-asset-prefix/_next/static/chunks/webpack-a426aae3231a8df1.js HTTP/1.1\" 404 Not Found\nINFO:     127.0.0.1:57422 - \"GET /litellm-asset-prefix/_next/static/chunks/fd9d1056-205af899b895cbac.js HTTP/1.1\" 404 Not Found\nINFO:     127.0.0.1:57464 - \"GET /.well-known/appspecific/com.chrome.devtools.json HTTP/1.1\" 404 Not Found\nINFO:     127.0.0.1:57454 - \"GET /litellm-asset-prefix/_next/static/chunks/117-c4922b1dd81b62ce.js HTTP/1.1\" 404 Not Found\nINFO:     127.0.0.1:57438 - \"GET /litellm-asset-prefix/_next/static/chunks/main-app-475d6efe4080647d.js HTTP/1.1\" 404 Not Found\nINFO:     10.2.0.49:55618 - \"GET /health/liveliness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:55630 - \"GET /health/readiness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:41176 - \"GET /health/liveliness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:41172 - \"GET /health/readiness HTTP/1.1\" 200 OK\n```\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nmain-latest\n\n### Twitter / LinkedIn details\n\n@mdiloreto",
      "state": "closed",
      "author": "mateo-di",
      "author_type": "User",
      "created_at": "2025-06-18T18:29:38Z",
      "updated_at": "2025-06-20T05:29:02Z",
      "closed_at": "2025-06-19T02:20:10Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11865/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11865",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11865",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:17.268960",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "This is a duplicate of an existing issue already filed.",
          "created_at": "2025-06-19T02:20:35Z"
        },
        {
          "author": "krrishdholakia",
          "body": "The api is being served. The swagger docs don't load. ",
          "created_at": "2025-06-19T02:21:02Z"
        }
      ]
    },
    {
      "issue_number": 11881,
      "title": "[Bug]: `litellm.utils.supports_pdf_input` returns `False` for gemini 2.5 models",
      "body": "### What happened?\n\n`litellm.utils.supports_pdf_input(\"gemini/gemini-2.5-flash\")` returns `False`, but Gemini 2.5 Flash actually supports PDF input (at least locally). \n\n\n### Code to reproduce\n\n```\n#!/usr/bin/env python3\nimport base64\nfrom pathlib import Path\n\nfrom litellm import completion\nfrom litellm.utils import supports_pdf_input\n\nprint(f\"litellm.utils.supports_pdf_input('gemini/gemini-2.5-flash') = {supports_pdf_input('gemini/gemini-2.5-flash')}\")\n\n# Read and encode PDF\npdf_path = \"/workspace/tests/multi_modal_test_assets/cover_book.pdf\"\npdf_b64 = base64.b64encode(Path(pdf_path).read_bytes()).decode()\n\n# API call\nresponse = completion(\n    model=\"gemini/gemini-2.5-flash\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"What is this book about? Give a very brief summary.\"},\n                {\"type\": \"file\", \"file\": {\"file_data\": f\"data:application/pdf;base64,{pdf_b64}\"}},\n            ],\n        }\n    ],\n    temperature=0.0,\n    max_tokens=150,\n)\n\nprint(response.choices[0].message.content)\n```\n\nOutput:\n\n```\nlitellm.utils.supports_pdf_input('gemini/gemini-2.5-flash') = False\nThis book is **\"The Little Prince\" by Antoine de\n```\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.72.4\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "bmanczak",
      "author_type": "User",
      "created_at": "2025-06-19T12:02:04Z",
      "updated_at": "2025-06-20T04:07:26Z",
      "closed_at": "2025-06-20T04:07:26Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11881/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11881",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11881",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:17.489047",
      "comments": []
    },
    {
      "issue_number": 11861,
      "title": "[Bug]: Add vertex ai claude-sonnet-4 to model cost map",
      "body": "### What happened?\n\nA bug happened!\n\nwhen the vertex_ai env vars are set:\n```\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = creds_path\nos.environ[\"VERTEXAI_LOCATION\"] = \"us-east1\"\nos.environ[\"VERTEXAI_PROJECT\"] = \"project-name\"\n```\n\nAnd you talk to a model from vertex_ai such as:\n```\nresponse = litellm.completion(\n    model=\"vertex_ai/claude-sonnet-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n)\n```\nThe request works, however, it causes the following logs to be created before the response:\n```\nProvider List: https://docs.litellm.ai/docs/providers\n\n\nProvider List: https://docs.litellm.ai/docs/providers\n\n\nProvider List: https://docs.litellm.ai/docs/providers\n\n\nProvider List: https://docs.litellm.ai/docs/providers\n\n\nProvider List: https://docs.litellm.ai/docs/providers\n\n\nProvider List: https://docs.litellm.ai/docs/providers\n\n\nProvider List: https://docs.litellm.ai/docs/providers\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.4\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "danton267",
      "author_type": "User",
      "created_at": "2025-06-18T15:13:18Z",
      "updated_at": "2025-06-20T04:07:26Z",
      "closed_at": "2025-06-20T04:07:26Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11861/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11861",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11861",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:17.489067",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "It needs to get added to our model cost map ",
          "created_at": "2025-06-19T04:06:02Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Claude sonnet 4 is on our model cost map - https://github.com/BerriAI/litellm/blob/4796e9c5ba2a6e00748728b0f4733eba0b60956f/model_prices_and_context_window.json#L8162",
          "created_at": "2025-06-19T23:48:39Z"
        }
      ]
    },
    {
      "issue_number": 11886,
      "title": "Bedrock Agents - Passthrough Endpoints - Logs are not present in LiteLLM UI when invoked via LiteLLM Proxy",
      "body": "Bedrock Agents called using boto3 gives a success response, but logs are not available in LiteLLM UI\n\n## https://docs.litellm.ai/docs/pass_through/bedrock#advanced---bedrock-agents\n\n**Code**\nimport os \nimport boto3 \nfrom uuid import uuid4\nfrom botocore.config import Config\nagent_id = 'PYZPCY0CFY'# 'PYZPCY0CFY'\nagent_alias_id = 'E0SXOJWK05' #0ADZTMORCM'\nproxy_endpoint = \"http://<endpoint>:4000/bedrock/\" # üëà your proxy base url\n\ncustom_headers = {   \n    'Authorization': 'Bearer sk-1234'\n}\n\nruntime_client = boto3.client(\n    service_name=\"bedrock-agent-runtime\", \n    region_name=\"eu-central-1\", \n    endpoint_url=proxy_endpoint\n)\n\ndef inject_custom_headers(request, **kwargs):\n    request.headers.update(custom_headers)\n\nruntime_client.meta.events.register('before-send.*.*', inject_custom_headers)\n\nresponse = runtime_client.invoke_agent(\n            agentId= agent_id,\n            agentAliasId=agent_alias_id,\n            sessionId=str(uuid4()),\n            inputText=\"write a story about nature?\"\n        )\n\ncompletion = \"\"\n\nfor event in response.get(\"completion\"):\n    chunk = event[\"chunk\"]\n    completion += chunk[\"bytes\"].decode()\n\nprint(completion)\n\n**Response is successful** But Logs are not available in LiteLLM\n\nBut when calling same endpoint via postman http://<endpoint>:4000/bedrock/model/anthropic.claude-3-5-sonnet-20240620-v1:0/converse - Able to get the response and Logs are available in UI\n\n",
      "state": "open",
      "author": "marumugam-lg",
      "author_type": "User",
      "created_at": "2025-06-19T13:53:11Z",
      "updated_at": "2025-06-20T01:36:11Z",
      "closed_at": null,
      "labels": [
        "unable to repro",
        "awaiting: user response"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11886/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11886",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11886",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:17.726675",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Unable to repro this - i can see bedrock agent calls being logged as expected - \n\n<img width=\"839\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/7b543534-3e12-4799-9cc2-20c9b1f4efc7\" />",
          "created_at": "2025-06-20T01:34:17Z"
        },
        {
          "author": "krrishdholakia",
          "body": "My test curl: \n```bash\ncurl -L -X POST 'http://0.0.0.0:4000/bedrock/agents/L1RT58GYRW/agentAliases/MFPSBCXYTW/sessions/12345/text' \\\n-H 'Authorization: Bearer sk-1234' \\\n-H 'Content-Type: application/json' \\\n-d '{\n    \"inputText\": \"give me the weather for seattle\",\n    \"enableTrace\": true\n}'\n```\n\nCa",
          "created_at": "2025-06-20T01:35:57Z"
        }
      ]
    },
    {
      "issue_number": 6141,
      "title": "[Bug]: vertex ai service account json -unable to resolve 'environment_id' field",
      "body": "### What happened?\n\nWhen using litellm SDK version 1.48.7 like this:\r\n\r\n```\r\nfrom litellm import completion\r\nimport json\r\n\r\n## GET CREDENTIALS\r\nfile_path = 'PATH_TO_JSON'\r\n\r\n# Load the JSON file\r\nwith open(file_path, 'r') as file:\r\n    vertex_credentials = json.load(file)\r\n\r\n# Convert to JSON string\r\nvertex_credentials_json = json.dumps(vertex_credentials)\r\n\r\n\r\nresponse = completion(\r\n  model=\"vertex_ai/gemini-pro\",\r\n  messages=[{\"content\": \"You are a good bot.\",\"role\": \"system\"}, {\"content\": \"tell me poem on pasta\",\"role\": \"user\"}],\r\n  vertex_credentials=vertex_credentials_json,\r\n  vertex_project=\"my_project_id\",\r\n  vertex_location=\"us-central1\"\r\n)\r\n```\r\n\r\nWe are seeing error with stack trace:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/appuser/.local/lib/python3.11/site-packages/litellm/main.py\", line 2280, in completion\r\n    model_response = vertex_chat_completion.completion(  # type: ignore\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/appuser/.local/lib/python3.11/site-packages/litellm/llms/vertex_ai_and_google_ai_studio/gemini/vertex_and_google_ai_studio_gemini.py\", line 1208, in completion\r\n    _auth_header, vertex_project = self._ensure_access_token(\r\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/appuser/.local/lib/python3.11/site-packages/litellm/llms/vertex_ai_and_google_ai_studio/vertex_llm_base.py\", line 137, in _ensure_access_token\r\n    self._credentials, cred_project_id = self.load_auth(\r\n                                         ^^^^^^^^^^^^^^^\r\n  File \"/home/appuser/.local/lib/python3.11/site-packages/litellm/llms/vertex_ai_and_google_ai_studio/vertex_llm_base.py\", line 79, in load_auth\r\n    creds = identity_pool.Credentials.from_info(json_obj)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/appuser/.local/lib/python3.11/site-packages/google/auth/identity_pool.py\", line 425, in from_info\r\n    return super(Credentials, cls).from_info(info, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/appuser/.local/lib/python3.11/site-packages/google/auth/external_account.py\", line 591, in from_info\r\n    return cls(\r\n           ^^^^\r\n  File \"/home/appuser/.local/lib/python3.11/site-packages/google/auth/identity_pool.py\", line 273, in __init__\r\n    raise exceptions.MalformedError(\r\ngoogle.auth.exceptions.MalformedError: Invalid Identity Pool credential_source field 'environment_id'\r\n```\r\n\r\nWhereas when I use vertex ai SDK itself like this with my same service account credentials file like this:\r\n\r\n```\r\nimport os\r\n\r\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"PATH_TO_JSON\"\r\nos.environ[\"VERTEXAI_LOCATION\"]=\"us-central1\"\r\nos.environ[\"VERTEXAI_PROJECT\"]=\"my_project_id\"\"\r\n\r\nimport vertexai\r\nfrom vertexai.generative_models import GenerativeModel\r\n\r\nvertexai.init(project=\"my_project_id\"\", location=\"us-central1\")\r\n\r\nmodel = GenerativeModel(\"gemini-pro\")\r\n\r\nresponse = model.generate_content(\r\n    \"tell me poem on pasta\"\r\n)\r\n\r\nprint(response.text)\r\n```\r\n\r\nThen it works fine.\r\n\r\n\r\nMy service account file looks like this:\r\n```\r\n{\r\n  \"type\": \"external_account\",\r\n  \"audience\": \"....\",\r\n  \"subject_token_type\": \"....\",\r\n  \"service_account_impersonation_url\": \"https://iamcredentials.googleapis.com/v1/projects/-/serviceAccounts/.....iam.gserviceaccount.com:generateAccessToken\",\r\n  \"token_url\": \"https://sts.googleapis.com/v1/token\",\r\n  \"credential_source\": {\r\n    \"environment_id\": \"aws1\",\r\n    \"region_url\": \"....\",\r\n    \"url\": \"....\",\r\n    \"regional_cred_verification_url\": \"....\"\r\n  }\r\n}\r\n```\n\n### Relevant log output\n\n_No response_\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "vaghelarahul94",
      "author_type": "User",
      "created_at": "2024-10-10T00:25:20Z",
      "updated_at": "2025-06-20T01:32:00Z",
      "closed_at": "2025-06-20T01:32:00Z",
      "labels": [
        "bug",
        "unable to repro"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 14,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/6141/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/6141",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/6141",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:17.930157",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "> google.auth.exceptions.MalformedError: Invalid Identity Pool credential_source field 'environment_id'\r\n\r\n\r\nthis error is coming from the google sdk not litellm\r\n\r\n> os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"PATH_TO_JSON\"\r\n\r\n\r\nTry doing using the env var for litellm, and see if that works inste",
          "created_at": "2024-10-10T23:07:28Z"
        },
        {
          "author": "vaghelarahul94",
          "body": "Hi @krrishdholakia Thanks for your response! I appreciate it.\r\n\r\nI updated the code to use the `os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"PATH_TO_JSON\"` environment variable. Additionally, I commented out a few other sections.\r\n\r\n```\r\nresponse = completion(\r\n  model=\"vertex_ai/gemini-pro\",\r\n  me",
          "created_at": "2024-10-11T17:34:38Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-01-28T02:57:42Z"
        },
        {
          "author": "pritkudale",
          "body": "Facing the same issue\n",
          "created_at": "2025-03-07T04:20:36Z"
        },
        {
          "author": "sammcj",
          "body": "Also have this issue, GCP and the SA is correctly configured.",
          "created_at": "2025-03-14T23:20:25Z"
        }
      ]
    },
    {
      "issue_number": 6739,
      "title": "[Bug]: Custom logging callback not called when json_logs is set to true",
      "body": "### What happened?\n\nTested to create a custom logging callback according to the docs [here](), and did a very simple just to try it out, which implements the following methods:\r\n```py\r\ndef log_pre_api_call(self, model, messages, kwargs): \r\n    print(\"*********** Pre-API Call\")\r\n\r\ndef log_post_api_call(self, kwargs, response_obj, start_time, end_time): \r\n    print(\"*********** Post-API Call\")\r\n\r\ndef log_stream_event(self, kwargs, response_obj, start_time, end_time):\r\n    print(\"*********** On Stream\")\r\n    \r\ndef log_success_event(self, kwargs, response_obj, start_time, end_time): \r\n    print(\"*********** On Success\")\r\n\r\ndef log_failure_event(self, kwargs, response_obj, start_time, end_time): \r\n    print(\"*********** On Failure\")\r\n```\r\nAnd if I set my config to:\r\n```yaml\r\nlitellm_settings:\r\n  allowed_fails: 3\r\n  callbacks: custom_callbacks.proxy_handler_instance\r\n  num_retries: 3\r\n  request_timeout: 600\r\n```\r\nit all works as expected and I can see in my logs:\r\n```\r\n08:27:21 - LiteLLM Router:INFO: simple_shuffle.py:72 - get_available_deployment for model: llama3.1-8b, Selected deployment: {'model_name': 'llama3.1-8b', 'litellm_params': {'model': 'bedrock/meta.llama3-1-8b-instruct-v1:0'}, 'model_info': {'id': 'llama3.1-8b-us-west-2', 'db_model': False}} for model: llama3.1-8b\r\n08:27:23 - LiteLLM Router:INFO: router.py:936 - litellm.acompletion(model=bedrock/meta.llama3-1-8b-instruct-v1:0) 200 OK\r\n*********** Pre-API Call\r\n*********** Post-API Call\r\n*********** On Async Success\r\nINFO:     172.20.0.1:58222 - \"POST /chat/completions HTTP/1.1\" 200 OK\r\n```\r\nBut if I set the LiteLLM config to log using JSON, to be able to more easily handle the logs and set alerts given `ERROR`s, like this:\r\n```\r\nlitellm_settings:\r\n  allowed_fails: 3\r\n  callbacks: custom_callbacks.proxy_handler_instance\r\n  json_logs: true\r\n  num_retries: 3\r\n  request_timeout: 600\r\n```\r\nmy custom logger isn't called! The output then looks like this:\r\n```json\r\n{\"message\": \"get_available_deployment for model: llama3.1-8b, Selected deployment: {'model_name': 'llama3.1-8b', 'litellm_params': {'model': 'bedrock/meta.llama3-1-8b-instruct-v1:0'}, 'model_info': {'id': 'llama3.1-8b-us-west-2', 'db_model': False}} for model: llama3.1-8b\", \"level\": \"INFO\", \"timestamp\": \"2024-11-14T08:26:46.522708\"}\r\n{\"message\": \"litellm.acompletion(model=bedrock/meta.llama3-1-8b-instruct-v1:0)\\u001b[32m 200 OK\\u001b[0m\", \"level\": \"INFO\", \"timestamp\": \"2024-11-14T08:26:49.473654\"}\r\n{\"message\": \"get_available_deployment for model: llama3.1-8b, Selected deployment: {'model_name': 'llama3.1-8b', 'litellm_params': {'model': 'bedrock/meta.llama3-1-8b-instruct-v1:0'}, 'model_info': {'id': 'llama3.1-8b-us-west-2', 'db_model': False}} for model: llama3.1-8b\", \"level\": \"INFO\", \"timestamp\": \"2024-11-14T08:26:52.800844\"}\r\n{\"message\": \"litellm.acompletion(model=bedrock/meta.llama3-1-8b-instruct-v1:0)\\u001b[32m 200 OK\\u001b[0m\", \"level\": \"INFO\", \"timestamp\": \"2024-11-14T08:26:54.350644\"}\r\n```\r\nNote that the only change is to set `json_logs: true` in the config and it results in no printouts for my custom logs.\r\n\r\n## Expected behaviour\r\n\r\nI would expect my custom logger to still be called, regardless of LiteLLM sets its internal logging to JSON format or not.\r\n\r\n**Why I want this:** We want to use structured logging in order to more easily setup alerts on errors. The LiteLLM standard logging is not enough to get all the information out that we want to get, but that is OK since it supports for custom loggers. In these, we will format a nice JSON message that we later can use for alerts. However, not all logging messages go via the custom logger, hence we are missing some of the `ERROR` logs, so we want LiteLLM to still log in JSON format so that we can catch those as well.\r\n\r\nCurrently this is not possible, since we have to decide if we want to catch LiteLLM logs that doesn't go via the custom logger ***or*** we set a custom logger. I would love to be able to get both üòÑ \r\n\n\n### Relevant log output\n\n_No response_\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "Jacobh2",
      "author_type": "User",
      "created_at": "2024-11-14T08:47:01Z",
      "updated_at": "2025-06-20T00:02:20Z",
      "closed_at": null,
      "labels": [
        "bug",
        "unable to repro",
        "stale"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/6739/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "krrishdholakia"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/6739",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/6739",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:18.226991",
      "comments": [
        {
          "author": "Jacobh2",
          "body": "@krrishdholakia, is this expected behaviour? Do I have to disable JSON logging in order to use the custom loggin callback? üòÑ ",
          "created_at": "2024-11-18T06:50:08Z"
        },
        {
          "author": "Jacobh2",
          "body": "@ishaan-jaff, Is this expected behaviour?",
          "created_at": "2024-11-21T11:53:24Z"
        },
        {
          "author": "krrishdholakia",
          "body": "no ",
          "created_at": "2024-11-21T13:02:16Z"
        },
        {
          "author": "krrishdholakia",
          "body": "unable to repro \r\n<img width=\"964\" alt=\"Screenshot 2024-11-21 at 6 46 50 PM\" src=\"https://github.com/user-attachments/assets/22c729a7-2e9b-43c4-9c9b-283c476320ee\">\r\n<img width=\"996\" alt=\"Screenshot 2024-11-21 at 6 46 48 PM\" src=\"https://github.com/user-attachments/assets/ea6554ec-4837-41a1-a8a6-c55d",
          "created_at": "2024-11-21T13:17:21Z"
        },
        {
          "author": "krrishdholakia",
          "body": "your logging callbacks should be working - i suspect there might be some issue with print / json logging, although i'm unable to repro it. \r\n\r\nWe have other users using our otel logging which is a custom logger with json logging enabled. Closing this ticket for now, as i can confirm customlogger wor",
          "created_at": "2024-11-21T13:20:57Z"
        }
      ]
    },
    {
      "issue_number": 7320,
      "title": "[Feature]: Add Retry Logic for Guardrails or Allow Skip Post Call Rules or Add Response Format Validator with Type JSON",
      "body": "### The Feature\n\n## Current configuration\r\nI have implemented a custom guardrail:\r\n```\r\nfrom typing import Any, Dict, List, Literal, Optional, Union\r\n\r\nimport litellm\r\nfrom litellm._logging import verbose_proxy_logger\r\nfrom litellm.caching.caching import DualCache\r\nfrom litellm.integrations.custom_guardrail import CustomGuardrail\r\nfrom litellm.proxy._types import UserAPIKeyAuth\r\nfrom litellm.proxy.guardrails.guardrail_helpers import should_proceed_based_on_metadata\r\nfrom litellm.types.guardrails import GuardrailEventHooks\r\nimport json\r\n\r\nclass myCustomGuardrail(CustomGuardrail):\r\n    def __init__(\r\n        self,\r\n        **kwargs,\r\n    ):\r\n        # store kwargs as optional_params\r\n        self.optional_params = kwargs\r\n\r\n        super().__init__(**kwargs)\r\n\r\n    async def async_post_call_success_hook(\r\n            self,\r\n            data: dict,\r\n            user_api_key_dict: UserAPIKeyAuth,\r\n            response,\r\n        ):\r\n            \"\"\"\r\n            Runs on response from LLM API call\r\n\r\n            It can be used to reject a response\r\n\r\n            If a response contains invalid JSON -> we will raise an exception\r\n            \"\"\"\r\n            if isinstance(response, litellm.ModelResponse):\r\n                for choice in response.choices:\r\n                    if isinstance(choice, litellm.Choices):\r\n                        if isinstance(choice.message.content, str):\r\n                            try:\r\n                                json_content = json.loads(choice.message.content)\r\n                            except json.JSONDecodeError as e:\r\n                                raise ValueError(f\"Invalid JSON in response content: {e}\")\r\n```\r\n\r\nAnd the following custom rule:\r\n```\r\nimport json\r\nfrom litellm._logging import verbose_proxy_logger\r\n\r\ndef my_custom_rule(input): # receives the model response\r\n    try:\r\n        verbose_proxy_logger.debug(\"[TEST]: input %s\", input)\r\n        json_content = json.loads(input)\r\n        return {\"decision\": True}\r\n    except json.JSONDecodeError as e:\r\n        return {\r\n            \"decision\": False,\r\n            \"message\": f\"Invalid JSON in response content: {e}\"\r\n        }\r\n```\r\n\r\nHere is the configuration:\r\n```\r\nmodel_list:\r\n  - model_name: openai-default-model\r\n    litellm_params:\r\n      model: gpt-4o\r\n      api_key: os.environ/OPENAI_API_KEY\r\n\r\nguardrails:\r\n  - guardrail_name: \"check-json-guard\"\r\n    litellm_params:\r\n      guardrail: custom_guardrail.myCustomGuardrail\r\n      mode: \"post_call\"\r\n\r\nlitellm_settings:\r\n  json_logs: true\r\n  num_retries: 3\r\n  retry_after: 1\r\n  request_timeout: 600\r\n  disable_cooldowns: true\r\n  failure_callback: [\"sentry\"]\r\n  redact_user_api_key_info: true\r\n  post_call_rules: post_call_rules.my_custom_rule\r\n```\r\n\r\n## Problem Statement\r\n\r\nI am using LiteLLM as a proxy server with Docker Compose to handle both JSON and plain text responses. However, there are limitations in the retry and validation mechanisms:\r\n\r\n1. **Guardrail Failures Stop Retries**:\r\n   - If a guardrail raises an exception (e.g., `ValueError` due to invalid JSON), the request is stopped, and an error is returned. This prevents retries, even though subsequent requests to LLM might return valid responses.\r\n\r\n2. **Inflexibility in `post_call_rules`**:\r\n   - `post_call_rules` always runs if defined, which causes issues for requests that do not require JSON validation.\r\n   - There is no way to:\r\n     - Skip `post_call_rules` for specific requests.\r\n     - OR Dynamically define `post_call_rules` in the request body.\r\n\r\n3. **Validate JSON Schema**:\r\n   - The current approach relies on JSON schema validation, which is unnecessary for my use case. I only need to verify that the response is valid JSON, not validate against a specific schema.\r\n\r\n## Proposed Solutions\r\nOne of the following solutions would address the issues:\r\n\r\n1. **Enable Retries on Guardrail Exceptions**:\r\n   - Add a mechanism to allow retries when a custom guardrail raises an exception (e.g., `ValueError`). This would enable subsequent requests to attempt to generate a valid response. It is possible to have a specific exception or a configuration parameter for the Guardrail that will allow further retrying.\r\n\r\n2. **Dynamic Handling of `post_call_rules`**:\r\n   - Introduce a parameter in the request body to:\r\n     - Skip `post_call_rules` entirely (e.g., `\"skip_post_call_rules\": true`).\r\n     - Explicitly define `post_call_rules` to trigger validation only when specified in the CURL request body.\r\n\r\n3. **Lightweight JSON Validation**:\r\n   - Add a built-in validator to check if a response is valid JSON without requiring schema validation. CURL requests could include a specific parameter (e.g., `\"response_json_format_validator\": true`) to enable this validation. If the response is invalid, retries should follow the configuration rules.\r\n\r\n## Problem Solutions and Implementation Examples\r\n\r\n### Solution 1: Enable Retries on Guardrail Exceptions\r\n```\r\nguardrails:\r\n  - guardrail_name: \"check-json-guard\"\r\n    litellm_params:\r\n      guardrail: custom_guardrail.myCustomGuardrail\r\n      mode: \"post_call\"\r\n      allow_retries_on_guardrail_failure: true\r\n\r\nlitellm_settings:\r\n  num_retries: 3\r\n  retry_after: 1\r\n```\r\n#### Example Request 1: Expects JSON Response\r\n```\r\ncurl --location 'http://0.0.0.0:4000/chat/completions' \\\r\n--header 'Content-Type: application/json' \\\r\n--header 'Authorization: Bearer sk-12345' \\\r\n--data '{\r\n  \"model\": \"openai-default-model\",\r\n  \"messages\": [\r\n    {\r\n      \"role\": \"user\",\r\n      \"content\": \"Give me the example of JSON object with a cat name and its age. Please respond in JSON. JSON:\"\r\n    }\r\n  ],\r\n  \"guardrails\": [\"check-json-guard\"],\r\n}'\r\n```\r\n\r\n#### Example Request 2: Expects Plain Text Response\r\n```\r\ncurl --location 'http://0.0.0.0:4000/chat/completions' \\\r\n--header 'Content-Type: application/json' \\\r\n--header 'Authorization: Bearer sk-12345' \\\r\n--data '{\r\n  \"model\": \"openai-default-model\",\r\n  \"messages\": [\r\n    {\r\n      \"role\": \"user\",\r\n      \"content\": \"How are you?\"\r\n    }\r\n  ]\r\n}'\r\n```\r\n\r\n### Solution 2.1: Allow skip `post_call_rules`\r\n#### Example Request 1: JSON Expected, Custom Post Call Rule\r\n```\r\ncurl --location 'http://0.0.0.0:4000/chat/completions' \\\r\n--header 'Content-Type: application/json' \\\r\n--header 'Authorization: Bearer sk-12345' \\\r\n--data '{\r\n  \"model\": \"openai-default-model\",\r\n  \"messages\": [\r\n    {\r\n      \"role\": \"user\",\r\n      \"content\": \"Give me the example of JSON object with a cat name and its age. Please respond in JSON. JSON:\"\r\n    }\r\n  ]\r\n}'\r\n```\r\n\r\n#### Example Request 2: Plain Text, Skip Post Call Rule\r\n```\r\ncurl --location 'http://0.0.0.0:4000/chat/completions' \\\r\n--header 'Content-Type: application/json' \\\r\n--header 'Authorization: Bearer sk-12345' \\\r\n--data '{\r\n  \"model\": \"openai-default-model\",\r\n  \"messages\": [\r\n    {\r\n      \"role\": \"user\",\r\n      \"content\": \"How are you?\"\r\n    }\r\n  ],\r\n  \"skip_post_call_rules\": true\r\n}'\r\n```\r\n\r\n### Solution 2.2: Dynamic Handling of `post_call_rules`\r\nWe remove `post_call_rules` from `litellm_settings` and send them as a part of request body.\r\n\r\n#### Example Request 1: JSON Expected, Custom Post Call Rule\r\n```\r\ncurl --location 'http://0.0.0.0:4000/chat/completions' \\\r\n--header 'Content-Type: application/json' \\\r\n--header 'Authorization: Bearer sk-12345' \\\r\n--data '{\r\n  \"model\": \"openai-default-model\",\r\n  \"messages\": [\r\n    {\r\n      \"role\": \"user\",\r\n      \"content\": \"Give me the example of JSON object with a cat name and its age. Please respond in JSON. JSON:\"\r\n    }\r\n  ],\r\n  \"post_call_rules\": [\"post_call_rules.my_custom_rule\"]\r\n}'\r\n```\r\n\r\n#### Example Request 2: Plain Text, No Post Call Rule\r\n```\r\ncurl --location 'http://0.0.0.0:4000/chat/completions' \\\r\n--header 'Content-Type: application/json' \\\r\n--header 'Authorization: Bearer sk-12345' \\\r\n--data '{\r\n  \"model\": \"openai-default-model\",\r\n  \"messages\": [\r\n    {\r\n      \"role\": \"user\",\r\n      \"content\": \"How are you?\"\r\n    }\r\n  ]\r\n}'\r\n```\r\n### Solution 3: Lightweight JSON Validation\r\nIf validation fails - the retry policy from configuration and fallbacks, if they are provided, should work.\r\n\r\n#### Example Request 1: JSON Response with Lightweight Validation\r\n```\r\ncurl --location 'http://0.0.0.0:4000/chat/completions' \\\r\n--header 'Content-Type: application/json' \\\r\n--header 'Authorization: Bearer sk-12345' \\\r\n--data '{\r\n  \"model\": \"openai-default-model\",\r\n  \"messages\": [\r\n    {\r\n      \"role\": \"user\",\r\n      \"content\": \"Give me the example of JSON object with a cat name and its age. Please respond in JSON. JSON:\"\r\n    }\r\n  ],\r\n  \"response_json_format_validator\": true\r\n}'\r\n```\r\n\r\n#### Example Request 2: Plain Text Response, No Validation\r\n```\r\ncurl --location 'http://0.0.0.0:4000/chat/completions' \\\r\n--header 'Content-Type: application/json' \\\r\n--header 'Authorization: Bearer sk-12345' \\\r\n--data '{\r\n  \"model\": \"openai-default-model\",\r\n  \"messages\": [\r\n    {\r\n      \"role\": \"user\",\r\n      \"content\": \"How are you?\"\r\n    }\r\n  ]\r\n}'\r\n```\r\n## Questions\r\n\r\n- Can LiteLLM implement retries after guardrail exceptions?\r\n- Is it feasible to dynamically define or skip `post_call_rules` in the request body?\r\n- Could a lightweight JSON validator be added to validate structure without schemas?\n\n### Motivation, pitch\n\n### Motivation\r\n\r\nThe motivation for this proposal stems from practical challenges encountered while using LiteLLM as a proxy server to manage both JSON and plain text responses in a real-world application. Specifically, the existing retry and validation mechanisms have limitations that hinder flexibility and reliability in dynamic scenarios. \r\n\r\nKey challenges include:\r\n\r\n1. **Inflexible Guardrail Behavior**:\r\n   - Currently, guardrail failures stop retries, which is suboptimal for use cases where transient issues (e.g., invalid JSON in responses) could be resolved with subsequent retries. This limits the effectiveness of LiteLLM in handling real-time, high-availability workflows.\r\n\r\n2. **Static `post_call_rules` Configuration**:\r\n   - The inability to dynamically enable or disable `post_call_rules` based on the request context introduces unnecessary overhead for requests that do not require JSON validation. For instance, plain text responses are subject to the same rules as JSON responses, leading to redundant or invalid processing.\r\n\r\n3. **Excessive Overhead in JSON Schema Validation**:\r\n   - JSON schema validation is overkill for use cases where only basic JSON structure validation is required. This increases complexity and processing time, detracting from LiteLLM's lightweight nature.\r\n\r\n### Pitch\r\n\r\nThe proposed solutions directly address these challenges by introducing three complementary features:\r\n\r\n1. **Retries on Guardrail Exceptions**:\r\n   - Allowing retries for requests even when a guardrail exception is raised ensures robustness in handling transient errors, improving reliability in production workflows.\r\n\r\n2. **Dynamic `post_call_rules` Handling**:\r\n   - Introducing request-level parameters to skip or define `post_call_rules` dynamically enables greater flexibility and efficiency. This ensures that requests are processed only with the rules they require, reducing unnecessary overhead.\r\n\r\n3. **Lightweight JSON Validation**:\r\n   - A basic JSON structure validator provides a more efficient alternative to full schema validation, catering to simpler use cases without compromising functionality.\r\n\r\nBy implementing these changes, LiteLLM can provide a more flexible and reliable proxy solution, better aligned with diverse use cases ranging from simple text processing to complex JSON handling.\r\n\r\nThis proposal directly addresses the needs of developers working on systems requiring dynamic response validation (e.g., conversational AI platforms, dynamic API integrations). It reduces operational friction and improves LiteLLM's utility in production environments. \r\n\r\nIf applicable, this proposal could tie into other GitHub issues related to retry behavior, validation mechanisms, or guardrail enhancements (please link them if available).\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\nhttps://www.linkedin.com/in/alexandrphilippov/",
      "state": "open",
      "author": "aleksandrphilippov",
      "author_type": "User",
      "created_at": "2024-12-20T01:09:05Z",
      "updated_at": "2025-06-20T00:02:18Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/7320/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/7320",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/7320",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:18.447598",
      "comments": [
        {
          "author": "aleksandrphilippov",
          "body": "@krrishdholakia Could you please check it? Feel free to ask any questions about it.",
          "created_at": "2024-12-20T01:11:51Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-03-21T00:01:45Z"
        },
        {
          "author": "krrishdholakia",
          "body": "not-stale \n\nwould appreciate any help with this ticket",
          "created_at": "2025-03-21T00:55:27Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-20T00:02:18Z"
        }
      ]
    },
    {
      "issue_number": 7454,
      "title": "[Bug]: supports_vision Method Incorrectly Returns False for Vision-Capable Models",
      "body": "### What happened?\n\nAlthough the `groq/llama-3.2-11b-vision-preview` model supports vision, running `print(litellm.supports_vision(model=\"groq/llama-3.2-11b-vision-preview\"))` returns `False`. This behavior is common across models from various providers.\n\n### Relevant log output\n\n_No response_\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.55.12\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "githubuser16384",
      "author_type": "User",
      "created_at": "2024-12-28T16:38:35Z",
      "updated_at": "2025-06-20T00:02:17Z",
      "closed_at": null,
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/7454/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/7454",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/7454",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:18.674755",
      "comments": [
        {
          "author": "andrelohmann",
          "body": "It seems like this problem still exists.\n\nI have created another Issue ragarding the same problem.\n\nhttps://github.com/BerriAI/litellm/issues/9297",
          "created_at": "2025-03-17T00:11:08Z"
        },
        {
          "author": "andrelohmann",
          "body": "I have some update on the topic. Please have a look at the Bug Ticket, I created. https://github.com/BerriAI/litellm/issues/9297",
          "created_at": "2025-03-21T08:44:28Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-20T00:02:16Z"
        }
      ]
    },
    {
      "issue_number": 7560,
      "title": "[Bug]: Valid config keys have changed in V2",
      "body": "### What happened?\r\n\r\nAn annoying pydantic warning triggered by: https://github.com/BerriAI/litellm/blob/f1540ceeab9a8ca1335a49b84be95e27ea7b89de/litellm/types/integrations/prometheus.py#L224\r\n\r\nWould be amazing if you could fix this.\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\r\n* 'fields' has been removed\r\n  warnings.warn(message, UserWarning)\r\n```\r\n\r\n\r\n### Are you a ML Ops Team?\r\n\r\nNo\r\n\r\n### What LiteLLM version are you on ?\r\n\r\nlatest\r\n\r\n### Twitter / LinkedIn details\r\n\r\n_No response_",
      "state": "closed",
      "author": "juliuslipp",
      "author_type": "User",
      "created_at": "2025-01-05T02:03:06Z",
      "updated_at": "2025-06-20T00:02:16Z",
      "closed_at": "2025-06-20T00:02:16Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 16,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/7560/reactions",
        "total_count": 19,
        "+1": 19,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/7560",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/7560",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:18.872312",
      "comments": [
        {
          "author": "lsorber",
          "body": "I suspect this is caused by the changes introduced in v1.56.2 with https://github.com/BerriAI/litellm/pull/7421, where you can see the following in `litellm/types/integrations/prometheus.py`:\r\n```python\r\nclass UserAPIKeyLabelValues(BaseModel):\r\n    end_user: Optional[str] = None\r\n    user: Optional[",
          "created_at": "2025-01-05T20:56:39Z"
        },
        {
          "author": "gouzhuang",
          "body": "same issue. reverted back to 1.55.12 as a workaround.",
          "created_at": "2025-01-06T08:05:04Z"
        },
        {
          "author": "nbrosse",
          "body": "Same issue.",
          "created_at": "2025-01-07T13:29:07Z"
        },
        {
          "author": "ryanpeach",
          "body": "Yup",
          "created_at": "2025-01-08T17:04:21Z"
        },
        {
          "author": "cauchymike",
          "body": "getting this same issue working with Raglite ",
          "created_at": "2025-01-11T16:05:03Z"
        }
      ]
    },
    {
      "issue_number": 7764,
      "title": "[Bug]: litellm slower then python's request ",
      "body": "### What happened?\n\nIm currently using litellm to benchmark different models in different environments.\r\nIm seeing strange behavior from litellm compared to standard  request using python Requests library when trying high request rates in multithreaded setup.\r\nspecifically when using litellm it somehow interfere with python's main thread and slows down the task submission using concurrent.futures thread pool.\r\nIm aware of the issues that might arise when using python and multithreading (i.e GIL and so.) but i believe that in this case it isn't the issue.\r\n\r\nDo you have any idea what i the code of chat.completion could cause that ?\r\n\r\nThank you.\r\nimport threading\r\nimport numpy as np\r\nimport requests\r\nimport time\r\nfrom concurrent.futures import ThreadPoolExecutor \r\nimport requests\r\nimport json \r\nimport litellm\r\nimport os \r\nimport tqdm\r\n\r\ntp = ThreadPoolExecutor(100)\r\n\r\n\r\ndef send_chat_completions_request():\r\n        rs = []\r\n        start = time.time()\r\n        response = litellm.completion(model=\"gpt-4o-mini\"\r\n                                      , messages= [\r\n    \r\n                {\"role\": \"user\", \"content\": \" \".join([\"count till 1000\"]*1000)}\r\n                ], \r\n                stream= True,\r\n                max_tokens= 300,\r\n                stream_options= {\"include_usage\": True},\r\n            )\r\n        for r in response:\r\n            rs.append(r)\r\n        return start\r\n\r\ndef send_chat_completions_request2():\r\n    chunks = []\r\n    data = {\r\n        \"model\": \"gpt-4o-mini\", \r\n        \"messages\": [\r\n            {\"role\": \"user\", \"content\": \" \".join([\"count till 1000\"]*1000)}\r\n        ],\r\n        \"stream\": True,\r\n        \"max_tokens\": 300,   # Limit the response length\r\n    }\r\n    start = time.time()\r\n    response = requests.post(API_URL, headers=headers, json=data, stream=True)\r\n    if response.status_code == 200:\r\n            for chunk in response.iter_lines():\r\n                if chunk:\r\n                    chunks.append( chunk)\r\n    else:\r\n        return response\r\n    return start\r\n\r\n\r\n\r\nimport threading\r\nres = []\r\nfor i in tqdm.tqdm(range(20)):\r\n    res.append(tp.submit(send_chat_completions_request))\r\n    time.sleep( 1 / 5)\r\n\r\nres_num = [x.result() for x in res]\r\nprint(np.diff(res_num).mean())\r\nprint(np.diff(res_num).std())\r\n\r\n\r\nres2 = []\r\nfor i in tqdm.tqdm(range(20)):\r\n    res2.append(tp.submit(send_chat_completions_request2))\r\n    time.sleep( 1 / 5)\r\n\r\nres_num2 = [x.result() for x in res2]\r\nprint(np.diff(res_num2).mean())\r\nprint(np.diff(res_num2).std())\r\n\r\n\r\nwould give me:\r\n\r\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.10it/s]\r\n0.24398223977339895 \r\n0.06533077322436216\r\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.99it/s]\r\n0.20050784161216334\r\n0.0002891661660958098\n\n### Relevant log output\n\n_No response_\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\n1.58.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "jouDance",
      "author_type": "User",
      "created_at": "2025-01-14T13:58:27Z",
      "updated_at": "2025-06-20T00:02:15Z",
      "closed_at": "2025-06-20T00:02:15Z",
      "labels": [
        "bug",
        "awaiting: user response",
        "mlops user request",
        "stale"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/7764/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "krrishdholakia"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/7764",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/7764",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:19.113009",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Hi @jouDance do you see this behavior using the async route instead? ",
          "created_at": "2025-01-15T02:09:29Z"
        },
        {
          "author": "jouDance",
          "body": "Hey, thanks for responding! The initial reason i moved from the async (acompletion ) to multithreading was poor performance as well when scheduling big amount of requests at once. I suspected it to be the big amount of responses coming back at once as stream and handling them would prevent the main ",
          "created_at": "2025-01-15T06:29:26Z"
        },
        {
          "author": "Dannynis",
          "body": "After digging a bit deeper to what might cause that i've noticed a lot of response handler threads spawned from litellm.\r\nNot sure if thats what causes the interference but i think it might be the issue.\r\n\r\n![image](https://github.com/user-attachments/assets/4a299ff3-d79c-4d94-9562-2acc0f814a15)\r\n",
          "created_at": "2025-01-15T10:05:25Z"
        },
        {
          "author": "Dannynis",
          "body": "** UPDATE **\r\nit most definitely the issue, \r\nafter commenting out https://github.com/BerriAI/litellm/blob/fe60a38c8e43e908f44d8c668a5ba9fae1dca762/litellm/litellm_core_utils/streaming_handler.py#L1498\r\n\r\nI saw major improvement in the execution speed, i suspect that at large amount of requests it s",
          "created_at": "2025-01-15T10:30:23Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Amazing. Thank you @Dannynis @jouDance \n\nI'll push a fix for this today. \n\nAppreciate your work on this. ",
          "created_at": "2025-01-15T14:03:51Z"
        }
      ]
    },
    {
      "issue_number": 8244,
      "title": "[Feature]: Accurate Token Counting in LiteLLM with vLLM-Hosted Models (Gemma, Mistral, etc.)",
      "body": "### The Feature\n\nvLLM (and similar backends like Ollama) provide dedicated routes to retrieve the exact token count for a given prompt, based on the tokenizer.json of the loaded model.\n\nLiteLLM should be able to natively query the tokenizer used by vLLM (instead of defaulting to tiktoken).\nIdeally, LiteLLM would extract tokenization logic directly from the vLLM-hosted model (when available), ensuring full compatibility with models like Gemma 2, Mistral, etc.\nAlternatively, if vLLM exposes its token counting as an API route, LiteLLM could simply delegate token counting to vLLM when connected.\n\nThe only current workaround is using vLLM's own API instead of LiteLLM for token counting when running locally.\n\nOtherwise thank you for your work :)\n\n### Motivation, pitch\n\nI'm working with LiteLLM to centralise endpoints into one unique endpoint.\n\nCurrently, LiteLLM offers a /utils/token_counter route that can count tokens when plugged into a vLLM instance. However, this feature appears to have limited compatibility:\n\n- It works well for some models but defaults to tiktoken when unsupported, which is inaccurate for many architectures.\n- When using vLLM-hosted models like Gemma 2 or Mistral, the token count retrieved by LiteLLM can be inconsistent or incorrect, as these models often rely on specialized tokenization schemes.\n\n\n### Are you a ML Ops Team?\n\nYes\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "thverney-dozo",
      "author_type": "User",
      "created_at": "2025-02-04T14:37:20Z",
      "updated_at": "2025-06-20T00:02:13Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "mlops user request",
        "stale",
        "feb 2025",
        "usage tracking",
        "vllm"
      ],
      "label_count": 6,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8244/reactions",
        "total_count": 3,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8244",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8244",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:19.331082",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Hi @thverney-dozo how would you want us to handle this? \n\nWe already allow for custom token counting - https://docs.litellm.ai/docs/proxy/configs#set-custom-tokenizer",
          "created_at": "2025-02-05T20:58:35Z"
        },
        {
          "author": "thverney-dozo",
          "body": "Hi @krrishdholakia, \nThank you for your quick response.\n\nThe custom tokenizer approach works well when vLLM is hosted on the same server and the tokenizer.json can be easily retrieved for token counting.\n\nHowever, LiteLLM functions as an endpoint aggregator, and in scenarios where multiple vLLM inst",
          "created_at": "2025-02-07T09:32:34Z"
        },
        {
          "author": "Yuhui0620",
          "body": "+1. Any plan for supporting tokenize api of vllm?[https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html#tokenizer-api](url)",
          "created_at": "2025-02-18T06:26:45Z"
        },
        {
          "author": "cassandeer",
          "body": "+1 ",
          "created_at": "2025-02-20T13:26:15Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Hi @thverney-dozo @Yuhui0620 @cassandeer if anyone wants to contribute this PR with a mock test, this would be welcome. \n\nhere's the relevant code block - https://github.com/BerriAI/litellm/blob/11e9fc7b549116c8267d9c7e3db93d9c084e2837/litellm/proxy/proxy_server.py#L5791",
          "created_at": "2025-02-21T07:41:52Z"
        }
      ]
    },
    {
      "issue_number": 8355,
      "title": "[Bug]: Unable to Pass User ID to Langfuse via LiteLLM Key Metadata",
      "body": "### What happened?\n\nHello,\n\nI am using LiteLLM to access LLMs and have integrated Langfuse as a callback. I can successfully see the logs in Langfuse.\n\nWhile creating a key in LiteLLM, I have tried adding metadata fields such as trace_user_id, user, and user_id to allow Langfuse to capture the user ID. However, no matter what I try, the user ID does not appear in Langfuse.\n\nMy question is: When creating a user key in LiteLLM, is there a way to associate the key owner‚Äôs information with the key using metadata in LiteLLM? Or is there another mechanism in Langfuse or Litellm that would allow me to filter logs by user ID?\n\nThanks!\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.60.6\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "bleriot14",
      "author_type": "User",
      "created_at": "2025-02-07T12:32:42Z",
      "updated_at": "2025-06-20T00:02:12Z",
      "closed_at": null,
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8355/reactions",
        "total_count": 2,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8355",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8355",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:19.515361",
      "comments": [
        {
          "author": "josephjobs",
          "body": "I'm having the same problem too",
          "created_at": "2025-02-07T12:46:10Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "Ideally should we just log the user_id, team_id associated with a key on langfuse ? ",
          "created_at": "2025-02-07T22:07:28Z"
        },
        {
          "author": "chrisranderson",
          "body": "> Ideally should we just log the user_id, team_id associated with a key on langfuse ?\n\nDevs are our end users, so linking a key's user ID with a trace would be nice. I'm new to Langfuse, so whatever it takes to see key owners and their usage easily would be great.",
          "created_at": "2025-03-21T23:08:35Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-20T00:02:11Z"
        }
      ]
    },
    {
      "issue_number": 9178,
      "title": "[Bug]: Connection to backing provider leaking when the source request cancels",
      "body": "### What happened?\n\nLiteLLM does not close the proxy connections to the backing LLM server, even if the requests are cancelled by the client.\n\nI send requests with `curl` with very short timeout. `curl` cancels them, but LiteLLM proxy doesn't in turn cancel its own requests to the backing LLM provider.\n\nReproducer: https://github.com/aliok/litellm-connection-leak\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.61.20-stable\n\n### Twitter / LinkedIn details\n\nhttps://www.linkedin.com/in/aliok/",
      "state": "open",
      "author": "aliok",
      "author_type": "User",
      "created_at": "2025-03-12T18:13:21Z",
      "updated_at": "2025-06-20T00:02:09Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9178/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9178",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9178",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:19.725970",
      "comments": [
        {
          "author": "halil-bugol",
          "body": "Up!",
          "created_at": "2025-03-12T19:45:52Z"
        },
        {
          "author": "315930399",
          "body": "dose this branch need to be merged ? @krrishdholakia @ishaan-jaff \n https://github.com/BerriAI/litellm/tree/litellm_fix_check_request_disconnection_loic",
          "created_at": "2025-03-19T01:58:23Z"
        },
        {
          "author": "krrishdholakia",
          "body": "It's pretty old - i believe we had an initial attempt at this, but had to remove it due to some perf issues. \n\nA PR here is welcome! ",
          "created_at": "2025-03-19T02:39:28Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-18T00:01:48Z"
        },
        {
          "author": "aliok",
          "body": "Is there a way to freeze this? I think this would bite so many butts eventually",
          "created_at": "2025-06-19T14:07:48Z"
        }
      ]
    },
    {
      "issue_number": 9228,
      "title": "[Feature]:  Add apac.amazon.nova-* Model Support",
      "body": "### The Feature\n\nHi LiteLLM Team,\n\nI'm using LiteLLM with the `amazon.nova-*` model in the APAC region . However, I noticed that the` model_prices_and_context_window.json` file doesn't include this model for the APAC region.\n\nThe current `model_prices_and_context_window.json` includes regional variations for other amazon.nova-pro-v1:0 models (e.g., us.amazon.nova-pro-v1:0, eu.amazon.nova-pro-v1:0), and I believe adding support for `apac.amazon.nova-pro-v1:0` would be beneficial for users in the Asia-Pacific region. (The nova model officially supports the apac region.)\n\nCould you please consider adding the following entry to the `model_prices_and_context_window.json` file?\n```\n\"apac.amazon.nova-micro-v1:0\": {\n    \"max_tokens\": 4096,\n    \"max_input_tokens\": 300000,\n    \"max_output_tokens\": 4096,\n    \"input_cost_per_token\": 0.000000035,  // Please verify the correct pricing for the APAC region\n    \"output_cost_per_token\": 0.00000014,  // Please verify the correct pricing for the APAC region\n    \"litellm_provider\": \"bedrock_converse\",\n    \"mode\": \"chat\",\n    \"supports_function_calling\": true,\n    \"supports_prompt_caching\": true,\n    \"supports_response_schema\": true\n},\n\"apac.amazon.nova-lite-v1:0\": {\n    \"max_tokens\": 4096,\n    \"max_input_tokens\": 128000,\n    \"max_output_tokens\": 4096,\n    \"input_cost_per_token\": 0.00000006,  // Please verify the correct pricing for the APAC region\n    \"output_cost_per_token\": 0.00000024, // Please verify the correct pricing for the APAC region\n    \"litellm_provider\": \"bedrock_converse\",\n    \"mode\": \"chat\",\n    \"supports_function_calling\": true,\n    \"supports_vision\": true,\n    \"supports_pdf_input\": true,\n    \"supports_prompt_caching\": true,\n    \"supports_response_schema\": true\n},\n\"apac.amazon.nova-pro-v1:0\": {\n    \"max_tokens\": 4096, \n    \"max_input_tokens\": 300000,\n    \"max_output_tokens\": 4096,\n    \"input_cost_per_token\": 0.0000008,  // Please verify the correct pricing for the APAC region\n    \"output_cost_per_token\": 0.0000032, // Please verify the correct pricing for the APAC region\n    \"litellm_provider\": \"bedrock_converse\",\n    \"mode\": \"chat\",\n    \"supports_function_calling\": true,\n    \"supports_vision\": true,\n    \"supports_pdf_input\": true,\n    \"supports_prompt_caching\": true,\n    \"supports_response_schema\": true\n}\n```\n\n### Motivation, pitch\n\n-\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "jaehyeongAN",
      "author_type": "User",
      "created_at": "2025-03-14T05:18:34Z",
      "updated_at": "2025-06-20T00:02:07Z",
      "closed_at": "2025-06-20T00:02:07Z",
      "labels": [
        "enhancement",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9228/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9228",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9228",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:19.903008",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-13T00:01:52Z"
        }
      ]
    },
    {
      "issue_number": 9231,
      "title": "[Feature]: Snowflake Cortex QA Items",
      "body": "### The Feature\n\n- Allow me to pass `snowflake_account_id` and `snowflake_jwt` as dynamic params (via config) \n- allow me to pass an api_base and append the `api/v2/cortex/inference:complete` to my api base (like openai does) \n\n\nIn the code, it looks like `api_key` dynamic param = `SNOWFLAKE_JWT` env var, this is confusing because the names are different, and it's not immediately apparent by looking at docs. \n\n### Motivation, pitch\n\nMake it easier to use the Snowflake integration. \n\nCurrently the docs are quite confusing - as the sdk example != the proxy example \n\n<img width=\"1078\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/d0515c58-756d-45af-b7fb-b2a83f0a7394\" />\n<img width=\"1091\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/d59a86ba-43b0-4d35-8959-309c4ed89300\" />\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "krrishdholakia",
      "author_type": "User",
      "created_at": "2025-03-14T06:47:26Z",
      "updated_at": "2025-06-20T00:02:05Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9231/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9231",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9231",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:20.103178",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Hey @SunnyWan59 can you look into simplifying this \n\nI think Azure, Bedrock and VertexAI are relevant examples here. I trust how you end up solving this, it's just confusing right now. \n\ncc: @ishaan-jaff ",
          "created_at": "2025-03-14T06:49:05Z"
        },
        {
          "author": "SunnyWan59",
          "body": "@krrishdholakia I'll get started on this!",
          "created_at": "2025-03-15T02:40:51Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Thanks!",
          "created_at": "2025-03-15T03:50:41Z"
        },
        {
          "author": "SunnyWan59",
          "body": "Hey @krrishdholakia do you mean something like this for the config.yaml?\n```\nmodel_list:\n  - model_name: mistral-7b\n    litellm_params:\n        model: snowflake/mistral-7b\n        snowflake_jwt: <JWT>\n        snowflake_account_id: <ACCOUNT-ID>\n        api_base: snowflakecomputing.com\n```",
          "created_at": "2025-03-21T05:59:28Z"
        },
        {
          "author": "SunnyWan59",
          "body": "I just wrote a pr for this @krrishdholakia  #9443, lmk what you think.",
          "created_at": "2025-03-21T21:18:01Z"
        }
      ]
    },
    {
      "issue_number": 9239,
      "title": "LiteLLM cannot handle image responses from TogetherAI's text2image model Stabeldiffusion due to an additional 'ID' field being sent from togetherAI in the response",
      "body": "TogetherAI sends an additional 'ID field':\n\nlitellm_1 | )\nlitellm_1 | ^\nlitellm_1 | File \"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py\", line 278, in convert_to_image_response\nlitellm_1 | model_response_object = ImageResponse(**model_response_dict)\n**litellm_1 | TypeError: ImageResponse.init() got an unexpected keyword argument 'id'**\nlitellm_1 |\nlitellm_1 |\nlitellm_1 | received_args={'response_object': {'created': None, 'data': [{'b64_json': '/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAQABAAD......\n\nmodel's config:\n\n - model_name: together_ai/stable-diffusion-xl\n   litellm_params:\n     model: \"together_ai/stabilityai/stable-diffusion-xl-base-1.0\"\n     api_base: \"https://api.together.xyz/v1\"\n     api_key: tgp_v1_-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n     response_format: \"base64\"  # Ensure base64 output\n   model_info:\n     mode: image_generation\n\nWhy This Happens\n    Together AI Response Format: Together AI‚Äôs /v1/images/generations endpoint returns a JSON object like {\"created\": null, \"data\": [{\"b64_json\": \"...\"}], \"id\": \"...\"}. The 'id' field is part of their response but not part of the standard OpenAI image generation response spec that litellm expects ({\"created\": ..., \"data\": [{\"b64_json\": \"...\"}]}).\n    litellm Bug: The ImageResponse class in litellm (at convert_dict_to_response.py:278) isn‚Äôt designed to handle extra fields like 'id', causing a TypeError. This is a compatibility issue between Together AI‚Äôs response and litellm‚Äôs parsing logic.\n\n",
      "state": "closed",
      "author": "JosefAschauer",
      "author_type": "User",
      "created_at": "2025-03-14T10:37:52Z",
      "updated_at": "2025-06-20T00:02:04Z",
      "closed_at": "2025-06-20T00:02:04Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9239/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9239",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9239",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:25.309316",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-13T00:01:48Z"
        }
      ]
    },
    {
      "issue_number": 9236,
      "title": "why won't any huggingface text2image models work?",
      "body": "Since I'm struggling with this since days and I know, that huggingface LLM's via their serverless inference api work just well same as text2image using openai's dall-e-3.\n\nProblem:\nregardless what I configure in the litellm config - currently it's this but I tried nearly everything i could think of:\n\n - model_name: huggingface/stable-diffusion-xl\n   litellm_params:\n     model: \"huggingface/stabilityai/stable-diffusion-xl-base-1.0\"\n     api_base: \"https://router.huggingface.co/hf-inference/models/stabilityai/stable-diffusion-xl-base-1.0\"\n     api_key: os.environ/HUGGINGFACE_API_KEY\n     request_type: \"post\"\n     request_body:\n       inputs: \"{{prompt}}\"\n     custom_llm_provider: \"huggingface\"  # Explicitly set provider\n     response_format: \"base64\"\n   model_info:\n     mode: image_generation\n\nBut the model info in liteLLM keeps showing mode: chat\n\nwith the bare minimum:\n\n - model_name: huggingface/stable-diffusion-xl\n   litellm_params:\n     model: \"huggingface/stabilityai/stable-diffusion-xl-base-1.0\"\n     api_base: \"https://router.huggingface.co/hf-inference/models/stabilityai/stable-diffusion-xl-base-1.0\"\n     api_key: os.environ/HUGGINGFACE_API_KEY\n   model_info:\n     mode: image_generation\n\nit's exactly the same.\n\nIn the logs  I'm seeing a 200:\nINFO:     192.168.0.27:58640 - \"POST /v1/images/generations HTTP/1.1\" 200 OK\n\nBut it's never coming back with a valid response, probably since it expects a chat response. In the debug logs I also see that it's sending the api key.\n\nitellm_1         | 08:09:24 - LiteLLM:DEBUG: utils.py:4579 - model_info: {'key': 'stabilityai/stable-diffusion-xl-base-1.0', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'output_cost_per_token': 0, 'litellm_provider': 'huggingface', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_pdf_input': None}\n\nand\n\nlitellm_1         |     \"request_id\": \"a44d8189-d1f0-411a-8632-58c96998cc4e\",\nlitellm_1         |     \"call_type\": \"aimage_generation\",\nlitellm_1         |     \"api_key\": \"88dc28d0f030c55ed4ab77xxxxxxxxxxxxxxxxxxxf778539800c9f1243fe6b4b\",\nlitellm_1         |     \"cache_hit\": \"None\",\nlitellm_1         |     \"startTime\": \"2025-03-14 08:09:24.237652+00:00\",\nlitellm_1         |     \"endTime\": \"2025-03-14 08:09:24.248845+00:00\",\nlitellm_1         |     \"completionStartTime\": \"2025-03-14 08:09:24.248845+00:00\",\nlitellm_1         |     \"model\": \"stabilityai/stable-diffusion-xl-base-1.0\",\nlitellm_1         |     \"user\": \"default_user_id\",\nlitellm_1         |     \"team_id\": \"\",\nlitellm_1         |     \"metadata\": \"{\\\"user_api_key\\\": \\\"88dc28d0f030c55ed4ab77xxxxxxxxxxxxxxxxxxxf778539800c9f1243fe6b4b\\\", \\\"user_api_key_alias\\\": null, \\\"user_api_key_team_id\\\": null, \\\"user_api_key_org_id\\\": null, \\\"user_api_key_user_id\\\": \\\"default_user_id\\\", \\\"user_api_key_team_alias\\\": null, \\\"requester_ip_address\\\": \\\"\\\", \\\"applied_guardrails\\\": [], \\\"batch_models\\\": null, \\\"additional_usage_values\\\": {\\\"completion_tokens_details\\\": null, \\\"prompt_tokens_details\\\": null}}\",\n\n\nTo answer the obvious: Yes, I tested the huggingface endpoint directly in bash using CURL and the same api key and it gets me an image, so the problem lays somewhere either in my model config or litellm.\n\nAny help would be just GREAT!\n\n\n",
      "state": "closed",
      "author": "JosefAschauer",
      "author_type": "User",
      "created_at": "2025-03-14T08:24:07Z",
      "updated_at": "2025-06-20T00:02:04Z",
      "closed_at": "2025-06-20T00:02:04Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9236/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9236",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9236",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:25.565149",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-13T00:01:50Z"
        }
      ]
    },
    {
      "issue_number": 9240,
      "title": "[Feature]: Allow API Keys to be loaded from files, not just environment variables or directly in the config.yaml",
      "body": "### The Feature\n\nAdd a new pattern to api_key in the config file to allow for file/ to denote that a file reference follows.\n \n### Motivation, pitch\n\n  This allows keys to be stored in temporary files that exist during startup or in secured locations and not in plaintext with the config or visible in the process tables/container inspection.  This also allows for the config to be committed to git without concern or stored in a non-secured location.\n\nThis is similar to the existing os.environ and oidc prefixes.\n\nNote: This issue was edited as I had missed how the environment variables were referenced as my initial use has been with placing hte keys directly into the configuration file.\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\nhttps://www.linkedin.com/in/bexelbie/",
      "state": "closed",
      "author": "bexelbie",
      "author_type": "User",
      "created_at": "2025-03-14T11:03:46Z",
      "updated_at": "2025-06-20T00:02:03Z",
      "closed_at": "2025-06-20T00:02:03Z",
      "labels": [
        "enhancement",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9240/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9240",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9240",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:25.750659",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-13T00:01:46Z"
        }
      ]
    },
    {
      "issue_number": 9256,
      "title": "[Bug]: Missing documentation on tags used for containers",
      "body": "### What happened?\n\nCan we get a document that details the tags on the containers and what their update cadence/guidance is?\n\nThank you \n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.63.8-nightly\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "bexelbie",
      "author_type": "User",
      "created_at": "2025-03-14T16:17:21Z",
      "updated_at": "2025-06-20T00:02:02Z",
      "closed_at": "2025-06-20T00:02:02Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9256/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9256",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9256",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:25.939600",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-13T00:01:45Z"
        }
      ]
    },
    {
      "issue_number": 9265,
      "title": "[Bug]: LiteLLM Cache: Excepton add_cache: sequence item 1: expected str instance, list found",
      "body": "### What happened?\n\nWorking on redis-semantic caching in Proxy, receiving this error. I do see cache hits when i do a repeated curl test.\n\nMy settings: \n\n```cache_params:\n    type: \"redis-semantic\"\n    host: \"localhost\" # Changed from 0.0.0.0 to service name\n    port: \"6379\"\n    password: \"xxxxxxxxxx\"\n    # redis_url: \"redis://default:xxxxxxxx@localhost:6379\"\n    namespace: \"litellm:caching\"\n    similarity_threshold: 0.8\n    redis_semantic_cache_embedding_model: amazon.titan-embed-text-v2```\n\n\n### Relevant log output\n\n```shell\ncaching.py:582 - LiteLLM Cache: Excepton add_cache: sequence item 1: expected str instance, list found\nTraceback (most recent call last):\n  File \"/Users/ross/Projects/LiteLLM/.venv/lib/python3.13/site-packages/litellm/caching/caching.py\", line 580, in async_add_cache\n    await self.cache.async_set_cache(cache_key, cached_data, **kwargs)\n  File \"/Users/ross/Projects/LiteLLM/.venv/lib/python3.13/site-packages/litellm/caching/redis_semantic_cache.py\", line 209, in async_set_cache\n    prompt = \"\".join(message[\"content\"] for message in messages)\nTypeError: sequence item 1: expected str instance, list found\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.63.8\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "awsrossw",
      "author_type": "User",
      "created_at": "2025-03-14T19:42:49Z",
      "updated_at": "2025-06-20T00:02:01Z",
      "closed_at": "2025-06-20T00:02:01Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9265/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9265",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9265",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:26.153419",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-13T00:01:44Z"
        }
      ]
    },
    {
      "issue_number": 9266,
      "title": "Azure OpenAI call using the AZURE_API_BASE URL with suffix.",
      "body": "See the code below. As you can see the base URL is `https://ai-dl564313822ai240100219788.services.ai.azure.com/models` but in the log, the requesting url is `https://ai-dl564313822ai240100219788.services.ai.azure.com/models/openai` with `openai` suffix. \n\nExpect: the URL remains the same.\nActual: the URL has changed from  `https://ai-dl564313822ai240100219788.services.ai.azure.com/models` to `https://ai-dl564313822ai240100219788.services.ai.azure.com/models/openai`\n\n```python\n\nfrom litellm import completion, _turn_on_debug\nimport os\n\n## set ENV variables\nos.environ[\"AZURE_API_KEY\"] = \"xxx\"\nos.environ[\"AZURE_API_BASE\"] = \"https://ai-dl564313822ai240100219788.services.ai.azure.com/models\"\nos.environ[\"AZURE_API_VERSION\"] = \"2024-09-12\"\n\n_turn_on_debug()\n\n# azure call\nresponse = completion(\n  \"azure/o1-mini\",\n  messages = [{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n  stream=True,\n)\n\nprint(\"Response:\", response.choices[0].message.content)\nprint(\"Model:\", response.model)\nprint(\"Usage:\")\nprint(\"\tPrompt tokens:\", response.usage.prompt_tokens)\nprint(\"\tTotal tokens:\", response.usage.total_tokens)\nprint(\"\tCompletion tokens:\", response.usage.completion_tokens)\n```\n\n\n```\n\n13:54:57 - LiteLLM:DEBUG: utils.py:311 - \n\n13:54:57 - LiteLLM:DEBUG: utils.py:311 - Request to litellm:\n13:54:57 - LiteLLM:DEBUG: utils.py:311 - litellm.completion('azure/o1-mini', messages=[{'content': 'Hello, how are you?', 'role': 'user'}], stream=True)\n13:54:57 - LiteLLM:DEBUG: utils.py:311 - \n\n13:54:57 - LiteLLM:DEBUG: litellm_logging.py:389 - self.optional_params: {}\n13:54:57 - LiteLLM:DEBUG: utils.py:311 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n13:54:57 - LiteLLM:DEBUG: utils.py:4295 - checking potential_model_names in litellm.model_cost: {'split_model': 'o1-mini', 'combined_model_name': 'openai/o1-mini', 'stripped_model_name': 'o1-mini', 'combined_stripped_model_name': 'openai/o1-mini', 'custom_llm_provider': 'openai'}\n13:54:57 - LiteLLM:DEBUG: utils.py:4295 - checking potential_model_names in litellm.model_cost: {'split_model': 'o1-mini', 'combined_model_name': 'openai/o1-mini', 'stripped_model_name': 'o1-mini', 'combined_stripped_model_name': 'openai/o1-mini', 'custom_llm_provider': 'openai'}\n13:54:57 - LiteLLM:DEBUG: utils.py:4295 - checking potential_model_names in litellm.model_cost: {'split_model': 'o1-mini', 'combined_model_name': 'openai/o1-mini', 'stripped_model_name': 'o1-mini', 'combined_stripped_model_name': 'openai/o1-mini', 'custom_llm_provider': 'openai'}\n13:54:57 - LiteLLM:INFO: utils.py:2999 - \nLiteLLM completion() model= o1-mini; provider = azure\n13:54:57 - LiteLLM:DEBUG: utils.py:3002 - \nLiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': True, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'reasoning_effort': None, 'messages': [{'content': 'Hello, how are you?', 'role': 'user'}], 'thinking': None, 'additional_drop_params': None, 'custom_llm_provider': 'azure', 'drop_params': None, 'model': 'o1-mini', 'n': None}\n13:54:57 - LiteLLM:DEBUG: utils.py:3005 - \nLiteLLM: Non-Default params passed to completion() {'stream': True}\n13:54:57 - LiteLLM:DEBUG: utils.py:4295 - checking potential_model_names in litellm.model_cost: {'split_model': 'o1-mini', 'combined_model_name': 'openai/o1-mini', 'stripped_model_name': 'o1-mini', 'combined_stripped_model_name': 'openai/o1-mini', 'custom_llm_provider': 'openai'}\n13:54:57 - LiteLLM:DEBUG: utils.py:4295 - checking potential_model_names in litellm.model_cost: {'split_model': 'o1-mini', 'combined_model_name': 'openai/o1-mini', 'stripped_model_name': 'o1-mini', 'combined_stripped_model_name': 'openai/o1-mini', 'custom_llm_provider': 'openai'}\n13:54:57 - LiteLLM:DEBUG: utils.py:4295 - checking potential_model_names in litellm.model_cost: {'split_model': 'o1-mini', 'combined_model_name': 'openai/o1-mini', 'stripped_model_name': 'o1-mini', 'combined_stripped_model_name': 'openai/o1-mini', 'custom_llm_provider': 'openai'}\n13:54:57 - LiteLLM:DEBUG: utils.py:311 - Final returned optional params: {'stream': True, 'extra_body': {}}\n13:54:57 - LiteLLM:DEBUG: litellm_logging.py:389 - self.optional_params: {'stream': True, 'extra_body': {}}\n13:54:57 - LiteLLM:DEBUG: common_utils.py:328 - Initializing Azure OpenAI Client for , Api Base: https://ai-dl564313822ai240100219788.services.ai.azure.com/models, Api Key:3kTrhCj7***************\n13:54:57 - LiteLLM:DEBUG: utils.py:4295 - checking potential_model_names in litellm.model_cost: {'split_model': 'o1-mini', 'combined_model_name': 'openai/o1-mini', 'stripped_model_name': 'o1-mini', 'combined_stripped_model_name': 'openai/o1-mini', 'custom_llm_provider': 'openai'}\n13:54:57 - LiteLLM:DEBUG: utils.py:4295 - checking potential_model_names in litellm.model_cost: {'split_model': 'o1-mini', 'combined_model_name': 'openai/o1-mini', 'stripped_model_name': 'o1-mini', 'combined_stripped_model_name': 'openai/o1-mini', 'custom_llm_provider': 'openai'}\n13:54:57 - LiteLLM:DEBUG: utils.py:4295 - checking potential_model_names in litellm.model_cost: {'split_model': 'o1-mini', 'combined_model_name': 'openai/o1-mini', 'stripped_model_name': 'o1-mini', 'combined_stripped_model_name': 'openai/o1-mini', 'custom_llm_provider': 'openai'}\n13:54:57 - LiteLLM:DEBUG: utils.py:4295 - checking potential_model_names in litellm.model_cost: {'split_model': 'o1-mini', 'combined_model_name': 'azure/o1-mini', 'stripped_model_name': 'o1-mini', 'combined_stripped_model_name': 'azure/o1-mini', 'custom_llm_provider': 'azure'}\n13:54:57 - LiteLLM:DEBUG: utils.py:4579 - model_info: {'key': 'azure/o1-mini', 'max_tokens': 65536, 'max_input_tokens': 128000, 'max_output_tokens': 65536, 'input_cost_per_token': 1.21e-06, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': 6.05e-07, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 4.84e-06, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'azure', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': None, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': False, 'supports_assistant_prefill': False, 'supports_prompt_caching': True, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'tpm': None, 'rpm': None}\n13:54:57 - LiteLLM:DEBUG: utils.py:4295 - checking potential_model_names in litellm.model_cost: {'split_model': 'o1-mini', 'combined_model_name': 'openai/o1-mini', 'stripped_model_name': 'o1-mini', 'combined_stripped_model_name': 'openai/o1-mini', 'custom_llm_provider': 'openai'}\n13:54:57 - LiteLLM:DEBUG: litellm_logging.py:685 - \n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://ai-dl564313822ai240100219788.services.ai.azure.com/models/openai/ \\\n-d '{'model': 'o1-mini', 'messages': [{'content': 'Hello, how are you?', 'role': 'user'}], 'extra_body': {}}'\n\n\n13:54:57 - LiteLLM:DEBUG: main.py:5512 - openai.py: Received openai error - Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\n13:54:57 - LiteLLM:DEBUG: exception_mapping_utils.py:2243 - Logging Details: logger_fn - None | callable(logger_fn) - False\n13:54:57 - LiteLLM:DEBUG: litellm_logging.py:1948 - Logging Details LiteLLM-Failure Call: []\nTraceback (most recent call last):\n  File \"/home/dl/.local/lib/python3.10/site-packages/litellm/llms/openai/openai.py\", line 727, in completion\n    raise e\n  File \"/home/dl/.local/lib/python3.10/site-packages/litellm/llms/openai/openai.py\", line 654, in completion\n    self.make_sync_openai_chat_completion_request(\n  File \"/home/dl/.local/lib/python3.10/site-packages/litellm/litellm_core_utils/logging_utils.py\", line 145, in sync_wrapper\n    result = func(*args, **kwargs)\n  File \"/home/dl/.local/lib/python3.10/site-packages/litellm/llms/openai/openai.py\", line 473, in make_sync_openai_chat_completion_request\n    raise e\n  File \"/home/dl/.local/lib/python3.10/site-packages/litellm/llms/openai/openai.py\", line 455, in make_sync_openai_chat_completion_request\n    raw_response = openai_client.chat.completions.with_raw_response.create(\n  File \"/home/dl/.local/lib/python3.10/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n  File \"/home/dl/.local/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 279, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/dl/.local/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 914, in create\n    return self._post(\n  File \"/home/dl/.local/lib/python3.10/site-packages/openai/_base_client.py\", line 1242, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/home/dl/.local/lib/python3.10/site-packages/openai/_base_client.py\", line 919, in request\n    return self._request(\n  File \"/home/dl/.local/lib/python3.10/site-packages/openai/_base_client.py\", line 1023, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/dl/.local/lib/python3.10/site-packages/litellm/main.py\", line 1255, in completion\n    response = azure_o1_chat_completions.completion(\n  File \"/home/dl/.local/lib/python3.10/site-packages/litellm/llms/azure/chat/o_series_handler.py\", line 50, in completion\n    return super().completion(\n  File \"/home/dl/.local/lib/python3.10/site-packages/litellm/llms/openai/openai.py\", line 738, in completion\n    raise OpenAIError(\nlitellm.llms.openai.common_utils.OpenAIError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/dl/ai/podcast-ai/back-end/api1.py\", line 12, in <module>\n    response = completion(\n  File \"/home/dl/.local/lib/python3.10/site-packages/litellm/utils.py\", line 1235, in wrapper\n    raise e\n  File \"/home/dl/.local/lib/python3.10/site-packages/litellm/utils.py\", line 1113, in wrapper\n    result = original_function(*args, **kwargs)\n  File \"/home/dl/.local/lib/python3.10/site-packages/litellm/main.py\", line 3101, in completion\n    raise exception_type(\n  File \"/home/dl/.local/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2214, in exception_type\n    raise e\n  File \"/home/dl/.local/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2053, in exception_type\n    raise APIError(\nlitellm.exceptions.APIError: litellm.APIError: AzureException APIError - Resource not found\n```",
      "state": "closed",
      "author": "DerekLiang",
      "author_type": "User",
      "created_at": "2025-03-14T21:10:37Z",
      "updated_at": "2025-06-20T00:02:00Z",
      "closed_at": "2025-06-20T00:02:00Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9266/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9266",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9266",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:26.356765",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-13T00:01:42Z"
        }
      ]
    },
    {
      "issue_number": 9297,
      "title": "Parameter supports_vision: True is ignored on config.yaml of LiteLLM Proxy",
      "body": "Hi there, I'm trying to proxy visual models behind LiteLLM Proxy.\n\nHere is a combination of Settings I tried:\n\n```\nmodel_list:\n  - model_name: gemma3:12b-it-q8_0_Node1\n    litellm_params:\n      model: ollama/gemma3:12b-it-q8_0\n      api_base: http://api1.lan:11434\n    model_info:\n      supports_vision: True\n  - model_name: gemma3:12b-it-q8_0_Node2\n    litellm_params:\n      model: ollama/gemma3:12b-it-q8_0\n      api_base: http://api1.lan:11434\n    metadata:\n      supports_vision: True\n  - model_name: gemma3:12b-it-q8_0_Node3\n    litellm_params:\n      model: ollama/gemma3:12b-it-q8_0\n      api_base: http://api1.lan:11434\n      supports_vision: True\n```\n\nWhat ever I'm trying, the \"supports_vision: True\" parameter gets ignored. Neither do I get the feature shown as activated in the \"model hub\", nor am I able to push Images via OpenWebUI to the model. When directly pointing to the ollama endpoint, without LiteLLM in between, OpenWebUI works fine with visual models.\n\nWhat am I missing here?\n\nI'm running docker compose from the latest gitlab clone.",
      "state": "open",
      "author": "andrelohmann",
      "author_type": "User",
      "created_at": "2025-03-17T00:01:20Z",
      "updated_at": "2025-06-20T00:01:58Z",
      "closed_at": null,
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9297/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9297",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9297",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:26.538662",
      "comments": [
        {
          "author": "andrelohmann",
          "body": "I'm a step further. It works, if not using the provider ollama but openai instead.\n\n```\nmodel_list:\n  - model_name: gemma3:12b-it-q8_0_Node1\n    litellm_params:\n      model: openai/gemma3:12b-it-q8_0\n      api_base: http://api1.lan:11434\n    model_info:\n      supports_vision: True\n```\n",
          "created_at": "2025-03-21T08:45:26Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-20T00:01:58Z"
        }
      ]
    },
    {
      "issue_number": 9425,
      "title": "[Feature]: Show usage keys bounded by users and teams",
      "body": "### The Feature\n\nCurrently the users and teams filters only filter the selected key input options, make it so that they can filter the usage table\n\n### Motivation, pitch\n\nmore granularity\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "CakeCrusher",
      "author_type": "User",
      "created_at": "2025-03-21T00:21:08Z",
      "updated_at": "2025-06-20T00:01:51Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9425/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9425",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9425",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:26.784795",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-20T00:01:51Z"
        }
      ]
    },
    {
      "issue_number": 9429,
      "title": "Is there a unified way to set thinking token budget for Sonnet 3.7 across providers?",
      "body": "I appears that I need to set the thinking token budget differently if I am accessing Sonnet 3.7 via anthropic directly versus via open router. \n\n```python\nmessages=[{\"role\": \"user\", \"content\": \"Hi\"}]\nNUM_TOKENS=8192\n\nmodel = \"openrouter/anthropic/claude-3.7-sonnet\"\nprint()\nresponse = litellm.completion(\n    model,\n    messages,\n    reasoning = dict(max_tokens=NUM_TOKENS),\n)\nprint(response)\n\nmodel = \"anthropic/claude-3-7-sonnet-20250219\"\nprint()\nresponse = litellm.completion(\n    model,\n    messages,\n    max_tokens = 2 * NUM_TOKENS,\n    thinking = dict(type=\"enabled\", budget_tokens=NUM_TOKENS),\n)\nprint(response)\n```\n\nIs that correct?\n\nIs there a plan to try and unify this?",
      "state": "open",
      "author": "paul-gauthier",
      "author_type": "User",
      "created_at": "2025-03-21T02:25:22Z",
      "updated_at": "2025-06-20T00:01:49Z",
      "closed_at": null,
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9429/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9429",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9429",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:26.972014",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-20T00:01:48Z"
        }
      ]
    },
    {
      "issue_number": 9444,
      "title": "Authentication error when trying to use the admin panel",
      "body": "I'm installing open webui using docker and I needed LiteLLM for Proxy, so I cloned the repository using \"git clone https://github.com/BerriAI/litellm\" and then cd into folder, then used \"nano .env\" added a master key and salt key, saved and then used \"docker compose up -d\" \nI can access the website through localhost:4000 and I click \"LiteLLM Admin Panel on /ui\" which takes me to a separate page that prompts me to log in. When I type in admin and my master key I get the following error: \n{\"error\":{\"message\":\"Authentication Error, User not found, passed user_id=admin\",\"type\":\"auth_error\",\"param\":\"None\",\"code\":\"400\"}}\n\nI've tried different machines, RaspPi with Ubuntu and then also a Fresh Ubuntu VM on my computer incase it was an issue with ARM processors. Same exact result. I don't exactly know how to fix this and I've researched so much and can't find an answer that works for me. \n\n![Image](https://github.com/user-attachments/assets/c606521f-a83e-4f5d-8150-0cbc76bee038)\n![Image](https://github.com/user-attachments/assets/9308856a-8135-40a1-a59b-75d0f8bb40a8)",
      "state": "open",
      "author": "TheImaginaryCreator",
      "author_type": "User",
      "created_at": "2025-03-21T21:37:26Z",
      "updated_at": "2025-06-20T00:01:48Z",
      "closed_at": null,
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9444/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9444",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9444",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:27.184170",
      "comments": [
        {
          "author": "wagnerjt",
          "body": "Check out #9433 ",
          "created_at": "2025-03-21T22:14:29Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-20T00:01:47Z"
        }
      ]
    },
    {
      "issue_number": 11846,
      "title": "[Bug]: audio transcription cost is always 0",
      "body": "### What happened?\n\naudio transcription cost looks always 0\nLiteLLM calculate transcription cost depend on `audio_transcription_file_duration` which get from [here](https://github.com/BerriAI/litellm/blob/01af7fe1a848569c56741df99d4f76dde58a5af2/litellm/cost_calculator.py#L786-L788) looks always 0 (there is no duration attr in completion_response)\nI think we need to get the file duration from the actual file\n\nI used `gpt-4o-transcribe` model and below is the completion_response value\n```\nTranscriptionResponse(text='The quick brown fox jumped over the lazy dog.')\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "yeahyung",
      "author_type": "User",
      "created_at": "2025-06-18T11:26:32Z",
      "updated_at": "2025-06-19T23:47:37Z",
      "closed_at": null,
      "labels": [
        "bug",
        "spend tracking"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11846/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "krrishdholakia"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11846",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11846",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:27.348806",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Hey @yeahyung how can we get the duration for this response? \n\n\nOpen to ideas here\n",
          "created_at": "2025-06-19T23:47:25Z"
        }
      ]
    },
    {
      "issue_number": 11902,
      "title": "[Bug]: gemini-2.5-flash-lite-preview-06-17 model is missing for gemini provider",
      "body": "### What happened?\n\n`gemini-2.5-flash-lite-preview-06-17` pricing model doesnt exits for `gemini` provider in the https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\nas a result, we are having errors for cost calculation. this fix is essential for us.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.72.6\n\n### Twitter / LinkedIn details\n\n@fcakyon",
      "state": "closed",
      "author": "fcakyon",
      "author_type": "User",
      "created_at": "2025-06-19T22:15:33Z",
      "updated_at": "2025-06-19T23:38:19Z",
      "closed_at": "2025-06-19T23:38:19Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11902/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11902",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11902",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:27.522388",
      "comments": []
    },
    {
      "issue_number": 11900,
      "title": "[Bug]: gemini-2.5-flash price is outdated",
      "body": "### What happened?\n\nCurrent pricing: \n\n![Image](https://github.com/user-attachments/assets/bce4ae1f-ec14-409b-b162-6a31b603e683)\n\nIncorrect pricing:\nhttps://github.com/BerriAI/litellm/blob/649636b26bc7d4f2c67c37c7282e580b0355149b/model_prices_and_context_window.json#L6898-L6901\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.6\n\n### Twitter / LinkedIn details\n\n@fcakyon",
      "state": "closed",
      "author": "fcakyon",
      "author_type": "User",
      "created_at": "2025-06-19T22:08:24Z",
      "updated_at": "2025-06-19T23:38:18Z",
      "closed_at": "2025-06-19T23:38:18Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11900/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11900",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11900",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:27.522414",
      "comments": []
    },
    {
      "issue_number": 11882,
      "title": "[Bug]: Invalid tpm and rpm values for gemini 2.5 pro and flash models",
      "body": "### What happened?\n\nCommits https://github.com/BerriAI/litellm/commit/dfafa986ea371ba3c7338a56c65b022207cc33c0 and https://github.com/BerriAI/litellm/commit/b080220d02018912f3e237e8b741755ee293c1d2 set the `tpm` and `rpm` values for the gemini 2.5 pro and flash models to float values.\n\nThis is an invalid value for ModelGroupInfo\n\n```python\nclass ModelGroupInfo(BaseModel):\n   ...\n    tpm: Optional[int] = None\n    rpm: Optional[int] = None\n```\n\n### Relevant log output\n\n```shell\n{\"error\":{\"message\":\"2 validation errors for ModelGroupInfo\\ntpm\\n  Input should be a valid integer, got a number with a fractional part [type=int_from_float, input_value=8e-06, input_type=float]\\n    For further information visit https://errors.pydantic.dev/2.10/v/int_from_float\\nrpm\\n  Input should be a valid integer, got a number with a fractional part [type=int_from_float, input_value=1e-05, input_type=float]\\n    For further information visit https://errors.pydantic.dev/2.10/v/int_from_float\",\"type\":\"None\",\"param\":\"None\",\"code\":\"500\"}}\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nmain-v1.72.6.post1-nightly\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "adrianlzt",
      "author_type": "User",
      "created_at": "2025-06-19T12:02:37Z",
      "updated_at": "2025-06-19T21:59:59Z",
      "closed_at": "2025-06-19T21:59:59Z",
      "labels": [
        "bug",
        "urgent"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11882/reactions",
        "total_count": 3,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "krrishdholakia"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11882",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11882",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:27.522426",
      "comments": [
        {
          "author": "rotem925",
          "body": "Same issue here.",
          "created_at": "2025-06-19T14:24:48Z"
        },
        {
          "author": "krrishdholakia",
          "body": "My bad - i'll fix it asap and add a better test for this. ",
          "created_at": "2025-06-19T19:08:47Z"
        }
      ]
    },
    {
      "issue_number": 10149,
      "title": "[Bug]: Gemini models, error analyzing video",
      "body": "### What happened?\n\nI get an the error when analyzing a video with the models \"gemini/gemini-2.5-flash-preview-04-17\" and \"gemini/gemini-2.5-pro-preview-03-25\". I don't get the error when using \"gemini/gemini-2.0-flash\". I'm not sure why this is happening as the video is only 1 minute and 30 seconds so the context window should be big enough.\n\nHere is my code\n```\n        # Read the processed video file\n        video_bytes = Path(processed_video_path).read_bytes()\n        video_data_base64 = base64.b64encode(video_bytes).decode(\"utf-8\")\n        print(\"Video processed and converted to base64\")\n\n        model = get_model(\"gemini-2-5-pro\")\n\n        video_message = {\n            \"type\": \"image_url\",\n            \"image_url\": f\"data:video/mp4;base64,{video_data_base64}\"\n        }\n\n        response = completion(\n                model=model.model_name,\n                response_format=VideoAnalysis,\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": [\n                            {\n                                \"type\": \"text\",\n                                \"text\": video_analysis_prompt.main_system\n                            }\n                        ],\n                        \"cache_control\": {\"type\": \"ephemeral\"}\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\n                                \"type\": \"text\",\n                                \"text\": video_analysis_prompt.user_prompt\n                            },\n                            video_message\n                        ]\n                    }\n                ],\n                **model.extra_args\n                )\n```\n\n### Relevant log output\n\n```shell\nError analyzing video http://video.akamai.steamstatic.com/store_trailers/257098608/movie_max.mp4?t=1739556633: litellm.InternalServerError: VertexAIException InternalServerError - {\n  \"error\": {\n    \"code\": 500,\n    \"message\": \"An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\",\n    \"status\": \"INTERNAL\"\n  }\n}\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv.1.66.3\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "gego144",
      "author_type": "User",
      "created_at": "2025-04-19T00:29:09Z",
      "updated_at": "2025-06-19T21:59:46Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10149/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "colesmcintosh"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10149",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10149",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:27.718850",
      "comments": [
        {
          "author": "gego144",
          "body": "Ended up switching from LiteLLM to the google vertex API and the error went away, not sure what the root cause was here.",
          "created_at": "2025-04-28T22:59:00Z"
        },
        {
          "author": "azgo14",
          "body": "this is still a problem for AI Studio. pretty weird",
          "created_at": "2025-06-19T20:53:26Z"
        },
        {
          "author": "krrishdholakia",
          "body": "cc: @colesmcintosh ",
          "created_at": "2025-06-19T21:59:35Z"
        }
      ]
    },
    {
      "issue_number": 11878,
      "title": "[Bug]: web search error with responses API",
      "body": "### What happened?\n\n### 1. `/responses` with `web_search`\n\n```\ncurl --request POST \\\n  --url http://localhost/responses \\\n  --header 'Content-Type: application/json' \\\n  --header 'api-key: xxx' \\\n  --data '{\n\t\"model\": \"gemini-2.5-flash\",\n\t\"input\": \"what is the latest version of supabase python package and when was it released?\",\n\t\"tools\": [\n\t\t{\n\t\t\t\"type\": \"web_search_preview\",\n\t\t\t\"search_context_size\": \"low\"\n\t\t}\n\t]\n}'\n```\nreturns error:\n`{\n\t\"error\": {\n\t\t\"message\": \"litellm.APIConnectionError: 'name'\\nTraceback (most recent call last):\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/responses/main.py\\\", line 210, in responses\\n    return litellm_completion_transformation_handler.response_api_handler(\\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\\n        model=model,\\n        ^^^^^^^^^^^^\\n    ...<5 lines>...\\n        **kwargs,\\n        ^^^^^^^^^\\n    )\\n    ^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/responses/litellm_completion_transformation/handler.py\\\", line 42, in response_api_handler\\n    LiteLLMCompletionResponsesConfig.transform_responses_api_request_to_chat_completion_request(\\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\\n        model=model,\\n        ^^^^^^^^^^^^\\n    ...<4 lines>...\\n        **kwargs,\\n        ^^^^^^^^^\\n    )\\n    ^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/responses/litellm_completion_transformation/transformation.py\\\", line 118, in transform_responses_api_request_to_chat_completion_request\\n    \\\"tools\\\": LiteLLMCompletionResponsesConfig.transform_responses_api_tools_to_chat_completion_tools(\\n             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\\n        responses_api_request.get(\\\"tools\\\") or []  # type: ignore\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n    ),\\n    ^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/responses/litellm_completion_transformation/transformation.py\\\", line 455, in transform_responses_api_tools_to_chat_completion_tools\\n    name=tool[\\\"name\\\"],\\n         ~~~~^^^^^^^^\\nKeyError: 'name'\\n\",\n\t\t\"type\": null,\n\t\t\"param\": null,\n\t\t\"code\": \"500\"\n\t}\n}`\n\n### 2. `/responses` without `web_search`\n```\ncurl --request POST \\\n  --url http://localhost/responses \\\n  --header 'Content-Type: application/json' \\\n  --header 'api-key: xxx' \\\n  --data '{\n\t\"model\": \"gemini-2.5-flash\",\n\t\"input\": \"what is the latest version of supabase python package and when was it released?\"\n}'\n```\nworks fine:\n```\n\"text\": \"As of my last update, the latest version of the `supabase-py` Python package is:\\n\\n*   **Version:** **2.5.0**\\n*   **Release Date:** **May 23, 2024**\\n\\nYou can always check the most up-to-date information directly on the PyPI page for `supabase-py`: [https://pypi.org/project/supabase-py/](https://pypi.org/project/supabase-py/)\"\n```\n\n### 3. `/chat/completions` with `web_search`\n```\ncurl --request POST \\\n  --url 'http://localhost/chat/completions?api-version=2025-04-01-preview' \\\n  --header 'Authorization: Bearer xxx' \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n\t\"model\": \"gemini-2.5-flash\",\n\t\"messages\": [\n\t\t{\n\t\t\t\"role\": \"user\",\n\t\t\t\"content\": \"what is the latest version of supabase python package and when was it released?\"\n\t\t}\n\t],\n\t\"web_search_options\": {\n\t\t\"search_context_size\": \"medium\"\n\t}\n}'\n```\nworks fine:\n```\n\"message\": {\n\t\t\t\t\"content\": \"The latest version of the Supabase Python package is 2.15.3. It was released on June 10, 2025.\",\n```\n\nSomething seems not right with `/responses` when using web search tool\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.72.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "taralika",
      "author_type": "User",
      "created_at": "2025-06-19T06:10:53Z",
      "updated_at": "2025-06-19T21:11:36Z",
      "closed_at": "2025-06-19T21:11:36Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11878/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "ishaan-jaff"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11878",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11878",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:27.916766",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "fixing ",
          "created_at": "2025-06-19T20:11:55Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "Fixed here: https://github.com/BerriAI/litellm/pull/11894 ",
          "created_at": "2025-06-19T20:15:18Z"
        }
      ]
    },
    {
      "issue_number": 9881,
      "title": "[Bug]: logfire/opentelemetry logging callbacks are filtered on async completions",
      "body": "### What happened?\n\nI'm trying to get my litellm `acompletion` calls to go to logfire, but I've only managed to make it work with sync calls. Here is a script that should be able to reproduce:\n\n```python\nimport asyncio\nimport os\n\nimport litellm\nos.environ[\"LOGFIRE_TOKEN\"] = ...\nos.environ[\"OPENAI_API_KEY\"] = ...\n\n# I've tried with every combo of the below lists\nlitellm.callbacks = [\"logfire\"]\nlitellm._turn_on_debug()\nlitellm.success_callback = [\"logfire\"]\nlitellm.failure_callback = [\"logfire\"]\n\n\nasync def main():\n    response = await litellm.acompletion(\n        model=\"gpt-4o-mini\", messages=[{\"role\": \"user\", \"content\": \"Hello, world!\"}]\n    )\n    print(response)\n\n\nasyncio.run(main())\n\n```\n\nIs there any way this is possible? It seems like the issue is that the OpenTelemetry logger is being filtered out before it can be used, but only for async requests.\n\n### Relevant log output\n\n```shell\n09:28:25 - LiteLLM:DEBUG: opentelemetry.py:817 - OpenTelemetry: intiializing http exporter. Value of OTEL_EXPORTER: otlp_http\nOverriding of current TracerProvider is not allowed\n...\n09:28:26 - LiteLLM:DEBUG: utils.py:301 - Initialized litellm callbacks, Async Success Callbacks: [<litellm.integrations.opentelemetry.OpenTelemetry object at 0x113e1b440>, <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x12121a6f0>]\n09:28:26 - LiteLLM:DEBUG: logfire_logger.py:30 - in init logfire logger\n09:28:26 - LiteLLM:DEBUG: litellm_logging.py:374 - self.optional_params: {}\n...\n09:28:26 - LiteLLM:DEBUG: utils.py:301 - Final returned optional params: {'extra_body': {}}\n09:28:26 - LiteLLM:DEBUG: litellm_logging.py:374 - self.optional_params: {'extra_body': {}}\n09:28:26 - LiteLLM:DEBUG: litellm_logging.py:611 - \n\nPOST Request Sent from LiteLLM:\n...\n\n\nLogfire project URL: *my-url*\n09:28:26 - LiteLLM:DEBUG: utils.py:301 - RAW RESPONSE:\n...\n\n\n09:28:26 - LiteLLM:DEBUG: litellm_logging.py:2049 - Filtered callbacks: []\n...\n09:28:26 - LiteLLM:DEBUG: litellm_logging.py:2049 - Filtered callbacks: []\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.61.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "zsilver1",
      "author_type": "User",
      "created_at": "2025-04-10T13:49:01Z",
      "updated_at": "2025-06-19T19:17:41Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9881/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9881",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9881",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:28.129331",
      "comments": [
        {
          "author": "n1lanjan",
          "body": "It seems that the async logging is not fully implemented for logfire_logger here:\n- https://github.com/BerriAI/litellm/blob/main/litellm/integrations/logfire_logger.py\n\nThe logger does not fallback to use the sync logging since it gets filtered due to this line here:\n- https://github.com/BerriAI/lit",
          "created_at": "2025-04-10T21:17:01Z"
        },
        {
          "author": "zsilver1",
          "body": "Thanks for taking a look, I can submit a PR for it, but not really sure what the implications of this are? Don't want to break anything. Do you know why these are excluded from async calls in the first place?",
          "created_at": "2025-04-11T14:07:58Z"
        },
        {
          "author": "umoh1",
          "body": "@zsilver1 Hey! There's another [issue](https://github.com/BerriAI/litellm/issues/8842) related to callbacks being filtered on async completions on versions `> 1.57.1`. Would you mind commenting/reacting on this issue? Trying to drive some traction to it so the maintainers can prioritize it!",
          "created_at": "2025-06-19T19:17:41Z"
        }
      ]
    },
    {
      "issue_number": 11841,
      "title": "Fix \"azure/o3\" entry in \"model_prices_and_context_window.json\"",
      "body": "https://github.com/BerriAI/litellm/blob/01af7fe1a848569c56741df99d4f76dde58a5af2/model_prices_and_context_window.json#L2159-L2187\n\nWe also need to update [model_prices_and_context_window_backup.json](https://github.com/BerriAI/litellm/blob/01af7fe1a848569c56741df99d4f76dde58a5af2/litellm/model_prices_and_context_window_backup.json).\n",
      "state": "open",
      "author": "uje-m",
      "author_type": "User",
      "created_at": "2025-06-18T08:47:16Z",
      "updated_at": "2025-06-19T19:15:06Z",
      "closed_at": null,
      "labels": [
        "awaiting: user response"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11841/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11841",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11841",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:28.309667",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "what is the error @uje-m ",
          "created_at": "2025-06-19T19:14:51Z"
        }
      ]
    },
    {
      "issue_number": 11844,
      "title": "[Bug]: After upgrading to 1.72.2-stable, the Azure AI DeepSeek model is not working properly.",
      "body": "### What happened?\n\n\nAfter upgrading to 1.72.2-stable, the following issue occurred when using the Azure AI DeepSeek model:\n\n```\nlitellm.BadRequestError: Azure_aiException - {\"error\":{\"code\":\"Invalid input\",\"status\":422,\"message\":\"invalid input error\",\"details\":[{\"type\":\"model_attributes_type\",\"loc\":[\"body\"],\"msg\":\"Input should be a valid dictionary or object to extract fields from\",\"input\":\"{\\\"model\\\": \\\"deepseek-v3-0324-deployment-0426\\\", \\\"messages\\\": [{\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are a professional Traditional Chinese (Taiwan) Language native translator who needs to fluently translate text into Traditional Chinese (Taiwan) Language.\\\\n\\\\n## Translation Rules\\\\n1. Output only the translated content, without explanations or additional content (such as \\\\\\\"Here's the translation:\\\\\\\" or \\\\\\\"Translation as follows:\\\\\\\")\\\\n2. The returned translation must maintain exactly the same number of paragraphs and format as the original text\\\\n3. If the text contains HTML tags, consider where the tags should be placed in the translation while maintaining fluency\\\\n4. For content that should not be translated (such as proper nouns, code, etc.), keep the original text\\\\n\\\\n## Context Awareness\\\\nDocument Metadata:\\\\nTitle: \\\\u300aBuild Generative AI Applications with Foundation Models \\\\u2013 Amazon Bedrock Pricing \\\\u2013 AWS\\\\u300b\\\\n\\\\n## Input-Output Format Examples\\\\n\\\\n### Input Example:\\\\nParagraph A\\\\n\\\\n%%\\\\n\\\\nParagraph B\\\\n\\\\n%%\\\\n\\\\nParagraph C\\\\n\\\\n%%\\\\n\\\\nParagraph D\\\\n\\\\n### Output Example:\\\\nTranslation A\\\\n\\\\n%%\\\\n\\\\nTranslation B\\\\n\\\\n%%\\\\n\\\\nTranslation C\\\\n\\\\n%%\\\\n\\\\nTranslation D\\\"}, {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Translate to Traditional Chinese (Taiwan) Language:\\\\n\\\\nAnthropic \\\\n\\\\n%%\\\\n\\\\nCohere \\\\n\\\\n%%\\\\n\\\\nDeepSeek \\\\n\\\\n%%\\\\n\\\\nLuma AI \\\"}], \\\"temperature\\\": 0, \\\"stream\\\": false}\"}]}}. Received Model Group=deepseek-v3\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.2-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "kkc-tonywu",
      "author_type": "User",
      "created_at": "2025-06-18T10:37:26Z",
      "updated_at": "2025-06-19T19:12:45Z",
      "closed_at": "2025-06-19T19:12:45Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11844/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11844",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11844",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:28.487932",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Fixed with https://github.com/BerriAI/litellm/pull/11872",
          "created_at": "2025-06-19T19:12:45Z"
        }
      ]
    },
    {
      "issue_number": 11847,
      "title": "[Bug]: RecursionError when using gpt-4o-realtime-preview with WebSocket and fallback to self in model_group",
      "body": "### What happened?\n\n### Summary\n\nWhen using the gpt-4o-realtime-preview model with mode: realtime (WebSocket) in LiteLLM v1.65.4, we observe a significant slowdown in LiteLLM performance.\n\n\n\n\n### Relevant log output\n\n```shell\n11:29:14 - LiteLLM:DEBUG: utils.py:324 - Request to litellm:\n11:29:14 - LiteLLM:DEBUG: utils.py:324 - litellm._arealtime(api_key='<api_key>', api_base='<api_base>, use_in_pass_through=False, merge_reasoning_content_in_choices=False, model='gpt-4o-realtime-preview', caching=False, websocket=<starlette.websockets.WebSocket object at 0x7f0709a8b9b0>, litellm_trace_id='343ffd29-b304-40a5-b5d8-bbca6c268732', metadata={'model_group': 'gpt-4o-realtime-preview', 'caching_groups': None, 'model_group_size': 1, 'previous_models': [{'exception_type': 'RecursionError', 'exception_string': 'maximum recursion depth exceeded', 'websocket': <starlette.websockets.WebSocket object at 0x7f07088f5d30>, 'litellm_trace_id': '33aa63fc-6a3f-4735-ba9b-1c2962e55f2e', 'metadata': {'model_group': 'gpt-4o-realtime-preview', 'caching_groups': None, 'model_group_size': 1}, 'timeout': None, 'max_retries': 0, 'model': 'gpt-4o-realtime-preview'}, {'exception_type': 'RecursionError', 'exception_string': 'maximum recursion depth exceeded', 'websocket': <starlette.websockets.WebSocket object at 0x7f06eef0d250>, 'litellm_trace_id': 'deb616bf-078c-4b2c-b998-e980c86a8750', 'metadata': {'model_group': 'gpt-4o-realtime-preview', 'caching_groups': None, 'model_group_size': 1}, 'timeout': None, 'max_retries': 0, 'model': 'gpt-4o-realtime-preview'}, {'exception_type': 'RecursionError', 'exception_string': 'maximum recursion depth exceeded', 'websocket': <starlette.websockets.WebSocket object at 0x7f07088f5d30>, 'litellm_trace_id': '33aa63fc-6a3f-4735-ba9b-1c2962e55f2e', 'metadata': {'model_group': 'gpt-4o-realtime-preview', 'caching_groups': None, 'model_group_size': 1}, 'timeout': None, 'max_retries': 0, 'model': 'gpt-4o-realtime-preview'}, {'exception_type': 'RecursionError', 'exception_string': 'maximum recursion depth exceeded', 'websocket': <starlette.websockets.WebSocket object at 0x7f06eef0d250>, 'litellm_trace_id': 'deb616bf-078c-4b2c-b998-e980c86a8750', 'metadata': {'model_group': 'gpt-4o-realtime-preview', 'caching_groups': None, 'model_group_size': 1}, 'timeout': None, 'max_retries': 0, 'model': 'gpt-4o-realtime-preview'}]}, timeout=None, max_retries=0, messages=[{'role': 'user', 'content': 'dummy-text'}], num_retries=2)\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.65.4\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "azat-sariyev",
      "author_type": "User",
      "created_at": "2025-06-18T11:44:33Z",
      "updated_at": "2025-06-19T19:11:41Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11847/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 1,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11847",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11847",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:28.654977",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Can you share steps to repro? @azat-sariyev \n\nconfig.yaml + clientside script would help! ",
          "created_at": "2025-06-19T19:11:28Z"
        }
      ]
    },
    {
      "issue_number": 11864,
      "title": "[Bug]: Issue: Prisma NotConnectedError in Restricted GKE Environment with LiteLLM Image",
      "body": "### What happened?\n\nHi Team! Hope you are doing great! \n\n## Summary\n\nI'm experiencing a `NotConnectedError` from Prisma when deploying LiteLLM in a restricted Google Kubernetes Engine (GKE) environment. The issue occurs specifically with the main LiteLLM image (`ghcr.io/berriai/litellm:main-latest`) but does not occur with the `non_root` image variant. The error happens during application startup when Prisma attempts to establish a connection to the query engine.\n\n## Environment Details\n\n- **Kubernetes Environment**: Google Kubernetes Engine (GKE)\n- **LiteLLM Image**: `ghcr.io/berriai/litellm:main-latest` (failing)\n- **Working Image**: `non_root` variant (same version)\n- **Database**: External PostgreSQL (<hidden>:5432)\n- **Redis**: Internal Redis instance\n- **Deployment Method**: Helm chart deployment\n\n## Error Details\n\n### Error Message\n```\nprisma.engine.errors.NotConnectedError: Not connected to the query engine\n```\n\n### Full Stack Trace\nThe error occurs in the following sequence:\n1. `proxy_startup_event` in `proxy_server.py:581`\n2. `_setup_prisma_client` in `proxy_server.py:3461`\n3. `health_check` in `utils.py:2430`\n4. `query_raw` in `prisma/client.py:457`\n5. `_execute` in `prisma/client.py:561`\n6. `query` in `prisma/engine/query.py:244`\n7. `request` in `prisma/engine/http.py:97`\n\n### Application Startup Logs\n```\n2025-06-18 17:58:20,676 - litellm_proxy_extras - INFO - Running prisma migrate deploy\n2025-06-18 17:58:26,928 - litellm_proxy_extras - INFO - prisma migrate deploy stdout: Prisma schema loaded from schema.prisma\nDatasource \"client\": PostgreSQL database \"litellm\", schema \"public\" at \"<hidden>:5432\"\n\n24 migrations found in prisma/migrations\n\n\nNo pending migrations to apply.\n\n2025-06-18 17:58:26,928 - litellm_proxy_extras - INFO - prisma migrate deploy completed\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nERROR:    Traceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/starlette/routing.py\", line 693, in lifespan\n    async with self.lifespan_context(app) as maybe_state:\n               ~~~~~~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/proxy_server.py\", line 581, in proxy_startup_event\n    prisma_client = await ProxyStartupEvent._setup_prisma_client(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<3 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/proxy_server.py\", line 3461, in _setup_prisma_client\n    PrismaDBExceptionHandler.handle_db_exception(e)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/db/exception_handler.py\", line 61, in handle_db_exception\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/proxy_server.py\", line 3458, in _setup_prisma_client\n    await prisma_client.health_check()\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/utils.py\", line 2430, in health_check\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/utils.py\", line 2412, in health_check\n    response = await self.db.query_raw(sql_query)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/prisma/client.py\", line 457, in query_raw\n    resp = await self._execute(\n           ^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/prisma/client.py\", line 561, in _execute\n    return await self._engine.query(builder.build(), tx_id=self._tx_id)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/prisma/engine/query.py\", line 244, in query\n    return await self.request(\n           ^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/prisma/engine/http.py\", line 97, in request\n    raise errors.NotConnectedError('Not connected to the query engine')\nprisma.engine.errors.NotConnectedError: Not connected to the query engine\n\n17:58:29 - LiteLLM Proxy:ERROR: utils.py:2442 - Error getting LiteLLM_SpendLogs row count: Not connected to the query engine\nERROR:    Application startup failed. Exiting.\n\n#------------------------------------------------------------#\n#                                                            #\n#           'It would help me if you could add...'            #\n#        https://github.com/BerriAI/litellm/issues/new        #\n#                                                            #\n#------------------------------------------------------------#\n\n Thank you for using LiteLLM! - Krrish & Ishaan\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n\n\n```\n\n## Configuration Details\n\n### Deployment Configuration\n```yaml\n# Container Configuration\nimage: ghcr.io/berriai/litellm:main-latest\ncommand: [\"/bin/sh\"]\nargs: [\"-c\", \"exec litellm --host 0.0.0.0 --port 4000\"]\n\n# Security Context\nsecurityContext:\n  allowPrivilegeEscalation: false\n  capabilities:\n    drop: [\"all\"]\n  readOnlyRootFilesystem: true\n  runAsGroup: 1000\n  runAsNonRoot: true\n  runAsUser: 1000\n\n# Volume Mounts\nvolumeMounts:\n  - name: npm-cache-dir\n    mountPath: /.npm\n  - name: tmp-dir\n    mountPath: /tmp\n  - name: prisma-cache\n    mountPath: /.cache\n\n# Volumes\nvolumes:\n  - name: npm-cache-dir\n    emptyDir: {}\n  - name: prisma-cache\n    emptyDir: {}\n  - name: tmp-dir\n    emptyDir: {}\n```\n\n### Environment Variables (ConfigMap)\n```yaml\ndata:\n  PORT: \"4000\"\n  STORE_MODEL_IN_DB: \"True\"\n  LITELLM_MODE: \"PRODUCTION\"\n  LITELLM_LOG_LEVEL: \"INFO\"\n  USE_PRISMA_MIGRATE: \"True\"\n  LITELLM_LOCAL_MODEL_COST_MAP: \"True\"\n  LITELLM_MIGRATION_DIR: \"/tmp/prisma/migrations\"\n  XDG_CACHE_HOME: \"/tmp/prisma-cache\"\n  PRISMA_HOME: \"/tmp/prisma-cache\"\n  PRISMA_BINARY_CACHE_DIR: \"/tmp/prisma-cache\"\n  PRISMA_CLI_CACHE_DIR: \"/tmp/prisma-cache\"\n  DATABASE_HOST: \"<hidden>\"\n  DATABASE_USERNAME: \"litellm\"\n  DATABASE_NAME: \"litellm\"\n  DATABASE_PORT: \"5432\"\n  DATABASE_PASSWORD: \"[REDACTED]\"\n  DATABASE_URL: \"postgresql://litellm:[REDACTED]@<hidden>:5432/litellm\"\n  REDIS_HOST: \"redis\"\n  REDIS_PORT: \"6379\"\n  REDIS_DB: \"0\"\n  REDIS_URL: \"redis://:[REDACTED]@redis:6379/0\"\n  REDIS_PASSWORD: \"[REDACTED]\"\n  SERVER_ROOT_PATH: \"/litellm\"\n```\n\n### Database Configuration\n- **Host**: <hidden>\n- **Port**: 5432\n- **Database**: litellm\n- **User**: litellm\n- **SSL Mode**: require\n- **Connection String**: `postgresql://litellm:[PASSWORD]@<hidden>:5432/litellm`\n\n## Testing and Validation\n\n### Database Connectivity Test\nI tested database connectivity using a debug pod with the same network configuration:\n\n```bash\n# Test PostgreSQL connection\npsql \"postgresql://litellm:[PASSWORD]@<hidden>:5432/litellm\" -c \"SELECT 1;\"\n# Result: Connection successful\n```\n\n### Prisma Migration Test\nThe Prisma migrations complete successfully:\n```\nPrisma schema loaded from schema.prisma\nDatasource \"client\": PostgreSQL database \"litellm\", schema \"public\" at \"<hidden>:5432\"\n24 migrations found in prisma/migrations\nNo pending migrations to apply.\n```\n\n### Working Configuration\nThe same configuration works fine with the `non_root` image variant, confirming that:\n1. Database credentials are correct\n2. Network connectivity is functional\n3. Prisma schema and migrations are valid\n4. Environment variables are properly configured\n\n## Key Observations\n\n1. **Image-Specific Issue**: The error only occurs with the main LiteLLM image, not the `non_root` variant\n2. **Prisma Query Engine**: The error suggests Prisma's query engine fails to start or connect\n3. **Restricted Environment**: The issue appears to be related to the restricted GKE environment\n4. **Security Context**: The container runs with non-root user and read-only filesystem\n5. **Cache Directories**: Prisma cache directories are mounted to `/tmp` and `/.cache`\n\n## Potential Root Causes\n\n1. **File System Permissions**: The read-only filesystem might be preventing Prisma from writing temporary files\n2. **User Permissions**: The non-root user (UID 1000) might not have sufficient permissions\n3. **Binary Execution**: Prisma binaries might not be executable in the restricted environment\n4. **Cache Directory Access**: Issues with writing to the mounted cache directories\n5. **Query Engine Process**: Prisma's query engine process might fail to start due to security restrictions\n\n## Request for Assistance\n\nI would appreciate guidance on:\n1. Whether there are known issues with the main LiteLLM image in restricted Kubernetes environments\n2. Alternative approaches to handle Prisma in restricted environments\n5. Whether this is a known limitation that requires using the `non_root` image\n\n## Additional Context\n\n- The `non_root` image works perfectly with the same configuration\n- Database connectivity is confirmed working\n- Prisma migrations complete successfully\n- The issue appears to be specific to the query engine initialization phase\n\n## Environment Information\n\n- **Kubernetes Version**: GKE 1.28+\n- **Node OS**: Container-Optimized OS\n- **Network Policies**: Standard GKE network policies\n- **Pod Security Standards**: Restricted\n- **Image Pull Policy**: IfNotPresent\n\n## Bonus: same configuration only chaning the image to litellm-non_root: \n```bash\n‚ûú k logs -f carto-notdelete-litellm-584cf895cf-dl9x8                                       \n2025-06-18 18:12:26,460 - litellm_proxy_extras - INFO - Running prisma migrate deploy\n2025-06-18 18:12:39,692 - litellm_proxy_extras - INFO - prisma migrate deploy stdout: Installing Prisma CLI\nPrisma schema loaded from schema.prisma\nDatasource \"client\": PostgreSQL database \"litellm\", schema \"public\" at \"<hidden>:5432\"\n\n24 migrations found in prisma/migrations\n\n\nNo pending migrations to apply.\n\n2025-06-18 18:12:39,692 - litellm_proxy_extras - INFO - prisma migrate deploy completed\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:4000 (Press CTRL+C to quit)\n\n#------------------------------------------------------------#\n#                                                            #\n#              'I don't like how this works...'               #\n#        https://github.com/BerriAI/litellm/issues/new        #\n#                                                            #\n#------------------------------------------------------------#\n\n Thank you for using LiteLLM! - Krrish & Ishaan\n\n\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n\n\nINFO:     10.2.0.49:41234 - \"GET /health/liveliness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:41236 - \"GET /health/readiness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:55852 - \"GET /health/liveliness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:55854 - \"GET /health/readiness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:46420 - \"GET /health/liveliness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:46430 - \"GET /health/readiness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:54730 - \"GET /health/liveliness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:54722 - \"GET /health/readiness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:45852 - \"GET /health/liveliness HTTP/1.1\" 200 OK\nINFO:     10.2.0.49:45844 - \"GET /health/readiness HTTP/1.1\" 200 OK\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nmain-latest\n\n### Twitter / LinkedIn details\n\n@mdiloreto",
      "state": "closed",
      "author": "mateo-di",
      "author_type": "User",
      "created_at": "2025-06-18T18:17:45Z",
      "updated_at": "2025-06-19T19:10:42Z",
      "closed_at": "2025-06-19T19:10:41Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11864/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11864",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11864",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:28.842835",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "This isn't a bug. If non-root works, then that implies your operating in an environment where network requests aren't allowed. \n\nNon-root exists for that case. ",
          "created_at": "2025-06-19T19:10:42Z"
        }
      ]
    },
    {
      "issue_number": 8842,
      "title": "[Bug]: Router's async completion don't trigger CustomLogger callbacks",
      "body": "The router's `.acompletion` method does not correctly call the `CustomLogger` hooks. Here is a MRE\n\n```python\nimport asyncio\n\nimport litellm\nfrom litellm.integrations.custom_logger import CustomLogger\n\n\nclass Handler(CustomLogger):\n    def log_pre_api_call(self, model, messages, kwargs):\n        print(f\"Pre-API Call\")\n\n    def log_post_api_call(self, kwargs, response_obj, start_time, end_time):\n        print(f\"Post-API Call\")\n\n    def log_success_event(self, kwargs, response_obj, start_time, end_time):\n        print(f\"On Success\")\n\n    def log_failure_event(self, kwargs, response_obj, start_time, end_time):\n        print(f\"On Failure\")\n\n    async def async_log_success_event(self, kwargs, response_obj, start_time, end_time):\n        print(f\"On Async Success\")\n\n    async def async_log_failure_event(self, kwargs, response_obj, start_time, end_time):\n        print(f\"On Async Failure\")\n\n\nrouter = litellm.Router(\n    model_list=[\n        {\n            \"model_name\": \"default\",\n            \"litellm_params\": {\"model\": \"mistral/mistral-large-latest\"},\n        },\n    ],\n    routing_strategy=\"usage-based-routing-v2\",\n)\n\n\ndef sync_send_request():\n    response = router.completion(\n        model=\"default\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"Hello world!\"},\n        ],\n    )\n    return response\n\n\nasync def async_send_request():\n    response = await router.acompletion(\n        model=\"default\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"Hello world!\"},\n        ],\n    )\n    return response\n\n\nif __name__ == \"__main__\":\n    litellm.callbacks.append(Handler())\n    # Synchronous call triggers the hooks\n    sync_send_request()\n\n    # Asynchronous calls don't trigger the success/failure hooks\n    asyncio.run(async_send_request())\n```\n\nRunning the above example prints the following:\n\n```bash\nPre-API Call\nPost-API Call\nOn Success\nPre-API Call\nPost-API Call\n```\n\nThe asynchronous call did not print \"On Async Success\".\n\n### Relevant log output\n\n```shell\n15:53:57 - LiteLLM Router:DEBUG: client_initalization_utils.py:468 - Initializing OpenAI Client for mistral/mistral-large-latest, Api Base:https://api.mistral.ai/v1, Api Key:qpITD0cC***************\n15:53:57 - LiteLLM Router:DEBUG: router.py:4132 - \nInitialized Model List ['default']\n15:53:57 - LiteLLM Router:INFO: router.py:667 - Routing strategy: usage-based-routing-v2\n15:53:57 - LiteLLM Router:DEBUG: router.py:519 - Intialized router with Routing strategy: usage-based-routing-v2\n\nRouting enable_pre_call_checks: False\n\nRouting fallbacks: None\n\nRouting content fallbacks: None\n\nRouting context window fallbacks: None\n\nRouter Redis Caching=None\n\n15:53:57 - LiteLLM Router:DEBUG: router.py:757 - router.completion(model=default,..)\n15:53:57 - LiteLLM Router:DEBUG: router.py:3135 - Inside async function with retries.\n15:53:57 - LiteLLM Router:DEBUG: router.py:3155 - async function w/ retries: original_function - <bound method Router._completion of <litellm.router.Router object at 0x105689850>>, num_retries - 2\n15:53:57 - LiteLLM Router:DEBUG: router.py:5522 - initial list of deployments: [{'model_name': 'default', 'litellm_params': {'use_in_pass_through': False, 'model': 'mistral/mistral-large-latest'}, 'model_info': {'id': '8fa585dbc2a029ff60c701d27aae5b6b622782630011fd9d3e2610c17421274c', 'db_model': False}}]\n15:53:57 - LiteLLM Router:DEBUG: router.py:5865 - cooldown deployments: []\n15:53:57 - LiteLLM Router:DEBUG: lowest_tpm_rpm_v2.py:566 - get_available_deployments - Usage Based. model_group: default, healthy_deployments: [{'model_name': 'default', 'litellm_params': {'use_in_pass_through': False, 'model': 'mistral/mistral-large-latest'}, 'model_info': {'id': '8fa585dbc2a029ff60c701d27aae5b6b622782630011fd9d3e2610c17421274c', 'db_model': False}}]\n15:53:57 - LiteLLM Router:DEBUG: lowest_tpm_rpm_v2.py:403 - input_tokens=10\n15:53:57 - LiteLLM Router:INFO: router.py:5845 - get_available_deployment for model: default, Selected deployment: {'model_name': 'default', 'litellm_params': {'use_in_pass_through': False, 'model': 'mistral/mistral-large-latest'}, 'model_info': {'id': '8fa585dbc2a029ff60c701d27aae5b6b622782630011fd9d3e2610c17421274c', 'db_model': False}} for model: default\nPre-API Call\nPost-API Call\nOn Success\n15:53:59 - LiteLLM Router:INFO: router.py:811 - litellm.completion(model=mistral/mistral-large-latest) 200 OK\n15:53:59 - LiteLLM Router:DEBUG: router.py:2895 - Async Response: ModelResponse(id='54d141126f2a4fa897c925773bb4eddc', created=1740581637, model='mistral/mistral-large-latest', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Hello! Nice to meet you. How are you today? Let's have a friendly conversation. üòä How about I share an interesting fact or a simple joke to start? Here it is:\\n\\nInteresting fact: Sea otters hold hands when they sleep to keep from drifting apart.\\n\\nOr, if you prefer a joke:\\n\\nWhat do you call fake spaghetti? An impasta.\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}))], usage=Usage(completion_tokens=88, prompt_tokens=6, total_tokens=94, completion_tokens_details=None, prompt_tokens_details=None), service_tier=None)\n15:53:59 - LiteLLM Router:DEBUG: router.py:3135 - Inside async function with retries.\n15:53:59 - LiteLLM Router:DEBUG: router.py:3155 - async function w/ retries: original_function - <bound method Router._acompletion of <litellm.router.Router object at 0x105689850>>, num_retries - 2\n15:53:59 - LiteLLM Router:DEBUG: router.py:924 - Inside _acompletion()- model: default; kwargs: {'stream': False, 'litellm_trace_id': '0babbe23-0b58-4f8e-95da-695c3609b74c', 'metadata': {'model_group': 'default', 'model_group_size': 1}}\n15:53:59 - LiteLLM Router:DEBUG: router.py:5522 - initial list of deployments: [{'model_name': 'default', 'litellm_params': {'use_in_pass_through': False, 'model': 'mistral/mistral-large-latest'}, 'model_info': {'id': '8fa585dbc2a029ff60c701d27aae5b6b622782630011fd9d3e2610c17421274c', 'db_model': False}}]\n15:53:59 - LiteLLM Router:DEBUG: cooldown_handlers.py:310 - retrieve cooldown models: []\n15:53:59 - LiteLLM Router:DEBUG: router.py:5583 - async cooldown deployments: []\n15:53:59 - LiteLLM Router:DEBUG: router.py:5586 - cooldown_deployments: []\n15:53:59 - LiteLLM Router:DEBUG: router.py:5865 - cooldown deployments: []\n15:53:59 - LiteLLM Router:DEBUG: lowest_tpm_rpm_v2.py:447 - get_available_deployments - Usage Based. model_group: default, healthy_deployments: [{'model_name': 'default', 'litellm_params': {'use_in_pass_through': False, 'model': 'mistral/mistral-large-latest'}, 'model_info': {'id': '8fa585dbc2a029ff60c701d27aae5b6b622782630011fd9d3e2610c17421274c', 'db_model': False}}]\n15:53:59 - LiteLLM Router:DEBUG: lowest_tpm_rpm_v2.py:403 - input_tokens=10\n15:53:59 - LiteLLM Router:INFO: router.py:5689 - get_available_deployment for model: default, Selected deployment: {'model_name': 'default', 'litellm_params': {'use_in_pass_through': False, 'model': 'mistral/mistral-large-latest'}, 'model_info': {'id': '8fa585dbc2a029ff60c701d27aae5b6b622782630011fd9d3e2610c17421274c', 'db_model': False}} for model: default\nPre-API Call\nPost-API Call\n15:53:59 - LiteLLM Router:INFO: router.py:1020 - litellm.acompletion(model=mistral/mistral-large-latest) 200 OK\n15:53:59 - LiteLLM Router:DEBUG: router.py:2895 - Async Response: ModelResponse(id='da34d203308d41b39b271c768997c925', created=1740581639, model='mistral/mistral-large-latest', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Hello! How can I assist you today? Let's have a friendly conversation.  How are you doing?\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}))], usage=Usage(completion_tokens=23, prompt_tokens=6, total_tokens=29, completion_tokens_details=None, prompt_tokens_details=None), service_tier=None)\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.61.16\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "bilelomrani1",
      "author_type": "User",
      "created_at": "2025-02-26T14:56:14Z",
      "updated_at": "2025-06-19T19:10:41Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8842/reactions",
        "total_count": 3,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8842",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8842",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:29.006876",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "Can you try `await asyncio.sleep(1)` and check if the callback runs ",
          "created_at": "2025-02-26T15:18:52Z"
        },
        {
          "author": "bilelomrani1",
          "body": "I double checked with `await asyncio.sleep(1000)` just to be sure, the callback does not seem to be run.",
          "created_at": "2025-02-26T15:37:12Z"
        },
        {
          "author": "bilelomrani1",
          "body": "The callbacks run correctly on 1.57.1, but starting from 1.57.2, they no longer seem to be called properly. I'll try to bisect the exact faulty commit if that helps.",
          "created_at": "2025-02-26T16:13:05Z"
        },
        {
          "author": "bilelomrani1",
          "body": "The bug seems to have been introduced by e8ed40a27b11b5ad0f0820352dc205df2f7dd5e5 ",
          "created_at": "2025-02-26T16:29:44Z"
        },
        {
          "author": "dhimmel",
          "body": "With litellm 1.66.0, I'm also unable to get async callbacks from a `litellm.CustomLogger` to trigger and this is with `litellm.acompletion` (no custom router).\n\nIntesting log lines when turned to DEBUG:\n\n```\nDEBUG    LiteLLM:utils.py:311 Initialized litellm callbacks, Async Success Callbacks: [<src.",
          "created_at": "2025-04-14T21:46:14Z"
        }
      ]
    },
    {
      "issue_number": 11837,
      "title": "[Bug]: watsonx custom deployment fails with \"model_id cannot be specified\" error",
      "body": "### What happened?\n\n## Description\nWhen using LiteLLM with watsonx custom deployed foundation models, the request fails with a 400 Bad Request error. The error indicates that `model_id` or `model` parameters are being incorrectly included in the request body when calling deployment endpoints.\n\n## Error Details\n```\nBadRequestError: litellm.BadRequestError: watsonxException - {\"errors\":[{\"code\":\"json_validation_error\",\"message\":\"Json document validation error: 'model_id' or 'model' cannot be specified in the request body\",\"more_info\":\"https://cloud.ibm.com/apidocs/watsonx-ai#deployments-text-chat\"}],\"trace\":\"xxxxx\",\"status_code\":400}\n```\n\n## Steps to Reproduce\n1. Deploy a foundation model to watsonx deployment space\n2. Use LiteLLM to call the deployed model:\n\n```python\nimport litellm\n\nresponse = litellm.completion(\n    model=\"watsonx/deployment/<deployment_id>\",\n    messages=[{\"content\": \"Hello, how're you?\", \"role\": \"user\"}],\n)\n\nprint(response)\n```\n\n3. Observe the 400 Bad Request error\n\n## Expected Behavior\nThe request should succeed without sending `model_id` or `model` parameters in the request body when using deployment endpoints, as the model is already specified by the deployment ID.\n\n## Actual Behavior\nLiteLLM incorrectly includes `model_id` or `model` parameters in the request body, causing watsonx to reject the request with a validation error.\n\n## Environment\n- LiteLLM version: 1.72.3\n- Python version: 3.11\n\n## Additional Context\n- This issue is related to PR #11527 which fixed some other watsonx deployment issues, but this specific problem persists\n- According to watsonx API documentation, deployment endpoints should not include model identifiers in the request body\n- The deployment ID in the URL already specifies which model to use\n\n## References\n- [[watsonx Custom Foundation Model Deployment Documentation](https://www.ibm.com/docs/en/watsonx/saas?topic=model-inferencing-custom-foundation)](https://www.ibm.com/docs/en/watsonx/saas?topic=model-inferencing-custom-foundation)\n- [[LiteLLM watsonx Provider Documentation](https://docs.litellm.ai/docs/providers/watsonx#usage---models-in-deployment-spaces)](https://docs.litellm.ai/docs/providers/watsonx#usage---models-in-deployment-spaces)\n- [[watsonx AI API Documentation](https://cloud.ibm.com/apidocs/watsonx-ai#deployments-text-chat)](https://cloud.ibm.com/apidocs/watsonx-ai#deployments-text-chat)\n- Related PR: #11527\n\n### Relevant log output\n\n```shell\nBadRequestError: litellm.BadRequestError: watsonxException - {\"errors\":[{\"code\":\"json_validation_error\",\"message\":\"Json document validation error: 'model_id' or 'model' cannot be specified in the request body\",\"more_info\":\"https://cloud.ibm.com/apidocs/watsonx-ai#deployments-text-chat\"}],\"trace\":\"xxxxx\",\"status_code\":400}\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\n1.72.3\n\n### Twitter / LinkedIn details\n\nhttps://linkedin.com/in/juancb",
      "state": "open",
      "author": "cbjuan",
      "author_type": "User",
      "created_at": "2025-06-18T07:25:46Z",
      "updated_at": "2025-06-19T16:09:06Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11837/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11837",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11837",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:29.220311",
      "comments": [
        {
          "author": "cbjuan",
          "body": "The same code but using the provider `watsonx_text` works\n\n```\nimport litellm\n\nresponse = litellm.completion(\n    model=\"watsonx_text/deployment/<deployment_id>\",\n    messages=[{\"content\": \"Hello, how're you?\", \"role\": \"user\"}],\n)\n\nprint(response)\n```",
          "created_at": "2025-06-18T13:03:14Z"
        }
      ]
    },
    {
      "issue_number": 11889,
      "title": "[Feature]: Get moderation results (if any) as part of the results coming for `watsonx_text` provider",
      "body": "### The Feature\n\nThe `watsonx_text` provider accepts the field `moderations` as part of the `completions()` method as is compatible with the [watsonx API for text generation](https://cloud.ibm.com/apidocs/watsonx-ai#text-generation)\n\nWhen using the moderations feature, if something gets moderated, Watsonx returns a field called `moderations` as part of the response to confirm the prompt has been moderated as well as the reason for moderation, the confidence, etc., and that's the reason why there is no `generated_text` in the response.\n\nA example of a moderated response in watsonx:\n\n```\n{'model_id': 'xxx/xxxxxx, 'created_at': '2025-06-19T15:39:16.188Z', 'results': [{'generated_text': '', 'generated_token_count': 0, 'input_token_count': 5, 'stop_reason': '', 'moderations': {'hap': [{'score': 0.9957095384597778, 'input': True, 'position': {'start': 0, 'end': 10}, 'entity': 'has_HAP', 'word': '<the expression containing HAP>.'}]}}], 'system' [...]}\n```\n\n\nCurrent output from LiteLLM when a prompt has been moderated in watsonx\n\n```\nModelResponse(id='chatcmpl-3bb1eb2d-01c6-4b1e-9e78-dd5dcd5f733d', created=1750349116, model=None, object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='', index=0, message=Message(content='', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=0, prompt_tokens=4, total_tokens=4, completion_tokens_details=None, prompt_tokens_details=None))\n```\n\n### Motivation, pitch\n\nIt would be great if LiteLLM could inform the developer if a prompt has been moderated when using the `watsonx_text` provider, so there the dev can get the real reason why the prompt didn't get any `generated_text` in the response.\n\nI've seen there is a field `provider_specific_fields` in the `Message` object included with the result choices. That field would be perfect to append there the moderation result (if it exist). The output from LiteLLM when the result of the `.completion()` method has been moderated, could be as follows:\n\n\n```\nModelResponse(id='chatcmpl-c4f6b353-9410-4bf8-9d44-1a4ee2a7333d', created=1750349181, model=None, object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='', index=0, message=Message(content='', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'moderations': {'hap': [{'score': 0.986398696899414, 'input': True, 'position': {'start': 0, 'end': 11}, 'entity': 'has_HAP', 'word': '<the input expression containing HAP>'}]}}))], usage=Usage(completion_tokens=0, prompt_tokens=4, total_tokens=4, completion_tokens_details=None, prompt_tokens_details=None))\n```\n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\nhttps://linkedin.com/in/juancb",
      "state": "open",
      "author": "cbjuan",
      "author_type": "User",
      "created_at": "2025-06-19T16:08:29Z",
      "updated_at": "2025-06-19T16:08:29Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11889/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11889",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11889",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:29.388854",
      "comments": []
    },
    {
      "issue_number": 10020,
      "title": "[Feature]: Support passing 'environment' variable to langfuse on trace/generations per key",
      "body": "### The Feature\n\nhttps://langfuse.com/docs/tracing-features/environments\n\nJust need to support this as a langfuse param here - https://github.com/BerriAI/litellm/blob/aff0d1a18c016ad7db4fdbb42ad5065b15b9c5c5/litellm/types/utils.py#L1850\n\ne2e flow:\n- connect proxy to langfuse\n- create key w/ metadata = langfuse_environment=\"development\"\n- make chat completion request w/ key\n- expect langfuse trace + generation to have correct environment\n\n### Motivation, pitch\n\nuser request \n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "krrishdholakia",
      "author_type": "User",
      "created_at": "2025-04-15T15:21:28Z",
      "updated_at": "2025-06-19T14:55:09Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "april 2025"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10020/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10020",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10020",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:29.388878",
      "comments": [
        {
          "author": "ketangangal",
          "body": "Hi @krrishdholakia , \nI would like to work on this Feature !",
          "created_at": "2025-04-17T11:12:20Z"
        },
        {
          "author": "ketangangal",
          "body": "Working on it Now",
          "created_at": "2025-05-03T18:35:23Z"
        },
        {
          "author": "krrishdholakia",
          "body": "thanks @ketangangal ",
          "created_at": "2025-05-03T19:06:40Z"
        },
        {
          "author": "ketangangal",
          "body": "I am still working on it !",
          "created_at": "2025-05-31T06:36:41Z"
        },
        {
          "author": "ketangangal",
          "body": "#11289 ",
          "created_at": "2025-06-01T13:09:59Z"
        }
      ]
    },
    {
      "issue_number": 10098,
      "title": "[Feature]: transfer chat_completion to responses api",
      "body": "### The Feature\n\nit will be helpful\n\n### Motivation, pitch\n\nNone\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "Kenwwww",
      "author_type": "User",
      "created_at": "2025-04-17T10:44:05Z",
      "updated_at": "2025-06-19T14:27:05Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10098/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10098",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10098",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:29.894087",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "@Kenwwww you want to call chat completion models through the responses API ? this is already supported ",
          "created_at": "2025-04-22T20:12:40Z"
        },
        {
          "author": "Manouchehri",
          "body": "@ishaan-jaff How? I can't find any documentation for it.",
          "created_at": "2025-06-19T14:27:05Z"
        }
      ]
    },
    {
      "issue_number": 11057,
      "title": "[Feature]: Add Claude4",
      "body": "### The Feature\n\nsee https://www.anthropic.com/news/claude-4\n\n### Motivation, pitch\n\nNew model\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "pathikrit",
      "author_type": "User",
      "created_at": "2025-05-22T17:14:32Z",
      "updated_at": "2025-06-19T13:27:31Z",
      "closed_at": "2025-05-27T15:19:11Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 29,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11057/reactions",
        "total_count": 13,
        "+1": 12,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 1,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11057",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11057",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:30.136961",
      "comments": []
    },
    {
      "issue_number": 8852,
      "title": "[Feature]: Add ElevenLabs as Provider",
      "body": "### The Feature\n\nHi @krrishdholakia!\nElevenLabs recently released its Scribe model, which significantly improves speech-to-text conversion. Integrating it as a provider in LiteLLM would expand the application's multimodal capabilities, offering a better experience for users who require audio generation and audio transcriptions.\n\n### Motivation, pitch\n\nBy supporting ElevenLabs in LiteLLM, you would provide more options for developers and users interested in multimodal applications.\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "mvrodrig",
      "author_type": "User",
      "created_at": "2025-02-26T19:52:40Z",
      "updated_at": "2025-06-19T07:09:37Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 11,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8852/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "S1LV3RJ1NX"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8852",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8852",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:30.136983",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "is it this API @mvrodrig ? https://elevenlabs.io/docs/api-reference/text-to-speech/convert ",
          "created_at": "2025-03-10T15:22:11Z"
        },
        {
          "author": "mvrodrig",
          "body": "Hi @ishaan-jaff , yes, that one!",
          "created_at": "2025-03-11T13:20:21Z"
        },
        {
          "author": "MattCarneiro",
          "body": "+1",
          "created_at": "2025-04-23T05:26:17Z"
        },
        {
          "author": "yigitkonur",
          "body": "as it is SOTA of STT, I guess it is really needed üôèüèø",
          "created_at": "2025-04-28T12:03:37Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Hi all, scoping this - what endpoints would you want us to support? ",
          "created_at": "2025-04-28T14:41:40Z"
        }
      ]
    },
    {
      "issue_number": 11696,
      "title": "[Bug]: 400 Invalid input  with Azure AI Foundry (Mistral model)",
      "body": "### What happened?\n\nHello LiteLLM team,\n\nI'm encountering a 422 Invalid input error when using LiteLLM with Azure AI Foundry and the mistral-small-2503 model.\nThe same model responds correctly using the Azure Foundry playground and direct API calls ‚Äî only the LiteLLM proxy results in this issue.\n\nWould appreciate your guidance on whether this is due to an automatically included parameter or something else in the request payload.\n\nThanks in advance for your help.\n\nBest regards,\n\n### Relevant log output\n\n```shell\nError fetching response:Error: 400 litellm.BadRequestError: Azure_aiException - {\"error\":{\"code\":\"Invalid input\",\"message\":\"{\"detail\":[{\"type\":\"extra_forbidden\",\"loc\":[\"body\",\"stream_options\",\"include_usage\"],\"msg\":\"Extra inputs are not permitted\",\"input\":true,\"url\":\"https://errors.pydantic.dev/2.10/v/extra_forbidden\"}]}\",\"status\":422}}. Received Model Group=mistral-small-2503\nAvailable Model Group Fallbacks=None\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.2-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "jumsay",
      "author_type": "User",
      "created_at": "2025-06-13T07:38:27Z",
      "updated_at": "2025-06-19T06:54:11Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11696/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11696",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11696",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:30.395048",
      "comments": [
        {
          "author": "samy-ljn",
          "body": "Same issue ",
          "created_at": "2025-06-19T06:54:11Z"
        }
      ]
    },
    {
      "issue_number": 11477,
      "title": "[Bug]: Configuring Pass-Through Endpoints through the UI interface leads to memory leaks.",
      "body": "### What happened?\n\nConfiguring Pass-Through Endpoints through the UI interface causes continuous growth in service memory, leading to memory leak issues. This problem has been identified in versions from v1.51.0-stable to v1.68.0-stable. We hope this issue can be resolved as soon as possible.\n\n<img width=\"682\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/0f9b00ed-d1f6-4d52-b4d9-1ab3fe51425e\" />\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.51.0-v1.68.0\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "harvardfly",
      "author_type": "User",
      "created_at": "2025-06-06T08:26:33Z",
      "updated_at": "2025-06-19T06:50:11Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11477/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11477",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11477",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:30.578330",
      "comments": [
        {
          "author": "harvardfly",
          "body": "@ishaan-jaff @krrishdholakia Please pay attention to this issueÔºåConfiguring Pass-Through Endpoints through the UIÔºåMemory will continue to grow.\n\n![Image](https://github.com/user-attachments/assets/8f33ac19-d40a-4dc1-8522-c8c859154cec)\n",
          "created_at": "2025-06-19T06:50:10Z"
        }
      ]
    },
    {
      "issue_number": 11402,
      "title": "[Bug]: groq/whisper-large-v3 returns 400 BadRequestError with OPENAI_TRANSCRIPTION_PARAMS",
      "body": "### What happened?\n\nI'm using the groq/whisper-large-v3 model with litellm.transcription() as shown in the official [documentation](https://docs.litellm.ai/docs/providers/groq#speech-to-text---whisper)\n\nThis same code was running fine on Google Colab around 3‚Äì4 days ago, but now it consistently fails with a BadRequestError. No changes were made to the code or environment.\n\n```python\nimport os\nfrom litellm import transcription\n\nos.environ[\"GROQ_API_KEY\"] = \"your-key-here\"\n\naudio_file = open(\"/path/to/audio.mp3\", \"rb\")\n\ntranscript = transcription(\n    model=\"groq/whisper-large-v3\",\n    file=audio_file,\n    prompt=\"Specify context or spelling\",\n    temperature=0,\n    response_format=\"json\"\n)\n\nprint(\"response=\", transcript)\n```\n\nError message:\n\n`\nBadRequestError: Error code: 400 - {'error': {'message': 'unknown param `OPENAI_TRANSCRIPTION_PARAMS[]`', 'type': 'invalid_request_error'}}\n`\n\n### Relevant log output\n\n```shell\n9 audio_file = open(audio_path, \"rb\")\n---> 11 transcript = litellm.transcription(\n     12     model=\"groq/whisper-large-v3\",\n     13     file=audio_file,\n     14     prompt=\"Specify context or spelling\",\n     15     temperature=0,\n     16     response_format=\"json\"\n     17 )\n     19 transcribed_text = transcript.text\n\nFile d:\\agenticvenv\\Lib\\site-packages\\litellm\\utils.py:1283, in client.<locals>.wrapper(*args, **kwargs)\n   1279 if logging_obj:\n   1280     logging_obj.failure_handler(\n   1281         e, traceback_exception, start_time, end_time\n   1282     )  # DO NOT MAKE THREADED - router retry fallback relies on this!\n-> 1283 raise e\n\nFile d:\\agenticvenv\\Lib\\site-packages\\litellm\\utils.py:1161, in client.<locals>.wrapper(*args, **kwargs)\n   1159         print_verbose(f\"Error while checking max token limit: {str(e)}\")\n   1160 # MODEL CALL\n...\n-> 1037         raise self._make_status_error_from_response(err.response) from None\n   1039     break\n   1041 assert response is not None, \"could not resolve response (should never happen)\"\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nlitellm==1.72.0\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "afreenss",
      "author_type": "User",
      "created_at": "2025-06-04T12:48:26Z",
      "updated_at": "2025-06-19T06:22:52Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11402/reactions",
        "total_count": 3,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11402",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11402",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:30.781598",
      "comments": [
        {
          "author": "jerry-intrii",
          "body": "Same problem here. Downgrade to  1.71.1 works fine",
          "created_at": "2025-06-11T06:13:09Z"
        },
        {
          "author": "afreenss",
          "body": "Yes. Downgrading to litellm 1.71.1 works fine for me as well üëç ",
          "created_at": "2025-06-11T06:46:35Z"
        },
        {
          "author": "lishtys",
          "body": "still not working in latest version",
          "created_at": "2025-06-19T06:22:52Z"
        }
      ]
    },
    {
      "issue_number": 11227,
      "title": "[Bug]: azure_ai foundry models not working",
      "body": "### What happened?\n\nDirect Azure AI foundry endpoint model call:\n```\ncurl --request POST \\\n  --url https://my-model.eastus2.models.ai.azure.com/chat/completions \\\n  --header 'Authorization: Bearer xxx' \\\n  --data '{\n\t\"model\": \"phi-4\",\n\t\"messages\": [\n\t\t{\n\t\t\t\"role\": \"user\",\n\t\t\t\"content\": \"ping\"\n\t\t}\n\t],\n\t\"max_tokens\": 10\n}'\n```\nsuccessful response:\n```\n{\n\t\"id\": \"chatcmpl-691aa22b-766b-4431-82e9-b3acccb249f8\",\n\t\"object\": \"chat.completion\",\n\t\"created\": 1748498263,\n\t\"model\": \"phi4\",\n\t\"choices\": [\n\t\t{\n\t\t\t\"index\": 0,\n\t\t\t\"message\": {\n\t\t\t\t\"role\": \"assistant\",\n\t\t\t\t\"content\": \"It looks like you're referencing the \\\"ping\\\"\",\n\t\t\t\t\"tool_calls\": [],\n\t\t\t\t\"reasoning_content\": null\n\t\t\t},\n\t\t\t\"finish_reason\": \"length\"\n\t\t}\n\t],\n\t\"usage\": {\n\t\t\"prompt_tokens\": 8,\n\t\t\"total_tokens\": 18,\n\t\t\"completion_tokens\": 10,\n\t\t\"prompt_tokens_details\": null\n\t}\n}\n```\n\n`proxy-config.yaml` entry:\n```\n- model_name: phi-4\n    litellm_params:\n      model: azure_ai/my-model\n      api_base: https://my-model.eastus2.models.ai.azure.com\n      api_key: xxx\n    model_info:\n      mode: chat\n      region: \"East US 2\"\n      base_model: azure_ai/Phi-4\n      access_groups: [\"default-models\"]\n      health_check_timeout: 1\n```\n\ncall via litellm proxy:\n```\ncurl --request POST \\\n  --url https://xxx.com/chat/completions \\\n  --header 'api-key: xxx' \\\n  --data '{\n\t\"model\": \"phi-4\",\n\t\"messages\": [\n\t\t{\n\t\t\t\"role\": \"user\",\n\t\t\t\"content\": \"ping\"\n\t\t}\n\t],\n\t\"max_tokens\": 10\n}'\n```\nerror response: 400 Bad Request\n```\n{\n\t\"error\": {\n\t\t\"message\": \"litellm.BadRequestError: Azure_aiException - {\\\"error\\\":{\\\"code\\\":\\\"Invalid input\\\",\\\"status\\\":422,\\\"message\\\":\\\"invalid input error\\\",\\\"details\\\":[{\\\"type\\\":\\\"model_attributes_type\\\",\\\"loc\\\":[\\\"body\\\"],\\\"msg\\\":\\\"Input should be a valid dictionary or object to extract fields from\\\",\\\"input\\\":\\\"{\\\\\\\"model\\\\\\\": \\\\\\\"phi-4-dcai\\\\\\\", \\\\\\\"messages\\\\\\\": [{\\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"ping\\\\\\\"}], \\\\\\\"stream\\\\\\\": false, \\\\\\\"max_tokens\\\\\\\": 10}\\\"}]}}. Received Model Group=phi-4\\nAvailable Model Group Fallbacks=None\",\n\t\t\"type\": null,\n\t\t\"param\": null,\n\t\t\"code\": \"400\"\n\t}\n}\n```\n\nExpected: successful response via litellm proxy same as when directly calling azure ai foundry endpoint\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\n1.71.2.dev1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "taralika",
      "author_type": "User",
      "created_at": "2025-05-29T06:03:34Z",
      "updated_at": "2025-06-19T04:24:37Z",
      "closed_at": "2025-06-19T04:24:37Z",
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11227/reactions",
        "total_count": 3,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "krrishdholakia"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11227",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11227",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:32.629136",
      "comments": [
        {
          "author": "dotrunghieu96",
          "body": "Up, I'm having the same issue with deepseek serverless endpoint on Azure AI Foundry",
          "created_at": "2025-06-05T11:16:56Z"
        },
        {
          "author": "vaclcer",
          "body": "Exactly. Azure AI foundry models do not work (v1.72.0-stable)",
          "created_at": "2025-06-09T11:07:04Z"
        },
        {
          "author": "cableman",
          "body": "Same error here: \n\n> litellm.BadRequestError: Azure_aiException - {\"error\":{\"code\":\"Invalid input\",\"status\":422,\"message\":\"invalid input error\",\"details\":[{\"type\":\"model_attributes_type\",\"loc\":[\"body\"],\"msg\":\"Input should be a valid dictionary or object to extract fields from\",\"input\":\"{\\\"model\\\": \\",
          "created_at": "2025-06-12T08:47:25Z"
        },
        {
          "author": "jumsay",
          "body": "+1 , Mistral model not working.",
          "created_at": "2025-06-12T14:45:50Z"
        },
        {
          "author": "taralika",
          "body": "@krrishdholakia @ishaan-jaff üëÜ",
          "created_at": "2025-06-12T15:35:09Z"
        }
      ]
    },
    {
      "issue_number": 11866,
      "title": "[Feature]: Improve base image flexibility: allow non-root UID/GID config and frontend path customization without rebuild",
      "body": "### The Feature\n\nHi LiteLLM team üëã,\nFirst of all, thank you for the amazing work you‚Äôve been doing ‚Äî we were positively surprised by how fast you addressed our previous CVE report üôå ([#11829](https://github.com/BerriAI/litellm/issues/11829)).\n\nWe‚Äôre currently evaluating LiteLLM and would like to reduce the need for maintaining a custom fork or rebuilding the base image.\n\nüß© Use Case & Motivation\nTo integrate seamlessly into our internal infrastructure (Kubernetes + custom ingress routing), we‚Äôd love to:\n\n- Use the official image as a base, without needing to clone or rebuild the full source.\n\n- Configure a fixed non-root UID/GID (e.g., UID 1000) that works with our platform's security context constraints.\n\n- Customize the SERVER_ROOT_PATH (e.g., /litellm) without needing to rebuild the frontend UI assets.\n\nWe‚Äôd prefer not to depend on the upstream repo directly but to use a public image as base image.\n\nüìå Feature Proposal\nWe propose:\n\n- Add support to configure non-root UID/GID via environment variable or ARG in the Dockerfile.\n\n- Add a runtime environment variable to set the asset prefix path used by the frontend, aligning it with SERVER_ROOT_PATH.\n\n- (Optional) Provide a headless image (API only) variant without the frontend, for lightweight deployments.\n\nüí° Why This Matters\nSmall changes like these would make it much easier for other platform teams like ours to adopt and contribute back üôè\n\nLet us know if you'd be open to a PR or suggestions from our side!\n\nThanks again for the great work!\n\n### Motivation, pitch\n\nWe're deploying LiteLLM in Kubernetes with restricted security and want to use the official image without rebuilding it. Currently, we need to patch the image to support a fixed UID and to serve the UI under a custom path (e.g., /litellm). Supporting these options via environment variables or build-time args would simplify adoption in restrictred and secure environments.\n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\n@mateo-di",
      "state": "open",
      "author": "mateo-di",
      "author_type": "User",
      "created_at": "2025-06-18T18:45:22Z",
      "updated_at": "2025-06-19T01:39:03Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11866/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11866",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11866",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:32.864090",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "> Customize the SERVER_ROOT_PATH (e.g., /litellm) without needing to rebuild the frontend UI assets.\n\nThis is already possible from v1.72.3 onwards \n\nhttps://docs.litellm.ai/docs/proxy/custom_root_ui\n",
          "created_at": "2025-06-18T21:06:00Z"
        },
        {
          "author": "krrishdholakia",
          "body": "> Use the official image as a base, without needing to clone or rebuild the full source.\n\nThis is also already possible - https://docs.litellm.ai/docs/proxy/deploy#use-litellm-as-a-base-image",
          "created_at": "2025-06-18T21:07:07Z"
        },
        {
          "author": "krrishdholakia",
          "body": "> Configure a fixed non-root UID/GID (e.g., UID 1000) that works with our platform's security context constraints.\n\nI don't know what this means. ",
          "created_at": "2025-06-18T21:07:24Z"
        },
        {
          "author": "mateo-di",
          "body": "Hi @krrishdholakia ‚Äî thanks for the quick response\n\n> > Customize the SERVER_ROOT_PATH (e.g., /litellm) without needing to rebuild the frontend UI assets.\n> \n> This is already possible from v1.72.3 onwards\n> \n> https://docs.litellm.ai/docs/proxy/custom_root_ui\n\nIn this regard we are facing an issue ",
          "created_at": "2025-06-19T01:24:33Z"
        }
      ]
    },
    {
      "issue_number": 11334,
      "title": "[Bug]: occasional error \"Transaction API error: Unable to start a transaction in the given time.\" for updating DB spend",
      "body": "### What happened?\n\nA few times a day, we will receive the following error: \"Transaction API error: Unable to start a transaction in the given time.\" for updating DB spend (see below for full log output).\n\nWe've deployed LiteLLM using GCP Cloud Run and host the PostgresQL database using Neon DB. (note: we do not use Redis).\n\nA few questions:\n1) why does this happen? I've looked at the DB dashboard and I don't see any deadlocks and the DB (Neon) compute and memory are well-provisioned with plenty of headroom. \n2) if this does happen, I assume it's not too big of a deal if it happens only occasionally because when I look at the code there's retry logic (3 times) and the spend updates are triggered every minute ([code](https://github.com/BerriAI/litellm/blob/702e399d944b89afa893358cbde092082b6a2706/litellm/proxy/utils.py#L2617)) - is that a fair understanding?\n3) should I use [Redis](https://github.com/BerriAI/litellm/blob/702e399d944b89afa893358cbde092082b6a2706/litellm/proxy/db/db_spend_update_writer.py#L373) to buffer spend updates? To provide some context, we have <100 users and QPS is quite low (~1) but the main thing is that it's important to _not_ lose spend updates because it's critical to have accurate spend logs for our use case.\n\nThanks again for your help and developing this awesome library/tool!\n\n### Relevant log output\n\n```shell\nMessage: DB read/write call failed: 504: {\"is_panic\":false,\"message\":\"Transaction API error: Unable to start a transaction in the given time.\",\"meta\":{\"error\":\"Unable to start a transaction in the given time.\"},\"error_code\":\"P2028\"}[Non-Blocking]LiteLLM Prisma Client Exception - update spend logs: 504: {\"is_panic\":false,\"message\":\"Transaction API error: Unable to start a transaction in the given time.\",\"meta\":{\"error\":\"Unable to start a transaction in the given time.\"},\"error_code\":\"P2028\"}\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/db/db_spend_update_writer.py\", line 561, in _commit_spend_updates_to_db\n    async with prisma_client.db.tx(\n               ~~~~~~~~~~~~~~~~~~~^\n        timeout=timedelta(seconds=60)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ) as transaction:\n    ^\n  File \"/usr/lib/python3.13/site-packages/prisma/client.py\", line 700, in __aenter__\n    return await self.start(_from_context=True)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/prisma/client.py\", line 672, in start\n    tx_id = await self.__client._engine.start_transaction(\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>..\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.69.3-nightly\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "wwwillchen",
      "author_type": "User",
      "created_at": "2025-06-02T20:29:45Z",
      "updated_at": "2025-06-19T00:06:24Z",
      "closed_at": "2025-06-18T23:40:16Z",
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11334/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11334",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11334",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:33.049742",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "This is not a bug - it's an error raised by prisma orm trying to connect to the db. \n\nIf it can't, it times out. We retry this 3 times before dropping the log. \n\nWe can't keep logs in memory forever, as it will cause memory issues. ",
          "created_at": "2025-06-18T23:40:16Z"
        },
        {
          "author": "wwwillchen",
          "body": "Got it. Agree, it's not a bug in LiteLLM.\n\nDo you recommend using Redis to avoid this issue?",
          "created_at": "2025-06-19T00:06:24Z"
        }
      ]
    },
    {
      "issue_number": 5859,
      "title": "[Feature]: Time To First Token Timeout",
      "body": "### The Feature\n\nIt would be great if we could set the timeout based on the time to first token rather than the total request duration. For example, if the context size is 5K and a model hasn't responded within 10 seconds, it's probably under heavy load, and I'd want it to failover to another endpoint.\n\n### Motivation, pitch\n\nEach inference has different characteristics, but there's especially a lot of variability with Azure. So it would be great to be able to send a user who's been waiting for 10 seconds to another server without dropping their request.\n\n### Twitter / LinkedIn details\n\n@yigitkonur",
      "state": "closed",
      "author": "yigitkonur",
      "author_type": "User",
      "created_at": "2024-09-24T05:30:18Z",
      "updated_at": "2025-06-19T00:02:40Z",
      "closed_at": "2025-06-19T00:02:40Z",
      "labels": [
        "enhancement",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/5859/reactions",
        "total_count": 7,
        "+1": 7,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/5859",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/5859",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:33.242628",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-01-28T02:58:20Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-12T00:02:06Z"
        }
      ]
    },
    {
      "issue_number": 7520,
      "title": "[Feature]: Option to remove stack trace from exception logging",
      "body": "### The Feature\r\n\r\nThere does not appear to be any option to remove stack traces from exceptions when they are logged. This causes the function arguments to be logged, which defeats the purpose of the message redaction option (`turn_off_message_logging`) since the message content is still logged via the exception stack trace.\r\n\r\nI propose that the `turn_off_message_logging` option include disable stack trace logging with another option to force the stack trace to be enabled if so desired (`always_include_stack_trace` or something). This would help to ensure that incoming messages are never logged, while allowing an option for the stack trace to remain if so desired.\r\n\r\nI have attached a screenshot of a message sent to Sentry that shows the stack trace with the complete incoming request arguments available, even though `turn_off_message_logging` is set to `true`.\r\n\r\n### Motivation, pitch\r\n\r\nIf using the `turn_off_message_logging` option, then it may be presumed that there is a desire to avoid logging any user provided values. There should be a means for ensuring that absolutely no user provided values are logged for privacy conscious applications.\r\n\r\n### Are you a ML Ops Team?\r\n\r\nYes\r\n\r\n### Twitter / LinkedIn details\r\n\r\n_No response_\r\n\r\n### Sample Sentry exception with stack trace\r\n\r\n<img width=\"1029\" alt=\"Screenshot 2025-01-03 at 6 59 22‚ÄØAM\" src=\"https://github.com/user-attachments/assets/a7d625b8-1206-47cb-8512-ee38bd05aa6e\" />",
      "state": "open",
      "author": "scraymondjr",
      "author_type": "User",
      "created_at": "2025-01-03T14:58:40Z",
      "updated_at": "2025-06-19T00:02:38Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/7520/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/7520",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/7520",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:33.462956",
      "comments": [
        {
          "author": "rsb-23",
          "body": "Same issue ",
          "created_at": "2025-03-19T13:31:44Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-18T00:01:58Z"
        },
        {
          "author": "rsb-23",
          "body": "@scraymondjr Have you found a workaround for the time being? ",
          "created_at": "2025-06-18T03:17:21Z"
        },
        {
          "author": "scraymondjr",
          "body": "Nope. I am no longer working with this project, either.",
          "created_at": "2025-06-18T03:26:23Z"
        }
      ]
    },
    {
      "issue_number": 7471,
      "title": "[info]: Regarding models compatible with OpenAI-Compatible Endpoints, such as Qwen. etc",
      "body": "### What happened?\r\n\r\nI guess some users might have encountered issues with the CrewAI project and, as a result, indirectly found  LiteLLM. They encountered some issues while using various models.\r\n\r\nI've noticed that several China model providers do not appear in the Providers list, and there have been multiple issues raised regarding this point. Actually, if the model you are using supports OpenAI-Compatible Endpoints, the author has already paved the way for you. You can use your model with the following method:\r\nhttps://docs.litellm.ai/docs/providers/openai_compatible\r\n \r\n\r\n### Relevant log output\r\n\r\nQwen as an example: \r\n![image](https://github.com/user-attachments/assets/884d8453-704a-4e1a-b97f-a5c8ed9e28cf)\r\nAs you can see,it works.\r\n\r\n\r\n### Are you a ML Ops Team?\r\n\r\nNo\r\n\r\n### What LiteLLM version are you on ?\r\n\r\nv1.53.1\r\n\r\n### Twitter / LinkedIn details\r\n\r\n_No response_",
      "state": "closed",
      "author": "Silence-Well",
      "author_type": "User",
      "created_at": "2024-12-30T09:53:33Z",
      "updated_at": "2025-06-19T00:02:38Z",
      "closed_at": "2025-06-19T00:02:38Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/7471/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/7471",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/7471",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:33.633653",
      "comments": [
        {
          "author": "LGCALIVE",
          "body": "Thanks bro,finally find a solution",
          "created_at": "2025-03-12T09:44:02Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-11T00:02:03Z"
        }
      ]
    },
    {
      "issue_number": 8011,
      "title": "Parameter Incompatibility with Perplexity-Sonar Model in Litellm Integration",
      "body": "### What happened?\n\n**Description:**\n\nI've identified a technical incompatibility issue between Litellm's parameter handling and the Perplexity-Sonar model's API requirements. This issue manifests specifically in the handling of the `frequency_penalty` parameter.\n\n**Technical Details:**\n\n1. **Current Implementation:**\n   When sending requests through Litellm, the standard OpenAI-style parameter format is used:\n   ```json\n   {\n     \"model\": \"perplexity-sonar\",\n     \"messages\": [...],\n     \"temperature\": 1,\n     \"max_tokens\": 8000,\n     \"frequency_penalty\": 0.5,\n     \"presence_penalty\": 0.4\n   }\n   ```\n\n2. **Error Manifestation:**\n   The Perplexity-Sonar API returns a 400 error with the following response:\n   ```json\n   {\n     \"error\": {\n       \"message\": \"litellm.BadRequestError: PerplexityException - Error code: 400 - {'error': {'message': 'Frequency penalty must satisfy p > 0.', 'type': 'invalid_parameter', 'code': 400}}\"\n     }\n   }\n   ```\n\n3. **Technical Context:**\n   - The issue occurs during the parameter translation layer between Litellm's unified API and Perplexity's specific API requirements\n   - The error specifically relates to the validation of the `frequency_penalty` parameter\n   - Perplexity-Sonar requires this parameter to be strictly greater than 0, while Litellm is passing it as 0\n\n4. **Impact:**\n   - All requests to Perplexity-Sonar through Litellm with `frequency_penalty: 0` fail\n   - This affects the library's goal of providing a unified interface for multiple LLM providers\n\nThis issue highlights a gap in parameter validation and transformation between Litellm's unified API and Perplexity-Sonar's specific requirements.\n\n---\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n-\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "suysoftware",
      "author_type": "User",
      "created_at": "2025-01-26T18:48:10Z",
      "updated_at": "2025-06-19T00:02:36Z",
      "closed_at": null,
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8011/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8011",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8011",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:33.840169",
      "comments": [
        {
          "author": "skyking363",
          "body": "+1",
          "created_at": "2025-03-20T07:38:49Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-19T00:02:36Z"
        }
      ]
    },
    {
      "issue_number": 8679,
      "title": "[Feature]: Add support for Mistral Codestral through Azure",
      "body": "### The Feature\n\nAdd support for using the Codestral model through Azure.\n\n\n\n### Motivation, pitch\n\nCurrently, it doesn't work, when I try, I get this error:\n```\n16:30:29 - LiteLLM Proxy:DEBUG: proxy_failure_handler.py:37 - inside _PROXY_failure_handler kwargs=\n16:30:29 - LiteLLM Router:INFO: router.py:2130 - litellm.atext_completion(model=mistral-codestral) Exception litellm.APIError: AzureException APIError - Error code: 500 - {'error': {'code': 'InternalServerError', 'message': 'Backend returned unexpected response. Please contact Microsoft for help.'}}\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "mboret",
      "author_type": "User",
      "created_at": "2025-02-20T16:32:36Z",
      "updated_at": "2025-06-19T00:02:34Z",
      "closed_at": "2025-06-19T00:02:34Z",
      "labels": [
        "enhancement",
        "llm translation",
        "stale",
        "feb 2025",
        "azure openai"
      ],
      "label_count": 5,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8679/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8679",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8679",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:34.106578",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Hi @mboret \n\n> Backend returned unexpected response. Please contact Microsoft for help\n\nthis doesn't look like a litellm error - is this working when you call the provider directly vs. the proxy? ",
          "created_at": "2025-02-21T07:27:45Z"
        },
        {
          "author": "gustavhertz",
          "body": "I think an issue with Mistral models through azure is that the `fim/completions` endpoint isn't supported in LiteLLM, is this something you have in your roadmap to add?",
          "created_at": "2025-03-13T13:22:03Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-12T00:02:00Z"
        }
      ]
    },
    {
      "issue_number": 8548,
      "title": "[Feature]: Add Multi-Modal Output Support (image-to-image, image-to-video, text-to-video)",
      "body": "### The Feature\n\nHi @krrishdholakia! \nI‚Äôm opening this issue as a feature request to propose adding support for multi-modal outputs‚Äîspecifically:\n\n- image-to-image\n- image-to-video\n- text-to-video\n\n\n\n### Motivation, pitch\n\nWhile these features are still emerging in the broader AI ecosystem, there‚Äôs growing interest in using multi-modal output capabilities in production workflows. We can already benefit from existing solutions like [Amazon Nova Canvas](https://docs.aws.amazon.com/nova/latest/userguide/image-gen-access.html) (for text-to-image and image-to-image) and [Amazon Nova Reels](http://docs.aws.amazon.com/nova/latest/userguide/video-gen-access.html) (for image-to-video and text-to-video).\n\nWe‚Äôre also watching for eventual public APIs from Google‚Äôs Veo 2 and OpenAI's Sora. However, it would be wonderful to start building and experimenting with these multi-modal features in Litellm now.\n\nLooking forward to your thoughts, and thanks in advance!\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "mvrodrig",
      "author_type": "User",
      "created_at": "2025-02-14T20:28:08Z",
      "updated_at": "2025-06-19T00:02:34Z",
      "closed_at": "2025-06-19T00:02:34Z",
      "labels": [
        "enhancement",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8548/reactions",
        "total_count": 3,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8548",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8548",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:34.338097",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Hi @mvrodrig how would you want us to support this?",
          "created_at": "2025-02-14T21:30:02Z"
        },
        {
          "author": "mvrodrig",
          "body": "@krrishdholakia The _image-to-image_ feature could be supported through the existing endpoint **images/generations**, while\n\n- _image-to-video_\n- _text-to-video_\n\ncould be supported via a new endpoint, such as **videos/generations**, with proxy support.",
          "created_at": "2025-02-18T12:51:38Z"
        },
        {
          "author": "dhawalkp",
          "body": "Can we have some traction on this request? Our Amazon Nova customers are actively looking for this feature",
          "created_at": "2025-03-13T07:08:49Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-12T00:02:01Z"
        }
      ]
    },
    {
      "issue_number": 9020,
      "title": "[Bug]: anthropic extended thinking not working with extended thinking + \"output-128k-2025-02-19\"",
      "body": "### What happened?\n\nGot this error AFTER i tried to extend to 128k window with sonnet 3.7\n\nHere's the script for repro (too long to put in log output)\n[test_llm_think.txt](https://github.com/user-attachments/files/19099782/test_llm_think.txt)\n\n```\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: AnthropicException - {\"type\":\"error\",\"error\":{\"type\":\"invalid_request_error\",\"message\":\"messages.1.content.0.type: Expected `thinking` or `redacted_thinking`, but found `text`. When `thinking` is enabled, a final `assistant` message must start with a thinking block (preceeding the lastmost set of `tool_use` and `tool_result` blocks). We recommend you include thinking blocks from previous turns. To avoid this requirement, disable `thinking`. Please consult our documentation at https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking\"}}\n```\n\n### Relevant log output\n\n```shell\nSee attached script\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nbranch = \"litellm_dev_03_03_2025_p3\" commit = \"a1a711abfa120e62944c3639620c6bce96671ba5\"\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "xingyaoww",
      "author_type": "User",
      "created_at": "2025-03-06T01:43:28Z",
      "updated_at": "2025-06-19T00:02:31Z",
      "closed_at": null,
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9020/reactions",
        "total_count": 7,
        "+1": 6,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9020",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9020",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:34.565086",
      "comments": [
        {
          "author": "xingyaoww",
          "body": "OK confirmed this issue still happens with latest main!",
          "created_at": "2025-03-06T02:07:46Z"
        },
        {
          "author": "fcakyon",
          "body": "Is there any update on claude thinking support in litellm? @xingyaoww ",
          "created_at": "2025-03-10T21:07:52Z"
        },
        {
          "author": "rob-lega",
          "body": "same issue.  any update?",
          "created_at": "2025-03-20T12:08:02Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-19T00:02:31Z"
        }
      ]
    },
    {
      "issue_number": 9099,
      "title": "[feature] support `/v1/completions` on aiohttp_openai/ route",
      "body": "I have integrated an OpenAI-compatible model in LiteLLM using aiohttp_openai and set the API base to https://<base-model-endpoint>/v1, along with the authentication token. The health check confirms that the model is up and running. However, when making inference requests to the LiteLLM endpoint, I noticed that all requests are being routed to /v1/chat/completions, regardless of whether they were originally sent to /v1/completions or /v1/chat/completions. Specifically, when calling the completion endpoint (POST https://<litellm-endpoint>/v1/completions), the request is instead handled as if it were sent to /v1/chat/completions. On the other hand, requests to /v1/chat/completions work as expected.\n\nIdeally, requests to /v1/completions should be treated separately from /v1/chat/completions, but it seems that LiteLLM is internally redirecting or merging them. I have confirmed that the setup is correct by ensuring the API base is properly configured, the health check is successful, and the correct token is provided. I would like to understand if there is an internal routing mechanism in LiteLLM that forces this behavior and if there is a way to explicitly separate the two endpoints. Any insights or guidance on resolving this issue would be greatly appreciated.",
      "state": "closed",
      "author": "vinayK34",
      "author_type": "User",
      "created_at": "2025-03-10T12:33:17Z",
      "updated_at": "2025-06-19T00:02:30Z",
      "closed_at": "2025-06-19T00:02:30Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9099/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9099",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9099",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:34.749993",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "aiohttp_openai is in beta and a chat completion route - please use `openai/`, which does handle this correctly ",
          "created_at": "2025-03-10T23:22:04Z"
        },
        {
          "author": "vinayK34",
          "body": "Anytime soon can that feature will be available for aiohttp_openai/ route?",
          "created_at": "2025-03-13T06:32:43Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-12T00:01:52Z"
        }
      ]
    },
    {
      "issue_number": 9195,
      "title": "[Bug]: Disable regenerate  key option when user is a premium user",
      "body": "### What happened?\n\nDisable regenerate  key option when user is a premium user\n\n![Image](https://github.com/user-attachments/assets/f0cb74b6-5b47-4cca-946e-941bdce16083)\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.63.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "jaswanth8888",
      "author_type": "User",
      "created_at": "2025-03-13T06:10:00Z",
      "updated_at": "2025-06-19T00:02:29Z",
      "closed_at": "2025-06-19T00:02:29Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9195/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9195",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9195",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:34.949933",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-12T00:01:46Z"
        }
      ]
    },
    {
      "issue_number": 9199,
      "title": "[Bug]: create_pretrained_tokenizer uses wrong parameter name auth_token for Tokenizer.from_pretrained call",
      "body": "### What happened?\n\nI've tried litellm-proxy instance with `custom_tokenizer` and `auth_token` filled with my own HF token.\nThe `utils/token_counter` endpoint failed with the following error:\n\n```\nlitellm-proxy-1  | 09:25:50 - LiteLLM:ERROR: utils.py:1674 - Error creating pretrained tokenizer: Tokenizer.from_pretrained() got an unexpected keyword argument 'auth_token'. Defaulting to version without 'auth_token'.\nlitellm-proxy-1  | INFO:     127.0.0.1:48030 - \"POST /utils/token_counter HTTP/1.1\" 500 Internal Server Error\nlitellm-proxy-1  | ERROR:    Exception in ASGI application\n\n...\nlitellm-proxy-1  | requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/...\n```\n\nThe litellm-proxy uses `auth_token` parameter for `Tokenizer.from_pretrained`, but recent versions of this library (including the version mensioned in requreiments.txt - 0.20.2) uses `token` parameter name.\n\nI was able to reproduce the issue even without litellm-proxy:\n\n```\n$ pip install tokenizers==0.20.2\n\n# python code\n# Tokenizer.from_pretrained(model_repo_id, auth_token=\"my-token\")\nTypeError: Tokenizer.from_pretrained() got an unexpected keyword argument 'auth_token'\n\n# python code\n# Tokenizer.from_pretrained(model_repo_id, token=\"my-token\")\nworks correctly\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.61.6\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "vlad-vinogradov-47",
      "author_type": "User",
      "created_at": "2025-03-13T09:43:54Z",
      "updated_at": "2025-06-19T00:02:28Z",
      "closed_at": "2025-06-19T00:02:28Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9199/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9199",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9199",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:35.118575",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-12T00:01:43Z"
        }
      ]
    },
    {
      "issue_number": 9202,
      "title": "[Feature]: Tracing Span Support for Proxy",
      "body": "### The Feature\n\nRight now there is only a way to specify generation name, trace name, session id, etc.\n\nNo support for spans. I wish we could also pass something like `span` name to effectively see hierarchical spans in the same way we can send the generation name in the metadata for langfuse.\n\n### Motivation, pitch\n\nWe are using Litellm Proxy across the company and use Langfuse. We would like a nice way for everyone across the company to have their traces more organized through spans.\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "aviramkofman",
      "author_type": "User",
      "created_at": "2025-03-13T11:06:37Z",
      "updated_at": "2025-06-19T00:02:27Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9202/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9202",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9202",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:35.447059",
      "comments": [
        {
          "author": "tarekabouzeid",
          "body": "I think it will be nice if litellm api key alias is sent to langfuse as user name, that way it will be easier to track cost in this setup.",
          "created_at": "2025-03-19T21:24:02Z"
        },
        {
          "author": "tarekabouzeid",
          "body": "I have done these changes [here](https://github.com/tarekabouzeid/litellm/commit/e2a721861411103c0605185d331af90ab9e38d8d)  , now logged traces coming from litellm has api key alias set as user. So far things looks fine but i am not 100% sure if this could break something else. \n@marcklingen Do you ",
          "created_at": "2025-03-20T18:30:13Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-19T00:02:26Z"
        }
      ]
    },
    {
      "issue_number": 9205,
      "title": "[Bug]: UI - Max Budget on Usage page always displays No limit",
      "body": "### What happened?\n\nOn the Usage page of the UI dashboard, the max budget will always display 'No limit' even though max budgets are set.\n\nThe values 'userMaxBudget' and 'selectedTeam' are hardcoded as null for the ViewUserSpend component. So, max budget will always display 'No limit'.\n\n![Image](https://github.com/user-attachments/assets/104482c3-a6ea-4448-82b4-bd9018410c2e)\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.61.13\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "philipn7",
      "author_type": "User",
      "created_at": "2025-03-13T15:03:26Z",
      "updated_at": "2025-06-19T00:02:26Z",
      "closed_at": "2025-06-19T00:02:26Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9205/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9205",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9205",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:35.685587",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-12T00:01:42Z"
        }
      ]
    },
    {
      "issue_number": 9207,
      "title": "Add \"chatgpt\" in \"model_prices_and_context_window.json\"",
      "body": "Source: <SOURCE_URL>\n\nWe need to update both [model_prices_and_context_window.json](https://github.com/BerriAI/litellm/blob/29f2a16e9ee2a26609de59eb26f0e373f146e93d/model_prices_and_context_window.json) and [model_prices_and_context_window_backup.json](https://github.com/BerriAI/litellm/blob/29f2a16e9ee2a26609de59eb26f0e373f146e93d/litellm/model_prices_and_context_window_backup.json) to reflect the new model.\n",
      "state": "closed",
      "author": "joseantoniovilar",
      "author_type": "User",
      "created_at": "2025-03-13T17:23:56Z",
      "updated_at": "2025-06-19T00:02:25Z",
      "closed_at": "2025-06-19T00:02:25Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9207/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9207",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9207",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:36.374563",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-12T00:01:41Z"
        }
      ]
    },
    {
      "issue_number": 9212,
      "title": "[Bug]: o3-mini via Azure does not support Structured Outputs",
      "body": "### What happened?\n\nValidationError for o3-mini via Azure when trying to use Structured Outputs. The response object returns normal verbose chat-like message instead of following Pydantic structure imposed upon it.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.63.2-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "rubberduck583",
      "author_type": "User",
      "created_at": "2025-03-13T19:53:19Z",
      "updated_at": "2025-06-19T00:02:24Z",
      "closed_at": "2025-06-19T00:02:24Z",
      "labels": [
        "bug",
        "mlops user request",
        "stale"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9212/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9212",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9212",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:36.549894",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-12T00:01:40Z"
        }
      ]
    },
    {
      "issue_number": 9213,
      "title": "[Bug]: Bedrock Cohere Embeddings Error",
      "body": "### What happened?\n\nI found some scenarios that when I use \"bedrock/cohere.embed-english-v3\" , when I give a single input written like seen in the SS below, the [\"embedding\"] field from EmbeddingResponse's model data is returning 'float'. Please see the SS below:\n\n![Image](https://github.com/user-attachments/assets/37f436e8-3b83-42c0-91ed-43be9e07fae9)\n\n![Image](https://github.com/user-attachments/assets/0bcb34a0-2b17-435a-9952-b3646c8efbc6)\n\nCould you please investigate this bug ? Because of this issue, dspy.Embedder is returning me this error:\n\n```\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[17], line 3\n      1 test_text = [\"good morning from litellm?\"]\n----> 3 emb = embedder(test_text)\n      4 print(emb.shape)\n\nFile /usr/local/lib/python3.12/site-packages/dspy/clients/embedding.py:116, in Embedder.__call__(self, inputs, batch_size, caching, **kwargs)\n    110         raise ValueError(\n    111             f\"`model` in `dspy.Embedder` must be a string or a callable, but got {type(self.model)}.\"\n    112         )\n    114     embeddings_list.extend(batch_embeddings)\n--> 116 embeddings = np.array(embeddings_list, dtype=np.float32)\n    118 if is_single_input:\n    119     return embeddings[0]\n\nValueError: could not convert string to float: 'float'\n```\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.51.0\n\n### Twitter / LinkedIn details\n\n_No response_\n",
      "state": "closed",
      "author": "belcekaya",
      "author_type": "User",
      "created_at": "2025-03-13T20:19:37Z",
      "updated_at": "2025-06-19T00:02:23Z",
      "closed_at": "2025-06-19T00:02:23Z",
      "labels": [
        "bug",
        "mlops user request",
        "stale"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9213/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9213",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9213",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:36.756978",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-12T00:01:38Z"
        }
      ]
    },
    {
      "issue_number": 9367,
      "title": "[Bug]: Hosted_vllmException - Error code: 404 - {'detail': 'Not Found'}",
      "body": "### What happened?\n\nopenai format services using VLLM, after using Litellm proxy for 20 hours+, there will be an error message when requesting continuously.\n\nError message: \n\n```\nlitellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.NotFoundError: NotFoundError: Hosted_vllmException - Error code: 404 - {'detail': 'Not Found'}. Received Model Group=QwQ-32B\\nAvailable Model Group Fallbacks=None\n\n\nTraceback (most recent call last):\n  File \"/home/litellm/litellm-1.63.11-stable.patch1/litellm/llms/openai/openai.py\", line 952, in async_streaming\n    headers, response = await self.make_openai_chat_completion_request(\n  File \"/home/litellm/litellm-1.63.11-stable.patch1/litellm/litellm_core_utils/logging_utils.py\", line 131, in async_wrapper\n    result = await func(*args, **kwargs)\n  File \"/home/litellm/litellm-1.63.11-stable.patch1/litellm/llms/openai/openai.py\", line 438, in make_openai_chat_completion_request\n    raise e\n  File \"/home/litellm/litellm-1.63.11-stable.patch1/litellm/llms/openai/openai.py\", line 420, in make_openai_chat_completion_request\n    await openai_aclient.chat.completions.with_raw_response.create(\n  File \"/opt/conda/lib/python3.9/site-packages/openai/_legacy_response.py\", line 381, in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n  File \"/opt/conda/lib/python3.9/site-packages/openai/resources/chat/completions/completions.py\", line 2000, in create\n    return await self._post(\n  File \"/opt/conda/lib/python3.9/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/conda/lib/python3.9/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n  File \"/opt/conda/lib/python3.9/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'detail': 'Not Found'}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/litellm/litellm-1.63.11-stable.patch1/litellm/main.py\", line 471, in acompletion\n    response = await init_response\n  File \"/home/litellm/litellm-1.63.11-stable.patch1/litellm/llms/openai/openai.py\", line 1003, in async_streaming\n    raise OpenAIError(\nlitellm.llms.openai.common_utils.OpenAIError: Error code: 404 - {'detail': 'Not Found'}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/litellm/litellm-1.63.11-stable.patch1/litellm/proxy/proxy_server.py\", line 3516, in chat_completion\n    return await base_llm_response_processor.base_process_llm_request(\n  File \"/home/litellm/litellm-1.63.11-stable.patch1/litellm/proxy/common_request_processing.py\", line 208, in base_process_llm_request\n    responses = await llm_responses\n  File \"/home/litellm/litellm-1.63.11-stable.patch1/litellm/router.py\", line 938, in acompletion\n    raise e\n  File \"/home/litellm/litellm-1.63.11-stable.patch1/litellm/router.py\", line 914, in acompletion\n    response = await self.async_function_with_fallbacks(**kwargs)\n  File \"/home/litellm/litellm-1.63.11-stable.patch1/litellm/router.py\", line 3347, in async_function_with_fallbacks\n    raise original_exception\n  File \"/home/litellm/litellm-1.63.11-stable.patch1/litellm/router.py\", line 3161, in async_function_with_fallbacks\n    response = await self.async_function_with_retries(*args, **kwargs)\n  File \"/home/litellm/litellm-1.63.11-stable.patch1/litellm/router.py\", line 3455, in async_function_with_retries\n    self.should_retry_this_error(\n  File \"/home/litellm/litellm-1.63.11-stable.patch1/litellm/router.py\", line 3628, in should_retry_this_error\n    raise error\n  File \"/home/litellm/litellm-1.63.11-stable.patch1/litellm/router.py\", line 3430, in async_function_with_retries\n    response = await self.make_call(original_function, *args, **kwargs)\n  File \"/home/litellm/litellm-1.63.11-stable.patch1/litellm/router.py\", line 3546, in make_call\n    response = await response\n  File \"/home/litellm/litellm-1.63.11-stable.patch1/litellm/router.py\", line 1077, in _acompletion\n    raise e\n  File \"/home/litellm/litellm-1.63.11-stable.patch1/litellm/router.py\", line 1036, in _acompletion\n    response = await _response\n  File \"/home/litellm/litellm-1.63.11-stable.patch1/litellm/utils.py\", line 1441, in wrapper_async\n    raise e\n  File \"/home/litellm/litellm-1.63.11-stable.patch1/litellm/utils.py\", line 1300, in wrapper_async\n    result = await original_function(*args, **kwargs)\n  File \"/home/litellm/litellm-1.63.11-stable.patch1/litellm/main.py\", line 490, in acompletion\n    raise exception_type(\n  File \"/home/litellm/litellm-1.63.11-stable.patch1/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2214, in exception_type\n    raise e\n  File \"/home/litellm/litellm-1.63.11-stable.patch1/litellm/litellm_core_utils/exception_mapping_utils.py\", line 402, in exception_type\n    raise NotFoundError(\nlitellm.exceptions.NotFoundError: litellm.NotFoundError: NotFoundError: Hosted_vllmException - Error code: 404 - {'detail': 'Not Found'}. Received Model Group=QwQ-32B\nAvailable Model Group Fallbacks=None\n```\n\nconfig:\n\n```\nmodel_list:\n  - model_name: QwQ-32B\n    litellm_params:\n      model: hosted_vllm/QwQ-32B\n      api_base: http://10.24.45.213:11036/v1\n      api_key: sk-xxx\n\n  - model_name: deepseek-reasoner\n    litellm_params:\n      model: deepseek/deepseek-reasoner\n      api_base: https://api.deepseek.com/v1\n      api_key: sk-xxx\n\n  - model_name: deepseek-chat\n    litellm_params:\n      model: deepseek/deepseek-chat\n      api_base: https://api.deepseek.com/v1\n      api_key: sk-xxx\n\nlitellm_settings:\n  success_callback: [\"langfuse\"]\n  callbacks: [\"langfuse\"]\n  turn_off_message_logging: False\n  redact_user_api_key_info: False\n  langfuse_default_tags: [\"cache_hit\", \"cache_key\", \"proxy_base_url\", \"user_api_key_alias\", \"user_api_key_user_id\", \"user_api_key_user_email\", \"user_api_key_team_alias\", \"semantic-similarity\"]\n\n  request_timeout: 600\n\n  set_verbose: True\n  json_logs: True\n\nenvironment_variables:\n  LANGFUSE_PUBLIC_KEY: \"pk-lf-xxx\"\n  LANGFUSE_SECRET_KEY: \"sk-lf-xxx\"\n  LANGFUSE_HOST: \"http://10.24.45.210:3000\"\n  LITELLM_MASTER_KEY: \"sk-xxx\"\n  DATABASE_URL: \"postgresql://xxx\"\n  PORT: 8089\n  UI_USERNAME: \"xxx\"\n  UI_PASSWORD: \"xxx\"\n\n```\n\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.63.11 & v1.63.11-stable.patch1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "MiyazonoKaori",
      "author_type": "User",
      "created_at": "2025-03-19T11:12:50Z",
      "updated_at": "2025-06-19T00:02:15Z",
      "closed_at": null,
      "labels": [
        "bug",
        "awaiting: user response",
        "stale"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9367/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9367",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9367",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:36.947825",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Hi @MiyazonoKaori please run with `--detailed_debug` to see the raw request - https://docs.litellm.ai/docs/proxy/debugging#detailed-debug\n\nIt should show the request sent ",
          "created_at": "2025-03-20T06:04:16Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-19T00:02:15Z"
        }
      ]
    },
    {
      "issue_number": 9370,
      "title": "Fix \"vertex_ai/claude-3-5-sonnet-v2@20241022\" entry in \"model_prices_and_context_window.json\"",
      "body": "https://github.com/BerriAI/litellm/blob/01c6cbd2705c65119b46866c0b4a44c9d78c662f/model_prices_and_context_window.json#L4741-L4754\n\nWe also need to update [model_prices_and_context_window_backup.json](https://github.com/BerriAI/litellm/blob/01c6cbd2705c65119b46866c0b4a44c9d78c662f/litellm/model_prices_and_context_window_backup.json).\n\nClaude models on vertex ai should be configured like:\nhttps://github.com/BerriAI/litellm/blob/01c6cbd2705c65119b46866c0b4a44c9d78c662f/model_prices_and_context_window.json#L4755-L4774\n\nSource: https://cloud.google.com/vertex-ai/generative-ai/pricing#claude-models\n\n",
      "state": "open",
      "author": "rhoentier",
      "author_type": "User",
      "created_at": "2025-03-19T15:14:15Z",
      "updated_at": "2025-06-19T00:02:14Z",
      "closed_at": null,
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9370/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9370",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9370",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:37.144693",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Hi @rhoentier i don't follow - what's the bug? ",
          "created_at": "2025-03-20T06:02:51Z"
        },
        {
          "author": "rhoentier",
          "body": "The claude models need more config parameters as in `vertex_ai/claude-3-7-sonnet@20250219`.\nFor example, only claude-3.7 has the prompt caching parameter. I think most claude models have the same values as claude-3.7, but I am not 100% sure.",
          "created_at": "2025-03-20T20:12:20Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-19T00:02:14Z"
        }
      ]
    },
    {
      "issue_number": 9389,
      "title": "[Feature]: Add testing for team callback endpoints",
      "body": "### The Feature\n\nNeed coverage for team testing endpoints:\n- DELETE\n- PATCH\n- POST\n- GET\n\ntests `tests/litellm/proxy/management_endpoints`\n\n### Motivation, pitch\n\n_\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "CakeCrusher",
      "author_type": "User",
      "created_at": "2025-03-20T01:13:15Z",
      "updated_at": "2025-06-19T00:02:13Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9389/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9389",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9389",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:37.398996",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-19T00:02:12Z"
        }
      ]
    },
    {
      "issue_number": 9391,
      "title": "[Bug]: Redis calls fails when using router lowest_latency strategy",
      "body": "### What happened?\n\nRedis calls fails with JSON serialization exception when router is configured with lowest_latency strategy\n\n### Relevant log output\n\n```shell\n2025-03-17 17:03:19,894 - LiteLLM - ERROR - Error occurred in async batch get cache - Task <Task pending name='Task-1250947' coro=<DualCache.async_batch_get_cache() running at /usr/lib/python3.13/site-packages/litellm/caching/dual_cache.py:280> cb=[run_until_complete.<locals>.done_cb()]> got Future <Future pending> attached to a different loop\n{\"message\": \"LiteLLM Redis Caching: async set() - Got exception from REDIS Object of type timedelta is not JSON serializable, Writing value={'697b69a1-9f34-4ae2-93f7-40341cdfe032': {'latency': [datetime.timedelta(microseconds=946864), datetime.timedelta(microseconds=798497), datetime.timedelta(microseconds=879579), datetime.timedelta(microseconds=454938), datetime.timedelta(seconds=1, microseconds=116129), datetime.timedelta(microseconds=929022), datetime.timedelta(microseconds=943053), datetime.timedelta(microseconds=476479), datetime.timedelta(seconds=1, microseconds=269783), datetime.timedelta(microseconds=762550)], '2025-03-16-05-35': {'tpm': 0, 'rpm': 1}, '2025-03-16-05-37': {'tpm': 0, 'rpm': 3}, '2025-03-16-06-06': {'tpm': 0, 'rpm': 1}, '2025-03-16-06-09': {'tpm': 0, 'rpm': 3}, '2025-03-16-06-14': {'tpm': 0, 'rpm': 1}, '2025-03-16-06-18': {'tpm': 0, 'rpm': 1}, '2025-03-16-06-21': {'tpm': 0, 'rpm': 3}, '2025-03-16-06-26': {'tpm': 0, 'rpm': 1}, '2025-03-16-06-30': {'tpm': 0, 'rpm': 1}, '2025-03-16-06-31': {'tpm': 0, 'rpm': 1}, '2025-03-16-06-41': {'tpm': 0, 'rpm': 2}, '2025-03-16-06-59': {'tpm': 0, 'rpm': 2}, '2025-03-16-07-02': {'tpm': 0, 'rpm': 1}, '2025-03-16-07-04': {'tpm': 0, 'rpm': 4}, '2025-03-16-07-07': {'tpm': 0, 'rpm': 1},\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.63.12\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "emerzon",
      "author_type": "User",
      "created_at": "2025-03-20T01:30:47Z",
      "updated_at": "2025-06-19T00:02:11Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request",
        "stale"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9391/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9391",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9391",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:37.592683",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-19T00:02:11Z"
        }
      ]
    },
    {
      "issue_number": 9396,
      "title": "[Bug]: Ambiguous error messages on callback configuration UI",
      "body": "### What happened?\n\nError messages on team callback configuration are not clear\n\n### Relevant log output\n\n![Image](https://github.com/user-attachments/assets/9da35e87-8b84-4e34-af8d-ef23f269abcb)\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n_\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "CakeCrusher",
      "author_type": "User",
      "created_at": "2025-03-20T02:35:25Z",
      "updated_at": "2025-06-19T00:02:10Z",
      "closed_at": null,
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9396/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9396",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9396",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:37.779378",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-19T00:02:09Z"
        }
      ]
    },
    {
      "issue_number": 9403,
      "title": "End User Tracking for Google AI Studio SDK - Gemini use.",
      "body": "Would like to have end user tracking for Google Gemini API usage.   \n\nUsecase: Gemini API is configured on the server side using litellm.  Individual application users need to be tracked for usage (application connects with centrally deployed litellm)",
      "state": "open",
      "author": "bytlinc",
      "author_type": "User",
      "created_at": "2025-03-20T07:23:10Z",
      "updated_at": "2025-06-19T00:02:08Z",
      "closed_at": null,
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9403/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9403",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9403",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:38.025301",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-19T00:02:08Z"
        }
      ]
    },
    {
      "issue_number": 9421,
      "title": "[Bug]: [UI] If you refresh on a deep lik it may not populate the state",
      "body": "### What happened?\n\nFor example if you navigate from virtual keys that will populate user keys  and then you navigate to Usage->Customer Usage then there will be keys from which to select from. If you reload the page at usage (clearing state) there will be no keys to select from.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n_\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "CakeCrusher",
      "author_type": "User",
      "created_at": "2025-03-20T20:58:21Z",
      "updated_at": "2025-06-19T00:02:07Z",
      "closed_at": null,
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9421/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9421",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9421",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:38.206308",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-19T00:02:07Z"
        }
      ]
    },
    {
      "issue_number": 11667,
      "title": "[Bug]: Gemini Flash 2.5 cost calculation is incorrect",
      "body": "### What happened?\n\nFor the Gemini Flash 2.5 model (gemini/gemini-2.5-flash-preview-05-20), the cost is listed as this: \n  \"output_cost_per_token\": 6e-7,\n  \"output_cost_per_reasoning_token\": 0.0000035,\n\nThis is not correct because the way Gemini bills for output token is that **all** tokens, both regular completion tokens and thinking tokens, are charged at the higher rate $3.50/1M tokens if thinking is enabled. \n\nRight now, LiteLLM is doing a [blended cost calculation](https://github.com/BerriAI/litellm/blob/b8bdf98a4bfd42404af5fb41a7e3e3371597e972/litellm/litellm_core_utils/llm_cost_calc/utils.py#L337-L343) charging the regular completion tokens at the lower rate ($0.60/1M tokens) and the reasoning tokens at the higher rate ($3.50/1M tokens).\n\nYou can see how Google describes the pricing on their [Vertex AI pricing table](https://cloud.google.com/vertex-ai/generative-ai/pricing#gemini-models-2.5) (but it's the same pricing for Gemini/AI Studio API)\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nany version\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "wwwillchen",
      "author_type": "User",
      "created_at": "2025-06-12T17:17:37Z",
      "updated_at": "2025-06-18T22:12:37Z",
      "closed_at": "2025-06-18T22:12:37Z",
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11667/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11667",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11667",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:38.421001",
      "comments": [
        {
          "author": "wwwillchen",
          "body": "Closing because this not relevant for Gemini Flash 2.5 GA pricing which unifies reasoning and non-reasoning token pricing.",
          "created_at": "2025-06-18T22:12:37Z"
        }
      ]
    },
    {
      "issue_number": 11857,
      "title": "[Bug]:  Prometheus document uses the outdated model gpt-3.5",
      "body": "### What happened?\n\nThe document for Prometheus setup instructs user to add gpt-3.5-turbo to config which has been retired by azure. the config has already been updated to new model.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.6\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "Shankyg",
      "author_type": "User",
      "created_at": "2025-06-18T14:16:25Z",
      "updated_at": "2025-06-18T21:05:13Z",
      "closed_at": "2025-06-18T21:05:13Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11857/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11857",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11857",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:38.583746",
      "comments": []
    },
    {
      "issue_number": 8847,
      "title": "[Bug]: AWS Region Name is not taken from boto3 session",
      "body": "### What happened?\n\nHere's the region setting code:\n\nhttps://github.com/BerriAI/litellm/blob/main/litellm/llms/bedrock/chat/converse_handler.py#L313\n```python\n\n        ### SET REGION NAME ###\n        if aws_region_name is None:\n            # check env #\n            litellm_aws_region_name = get_secret(\"AWS_REGION_NAME\", None)\n\n            if litellm_aws_region_name is not None and isinstance(\n                litellm_aws_region_name, str\n            ):\n                aws_region_name = litellm_aws_region_name\n\n            standard_aws_region_name = get_secret(\"AWS_REGION\", None)\n            if standard_aws_region_name is not None and isinstance(\n                standard_aws_region_name, str\n            ):\n                aws_region_name = standard_aws_region_name\n\n            if aws_region_name is None:\n                aws_region_name = \"us-west-2\"\n\n        litellm_params[\"aws_region_name\"] = (\n            aws_region_name  # [DO NOT DELETE] important for async calls\n        )\n```\nHowever all these locations do not account for sessions initiated with `aws sso`, which has credentials stored in ~/.aws/sso/cache.\n\nPerhaps the last condition could try to retrieve the region from the session?\n\n```python\n            if aws_region_name is None:\n                import boto3\n                session = boto3.Session()\n                configured_region = session.region_name\n                if configured_region:\n                    aws_region_name = configured_region\n                else:\n                    aws_region_name = \"us-west-2\"\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.61.9\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "q2mark",
      "author_type": "User",
      "created_at": "2025-02-26T16:22:18Z",
      "updated_at": "2025-06-18T19:59:41Z",
      "closed_at": "2025-06-18T19:48:19Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8847/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8847",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8847",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:38.583760",
      "comments": [
        {
          "author": "q2mark",
          "body": "Also, I see you're checking for AWS_REGION_NAME and AWS_REGION but not AWS_DEFAULT_REGION, which is the more standard way of specifying that in the environment.",
          "created_at": "2025-02-26T16:46:41Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Hey @q2mark acknowledging this! A PR here is welcome, if you have the bandwidth for it. ",
          "created_at": "2025-02-27T07:44:55Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-05-29T00:01:35Z"
        },
        {
          "author": "jonalee99",
          "body": "+1 on this issue, it's annoying to have to set the env var too.",
          "created_at": "2025-06-17T16:36:04Z"
        },
        {
          "author": "krrishdholakia",
          "body": "will try to do this today @jonalee99 @q2mark ",
          "created_at": "2025-06-17T20:05:02Z"
        }
      ]
    },
    {
      "issue_number": 11820,
      "title": "[Bug]: [bridge for /chat/completion -> /responses API]  Images Sent via URL Not Working in /chat/completions for OpenAI o1-pro, o3-pro, codex-mini-latest",
      "body": "### What happened?\n\nHi @krrishdholakia , @ishaan-jaff !\n\nWhen sending images using the image_url format via the `/chat/completions` endpoint for the models `openai/o1-pro`, `openai/o3-pro`, and `openai/codex-mini-latest`, the request fails with a 400 error. The same images, when sent via the `/responses` API, work correctly.\n\n### **Details**\nExample request to `/chat/completions` (does not work):\n```\ncurl --request POST \\\n  --url http://localhost:4000/v1/chat/completions \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n    \"model\": \"openai/o3-pro\",\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\n            \"type\": \"text\",\n            \"text\": \"What‚Äôs in this image?\"\n          },\n          {\n            \"type\": \"image_url\",\n            \"image_url\": {\n              \"url\": \"https://w7.pngwing.com/pngs/666/274/png-transparent-image-pictures-icon-photo-thumbnail.png\"\n            }\n          }\n        ]\n      }\n    ],\n    \"stream\": false\n  }'\n\n```\n\nResponse:\n```\n{\n  \"error\": {\n    \"message\": \"litellm.BadRequestError: OpenAIException - {\\n  \\\"error\\\": {\\n    \\\"message\\\": \\\"Invalid type for 'input[0].content[1].image_url': expected an image URL, but got an object instead.\\\",\\n    \\\"type\\\": \\\"invalid_request_error\\\",\\n    \\\"param\\\": \\\"input[0].content[1].image_url\\\",\\n    \\\"code\\\": \\\"invalid_type\\\"\\n  }\\n}. Received Model Group=openai/o3-pro\\nAvailable Model Group Fallbacks=None\",\n    \"type\": null,\n    \"param\": null,\n    \"code\": \"400\"\n  }\n}\n```\n\n_Note:_ The same request, when the image is sent in base64 format via the `/chat/completions` endpoint, works correctly.\n\n### /responses API Works as Expected\nWhen sending the equivalent request via the /responses endpoint, the image URL is handled correctly and there is no error.\n\nExample request to `/responses` (works fine):\n```\ncurl --request POST \\\n  --url http://localhost:4000/v1/responses \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n    \"model\": \"openai/o3-pro\",\n    \"input\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          { \"type\": \"input_text\", \"text\": \"what is in this image?\" },\n          { \"type\": \"input_image\", \"image_url\": \"https://w7.pngwing.com/pngs/666/274/png-transparent-image-pictures-icon-photo-thumbnail.png\" }\n        ]\n      }\n    ]\n  }'\n\n```\n\nRelated issue:\n\n- [Add support for using Responses API in /chat/completions spec](https://github.com/BerriAI/litellm/issues/9754#top) \n\nThanks in advance!\n\n### Relevant log output\n\n```shell\n{\n  \"error\": {\n    \"message\": \"litellm.BadRequestError: OpenAIException - {\\n  \\\"error\\\": {\\n    \\\"message\\\": \\\"Invalid type for 'input[0].content[1].image_url': expected an image URL, but got an object instead.\\\",\\n    \\\"type\\\": \\\"invalid_request_error\\\",\\n    \\\"param\\\": \\\"input[0].content[1].image_url\\\",\\n    \\\"code\\\": \\\"invalid_type\\\"\\n  }\\n}. Received Model Group=openai/o3-pro\\nAvailable Model Group Fallbacks=None\",\n    \"type\": null,\n    \"param\": null,\n    \"code\": \"400\"\n  }\n}\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.6.rc\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "mvrodrig",
      "author_type": "User",
      "created_at": "2025-06-17T20:06:27Z",
      "updated_at": "2025-06-18T19:48:18Z",
      "closed_at": "2025-06-18T19:48:18Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11820/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "krrishdholakia"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11820",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11820",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:38.757282",
      "comments": []
    },
    {
      "issue_number": 11582,
      "title": "[Bug]: Streaming not functioning for 'gemini-2.5-flash' and 'gemini-2.5-pro' models in version litellm: main-v1.72.2.rc",
      "body": "### What happened?\n\nDescription:\nThank you for the well-optimized new version, it is indeed faster and more economical!\n\nHowever, in the latest version of Lite LLM, `main-v1.72.2.rc`, I encountered an issue where streaming does not function properly for the `gemini-2.5-flash` and `gemini-2.5-pro` models. Streaming works correctly with earlier models such as `gemini-2.0-flash`, but does not work with newer ones. All other models from open-ai, claude work fine! \nEverything worked on an earlier version of litellm `litellm_stable_release_branch-v1.72.0.rc1`\n\nSteps to reproduce:\n\n```\nimport OpenAI from ‚Äúopenai‚Äù;\n\nconst litellmClient = new OpenAI({\n    apiKey: ‚Äúsk-.........‚Äù,\n    baseURL: `http://litellm.........:4000`,\n});\n(async () => {\n    const stream = await litellmClient.chat.completions.create({\n        model: ‚Äúgemini-2.5-flash‚Äù, // also try ‚Äúgemini-2.5-pro‚Äù\n        messages: [\n            {\n                role: ‚Äúuser‚Äù,\n                content: ‚ÄúSay ‚Äòdouble bubble bath‚Äô ten times fast.‚Äù,\n            }\n        ],\n        stream: true,\n    });\n\n    let content = ‚Äú‚Äù;\n    let countChunk = 0;\n    for await (const chunk of stream) {\n        console.log(chunk);\n        console.log(chunk.choices[0].delta);\n        content += chunk.choices[0].delta.content || ‚Äú‚Äù;\n        countChunk++;\n        console.log(‚Äú****************‚Äù);\n}\nconsole.log(‚Äú>>> Total chunks received:‚Äù, countChunk);\nconsole.log(‚Äú>>> Final content:\\n‚Äù, content);\n})();\n```\nFor `gemini-2.0-flash` model. Its ok! These are the latest messages in the logs\n```\n****************\n{\n  id: '6gBIaLfDPPesgLUPp9GE4Q0',\n  created: 1749549291,\n  model: 'gemini-2.0-flash-001',\n  object: 'chat.completion.chunk',\n  choices: [ { index: 0, delta: [Object] } ]\n}\n{\n  content: ' double bubble bath, double bubble bath, double bubble bath.\\n'\n}\n****************\n{\n  id: '6gBIaLfDPPesgLUPp9GE4Q0',\n  created: 1749549291,\n  model: 'gemini-2.0-flash-001',\n  object: 'chat.completion.chunk',\n  choices: [ { finish_reason: 'stop', index: 0, delta: {} } ]\n}\n{}\n****************\n>>> Total chunks received: 6\n>>> Final content:\n Okay, I'll try!\n\nDouble bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath.\n```\n\nBut! For `gemini-2.5-flash` and `gemini-2.5-pro` models, I get\n```\n{\n  id: 'RQVIaI-qDZGsgLUP09ePqAE',\n  created: 1749550405,\n  model: 'gemini-2.5-flash-preview-05-20',\n  object: 'chat.completion.chunk',\n  choices: [ { finish_reason: 'stop', index: 0, delta: {} } ]\n}\n{}\n****************\n>>> Total chunks received: 1\n>>> Final content:\n```\nAs a result, empty content is displayed.\n\nAll other endpoints are working fine.\nPlease help as soon as possible\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\n1.72.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "YuriyTW",
      "author_type": "User",
      "created_at": "2025-06-10T10:26:33Z",
      "updated_at": "2025-06-18T18:31:19Z",
      "closed_at": "2025-06-12T15:31:13Z",
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11582/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11582",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11582",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:38.757309",
      "comments": [
        {
          "author": "YuriyTW",
          "body": "I also tried adding the litellm parameter `merge_reasoning_content_in_choices` to the model.\nI was hoping to see at least a <think> block in the stream.\nBut it's not there either. The answer is the same as I wrote above.",
          "created_at": "2025-06-10T11:49:25Z"
        },
        {
          "author": "terrydang",
          "body": "I ran into the same issue as you.",
          "created_at": "2025-06-10T12:28:15Z"
        },
        {
          "author": "mherreshoff",
          "body": "I also had the same problem and downgrading to the `litellm_stable_release_branch-v1.72.0.rc1` branch fixed it for me.",
          "created_at": "2025-06-11T20:41:43Z"
        },
        {
          "author": "jbellis",
          "body": "Seeing the same (?) issue in 1.72.2-stable. I get the streamed chunks but I never get the \"finished\" message to close out the request.",
          "created_at": "2025-06-12T15:10:20Z"
        },
        {
          "author": "jbellis",
          "body": "@krrishdholakia this is a pretty big regression in 1.72.2",
          "created_at": "2025-06-12T15:16:59Z"
        }
      ]
    },
    {
      "issue_number": 11415,
      "title": "[Feature]: Support reranker with vllm provider",
      "body": "### The Feature\n\nHi Everyone,\nwould like to know if it is possible to add in the stable version of `litellm` a provider for open source reranker (`vllm/` or even `openai/`). I saw your documentation on how to add a [new-reranker-provider](https://docs.litellm.ai/docs/adding_provider/new_rerank_provider), and it seems to work.\n\n### Motivation, pitch\n\nI wanted to check if it is possible to add support for open-source reranker into `main`, so it is available for everyone. In that way, I won't need to rebase every time there are new updates and still want use open-source reranker. \nThanks\n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "federicovallucci",
      "author_type": "User",
      "created_at": "2025-06-04T19:07:27Z",
      "updated_at": "2025-06-18T16:27:43Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11415/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11415",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11415",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:38.942973",
      "comments": [
        {
          "author": "harvardfly",
          "body": "@federicovallucci I've also found that the open-source rerank model deployed with vLLM cannot be used through LiteLLM. How are you addressing this issue now?",
          "created_at": "2025-06-10T05:56:51Z"
        },
        {
          "author": "federicovallucci",
          "body": "hey @harvardfly ! I have cloned the `litellm` repo and added some custom code following this: [add reranker provider](https://docs.litellm.ai/docs/adding_provider/new_rerank_provider). However, having the custom code is not ideal cause I need to rebase when there are new updates.\n\nFrom a client side",
          "created_at": "2025-06-10T10:29:45Z"
        },
        {
          "author": "i-makashev",
          "body": "[Here](https://github.com/BerriAI/litellm/issues/4712) is mentioned that we can configure pass-through endpoint, but I can't figure out how to make it to work. I get different errors: `Unsupported provider cohere_chat`, `invalid api_key` and so on.",
          "created_at": "2025-06-18T16:25:16Z"
        }
      ]
    },
    {
      "issue_number": 4712,
      "title": "[Bug]: LiteLLM does not support  custom rerank model server.",
      "body": "### What happened?\n\nA bug happened!\r\nI  add a new  model:  bge-reranker-large.\r\n\r\nbut the API  response is :\r\n{\r\n   \"detail\": \"Not Found\"\r\n}\n\n### Relevant log output\n\n_No response_\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "hao-cold",
      "author_type": "User",
      "created_at": "2024-07-15T13:40:40Z",
      "updated_at": "2025-06-18T16:23:21Z",
      "closed_at": "2024-07-15T16:16:59Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/4712/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/4712",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/4712",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:39.148297",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "@cheng92hao you can create a pass through endpoint : https://docs.litellm.ai/docs/proxy/pass_through#tutorial---pass-through-cohere-re-rank-endpoint ",
          "created_at": "2024-07-15T16:16:58Z"
        },
        {
          "author": "S1LV3RJ1NX",
          "body": "@ishaan-jaff I am trying to use [infinity-server](https://github.com/michaelfeil/infinity), that helps hosting hugging face reranker models, but this server does not provide `meta` dict that causes API to fail how to solve for that?",
          "created_at": "2024-10-15T19:45:47Z"
        },
        {
          "author": "i-makashev",
          "body": "> @cheng92hao you can create a pass through endpoint : https://docs.litellm.ai/docs/proxy/pass_through#tutorial---pass-through-cohere-re-rank-endpoint\n\nHi, could you specify example how to make it to work. I really can't figure out how to configure pass-through to my vLLM endpoint",
          "created_at": "2025-06-18T16:23:21Z"
        }
      ]
    },
    {
      "issue_number": 11845,
      "title": "[Bug]: can connect to mcp server. Get authorization error",
      "body": "### What happened?\n\nUsing  1.72.6-rc1 though I wasn't able to try using MCP prior to this version. I am trying to connect with roocode, but also tried using mcp inspection tool. litellm gives the following error.  Which sounds like I am not setting the header correct, but I think I am, here is the mcp client setup\n\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"https://mcp.context7.com/mcp\"\n    },\n    \"LiteLLM\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"https://mymcpserver.com/mcp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer <MY_KEY>\"\n      }\n    }\n  }\n}\n\n```\nlitellm-1  | INFO:     192.168.32.1:40644 - \"POST /mcp HTTP/1.1\" 307 Temporary Redirect\nlitellm-1  | 11:06:04 - LiteLLM Proxy:ERROR: auth_exception_handler.py:79 - litellm.proxy.proxy_server.user_api_key_auth(): Exception occured - Malformed API Key passed in. Ensure Key has `Bearer ` prefix. Passed in: \nlitellm-1  | Requester IP Address:192.168.32.1\nlitellm-1  | Traceback (most recent call last):\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/litellm/proxy/auth/user_api_key_auth.py\", line 576, in _user_api_key_auth_builder\nlitellm-1  |     raise Exception(\nlitellm-1  |         f\"Malformed API Key passed in. Ensure Key has `Bearer ` prefix. Passed in: {passed_in_key}\"\nlitellm-1  |     )\nlitellm-1  | Exception: Malformed API Key passed in. Ensure Key has `Bearer ` prefix. Passed in: \nlitellm-1  | 11:06:04 - LiteLLM:ERROR: server.py:342 - Error handling MCP request: \nlitellm-1  | Traceback (most recent call last):\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/litellm/proxy/auth/user_api_key_auth.py\", line 576, in _user_api_key_auth_builder\nlitellm-1  |     raise Exception(\nlitellm-1  |         f\"Malformed API Key passed in. Ensure Key has `Bearer ` prefix. Passed in: {passed_in_key}\"\nlitellm-1  |     )\nlitellm-1  | Exception: Malformed API Key passed in. Ensure Key has `Bearer ` prefix. Passed in: \nlitellm-1  | \nlitellm-1  | During handling of the above exception, another exception occurred:\nlitellm-1  | \nlitellm-1  | Traceback (most recent call last):\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/litellm/proxy/_experimental/mcp_server/server.py\", line 329, in handle_streamable_http_mcp\nlitellm-1  |     await UserAPIKeyAuthMCP.user_api_key_auth_mcp(scope)\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/litellm/proxy/_experimental/mcp_server/auth/user_api_key_auth_mcp.py\", line 51, in user_api_key_auth_mcp\nlitellm-1  |     validated_user_api_key_auth = await user_api_key_auth(\nlitellm-1  |                                   ^^^^^^^^^^^^^^^^^^^^^^^^\nlitellm-1  |         api_key=litellm_api_key, request=request\nlitellm-1  |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlitellm-1  |     )\nlitellm-1  |     ^\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/litellm/proxy/auth/user_api_key_auth.py\", line 1137, in user_api_key_auth\nlitellm-1  |     user_api_key_auth_obj = await _user_api_key_auth_builder(\nlitellm-1  |                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlitellm-1  |     ...<8 lines>...\nlitellm-1  |     )\nlitellm-1  |     ^\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/litellm/proxy/auth/user_api_key_auth.py\", line 1104, in _user_api_key_auth_builder\nlitellm-1  |     return await UserAPIKeyAuthExceptionHandler._handle_authentication_error(\nlitellm-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlitellm-1  |     ...<6 lines>...\nlitellm-1  |     )\nlitellm-1  |     ^\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/litellm/proxy/auth/auth_exception_handler.py\", line 119, in _handle_authentication_error\nlitellm-1  |     raise ProxyException(\nlitellm-1  |     ...<4 lines>...\nlitellm-1  |     )\nlitellm-1  | litellm.proxy._types.ProxyException\nlitellm-1  | INFO:     192.168.32.1:40644 - \"POST /mcp/ HTTP/1.1\" 500 Internal Server Error\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.72.6-rc1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "ForceConstant",
      "author_type": "User",
      "created_at": "2025-06-18T11:09:37Z",
      "updated_at": "2025-06-18T15:57:48Z",
      "closed_at": "2025-06-18T14:50:56Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11845/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11845",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11845",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:39.346356",
      "comments": [
        {
          "author": "harvardfly",
          "body": "My performance is the same as yours; the SSE-type MCP I'm using doesn't work either.  @ForceConstant ",
          "created_at": "2025-06-18T12:42:40Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "Hi @ForceConstant can you try setting it like this \n\n```\n{\n\"mcpServers\": {\n\"LiteLLM\": {\n\"type\": \"streamable-http\",\n\"url\": \"https://mymcpserver.com/mcp\",\n\"headers\": {\n\"x-litellm-api-key\": \"Bearer sk-1234\"\n}\n}\n}\n```",
          "created_at": "2025-06-18T14:08:06Z"
        },
        {
          "author": "ForceConstant",
          "body": "Thanks, yes that worked. I got the use of header \"Authorization\" from the \"MCP Servers\"->\"Connect\"->Cursor. ",
          "created_at": "2025-06-18T14:18:25Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "Awesome ! i'm working on improving MCPs over this month. Let me know if you have any feedback. ",
          "created_at": "2025-06-18T14:23:31Z"
        },
        {
          "author": "harvardfly",
          "body": "I am using cline, configured like this, but it still doesn't work. Previous versions before v1.72.2 were fine.\n\n<img width=\"351\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/1a509f57-53a4-4213-993f-6ab8a9d30abb\" />\n\nSetting Cache on Proxy\n{\"message\": \"litellm.proxy.proxy_server.user_a",
          "created_at": "2025-06-18T15:31:52Z"
        }
      ]
    },
    {
      "issue_number": 11856,
      "title": "[Bug]: Outdated model in billing docs example",
      "body": "### What happened?\n\nBilling.md has example snippets for Langchain and curl, which call the older gpt-3.5-turbo instead of gpt-4o.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.6\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "karen-veigas",
      "author_type": "User",
      "created_at": "2025-06-18T14:11:12Z",
      "updated_at": "2025-06-18T15:19:38Z",
      "closed_at": "2025-06-18T15:19:38Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11856/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11856",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11856",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:39.535935",
      "comments": []
    },
    {
      "issue_number": 11853,
      "title": "[Bug]: Alerting.md document uses old model gpt-3.5-turbo",
      "body": "### What happened?\n\nThe docs have an outdated model, gpt-3.5-turbo, present when adding alerting metadata to proxy calls for debugging. The docs should show an update to a newer model such as gpt-4o.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.6\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "karen-veigas",
      "author_type": "User",
      "created_at": "2025-06-18T13:10:30Z",
      "updated_at": "2025-06-18T15:19:38Z",
      "closed_at": "2025-06-18T15:19:38Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11853/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11853",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11853",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:39.535955",
      "comments": []
    },
    {
      "issue_number": 11798,
      "title": "[Bug]: v1/messages endpoint always uses us-central1 with vertex_ai-anthropic models",
      "body": "### What happened?\n\n```yaml\n\n  - model_name: claude-3-7-sonnet\n    litellm_params:\n      model: vertex_ai/claude-3-7-sonnet@20250219\n      vertex_ai_project: \"os.environ/GCP_PROJECT\"\n      vertex_ai_location: \"europe-west1\"\n\n...\n\n  - model_name: gemini-2.5-pro-us\n    litellm_params:\n      model: vertex_ai/gemini-2.5-pro-preview-05-06\n      vertex_project: \"os.environ/GCP_PROJECT\"\n      vertex_location: \"us-central1\"\n\n...\n\n  - model_name: gemini-1.5-pro\n    litellm_params:\n      model: vertex_ai/gemini-1.5-pro\n      vertex_project: \"os.environ/GCP_PROJECT\"\n      vertex_location: \"europe-west1\"\n\n```\nI have the above models in my proxy config, and all of them work fine normally. However when trying to use claude on vertex-ai via the \"v1/messages\" endpoint (in order to use it with Claude Code), I get an error, because LiteLLM is trying to reach Claude on us-central1:\n`httpx.HTTPStatusError: Server error '500 Internal Server Error' for url 'https://us-central1-aiplatform.googleapis.com/v1/projects/my-project/locations/us-central1/publishers/anthropic/models/claude-3-7-sonnet@20250219:rawPredict'`\n\nBoth Gemeni Models with different Regions Work fine.\n\nI think this is probably a bug, maybe the mismatched `vertex_location` and `vertex_ai_location` have something to do with it? The config should be fine according to the docs: [https://docs.litellm.ai/docs/providers/vertex#anthropic](https://docs.litellm.ai/docs/providers/vertex#anthropic)\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "fjodorsch",
      "author_type": "User",
      "created_at": "2025-06-17T16:08:21Z",
      "updated_at": "2025-06-18T14:00:05Z",
      "closed_at": "2025-06-18T14:00:05Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11798/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11798",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11798",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:39.535963",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "Fixed here: https://github.com/BerriAI/litellm/pull/11831 ",
          "created_at": "2025-06-18T04:44:46Z"
        }
      ]
    },
    {
      "issue_number": 5650,
      "title": "[Bug]: Error parsing chunk: Expecting property name enclosed in double quotes",
      "body": "### What happened?\r\n\r\nOccasionally encountering this error when using the Gemini 1.5 Flash model\r\nVerison: 1.44.24\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nStreaming error: litellm.APIConnectionError: Error parsing chunk: Expecting property name enclosed in double quotes: line 1 column 2 (char 1),\r\nReceived chunk: {\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Nice\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\litellm\\llms\\vertex_ai_and_google_ai_studio\\gemini\\vertex_and_google_ai_studio_gemini.py\", line 1565, in __next__\r\n    return self._common_chunk_parsing_logic(chunk=chunk)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\Nice\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\litellm\\llms\\vertex_ai_and_google_ai_studio\\gemini\\vertex_and_google_ai_studio_gemini.py\", line 1541, in _common_chunk_parsing_logic\r\n    return self.handle_valid_json_chunk(chunk=chunk)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\Nice\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\litellm\\llms\\vertex_ai_and_google_ai_studio\\gemini\\vertex_and_google_ai_studio_gemini.py\", line 1503, in handle_valid_json_chunk\r\n    raise e\r\n  File \"C:\\Users\\Nice\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\litellm\\llms\\vertex_ai_and_google_ai_studio\\gemini\\vertex_and_google_ai_studio_gemini.py\", line 1495, in handle_valid_json_chunk\r\n    json_chunk = json.loads(chunk)\r\n                 ^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\Nice\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py\", line 346, in loads\r\n    return _default_decoder.decode(s)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\Nice\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py\", line 337, in decode\r\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\Nice\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py\", line 353, in raw_decode\r\n    obj, end = self.scan_once(s, idx)\r\n               ^^^^^^^^^^^^^^^^^^^^^^\r\njson.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Nice\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\litellm\\utils.py\", line 10017, in __next__      \r\n    chunk = next(self.completion_stream)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\Nice\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\litellm\\llms\\vertex_ai_and_google_ai_studio\\gemini\\vertex_and_google_ai_studio_gemini.py\", line 1569, in __next__\r\n    raise RuntimeError(f\"Error parsing chunk: {e},\\nReceived chunk: {chunk}\")\r\nRuntimeError: Error parsing chunk: Expecting property name enclosed in double quotes: line 1 column 2 (char 1),\r\nReceived chunk: {\r\n```\r\n\r\n\r\n### Twitter / LinkedIn details\r\n\r\n_No response_",
      "state": "open",
      "author": "phanminhtai",
      "author_type": "User",
      "created_at": "2024-09-12T06:40:07Z",
      "updated_at": "2025-06-18T13:23:10Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 16,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/5650/reactions",
        "total_count": 3,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "krrishdholakia"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/5650",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/5650",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:39.720309",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Seems like gemini will return invalid json / partial json mid-stream\r\n\r\nsimilar to this - https://github.com/BerriAI/litellm/issues/5479",
          "created_at": "2024-09-12T15:21:57Z"
        },
        {
          "author": "krrishdholakia",
          "body": "we need to update our test to cover this - https://github.com/BerriAI/litellm/blob/3e34edcff3f22db14239b7a87a9501dd237036a7/litellm/tests/test_streaming.py#L836",
          "created_at": "2024-09-12T15:22:51Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-01-28T02:58:35Z"
        },
        {
          "author": "AyrennC",
          "body": "Encountering the same bug with gemini-2.0-thinking-exp in `litellm_stable_release_branch-v1.61.13-stable`",
          "created_at": "2025-02-27T07:37:19Z"
        },
        {
          "author": "paul-gauthier",
          "body": "FYI, aider users are reporting this error with the new Gemini 2.5 Pro model. Are we sure this is bad json coming back from google?",
          "created_at": "2025-03-27T16:59:32Z"
        }
      ]
    },
    {
      "issue_number": 11817,
      "title": "[Bug]: Typechecking bug when creating message manually",
      "body": "### What happened?\n\nIf I create a message manually (e.g to inject in history) like this:\n```python\nfrom litellm.types.utils import Message\nmessage = Message(role=\"user\", content=\"foo\")\n```\n\nmypy complains because type is not `Literal[\"assistant\"]` as specified here:\nhttps://github.com/BerriAI/litellm/blob/6fe335ea94eca0b8c9333e042aa1fa72427a900d/litellm/types/utils.py#L614\n\ni can work around the issue with a type: ignore (on construct the message using pydantic's `model_validate`) but i think it would be nice to fix this minor bug for the type checker.\n\nI'm happy to open a PR if you agree this is a bug.\n\n### Relevant log output\n\n```shell\nerror: Argument \"role\" to \"Message\" has incompatible type \"Literal['user']\"; expected \"Literal['assistant']\"  [arg-type]\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.71.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "amarrella",
      "author_type": "User",
      "created_at": "2025-06-17T17:58:01Z",
      "updated_at": "2025-06-18T12:54:14Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11817/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11817",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11817",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:39.956880",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "yes, can you provide a PR for this fix ",
          "created_at": "2025-06-17T22:58:03Z"
        },
        {
          "author": "amarrella",
          "body": "will do üëç",
          "created_at": "2025-06-18T12:54:14Z"
        }
      ]
    },
    {
      "issue_number": 11821,
      "title": "[Bug]: Illegal header value exception for on-prem models",
      "body": "### What happened?\n\nHello, I have models deployed on on-prem servers. When I directly request , there is no problem. \nAnd I'm trying to add them to proxy like below\n```json\nmodel_list:\n- model_name: Meta-Llama-3.1-70B-Instruct-FP8\n  litellm_params:\n    model: openai/neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8\n    api_base: http://myteam-llama-70b-v1.cluster.com/v1\n    api_key: \"None\"\n```\n\nI'm getting the error shared on relevant log output\n\nI couldn't figure out  what is wrong.  Do you have anny\n\n\n### Relevant log output\n\n```shell\n+ Exception Group Traceback (most recent call last):\n  |   File \\\"/.../starlette/_utils.py\\\", line 76, in collapse_excgroups\n  |     yield\n  |   File \\\"/.../starlette/middleware/base.py\\\", line 177, in __call__\n  |     async with anyio.create_task_group() as task_group:\n  |   File \\\"/.../anyio/_backends/_asyncio.py\\\", line 772, in __aexit__\n  |     raise BaseExceptionGroup(\n  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n  +-+---------------- 1 ----------------\n    | Traceback (most recent call last):\n    |   File \\\"/.../uvicorn/protocols/http/h11_impl.py\\\", line 407, in run_asgi\n    |     result = await app(\n    |   File \\\"/.../uvicorn/middleware/proxy_headers.py\\\", line 69, in __call__\n    |     return await self.app(scope, receive, send)\n    |   File \\\"/.../fastapi/applications.py\\\", line 1054, in __call__\n    |     await super().__call__(scope, receive, send)\n    |   File \\\"/.../starlette/applications.py\\\", line 112, in __call__\n    |     await self.middleware_stack(scope, receive, send)\n    |   File \\\"/.../starlette/middleware/errors.py\\\", line 187, in __call__\n    |     raise exc\n    |   File \\\"/.../starlette/middleware/errors.py\\\", line 165, in __call__\n    |     await self.app(scope, receive, _send)\n    |   File \\\"/.../starlette/middleware/base.py\\\", line 176, in __call__\n    |     with recv_stream, send_stream, collapse_excgroups():\n    |   File \\\"/.../contextlib.py\\\", line 158, in __exit__\n    |     self.gen.throw(value)\n    |   File \\\"/.../starlette/_utils.py\\\", line 82, in collapse_excgroups\n    |     raise exc\n    |   File \\\"/.../starlette/middleware/base.py\\\", line 179, in __call__\n    |     await response(scope, wrapped_receive, send)\n    |   File \\\"/.../starlette/middleware/base.py\\\", line 208, in __call__\n    |     await send(\n    |   File \\\"/.../starlette/middleware/errors.py\\\", line 162, in _send\n    |     await send(message)\n    |   File \\\"/.../uvicorn/protocols/http/h11_impl.py\\\", line 488, in send\n    |     response = h11.Response(status_code=status, headers=headers, reason=reason)\n    |   File \\\"/.../h11/_events.py\\\", line 151, in __init__\n    |     self, \\\"headers\\\", normalize_and_validate(headers)\n    |   File \\\"/.../h11/_headers.py\\\", line 166, in normalize_and_validate\n    |     validate(_field_value_re, value, \\\"Illegal header value {!r}\\\", value)\n    |   File \\\"/.../h11/_util.py\\\", line 91, in validate\n    |     raise LocalProtocolError(msg)\n    | h11._util.LocalProtocolError: Illegal header value b'SO-ME-VA-LUE '\n  +------------------------------------\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.72.4\n\n### Twitter / LinkedIn details\n\nhttps://www.linkedin.com/in/furkancan/",
      "state": "closed",
      "author": "furkanc",
      "author_type": "User",
      "created_at": "2025-06-17T21:43:31Z",
      "updated_at": "2025-06-18T11:00:52Z",
      "closed_at": "2025-06-18T11:00:52Z",
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11821/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11821",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11821",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:40.139671",
      "comments": [
        {
          "author": "furkanc",
          "body": "I found the issue. This is related with the trailing space on the header value. Closing this issue. ",
          "created_at": "2025-06-18T11:00:52Z"
        }
      ]
    },
    {
      "issue_number": 11726,
      "title": "[Bug]: New Version Causes Connection Error Under Corporate Proxy",
      "body": "### What happened?\n\nHello, I would like to report a bug I encountered when using LiteLLM behind a corporate proxy. I deployed litellm as a proxy.\nI have configured both https_proxy and http_proxy environment variables, and everything worked as expected up to version **1.71.1.** \n\nHowever, after upgrading to versions **1.72.0** and **1.72.2**, I started receiving connection errors, even though my proxy settings and configuration files remained unchanged.\n\nI tried to read through change logs not sure that this is related to aiohttp or not.\n\n### Relevant log output\n\n```shell\nlitellm.APIConnectionError: GeminiException - Cannot connect to host generativelanguage.googleapis.com:443 ssl:default [Connection reset by peer]\n(not only with gemini but every model showed same error)\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.72.2-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "Impvest",
      "author_type": "User",
      "created_at": "2025-06-14T13:35:24Z",
      "updated_at": "2025-06-18T08:24:08Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11726/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11726",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11726",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:40.350399",
      "comments": [
        {
          "author": "ruskinwadia",
          "body": "Trying to debug this same issue !",
          "created_at": "2025-06-15T09:29:42Z"
        },
        {
          "author": "grosjeang",
          "body": "Same error, everything works fine for version 1.69.0 behind a company proxy.\n\nUpgraded to version 1.72.6 and got the same error : Connection reset by peer",
          "created_at": "2025-06-17T15:48:58Z"
        },
        {
          "author": "yanghan-cyber",
          "body": "Same error. Any solution?",
          "created_at": "2025-06-18T03:27:34Z"
        }
      ]
    },
    {
      "issue_number": 4162,
      "title": "[Feature]: Add streaming support for ClarifAI ",
      "body": "### The Feature\n\n@mogith-pn Can you show me a curl on how to make a streaming request to ClarifAI. This is not mentioned on a single place in your docs or on the swagger API \n\n### Motivation, pitch\n\n- \n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "ishaan-jaff",
      "author_type": "User",
      "created_at": "2024-06-13T00:16:14Z",
      "updated_at": "2025-06-18T07:37:59Z",
      "closed_at": "2025-05-08T00:02:40Z",
      "labels": [
        "enhancement",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/4162/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/4162",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/4162",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:40.521851",
      "comments": [
        {
          "author": "mogith-pn",
          "body": "> ### The Feature\r\n> @mogith-pn Can you show me a curl on how to make a streaming request to ClarifAI. This is not mentioned on a single place in your docs or on the swagger API\r\n> \r\n> ### Motivation, pitch\r\n> \r\n> ### Twitter / LinkedIn details\r\n> _No response_\r\n\r\n@ishaan-jaff ,\r\nCurrently clarifai ",
          "created_at": "2024-06-13T10:56:37Z"
        },
        {
          "author": "mogith-pn",
          "body": "That's why as per krish's suggestion, we wrapped the completion response to iterator, to ensure it follows stream response format.\r\nhttps://github.com/BerriAI/litellm/pull/3369#issuecomment-2090775919",
          "created_at": "2024-06-13T10:58:14Z"
        },
        {
          "author": "mogith-pn",
          "body": "@ishaan-jaff -\r\nCould you please take a look at this PR ?\r\nhttps://github.com/BerriAI/litellm/pull/4170/",
          "created_at": "2024-07-10T12:05:19Z"
        },
        {
          "author": "nitinbhojwani",
          "body": "> That's why as per krish's suggestion, we wrapped the completion response to iterator, to ensure it follows stream response format. [#3369 (comment)](https://github.com/BerriAI/litellm/pull/3369#issuecomment-2090775919)\r\n\r\n@ishaan-jaff does this answer sound good? The PR will allow users to utilise",
          "created_at": "2024-07-11T07:05:52Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-01-28T02:59:50Z"
        }
      ]
    },
    {
      "issue_number": 11191,
      "title": "[Bug]: default value for DATABASE_URL not being set in 1.17.1",
      "body": "### What happened?\n\nafter upgrading to 1.71.1 from 1.70.1 I get this error:\n\n```\nerror: Error validating datasource `client`: the URL must start with the protocol `postgresql://` or `postgres://`.\n  -->  schema.prisma:3\n   |\n 2 |   provider = \"postgresql\"\n 3 |   url      = env(\"DATABASE_URL\")\n   |\n```\n\n`DATABASE_URL` is unset and previously it was created in the `Constructing DATABASE_URL from environment variables` block of code.\n\n### Relevant log output\n\n```shell\n[Context: getConfig]\nPrisma CLI Version : 5.4.2\n{\"message\": \"The process failed to execute. Details: Command '['prisma', 'db', 'push', '--accept-data-loss']' returned non-zero exit status 1.. Retrying... (3 attempts left)\", \"level\": \"WARNING\", \"timestamp\": \"2025-05-27T21:07:50.377707\"}\nprisma:warn Prisma doesn't know which engines to download for the Linux distro \"wolfi\". Falling back to Prisma engines built \"debian\".\nPlease report your experience by creating an issue at https://github.com/prisma/prisma/issues so we can add your distro to the list of known supported distros.\nprisma:warn Prisma doesn't know which engines to download for the Linux distro \"wolfi\". Falling back to Prisma engines built \"debian\".\nPlease report your experience by creating an issue at https://github.com/prisma/prisma/issues so we can add your distro to the list of known supported distros.\nPrisma schema loaded from schema.prisma\nDatasource \"client\": PostgreSQL database\nError: Prisma schema validation - (get-config wasm)\nError code: P1012\nerror: Error validating datasource `client`: the URL must start with the protocol `postgresql://` or `postgres://`.\n  -->  schema.prisma:3\n   |\n 2 |   provider = \"postgresql\"\n 3 |   url      = env(\"DATABASE_URL\")\n   |\nValidation Error Count: 1\n[Context: getConfig]\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.71.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "NorthIsUp",
      "author_type": "User",
      "created_at": "2025-05-27T21:11:17Z",
      "updated_at": "2025-06-18T06:45:25Z",
      "closed_at": null,
      "labels": [
        "bug",
        "awaiting: user response"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11191/reactions",
        "total_count": 3,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11191",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11191",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:40.753105",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Hey @NorthIsUp how are you configuring the DB url? \n\nWe set this in the environment in local + staging, and don't see this issue. ",
          "created_at": "2025-05-28T02:41:49Z"
        },
        {
          "author": "pabloheimplatz",
          "body": "same problem here. I use this command from the LiteLLM docu:\n```\ndocker run \\\n    -v $(pwd)/config.yaml:/app/config.yaml \\\n    -e LITELLM_MASTER_KEY=sk-1234 \\\n    -e DATABASE_URL=postgresql://postgres:dev@host.docker.internal:5432/postgres \\\n    -p 4000:4000 \\\n    ghcr.io/berriai/litellm-database:ma",
          "created_at": "2025-06-09T08:22:36Z"
        },
        {
          "author": "burnbrigther",
          "body": "Same error.  Something wrong with prism client connection set up?",
          "created_at": "2025-06-18T04:39:56Z"
        },
        {
          "author": "pabloheimplatz",
          "body": "My problem was, that the DB was not reachable. (AWS RDS some networking problems) as well as locally in Postgres docker containers. Error message was a bit misleading but it was just not reachable. I haven't increase any library version or something like that. So I would assume there isn't a generel",
          "created_at": "2025-06-18T06:45:25Z"
        }
      ]
    },
    {
      "issue_number": 11471,
      "title": "[Bug]: Getting an error using Claude-Sonnet-4 (thinking mode) via Google Vertex AI",
      "body": "### What happened?\n\nA bug happened!\n\n![Image](https://github.com/user-attachments/assets/589254c0-6ecc-44ce-9cbc-f40456870f09)\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.0\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "Ccheers",
      "author_type": "User",
      "created_at": "2025-06-06T02:53:13Z",
      "updated_at": "2025-06-18T06:31:30Z",
      "closed_at": "2025-06-18T06:31:30Z",
      "labels": [
        "bug",
        "awaiting: user response"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11471/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11471",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11471",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:40.935890",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "how is 'text' being passed in? \n\n@miguelwon this looks similar to the error you saw ",
          "created_at": "2025-06-06T05:59:07Z"
        },
        {
          "author": "miguelwon",
          "body": "Yes, I think is the same. @Ccheers, when a message is of type `thinking`, the content itself  must have key ¬¥thinking¬¥, not `text`. ",
          "created_at": "2025-06-06T07:58:30Z"
        },
        {
          "author": "Ccheers",
          "body": "> how is 'text' being passed in?\n> \n> [@miguelwon](https://github.com/miguelwon) this looks similar to the error you saw\n\nI used the Golang OpenAI SDK and didn't make any additional modifications, just passed the",
          "created_at": "2025-06-06T13:12:22Z"
        }
      ]
    },
    {
      "issue_number": 9750,
      "title": "[Feature]: Add Text to Speech for Chat Completion Model results",
      "body": "### The Feature\n\nThis is probably a little bit of work, but it would be super awesome if we could have a standardised way to get TTS responses back from any LLM. Currently OpenAI supports this with some models and it's really useful.\n\nBut how great would it be if we can configure this in the proxy so that it makes a call to a TTS provider and provides it back for any model? For example, I could call Gemini and it would give me back the audio (in the background it does the gemini call as normal, but additionally does a tts call to the configured endpoint and settings to speak the result).\n\nFor models which support this natively such as OpenAI (some models) we can simply use the chat completion model, but for those which do not support it natively litellm would handle the additional call and provide the standardised result back.\n\n### Motivation, pitch\n\nThis makes it much easier to build interactive voice agents! As we can simply pass the Chat Completion and have it give back audio data.\n\n### Are you a ML Ops Team?\n\nYes\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "hongkongkiwi",
      "author_type": "User",
      "created_at": "2025-04-04T10:10:58Z",
      "updated_at": "2025-06-18T04:47:03Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9750/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9750",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9750",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:41.149193",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Gemini also has a text to speech model - https://ai.google.dev/gemini-api/docs/speech-generation\n\nWe also now have a speech -> chat completion bridge \n\nFor models that don't support audio, how would you expect us to generate audio? ",
          "created_at": "2025-06-18T04:47:02Z"
        }
      ]
    },
    {
      "issue_number": 10226,
      "title": "[Bug]: Cache control injection points for Anthropic/Bedrock",
      "body": "### What happened?\n\nI followed recent changes about support for cache control injection points for Anthopic API:\nhttps://github.com/BerriAI/litellm/pull/9996\n\nSo far it works good, but i stumbled on some things that i don't fully understand, maybe due to documentation:\nhttps://docs.litellm.ai/docs/tutorials/prompt_caching\nMaybe even more important:\nhttps://docs.litellm.ai/docs/completion/prompt_caching#anthropic-example\n```\nThe conversation history (previous messages) is included in the messages array. The final turn is marked with cache-control, for continuing in followups. The second-to-last user message is marked for caching with the cache_control parameter, so that this checkpoint can read from the previous cache.\n```\n\nI would like to be able to configure everything in LiteLLM proxy, mainly for long chat conversations:\nIf i understood documentation, i would need cache set on second-to-last and last message.\nHowever that is not possible in llm proxy configuration\nIn case there is only role set (ex: user), then bedrock returns exception (as it can have maximum of 4 messages set to cache).\nIn case there is index set, this can be only:\nif 0 <= targetted_index < len(messages):\nand means it's not quite possible to set -1 or something like that to target last messages for cache.\nas per code in: \nhttps://github.com/BerriAI/litellm/blob/f5996b2f6ba45ec3859a716e28f6e6eff0f7a0b3/litellm/integrations/anthropic_cache_control_hook.py#L42\n\nI believe there should be options in litellm proxy config that should make this possible out of the box.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.67.0-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "kresimirfijacko",
      "author_type": "User",
      "created_at": "2025-04-23T10:18:40Z",
      "updated_at": "2025-06-18T03:28:09Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10226/reactions",
        "total_count": 2,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10226",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10226",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:41.340814",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "Is the ask here to support -1 as an index ? so we'll always insert the control on the last message ",
          "created_at": "2025-04-23T14:03:22Z"
        },
        {
          "author": "kresimirfijacko",
          "body": "> Is the ask here to support -1 as an index ? so we'll always insert the control on the last message\n\nI am not quite sure.\nFrom this documentation:\nhttps://docs.litellm.ai/docs/providers/anthropic#caching---continuing-multi-turn-convo\n\nit seems it is necessary on last and second-to-last",
          "created_at": "2025-04-23T15:33:21Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "lets take a step back, what is your goal with using cache control injection @kresimirfijacko  ? ",
          "created_at": "2025-04-23T15:51:08Z"
        },
        {
          "author": "kresimirfijacko",
          "body": "> lets take a step back, what is your goal with using cache control injection [@kresimirfijacko](https://github.com/kresimirfijacko) ?\n\ngood question... \ngoals:\n1. 'static prompt caching' - for prompts that always have same system message (that is long) i want to cache this system prompts. this is e",
          "created_at": "2025-04-23T15:57:27Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "1. can you confirm, you can achieve this now \n2. what do you want to cache here ? user message? assistant message ? (can you show me where you'd want litellm to insert cache controls) ? ",
          "created_at": "2025-04-24T04:07:27Z"
        }
      ]
    },
    {
      "issue_number": 7675,
      "title": "[Bug]: Langfuse integration: error generated in logging handler  - dictionary changed size during iteration",
      "body": "### What happened?\r\n\r\nI get errors generated from the langfuse logging handler.\r\n\r\nThis is very similar to fixed issue here: https://github.com/BerriAI/litellm/issues/2245\r\n\r\nI believe we just need to remove the response obj if we are to follow the same pattern.\r\n\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nTraceback (most recent call last):\r\n  File \"/app/.venv/lib/python3.12/site-packages/litellm/integrations/langfuse/langfuse.py\", line 260, in log_event\r\n    print_verbose(f\"OUTPUT IN LANGFUSE: {output}; original: {response_obj}\")\r\n                                                            ^^^^^^^^^^^^^^\r\n  File \"/app/.venv/lib/python3.12/site-packages/openai/_models.py\", line 199, in __str__\r\n    return f'{self.__repr_name__()}({self.__repr_str__(\", \")})'  # type: ignore[misc]\r\n                                     ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/app/.venv/lib/python3.12/site-packages/pydantic/_internal/_repr.py\", line 56, in __repr_str__\r\n    return join_str.join(repr(v) if a is None else f'{a}={v!r}' for a, v in self.__repr_args__())\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/app/.venv/lib/python3.12/site-packages/pydantic/_internal/_repr.py\", line 56, in <genexpr>\r\n    return join_str.join(repr(v) if a is None else f'{a}={v!r}' for a, v in self.__repr_args__())\r\n                                                                            ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/app/.venv/lib/python3.12/site-packages/pydantic/main.py\", line 1026, in __repr_args__\r\n    for k, v in self.__dict__.items():\r\n                ^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: dictionary changed size during iteration\r\n```\r\n\r\n\r\n### Are you a ML Ops Team?\r\n\r\nNo\r\n\r\n### What LiteLLM version are you on ?\r\n\r\nv1.58.0\r\n\r\n### Twitter / LinkedIn details\r\n\r\n_No response_",
      "state": "closed",
      "author": "ngamolsky",
      "author_type": "User",
      "created_at": "2025-01-10T18:47:30Z",
      "updated_at": "2025-06-18T01:55:20Z",
      "closed_at": "2025-06-13T00:02:12Z",
      "labels": [
        "bug",
        "high priority",
        "stale",
        "logging",
        "feb 2025",
        "langfuse"
      ],
      "label_count": 6,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/7675/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/7675",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/7675",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:41.541118",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "is this still an issue? @ngamolsky \n\ncc: @ishaan-jaff ",
          "created_at": "2025-01-28T18:32:57Z"
        },
        {
          "author": "ngamolsky",
          "body": "Yes it is!\n\nStill shows up occasionally. Thank you! @krrishdholakia ",
          "created_at": "2025-01-30T18:56:51Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "hi @ngamolsky can you give me a way to repro this ? ",
          "created_at": "2025-01-30T18:59:58Z"
        },
        {
          "author": "ngamolsky",
          "body": "Unfortunately that's the hard part, it is intermittent and very hard to repro, just shows up in our alerts every once in a while and is distracting.\n\nBut it always comes from this stacktrace:\n\n```\nTraceback (most recent call last):\n  File \"/app/.venv/lib/python3.12/site-packages/litellm/integrations",
          "created_at": "2025-01-30T19:10:48Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "tracking ",
          "created_at": "2025-02-07T22:14:44Z"
        }
      ]
    },
    {
      "issue_number": 7649,
      "title": "[Bug]: Files missing in docker Litellm",
      "body": "### What happened?\n\nWhen we clone the project and run with docker compose up, we can see the following error:\r\n- prod_entrypoint.sh: no such file or directory\r\n- ./docker/build_admin_ui.sh: not found\r\n![image](https://github.com/user-attachments/assets/2671bcf6-0bdc-4c91-937d-28e78143a915)\r\n\n\n### Relevant log output\n\n_No response_\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nlatest\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "daniyalsaif200",
      "author_type": "User",
      "created_at": "2025-01-09T08:11:21Z",
      "updated_at": "2025-06-18T00:10:15Z",
      "closed_at": "2025-06-18T00:10:15Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/7649/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/7649",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/7649",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:41.850780",
      "comments": [
        {
          "author": "freakshock88",
          "body": "I have the same issue because I was auto updating litellm. As a workaround I'm now using image version `main-v1.56.10`.",
          "created_at": "2025-01-10T10:54:49Z"
        },
        {
          "author": "mmolinari",
          "body": "> I have the same issue because I was auto updating litellm. As a workaround I'm now using image version `main-v1.56.10`.\r\n\r\nThank you, I had the same issue and downgrading to main-v1.56.10 worked for me too.",
          "created_at": "2025-01-12T16:50:09Z"
        },
        {
          "author": "TheFitzZZ",
          "body": "Same issue, just different file missing for me (using UNRAIDs template, can't access logs right now to verify, sorry). Workaround worked for me all the same.",
          "created_at": "2025-01-13T11:59:54Z"
        },
        {
          "author": "taherfattahi",
          "body": "I encountered this issue on my Windows device, but everything worked fine on Linux even the latest version of lilellm",
          "created_at": "2025-01-21T12:38:05Z"
        },
        {
          "author": "florianoverkamp",
          "body": "I though I was running across this too, but it turns out in my case the mount point for config.yaml was the problem. in previous versions you could get away with mounting /app for config settings (since the litellm program itself does not live in there), but that no longer works because there are re",
          "created_at": "2025-01-22T08:40:38Z"
        }
      ]
    },
    {
      "issue_number": 9158,
      "title": "Duplicate entries in model_prices_and_context_window.json",
      "body": "perplexity.sonar and perplexity.sonar-pro each have two entries here:\n\nhttps://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json#L8733-L8768",
      "state": "closed",
      "author": "MattFisher",
      "author_type": "User",
      "created_at": "2025-03-12T03:00:38Z",
      "updated_at": "2025-06-18T00:09:53Z",
      "closed_at": "2025-06-18T00:01:50Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9158/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9158",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9158",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:42.169200",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-11T00:01:47Z"
        },
        {
          "author": "MattFisher",
          "body": "Seems to have been fixed in any case üëçüèª ",
          "created_at": "2025-06-18T00:09:53Z"
        }
      ]
    },
    {
      "issue_number": 8705,
      "title": "[Bug]: 'CompletionUsage' object is not subscriptable",
      "body": "### What happened?\n\n\n```\ncost_per_token_usage_object: Optional[Usage] = _get_usage_object(\n            completion_response=completion_response\n        )\n```\n`cost_per_token_usage_object` is actually a `CompletionUsage` instead of `Usage`\n\nThis leads to an error when accessing `usage[\"...\"]`.\n\n### Relevant log output\n\n```shell\nlitellm_logging.py:863 - response_cost_failure_debug_information: {'error_str': \"'CompletionUsage' object is not subscriptable\", 'traceback_str': 'Traceback (most recent call last):\nFile \".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py\", line 845, in _response_cost_calculator\n    response_cost = litellm.response_cost_calculator(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \".venv/lib/python3.12/site-packages/litellm/cost_calculator.py\", line 839, in response_cost_calculator\nraise e\n    File \".venv/lib/python3.12/site-packages/litellm/cost_calculator.py\", line 827, in response_cost_calculator\n    response_cost = completion_cost(\n                    ^^^^^^^^^^^^^^^^\n  File \".venv/lib/python3.12/site-packages/litellm/cost_calculator.py\", line 775, in completion_cost\n    raise e\\n  File \".venv/lib/python3.12/site-packages/litellm/cost_calculator.py\", line 754, in completion_cost\n    ) = cost_per_token(\n        ^^^^^^^^^^^^^^^\n  File \".venv/lib/python3.12/site-packages/litellm/cost_calculator.py\", line 275, in cost_per_token\n    return openai_cost_per_token(model=model, usage=usage_block)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".venv/lib/python3.12/site-packages/litellm/llms/openai/cost_calculation.py\", line 62, in cost_per_token\n    usage[\"completion_tokens\"] * model_info[\"output_cost_per_token\"]\n    ~~~~~^^^^^^^^^^^^^^^^^^^^^\nTypeError: \\'CompletionUsage\\' object is not subscriptable\\n', 'model': 'gpt-4o-mini', 'cache_hit': None, 'custom_llm_provider': 'openai', 'base_model': None, 'call_type': 'acompletion', 'custom_pricing': False}\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.61.13\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "ChenghaoMou",
      "author_type": "User",
      "created_at": "2025-02-21T12:40:30Z",
      "updated_at": "2025-06-18T00:01:56Z",
      "closed_at": null,
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8705/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8705",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8705",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:42.420030",
      "comments": [
        {
          "author": "hakan458",
          "body": "I am hitting the same issue. Was not an issue in previous versions ",
          "created_at": "2025-03-19T22:29:49Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-18T00:01:55Z"
        }
      ]
    },
    {
      "issue_number": 9153,
      "title": "[Feature]: Support FanoutCache",
      "body": "### The Feature\n\nHi team,\n\nThank you for maintaining this awesome package!\n\nI am a maintainer of DSPy, and we are looking to use diskcache.FanoutCache to improve the cache experience with high parallelism. DSPy's currently using litellm to call LM, and using litellm's caching system, so I want to check in if diskcache.FanoutCache (as another option besides the current cache solution) is something in your radar. Thank you!\n\nBest,\nChen\n\n### Motivation, pitch\n\nUsing FanoutCache to support high concurrency.\n\n### Are you a ML Ops Team?\n\nYes\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "chenmoneygithub",
      "author_type": "User",
      "created_at": "2025-03-11T23:06:53Z",
      "updated_at": "2025-06-18T00:01:52Z",
      "closed_at": "2025-06-18T00:01:52Z",
      "labels": [
        "enhancement",
        "mlops user request",
        "stale"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9153/reactions",
        "total_count": 4,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 2,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9153",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9153",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:42.608697",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-11T00:01:49Z"
        }
      ]
    },
    {
      "issue_number": 9136,
      "title": "[Bug]: LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.",
      "body": "### What happened?\n\nA bug happened!\n<a href='https://postimg.cc/v1V7wN3p' target='_blank'><img src='https://i.postimg.cc/TPktg8mY/Screenshot-2025-03-11-at-5-54-26-PM.png' border='0' alt='Screenshot-2025-03-11-at-5-54-26-PM'/></a>\n\n### Relevant log output\n\n```shell\nfrom crewai import Agent\nfrom dotenv import load_dotenv\nload_dotenv()\n\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom tools import serper_tool\n\nimport os\n\nimport litellm\n\nllm = litellm.completion(\n    model=\"gemini/gemini-1.5-flash\",\n    api_key=os.getenv(\"GOOGLE_API_KEY\"),\n    custom_llm_provider=\"google\",\n    # verbose=True\n)\n\n# llm = ChatGoogleGenerativeAI(\n#     model=\"gemini-1.5-pro\",  # ‚úÖ Correct parameter\n#     google_api_key=os.getenv(\"GEMINI_API_KEY\"),  # ‚úÖ Correct key name\n#     temperature=0.5,\n#     verbose=True\n# )\n\n# Creating a senior researcher agent with memory and verbose mode\nnews_researcher = Agent(\n    role=\"Senior Researcher\",\n    goal=\"Unccover ground breaking technologies in {topic}\",\n    verbose= True,\n    memory=True,\n    backstory=(\n        \"Driven by curiosity, you're at the forefront of innovation, eager to explore and share knowledge that could change the world.\"\n    ),\n    tools=[serper_tool],\n    llm=llm,\n    allow_delegation=True\n)\n\n# Creating a write agent with custom tools responsible in writing new blog\n\nnews_writer = Agent(\n    role=\"Writer\",\n    goal=\"Narrate compelling tech stories about {topic}\",\n    verbose=True,\n    memory=True,\n    backstory=(\n        \"With a flair for simplifying complex topics, you craft engaging narratives that captivate and educate, bringing new discoveries to light in an accessible manner.\"\n    ),\n    tools=[serper_tool],\n    llm=llm,\n    allow_delegation=False,\n)\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.60.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "theshivay",
      "author_type": "User",
      "created_at": "2025-03-11T12:29:17Z",
      "updated_at": "2025-06-18T00:01:52Z",
      "closed_at": "2025-06-18T00:01:52Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9136/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9136",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9136",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:42.822865",
      "comments": [
        {
          "author": "theshivay",
          "body": "# Solved\n```python\nfrom crewai import LLM\nllm = LLM(\n    model=\"gemini/gemini-1.5-flash\",\n    verbose=True,\n    temperature=0.5,\n    google_api_key = google_api_key\n)\n```",
          "created_at": "2025-03-12T16:39:58Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-11T00:01:51Z"
        }
      ]
    },
    {
      "issue_number": 9156,
      "title": "[Bug]: Router's completion() method does not work with a redis cache for caching cooldown, only the asynchronous acompletion() method works",
      "body": "We are trying to use an on-demand AWS ElastiCache service (Cluster mode: disabled) with Redis OSS to cache information on model cooldowns. However, when we use the Router's `completion()` method, we encounter a strange error, as shown below. Interestingly, the asynchronous `acompletion()` method works fine without any issues.\n\nFor our internal use case, we prefer to use the synchronous method rather than the asynchronous one. Is the synchronous method not supported by design when using a Redis cache, or is this a bug?\n\nI am running this from a databricks notebook.\n\nHere's my code:\n\n\n \n    router = Router(\n        model_list=[\n        {\n        \"model_name\": \"azure/gpt-4o\",\n        \"litellm_params\": {\n            \"model\": \"azure/NonProdOpenAI_GPT-4o-NorthCentral\",\n            \"api_base\": north_central_api_base,\n            \"api_key\": north_central_api_key,\n            \"base_model\": \"azure/gpt-4o\",\n\n\n        },\n        \"model_info\":{\n                \"id\": \"NonProdOpenAI_GPT-4o-NorthCentral\",\n                \n            },\n        },\n        {\n        \"model_name\": \"azure/gpt-35-turbo-16k\",\n        \"litellm_params\": {\n            \"model\": \"azure/NonProdOpenAI_GPT-35-T-16k-EastUS\",\n            \"api_base\": eastus_api_base,\n            \"api_key\": eastus_api_key,\n            \"base_model\": \"azure/gpt-35-turbo-16k\",\n\n        },\n        \"model_info\":{\n            \"id\": \"NonProdOpenAI_GPT-35-T-16k-EastUS\",\n  \n        },\n        },\n        ],\n        fallbacks=[{\"azure/gpt-4o\": [\"azure/gpt-35-turbo-16k\"]}], \n        num_retries=0,\n        cooldown_time=200,\n        redis_url=\"rediss://master.redis-litellm-ondemand.xxxx.use2.cache.amazonaws.com:6379\", # Hiding actual URL here for security reasons\n    )\n \n    response = router.completion(\n        model=\"azure/gpt-4o\",  # default model\n        messages=[{\"role\": \"user\", \"content\": \"Hey, how's it going?\"}]\n    )\n    print(\"Response: \", response)\n\n\nHowever, I don't see any such issue when using the `acompletion()` method with asyncio.\n\n### Relevant log output\n\n```shell\nResponse:  ModelResponse(id='chatcmpl-BA4av5YJ6w4tdrb97JBFgf9gdkpEC', created=1741740641, model='gpt-4o-2024-05-13', object='chat.completion', system_fingerprint='fp_ded0d14823', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Hello! I'm here and ready to help you with whatever you need. How can I assist you today?\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None, 'annotations': None}))], usage=Usage(completion_tokens=21, prompt_tokens=14, total_tokens=35, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier=None, prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n\n\n00:50:42 - LiteLLM:ERROR: redis_cache.py:898 - LiteLLM Redis Cache PING: - Got exception from REDIS : Task <Task pending name='Task-5' coro=<RedisCache.ping() running at /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/litellm/caching/redis_cache.py:873>> got Future <Future pending> attached to a different loop\nTask exception was never retrieved\nfuture: <Task finished name='Task-5' coro=<RedisCache.ping() done, defined at /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/litellm/caching/redis_cache.py:867> exception=RuntimeError(\"Task <Task pending name='Task-5' coro=<RedisCache.ping() running at /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/litellm/caching/redis_cache.py:873>> got Future <Future pending> attached to a different loop\")>\nTraceback (most recent call last):\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/litellm/caching/redis_cache.py\", line 901, in ping\n    raise e\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/litellm/caching/redis_cache.py\", line 873, in ping\n    response = await _redis_client.ping()\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/redis/asyncio/client.py\", line 616, in execute_command\n    return await conn.retry.call_with_retry(\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/redis/asyncio/retry.py\", line 59, in call_with_retry\n    return await do()\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/redis/asyncio/client.py\", line 590, in _send_command_parse_response\n    return await self.parse_response(conn, command_name, **options)\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/redis/asyncio/client.py\", line 637, in parse_response\n    response = await connection.read_response()\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/redis/asyncio/connection.py\", line 549, in read_response\n    response = await self._parser.read_response(\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/redis/_parsers/resp2.py\", line 82, in read_response\n    response = await self._read_response(disable_decoding=disable_decoding)\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/redis/_parsers/resp2.py\", line 90, in _read_response\n    raw = await self._readline()\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/redis/_parsers/base.py\", line 219, in _readline\n    data = await self._stream.readline()\n  File \"/usr/lib/python3.10/asyncio/streams.py\", line 524, in readline\n    line = await self.readuntil(sep)\n  File \"/usr/lib/python3.10/asyncio/streams.py\", line 616, in readuntil\n    await self._wait_for_data('readuntil')\n  File \"/usr/lib/python3.10/asyncio/streams.py\", line 501, in _wait_for_data\n    await self._waiter\nRuntimeError: Task <Task pending name='Task-5' coro=<RedisCache.ping() running at /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/litellm/caching/redis_cache.py:873>> got Future <Future pending> attached to a different loop\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.63.6\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "srinivastr1999",
      "author_type": "User",
      "created_at": "2025-03-12T01:05:43Z",
      "updated_at": "2025-06-18T00:01:51Z",
      "closed_at": "2025-06-18T00:01:51Z",
      "labels": [
        "bug",
        "mlops user request",
        "stale"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9156/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9156",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9156",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:43.027600",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-11T00:01:48Z"
        }
      ]
    },
    {
      "issue_number": 9270,
      "title": "[Bug]: not running latest version using docker",
      "body": "### What happened?\n\nim having an issue with getting the latest version installed on Docker. Only got version 1.53.7 when running.\n\nalso encounter errors during docker buildup. please let me know where i went wrong? \n\n\n`litellm-1     | Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nlitellm-1     | \nlitellm-1     | \nlitellm-1     | LiteLLM_VerificationTokenView Exists!\nlitellm-1     | MonthlyGlobalSpend Exists!\nlitellm-1     | Last30dKeysBySpend Exists!\nlitellm-1     | Last30dModelsBySpend Exists!\nlitellm-1     | MonthlyGlobalSpendPerKey Exists!\nlitellm-1     | MonthlyGlobalSpendPerUserPerKey Exists!\nlitellm-1     | DailyTagSpend Exists!\nlitellm-1     | Last30dTopEndUsersSpend Exists!\nlitellm-1     | INFO:     172.19.0.3:47656 - \"GET /metrics HTTP/1.1\" 404 Not Found\nlitellm-1     | INFO:     172.19.0.3:59080 - \"GET /metrics HTTP/1.1\" 404 Not Found\nlitellm-1     | INFO:     172.19.0.3:38276 - \"GET /metrics HTTP/1.1\" 404 Not Found\nlitellm-1     | INFO:     172.19.0.3:52796 - \"GET /metrics HTTP/1.1\" 404 Not Found\nlitellm-1     | INFO:     172.19.0.3:53548 - \"GET /metrics HTTP/1.1\" 404 Not Found\nlitellm-1     | INFO:     172.19.0.3:58618 - \"GET /metrics HTTP/1.1\" 404 Not Found\nlitellm-1     | INFO:     172.19.0.3:33200 - \"GET /metrics HTTP/1.1\" 404 Not Found\nlitellm-1     | INFO:     172.19.0.3:51636 - \"GET /metrics HTTP/1.1\" 404 Not Found\nlitellm-1     | INFO:     172.19.0.3:33978 - \"GET /metrics HTTP/1.1\" 404 Not Found\nlitellm-1     | INFO:     172.19.0.3:59790 - \"GET /metrics HTTP/1.1\" 404 Not Found\nlitellm-1     | INFO:     172.19.0.3:36826 - \"GET /metrics HTTP/1.1\" 404 Not Found\nlitellm-1     | INFO:     172.19.0.3:46838 - \"GET /metrics HTTP/1.1\" 404 Not Found\nlitellm-1     | INFO:     172.19.0.3:48678 - \"GET /metrics HTTP/1.1\" 404 Not Found\nlitellm-1     | INFO:     172.19.0.3:49618 - \"GET /metrics HTTP/1.1\" 404 Not Found\nlitellm-1     | INFO:     172.19.0.3:33512 - \"GET /metrics HTTP/1.1\" 404 Not Found\nlitellm-1     | INFO:     192.168.65.1:25419 - \"GET / HTTP/1.1\" 200 OK\nlitellm-1     | INFO:     192.168.65.1:25419 - \"GET /openapi.json HTTP/1.1\" 200 OK\nlitellm-1     | INFO:     172.19.0.3:49230 - \"GET /metrics HTTP/1.1\" 404 Not Found\nlitellm-1     | INFO:     172.19.0.3:49738 - \"GET /metrics HTTP/1.1\" 404 Not Found\nlitellm-1     | INFO:     172.19.0.3:60764 - \"GET /metrics HTTP/1.1\" 404 Not Found\nlitellm-1     | INFO:     172.19.0.3:53666 - \"GET /metrics HTTP/1.1\" 404 Not Found` \n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.53.7\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "048823",
      "author_type": "User",
      "created_at": "2025-03-15T02:41:47Z",
      "updated_at": "2025-06-18T00:01:43Z",
      "closed_at": null,
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9270/reactions",
        "total_count": 3,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9270",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9270",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:43.221564",
      "comments": [
        {
          "author": "edisonyls",
          "body": "Same here. Why it keeps giving \"GET /metrics HTTP/1.1\" 404 Not Found?",
          "created_at": "2025-03-19T23:35:48Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-18T00:01:42Z"
        }
      ]
    },
    {
      "issue_number": 9358,
      "title": "[Bug]: openrouter/perplexity doesn't return citations when streaming=True",
      "body": "### What happened?\n\nStreaming with perplexity models generally works correctly, returning citations: https://github.com/BerriAI/litellm/issues/4981\n\nUnfortunately this does not work with perplexity models provided by openrouter. MWE:\n\n```python\nfrom litellm import completion, stream_chunk_builder\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"you are an assistant.\"},\n    {\"role\": \"user\", \"content\": \"are there any pibbss in the fridge?\"},\n]\n_response = completion(\n    model=\"openrouter/perplexity/sonar\", messages=messages, stream=True\n)\nchunks = []\nfor chunk in _response:\n    print(chunk.choices[0].delta.content or \"\", end=\"\")\n    chunks.append(chunk)\nresponse = stream_chunk_builder(chunks, messages=messages)\n\nprint(hasattr(response, \"citations\")) # False\n```\n\nI have checked that it does work correctly when `streaming=False`:\n\n```python\nresponse2 = completion(\n    model=\"openrouter/perplexity/sonar\", messages=messages, stream=False\n)\nprint(hasattr(response2, \"citations\")) # True\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.63.11\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "abhimanyupallavisudhir",
      "author_type": "User",
      "created_at": "2025-03-19T02:59:08Z",
      "updated_at": "2025-06-18T00:01:37Z",
      "closed_at": null,
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9358/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9358",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9358",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:43.415670",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-18T00:01:37Z"
        }
      ]
    },
    {
      "issue_number": 9359,
      "title": "[Feature]: Retrieve the contents of the file search results that were used by the model",
      "body": "### The Feature\n\nOur provider is Azure OpenAI and we would like `include=['step_details.tool_calls[*].file_search.results[*].content']` to return the \"content\" using the generic OpenAI interface:\n\n```\nwith client.beta.threads.runs.stream(\n              thread_id=thread.id,\n              assistant_id=assistant_id,\n              event_handler=handler,\n              include=[\n                  'step_details.tool_calls[*].file_search.results[*].content'\n                  ]\n      ) as stream:\n          stream.until_done()\n```\n\nSee OpenAI documentation: \n\nhttps://platform.openai.com/docs/assistants/tools/file-search#step-5-create-a-run-and-check-the-output\n\n### Motivation, pitch\n\nWe are experimenting with litellm and are working on an app that displays to the user what file_search content was used to generate the response.  Maybe this should work and we are just missing something.  It works fine when we interface directly with our AzureOpenAI provider.\n\nThanks!\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "crosenth",
      "author_type": "User",
      "created_at": "2025-03-19T04:29:10Z",
      "updated_at": "2025-06-18T00:01:36Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9359/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9359",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9359",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:43.607429",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-18T00:01:36Z"
        }
      ]
    },
    {
      "issue_number": 10734,
      "title": "[Bug]: token_counter() raises on \"file\" type in message content",
      "body": "### What happened?\n\n`token_counter()` raises an exception when the message content includes a `{\"type\": \"file\"}` item. This format is often used when passing PDF input to models.\n\nHere is the reproduction code:\n```python\nfrom litellm.utils import token_counter\n\nfile_url = \"https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf\"\n\nmodel = \"bedrock/us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n\nfile_content = [\n    {\"type\": \"text\", \"text\": \"What's this file about?\"},\n    {\n        \"type\": \"file\",\n        \"file\": {\n            \"file_id\": file_url,\n        }\n    },\n]\nmessages = [{\"role\": \"user\", \"content\": file_content}]\n\ntoken_counter(model=model, messages=messages)\n```\n\nThe error started occurring in **v1.67.6**. It does not happen in **v1.67.5**.\n\n### Relevant log output\n\n```shell\nTraceback (most recent call last):\n  File \"/home/iwamot/.pyenv/versions/3.13.2/lib/python3.13/site-packages/litellm/litellm_core_utils/token_counter.py\", line 589, in _count_content_list\n    raise ValueError(\n        f\"Invalid content type: {type(c)}. Expected str or dict.\"\n    )\nValueError: Invalid content type: <class 'dict'>. Expected str or dict.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/iwamot/collmbo/test_pdf.py\", line 18, in <module>\n    token_counter(model=model, messages=messages)\n    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/iwamot/.pyenv/versions/3.13.2/lib/python3.13/site-packages/litellm/utils.py\", line 1696, in token_counter\n    return token_counter_new(\n        model,\n    ...<7 lines>...\n        default_token_count,\n    )\n  File \"/home/iwamot/.pyenv/versions/3.13.2/lib/python3.13/site-packages/litellm/litellm_core_utils/token_counter.py\", line 388, in token_counter\n    num_tokens = _count_messages(\n        params, new_messages, use_default_image_token_count, default_token_count\n    )\n  File \"/home/iwamot/.pyenv/versions/3.13.2/lib/python3.13/site-packages/litellm/litellm_core_utils/token_counter.py\", line 449, in _count_messages\n    num_tokens += _count_content_list(\n                  ~~~~~~~~~~~~~~~~~~~^\n        params.count_function,\n        ^^^^^^^^^^^^^^^^^^^^^^\n    ...<2 lines>...\n        default_token_count,\n        ^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/home/iwamot/.pyenv/versions/3.13.2/lib/python3.13/site-packages/litellm/litellm_core_utils/token_counter.py\", line 596, in _count_content_list\n    raise ValueError(\n        f\"Error getting number of tokens from content list: {e}, default_token_count={default_token_count}\"\n    )\nValueError: Error getting number of tokens from content list: Invalid content type: <class 'dict'>. Expected str or dict., default_token_count=None\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.68.2\n\n### Twitter / LinkedIn details\n\n@iwamot / https://www.linkedin.com/in/iwamot/",
      "state": "open",
      "author": "iwamot",
      "author_type": "User",
      "created_at": "2025-05-11T02:35:53Z",
      "updated_at": "2025-06-17T22:08:19Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10734/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10734",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10734",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:43.834407",
      "comments": [
        {
          "author": "srishti-figr",
          "body": "i am facing the same issue. any solution yet ?",
          "created_at": "2025-06-17T22:08:19Z"
        }
      ]
    },
    {
      "issue_number": 11559,
      "title": "[Bug]: MLFlow autologging not working (cannot schedule new futures after interpreter shutdown.)",
      "body": "### What happened?\n\nI am trying to use LiteLLM with MLflow and autologging. I am expecting a trace to appear in the Traces tab of the experiment.\n\nHere is the code that should autolog the LiteLLM trace: https://gist.github.com/ayqazi/7348ccf4739aa9700fdbea6970a5af21\n\nLogs are in the \"log output\" section. I can't get debug output to work, as the last message asks for.\n\nI've tried with mlflow 3.1.0 and 2.22.1\n\nI've tried litellm v1.71.3 and v1.59.12\n\n---\n\nWhen I use openai autologging with a compatible model, the trace appears:\n\nCode: https://gist.github.com/ayqazi/0a3cff574e0a4049706eee8c192a4f21\n\n<img width=\"1508\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/a0425832-75f1-4b79-bea2-946060401b3b\" />\n\n\n### Relevant log output\n\n```shell\n21:59:03 - LiteLLM:DEBUG: http_handler.py:519 - Using AiohttpTransport...\n21:59:03 - LiteLLM:DEBUG: http_handler.py:544 - Creating AiohttpTransport...\n21:59:03 - LiteLLM:DEBUG: http_handler.py:519 - Using AiohttpTransport...\n21:59:03 - LiteLLM:DEBUG: http_handler.py:544 - Creating AiohttpTransport...\n21:59:03 - LiteLLM:DEBUG: litellm_logging.py:168 - [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'\n21:59:03 - LiteLLM:DEBUG: transformation.py:17 - [Non-Blocking] Unable to import _ENTERPRISE_ResponsesSessionHandler - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'\n21:59:03 - LiteLLM:DEBUG: http_handler.py:519 - Using AiohttpTransport...\n21:59:03 - LiteLLM:DEBUG: http_handler.py:544 - Creating AiohttpTransport...\n21:59:03 - LiteLLM:DEBUG: http_handler.py:519 - Using AiohttpTransport...\n21:59:03 - LiteLLM:DEBUG: http_handler.py:544 - Creating AiohttpTransport...\n2025/06/09 21:59:04 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n2025/06/09 21:59:04 INFO mlflow.store.db.utils: Updating database tables\nINFO  [alembic.runtime.migration] Context impl SQLiteImpl.\nINFO  [alembic.runtime.migration] Will assume non-transactional DDL.\nINFO  [alembic.runtime.migration] Context impl SQLiteImpl.\nINFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n2025/06/09 21:59:04 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n2025/06/09 21:59:04 INFO mlflow.store.db.utils: Updating database tables\nINFO  [alembic.runtime.migration] Context impl SQLiteImpl.\nINFO  [alembic.runtime.migration] Will assume non-transactional DDL.\nA **Visual Display Unit (VDU)** is an electronic output device that presents information from a computer or other electronic system in a visual format.\n\nEssentially, it's what most people refer to as a **monitor**, **screen**, or **display**.\n\nHere's a breakdown of what that means:\n\n*   **Visual:** It produces output that can be seen, such as text, images, videos, and graphical user interfaces (GUIs).\n*   **Display:** Its primary function is to show this information.\n*   **Unit:** It's a distinct piece of hardware.\n\n**Key Characteristics and Functions:**\n\n*   **Output Device:** It receives data from a computer's graphics card (or integrated graphics) and translates it into a visual representation on its screen.\n*   **Human-Computer Interaction:** It's the primary way for users to see the results of their input and the processes running on a computer.\n*   **Evolution of Technology:**\n    *   Historically, VDUs were predominantly **Cathode Ray Tube (CRT)** monitors, which were bulky and heavy.\n    *   Today, they are almost exclusively **flat-panel displays** using technologies like:\n        *   **Liquid Crystal Display (LCD)**\n        *   **Light-Emitting Diode (LED)** (often LED-backlit LCDs)\n        *   **Organic Light-Emitting Diode (OLED)**\n        *   **Quantum Dot (QLED)**\n*   **Connectivity:** They connect to computers via various ports such as VGA (older), DVI, HDMI, DisplayPort, and USB-C.\n\n**Examples of VDUs:**\n\n*   **Desktop Computer Monitors:** The standalone screen connected to a desktop PC.\n*   **Laptop Screens:** The integrated display within a laptop.\n*   **Tablet Screens:** The display on a tablet device.\n*   **Smartphone Screens:** The display on a mobile phone.\n*   **Smart TV Screens:** While often used for entertainment, modern smart TVs are sophisticated VDUs.\n*   **Public Information Displays:** Screens at airports, train stations, or ATMs.\n\nIn summary, a VDU is the window through which you interact with and receive information from almost any modern electronic device.\n2025/06/09 21:59:16 WARNING mlflow.tracing.fluent: Failed to start span litellm-completion: cannot schedule new futures after interpreter shutdown. For full traceback, set logging level to debug.\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.71.3\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "ayqazi",
      "author_type": "User",
      "created_at": "2025-06-09T21:01:51Z",
      "updated_at": "2025-06-17T20:20:06Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11559/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11559",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11559",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:43.993109",
      "comments": [
        {
          "author": "zjmwqx",
          "body": "the same",
          "created_at": "2025-06-14T10:11:20Z"
        },
        {
          "author": "ayqazi",
          "body": "Posted on MLflow bug tracker instead: https://github.com/mlflow/mlflow/issues/16296\n",
          "created_at": "2025-06-17T20:20:06Z"
        }
      ]
    },
    {
      "issue_number": 11483,
      "title": "[Bug]: ollama_chat \"keep_alive\" isn't working",
      "body": "### What happened?\n\nIt seems that even after setting the `keep_alive` option in an `ollama_chat` model as described in the [docs](https://docs.litellm.ai/docs/providers/ollama):\n\n![Image](https://github.com/user-attachments/assets/52095225-7232-49a4-b160-2154c0357171)\n\nThe model is still only loaded for 4 minutes:\n\n![Image](https://github.com/user-attachments/assets/611ac8f8-a017-4b1f-9640-7718ace61951)\n\nI've been trying unsuccessfully for some time to apply this setting. Having 50+GB models loaded/unloaded at such small intervals makes the product pretty unusable for coding tasks.\n\nIs it possible I've made a mistake in my configuration? Ideally, I'd like `keep_alive` to apply globally to all ollama models.",
      "state": "open",
      "author": "TheTechromancer",
      "author_type": "User",
      "created_at": "2025-06-06T14:50:14Z",
      "updated_at": "2025-06-17T20:19:58Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11483/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11483",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11483",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:44.190687",
      "comments": [
        {
          "author": "regismesquita",
          "body": "have you checked your request? the request keep_alive overrides the default one.",
          "created_at": "2025-06-06T21:23:30Z"
        },
        {
          "author": "TheTechromancer",
          "body": "@regismesquita thanks for the suggestion, I enabled `--detailed_debug` and I think I found the cause: \n\n```\nlitellm-1     | POST Request Sent from LiteLLM:\nlitellm-1     | curl -X POST \\\nlitellm-1     | http://localhost:11434/api/generate \\ \nlitellm-1     | -d '{'model': 'tom_himanen/deepseek-r1-roo",
          "created_at": "2025-06-09T13:51:29Z"
        },
        {
          "author": "TheTechromancer",
          "body": "This apparently was just fixed in https://github.com/BerriAI/litellm/commit/4c82dd9b27795bc45c420939ebe766ef39eb22b5.",
          "created_at": "2025-06-09T14:12:17Z"
        },
        {
          "author": "TheTechromancer",
          "body": "Pulled latest litellm; verified it's fixed now. Thank you @krrishdholakia üôè",
          "created_at": "2025-06-09T14:18:36Z"
        },
        {
          "author": "regismesquita",
          "body": "I am trying to understand what happened, the line responsible for moving it into the top level was:\n```\n        if keep_alive is not None:\n            data[\"keep_alive\"] = keep_alive\n```\n\nI can see it on the commit you sent, and I can see when it was originally added. but I am not sure if it was mis",
          "created_at": "2025-06-09T14:18:41Z"
        }
      ]
    },
    {
      "issue_number": 11801,
      "title": "[Feature]: add gemini 2.5 GA models",
      "body": "### The Feature\n\nGoogle launched Gemini 2.5 model family as generally available now and made some changes alongside.\n\nGemini 2.5 flash lite was added with new pricing and new features such as OPTIONAL thinking by setting a thinking budget\n\nGemini 2.5 Flash was released as GA with optional thinking budgets. Also gemini 2.5 flash had pricing changes\n\nAnd gemini 2.5 pro was released as GA, now also with optional thinking budgets.\n\nThe preview versions would not allow thinking budgets.\n\nThis means:\n- gemini 2.5 pro can now be used with a thinking budget\n- gemini 2.5 flash new pricing\n\nSo i think some changes need to be made to accomondate the gemini 2.5 GA family \n\n### Motivation, pitch\n\nNew top of the line models by google should be supported \n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "Classic298",
      "author_type": "User",
      "created_at": "2025-06-17T17:04:45Z",
      "updated_at": "2025-06-17T19:59:15Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11801/reactions",
        "total_count": 8,
        "+1": 8,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11801",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11801",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:44.376121",
      "comments": []
    },
    {
      "issue_number": 11819,
      "title": "[Bug]: Unknown parameter: 'reasoning_effort' for o1-mini from Azure OpenAI",
      "body": "### What happened?\n\nAccording to OpenAI API docs, the chat completion payload parameter `reasoning_effort` should be supported for o-series models: https://platform.openai.com/docs/api-reference/chat/create#chat-create-reasoning_effort\n\nBut it turns out that `reasoning_effort` is not supported for `o1-mini` (it's supported for `o1`, `o3-mini`, etc).\n\nIt would be good to add special handling for `o1-mini` model for `get_supported_openai_params` method.\n\nSuggestion to make fix in `AzureOpenAIO1Config`, namely around here:\nhttps://github.com/BerriAI/litellm/blob/6fe335ea94eca0b8c9333e042aa1fa72427a900d/litellm/llms/azure/chat/o_series_transformation.py#L41-L42\n\n### Relevant log output\n\n```shell\nimport litellm\n\nresponse = litellm.completion(\n    model=\"o1-mini\", \n    messages=[{\"role\": \"user\", \"content\": \"Hey whats your name?\"}],\n    reasoning_effort=\"medium\",\n)\n>>\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: AzureException BadRequestError - Unknown parameter: 'reasoning_effort'.\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.66.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "akshoop",
      "author_type": "User",
      "created_at": "2025-06-17T18:54:38Z",
      "updated_at": "2025-06-17T19:25:34Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11819/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11819",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11819",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:44.376182",
      "comments": []
    },
    {
      "issue_number": 11500,
      "title": "[Bug]: Langfuse custom logger initialization error",
      "body": "### What happened?\n\nSetting up langfuse logging like this:\n```python\nlitellm.log_raw_request_response = True\nlitellm.success_callback = [\"langfuse\"]\n```\n\nAfter Langfuse SDK's new release, we get an error for unexpected keyword argument - sdk_integration\n\n\n### Relevant log output\n\n```shell\n[Non-Blocking Error] Error initializing custom logger: Langfuse.__init__() got an unexpected keyword argument 'sdk_integration'\nTraceback (most recent call last):\n  File \"/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3067, in _init_custom_logger_compatible_class\n    langfuse_logger = LangfusePromptManagement()\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/.venv/lib/python3.12/site-packages/litellm/integrations/langfuse/langfuse_prompt_management.py\", line 121, in __init__\n    self.Langfuse = langfuse_client_init(\n                    ^^^^^^^^^^^^^^^^^^^^^\n  File \"/.venv/lib/python3.12/site-packages/litellm/integrations/langfuse/langfuse_prompt_management.py\", line 105, in langfuse_client_init\n    client = Langfuse(**parameters)\n             ^^^^^^^^^^^^^^^^^^^^^^\nTypeError: Langfuse.__init__() got an unexpected keyword argument 'sdk_integration'\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "chrisgoddard",
      "author_type": "User",
      "created_at": "2025-06-06T21:39:24Z",
      "updated_at": "2025-06-17T09:19:11Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11500/reactions",
        "total_count": 7,
        "+1": 7,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11500",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11500",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:46.093412",
      "comments": [
        {
          "author": "DevBey",
          "body": "yep same error. \n\nlitellm==1.72.2\nlangfuse==3.0.0",
          "created_at": "2025-06-07T20:33:57Z"
        },
        {
          "author": "williamjr",
          "body": "Also running into this trying to wire langfuse into my existing litellm deployment.\n\n`litellm==1.72.2` and `langfuse==3.0.0`\n\n",
          "created_at": "2025-06-09T00:18:18Z"
        },
        {
          "author": "jmilldotdev",
          "body": "needs a fix...",
          "created_at": "2025-06-10T16:09:09Z"
        },
        {
          "author": "marcklingen",
          "body": "Langfuse python sdk v3 includes breaking changes as it moved to opentelemetry ([docs](https://langfuse.com/docs/sdk/python/sdk-v3#upgrade-from-v2)), as a fix for this issue right now, I'd suggest to require `langfuse<3` in litellm.\n\nThen, it would be interesting to move this integration to OTel (eit",
          "created_at": "2025-06-10T20:27:09Z"
        },
        {
          "author": "DevBey",
          "body": "strange @marcklingen is everywhere, LOL how did you even find this issue, by just searching langfuse in litellm issues randomly ?? \n\nthat make you GOAT ",
          "created_at": "2025-06-10T20:36:13Z"
        }
      ]
    },
    {
      "issue_number": 11751,
      "title": "[Bug]: Structured Outputs on Bedrock Haiku 3.5 breaks often !!",
      "body": "### What happened?\n\nA bug happened!\n\n**Issue**: I am seeing structured outputs breaking a lot with Claude Haiku 3.5 through Litellm for certain prompts (Unfortunately I cannot share the complete log, but I have added a redacted snippet of the call that LiteLLM makes to Bedrock).\n\n**Potential Solution**: Use [`ToolChoice`](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_ToolChoice.html) in `ToolConfig`. I was able to get a close to 100% structured output adherence by using the boto3 lib and forcing the usage of `inputSchema` tool.\n\n\n### Relevant log output\n\n```shell\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://bedrock-runtime.us-east-2.amazonaws.com/model/arn%3Aaws%3Abedrock%3Aus-east-2%3A<AWS-ACCOUNT-ID>%3Ainference-profile%2Fus.anthropic.claude-3-5-haiku-20241022-v1%3A0/converse \\\n-H 'Content-Type: ap****on' -H 'X-Amz-Date: 20****7Z' -H 'X-Amz-Security-Token: IQ****n9' -H 'Authorization: AW****37' -H 'Content-Length: *****' \\\n-d '{\"messages\": [{\"role\": \"user\", \"content\": [{\"text\": \"<PROMPT>\\n\"}]}], \"additionalModelRequestFields\": {}, \"system\": [{\"text\": \"You are a helpful assistant designed to output strict JSON adhering to the response schema mentioned in the tools\"}], \"inferenceConfig\": {}, \"toolConfig\": {\"tools\": [{\"toolSpec\": {\"inputSchema\": {\"json\": {\"type\": \"object\", \"properties\": {\"sql\": {\"description\": \"<FIELD-DESCRIPTION>\", \"title\": \"<FIELD-TITLE>\", \"type\": \"string\"}}, \"required\": [\"sql\"]}}, \"name\": \"SQL\", \"description\": \"SQL\"}}]}}'\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.72.0\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "saahityaedams",
      "author_type": "User",
      "created_at": "2025-06-16T07:06:47Z",
      "updated_at": "2025-06-17T08:55:03Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11751/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11751",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11751",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:46.309480",
      "comments": []
    },
    {
      "issue_number": 11792,
      "title": "[bug]",
      "body": null,
      "state": "closed",
      "author": "yeahyung",
      "author_type": "User",
      "created_at": "2025-06-17T07:52:32Z",
      "updated_at": "2025-06-17T08:39:49Z",
      "closed_at": "2025-06-17T08:11:43Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11792/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11792",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11792",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:46.309501",
      "comments": []
    },
    {
      "issue_number": 11794,
      "title": "[Bug]: Managed Object Table create UniqueViolationError error",
      "body": "### What happened?\n\nWhen call retrieve batch api several times, LiteLLM got below error due to duplicate `unified_object_id` field value\n`prisma.errors.UniqueViolationError: Unique constraint failed on the fields: (`unified_object_id`)`\n\n\n### Relevant log output\n\n```shell\nprisma.errors.UniqueViolationError: Unique constraint failed on the fields: (`unified_object_id`)\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nmain branch\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "yeahyung",
      "author_type": "User",
      "created_at": "2025-06-17T08:12:24Z",
      "updated_at": "2025-06-17T08:12:24Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11794/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11794",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11794",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:46.309507",
      "comments": []
    },
    {
      "issue_number": 11259,
      "title": "[Bug]: SSLCertificate error going from 1.71.1 to 1.71.2",
      "body": "### What happened?\n\nEncounter this error when updating from 1.71.1 to 1.71.2\n\n```\nTraceback (most recent call last):\n  File \"/workplace/phahng/SoundTroopers/src/SoundTroopers/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/fallback_utils.py\", line 52, in async_completion_with_fallbacks\n    response = await litellm.acompletion(\n  File \"/workplace/phahng/SoundTroopers/src/SoundTroopers/.venv/lib/python3.12/site-packages/litellm/utils.py\", line 1492, in wrapper_async\n    raise e\n  File \"/workplace/phahng/SoundTroopers/src/SoundTroopers/.venv/lib/python3.12/site-packages/litellm/utils.py\", line 1353, in wrapper_async\n    result = await original_function(*args, **kwargs)\n  File \"/workplace/phahng/SoundTroopers/src/SoundTroopers/.venv/lib/python3.12/site-packages/litellm/main.py\", line 531, in acompletion\n    raise exception_type(\n  File \"/workplace/phahng/SoundTroopers/src/SoundTroopers/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2239, in exception_type\n    raise e\n  File \"/workplace/phahng/SoundTroopers/src/SoundTroopers/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 918, in exception_type\n    raise ServiceUnavailableError(\nlitellm.exceptions.ServiceUnavailableError: litellm.ServiceUnavailableError: BedrockException - Cannot connect to host bedrock-runtime.us-east-1.amazonaws.com:443 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1010)')]\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.71.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "hung-phan",
      "author_type": "User",
      "created_at": "2025-05-30T07:41:25Z",
      "updated_at": "2025-06-17T06:59:17Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11259/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11259",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11259",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:46.309514",
      "comments": [
        {
          "author": "sivaTwks010928",
          "body": "I am also getting this issue, but my team using latest version are not getting this? did you solve this? I need help",
          "created_at": "2025-06-17T06:59:16Z"
        }
      ]
    },
    {
      "issue_number": 11791,
      "title": "[Bug]: token counter does not expect prefix",
      "body": "### What happened?\n\n## Input\n```python\nfrom litellm import completion\nresponse = completion(\n  model=\"ollama_chat/deepseek-r1:14b\",\n  messages=[\n    {\"role\": \"user\", \"content\": \"Who won the world cup in 2022?\"},\n    {\"role\": \"assistant\", \"content\": \"Argentina\", \"prefix\": True}\n  ]\n)\n```\n## Error\n```\nAPIConnectionError: litellm.APIConnectionError: Unsupported type <class 'bool'> for key prefix in message {'role': 'assistant', 'content': 'Argentina', 'prefix': True}\nTraceback (most recent call last):\n  File \".\\.venv\\Lib\\site-packages\\litellm\\main.py\", line 2993, in completion\n    response = base_llm_http_handler.completion(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".\\.venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\llm_http_handler.py\", line 466, in completion\n    return provider_config.transform_response(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".\\.venv\\Lib\\site-packages\\litellm\\llms\\ollama\\chat\\transformation.py\", line 372, in transform_response\n    prompt_tokens = response_json.get(\"prompt_eval_count\", litellm.token_counter(messages=messages))  # type: ignore\n                                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".\\.venv\\Lib\\site-packages\\litellm\\utils.py\", line 1712, in token_counter\n    return token_counter_new(\n           ^^^^^^^^^^^^^^^^^^\n  File \".\\.venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\token_counter.py\", line 397, in token_counter\n    num_tokens = _count_messages(\n                 ^^^^^^^^^^^^^^^^\n  File \".\\.venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\token_counter.py\", line 465, in _count_messages\n    raise ValueError(\nValueError: Unsupported type <class 'bool'> for key prefix in message {'role': 'assistant', 'content': 'Argentina', 'prefix': True}\n```\n## Summary\nToken counter code can't handle prefix. I tested with ollama. `litellm.disable_token_counter = True` circumvents the issue.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.6\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "jbarthelmes",
      "author_type": "User",
      "created_at": "2025-06-17T06:49:16Z",
      "updated_at": "2025-06-17T06:49:16Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11791/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11791",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11791",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:46.492688",
      "comments": []
    },
    {
      "issue_number": 11580,
      "title": "[Bug]: tool_calls index starts from 1",
      "body": "### What happened?\n\nWhen I use Amazon Bedrock as LLM provider, claude-3-7-sonnet as LLM model.\n\nI got an error: TypeError: Cannot read properties of undefined (reading 'id') when using litellm OpenAI-Compatible Endpoints, the root cause is tool_calls start from 1, it lead to an incorrect array [undefined, {tool_call_object}], then raise the error then (array[0].id)\n**tool_calls index should always starts from 0 rather than 1**\n\n### Relevant log output\n\n```shell\n{\n    \"id\": \"chatcmpl-8cff77bc-ccea-4b88-a61a-e89d750fe236\",\n    \"created\": 1749546275,\n    \"model\": \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n    \"object\": \"chat.completion.chunk\",\n    \"choices\": [\n        {\n            \"index\": 0,\n            \"delta\": {\n                \"content\": \"\",\n                \"role\": \"assistant\",\n                \"tool_calls\": [\n                    {\n                        \"id\": \"tooluse_9116K8c5Q8ivMmdx3VdS7g\",\n                        \"function\": {\n                            \"arguments\": \"\",\n                            \"name\": \"jira_get_issue\"\n                        },\n                        \"type\": \"function\",\n                        \"index\": 1\n                    }\n                ]\n            }\n        }\n    ],\n    \"provider_specific_fields\": {}\n}\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.71.1-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "langpingxue",
      "author_type": "User",
      "created_at": "2025-06-10T09:36:41Z",
      "updated_at": "2025-06-17T06:20:18Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11580/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11580",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11580",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:46.492709",
      "comments": [
        {
          "author": "langpingxue",
          "body": "seems related to litellm/litellm/litellm_core_utils/streaming_chunk_builder_utils.py need to enhance the index logic",
          "created_at": "2025-06-10T16:43:32Z"
        },
        {
          "author": "langpingxue",
          "body": "for openAI models no such issue, specifically for bedrock models",
          "created_at": "2025-06-17T06:20:17Z"
        }
      ]
    },
    {
      "issue_number": 10988,
      "title": "[Bug]: Request truncated in spend logs",
      "body": "### What happened?\n\nFor me the main use case of using LiteLLM is that I save the request/response pairs in a database, but it looks like last month this functionality was silently broken: https://github.com/BerriAI/litellm/commit/87733c8193b9fa712cad44723fa9416c0eaf7d91#diff-05c7dd71d0a4d0c324e26fd5ac1865c3cb4512ead1da7ac672006868afb5fac5R366-R371\n\n```json\n{\n  \"model\": \"gemini/gemini-2.0-flash-001\",\n  \"stream\": false,\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"### Task:\\nGenerate a concise, 3-5 word title with an emoji summarizing the chat history.\\n### Guidelines:\\n- The title should clearly represent the main theme or subject of the conversation.\\n- Use emojis that enhance understanding of the topic, but avoid quotation marks or special formatting.\\n- Write the title in the chat's primary language; default to English if multilingual.\\n- Prioritize accuracy over excessive creativity; keep it clear and simple.\\n- Your entire response must consist solely of the JSON object, without any introductory or concluding text.\\n- The output must be a single, raw JSON object, without any markdown code fences or other encapsulating text.\\n- Ensure no conversational text, affirmations, or explanations precede or follow the raw JSON output, as this will cause direct parsing failure.\\n### Output:\\nJSON format: { \\\"title\\\": \\\"your concise title here\\\" }\\n### Examples:\\n- { \\\"title\\\": \\\"üìâ Stock Market Trends\\\" },\\n- { \\\"title\\\": \\\"üç™ Perfect Chocolate Chip Recipe\\\" },\\n- { \\\"title\\\": \\\"Ev... (truncated 679 chars)\"\n    }\n  ],\n  \"max_tokens\": 1000\n}\n```\n\nIt looks like this was introduced in https://github.com/BerriAI/litellm/pull/9838 with no way to turn it off. The original issue was https://github.com/BerriAI/litellm/issues/9732, which complained that `store_prompts_in_spend_logs` stores the prompts in the spend log ü§¶‚Äç‚ôÇ \n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.69.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "mrexodia",
      "author_type": "User",
      "created_at": "2025-05-20T23:28:10Z",
      "updated_at": "2025-06-17T06:04:08Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10988/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10988",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10988",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:46.655446",
      "comments": [
        {
          "author": "flyisland",
          "body": "Yeah, I've run into the same issue. It'd be great if LiteLLM offered a config option to toggle log truncation.",
          "created_at": "2025-06-07T08:40:44Z"
        },
        {
          "author": "mrexodia",
          "body": "Locally I just patch `spend_tracking_utils.py` and comment out the call to `_sanitize_request_body_for_spend_logs_payload`:\n\n```sh\nnano .venv/lib/python3.12/site-packages/litellm/proxy/spend_tracking/spend_tracking_utils.py\n```\n\nModify it so the function looks like this:\n\n```python\ndef _get_proxy_se",
          "created_at": "2025-06-07T12:48:28Z"
        },
        {
          "author": "vanillechai",
          "body": "Patch for the Docker image:\n\n```Dockerfile\nFROM ghcr.io/berriai/litellm-database:main-latest\n\nRUN sed -i.bak 's/MAX_STRING_LENGTH = 1000$/MAX_STRING_LENGTH = 100000/' \\\n    /app/litellm/proxy/spend_tracking/spend_tracking_utils.py && \\\n    cmp -s /app/litellm/proxy/spend_tracking/spend_tracking_util",
          "created_at": "2025-06-17T05:49:27Z"
        }
      ]
    },
    {
      "issue_number": 11766,
      "title": "[Bug]: Prisma Client Database Connect Failed",
      "body": "### What happened?\n\nA bug happened!\n\n## Command\n\n`sudo podman run --name litellm -it --rm -e DATABASE_URL=postgresql://testuser:testpass@pg_database:5432/litellm -e LITELLM_MASTER_KEY=sk-1234 -e no_proxy=localhost -v ./litellm/config.yaml:/app/config.yaml -p 20119:4000 --net service_delivery_network  litellm/litellm:v1.71.1-stable --config /app/config.yaml --detailed_debug\n`\n## config.yaml\n\n```\nlitellm_settings:\n  # Logging/Callback settings\n  # success_callback: [\"langfuse\"]\n  # langfuse_default_tags: [\"cache_hit\", \"cache_key\", \"proxy_base_url\", \"user_api_key_alias\", \"user_api_key_user_id\", \"user_api_key_user_email\", \"user_api_key_team_alias\", \"semantic-similarity\", \"proxy_base_url\"]\n  turn_off_message_logging: false\n  ssl_verify: false\n  \ngeneral_settings:\n  store_model_in_db: true\n  store_prompts_in_spend_logs: true\n  disable_spend_logs: false\n  disable_spend_updates: false\n```\n\n\n### Relevant log output\n\n```shell\nprisma:warn Prisma doesn't know which engines to download for the Linux distro \"wolfi\". Falling back to Prisma engines built \"debian\".\nPlease report your experience by creating an issue at https://github.com/prisma/prisma/issues so we can add your distro to the list of known supported distros.\nprisma:warn Prisma doesn't know which engines to download for the Linux distro \"wolfi\". Falling back to Prisma engines built \"debian\".\nPlease report your experience by creating an issue at https://github.com/prisma/prisma/issues so we can add your distro to the list of known supported distros.\nPrisma schema loaded from schema.prisma\nDatasource \"client\": PostgreSQL database \"litellm\", schema \"public\" at \"pg_database:5432\"\n\n‚ö†Ô∏è  There might be data loss when applying the changes:\n\n  ‚Ä¢ A unique constraint covering the columns `[sso_user_id]` on the table `LiteLLM_UserTable` will be added. If there are existing duplicate values, this will fail.\n\n\nüöÄ  Your database is now in sync with your Prisma schema. Done in 192ms\n\nRunning generate... - Prisma Client Python (v0.11.0)\n\nSome types are disabled by default due to being incompatible with Mypy, it is highly recommended\nto use Pyright instead and configure Prisma Python to use recursive types. To re-enable certain types:\n\ngenerator client {\n  provider             = \"prisma-client-py\"\n  recursive_type_depth = -1\n}\n\nIf you need to use Mypy, you can also disable this message by explicitly setting the default value:\n\ngenerator client {\n  provider             = \"prisma-client-py\"\n  recursive_type_depth = 5\n}\n\nFor more information see: https://prisma-client-py.readthedocs.io/en/stable/reference/limitations/#default-type-limitations\n\n‚úî Generated Prisma Client Python (v0.11.0) to ./../../prisma in 405ms\n\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\n16:30:40 - LiteLLM Proxy:DEBUG: proxy_server.py:505 - litellm.proxy.proxy_server.py::startup() - CHECKING PREMIUM USER - False\n16:30:40 - LiteLLM Proxy:DEBUG: litellm_license.py:98 - litellm.proxy.auth.litellm_license.py::is_premium() - ENTERING 'IS_PREMIUM' - LiteLLM License=None\n16:30:40 - LiteLLM Proxy:DEBUG: litellm_license.py:107 - litellm.proxy.auth.litellm_license.py::is_premium() - Updated 'self.license_str' - None\n16:30:40 - LiteLLM Proxy:DEBUG: proxy_server.py:518 - worker_config: {\"model\": null, \"alias\": null, \"api_base\": null, \"api_version\": \"2024-07-01-preview\", \"debug\": false, \"detailed_debug\": true, \"temperature\": null, \"max_tokens\": null, \"request_timeout\": null, \"max_budget\": null, \"telemetry\": true, \"drop_params\": false, \"add_function_to_prompt\": false, \"headers\": null, \"save\": false, \"config\": \"/app/config.yaml\", \"use_queue\": false}\n\n#------------------------------------------------------------#\n#                                                            #\n#               'A feature I really want is...'               #\n#        https://github.com/BerriAI/litellm/issues/new        #\n#                                                            #\n#------------------------------------------------------------#\n\n Thank you for using LiteLLM! - Krrish & Ishaan\n\n\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n\n\n16:30:40 - LiteLLM Proxy:DEBUG: proxy_server.py:1794 -  setting litellm.turn_off_message_logging=False\n16:30:40 - LiteLLM Proxy:DEBUG: proxy_server.py:1794 -  setting litellm.ssl_verify=False\n16:30:40 - LiteLLM Proxy:DEBUG: proxy_server.py:2070 - _alerting_callbacks: {'store_model_in_db': True, 'store_prompts_in_spend_logs': True, 'disable_spend_logs': False, 'disable_spend_updates': False}\nGeneral settings: {'store_model_in_db': True, 'store_prompts_in_spend_logs': True, 'disable_spend_logs': False, 'disable_spend_updates': False}\nCustom auth settings: None\nCustom auth settings: None\n16:30:40 - LiteLLM Router:INFO: router.py:660 - Routing strategy: simple-shuffle\n16:30:40 - LiteLLM Router:DEBUG: router.py:545 - Intialized router with Routing strategy: simple-shuffle\n\nRouting enable_pre_call_checks: False\n\nRouting fallbacks: None\n\nRouting content fallbacks: None\n\nRouting context window fallbacks: None\n\nRouter Redis Caching=None\n\n16:30:40 - LiteLLM Proxy:DEBUG: utils.py:1168 - Creating Prisma Client..\n16:30:42 - LiteLLM Proxy:DEBUG: utils.py:1191 - Success - Created Prisma Client\n16:30:42 - LiteLLM Proxy:DEBUG: utils.py:2237 - PrismaClient: connect() called Attempting to Connect to DB\n16:30:42 - LiteLLM Proxy:DEBUG: utils.py:2241 - PrismaClient: DB not connected, Attempting to Connect to DB\n16:30:52 - LiteLLM Proxy:DEBUG: utils.py:108 - LiteLLM Prisma Client Exception connect(): Could not connect to the query engine\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/usr/lib/python3.13/site-packages/httpx/_transports/default.py\", line 394, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\n    raise exc from None\n  File \"/usr/lib/python3.13/site-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\n    response = await connection.handle_async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        pool_request.request\n        ^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_async/connection.py\", line 101, in handle_async_request\n    raise exc\n  File \"/usr/lib/python3.13/site-packages/httpcore/_async/connection.py\", line 78, in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_async/connection.py\", line 124, in _connect\n    stream = await self._network_backend.connect_tcp(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_backends/auto.py\", line 31, in connect_tcp\n    return await self._backend.connect_tcp(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<5 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_backends/anyio.py\", line 113, in connect_tcp\n    with map_exceptions(exc_map):\n         ~~~~~~~~~~~~~~^^^^^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 162, in __exit__\n    self.gen.throw(value)\n    ~~~~~~~~~~~~~~^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: All connection attempts failed\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/prisma/engine/query.py\", line 207, in spawn\n    data = await self.request('GET', '/status')\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/prisma/engine/http.py\", line 119, in request\n    resp = await self.session.request(method, url, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/prisma/_async_http.py\", line 28, in request\n    return Response(await self.session.request(method, url, **kwargs))\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpx/_client.py\", line 1540, in request\n    return await self.send(request, auth=auth, follow_redirects=follow_redirects)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpx/_client.py\", line 1629, in send\n    response = await self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/httpx/_client.py\", line 1657, in _send_handling_auth\n    response = await self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<3 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/httpx/_client.py\", line 1694, in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpx/_client.py\", line 1730, in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpx/_transports/default.py\", line 393, in handle_async_request\n    with map_httpcore_exceptions():\n         ~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 162, in __exit__\n    self.gen.throw(value)\n    ~~~~~~~~~~~~~~^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: All connection attempts failed\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/utils.py\", line 2244, in connect\n    await self.db.connect()\n  File \"/usr/lib/python3.13/site-packages/prisma/client.py\", line 358, in connect\n    await self.__engine.connect(\n    ...<2 lines>...\n    )\n  File \"/usr/lib/python3.13/site-packages/prisma/engine/query.py\", line 137, in connect\n    await self.spawn(file, timeout=timeout, datasources=datasources)\n  File \"/usr/lib/python3.13/site-packages/prisma/engine/query.py\", line 230, in spawn\n    raise errors.EngineConnectionError(\n        'Could not connect to the query engine'\n    ) from last_exc\nprisma.engine.errors.EngineConnectionError: Could not connect to the query engine\n\n16:30:52 - LiteLLM Proxy:DEBUG: utils.py:108 - Backing off... this was attempt #1\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/usr/lib/python3.13/site-packages/httpx/_transports/default.py\", line 394, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\n    raise exc from None\n  File \"/usr/lib/python3.13/site-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\n    response = await connection.handle_async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        pool_request.request\n        ^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_async/connection.py\", line 101, in handle_async_request\n    raise exc\n  File \"/usr/lib/python3.13/site-packages/httpcore/_async/connection.py\", line 78, in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_async/connection.py\", line 124, in _connect\n    stream = await self._network_backend.connect_tcp(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_backends/auto.py\", line 31, in connect_tcp\n    return await self._backend.connect_tcp(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<5 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_backends/anyio.py\", line 113, in connect_tcp\n    with map_exceptions(exc_map):\n         ~~~~~~~~~~~~~~^^^^^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 162, in __exit__\n    self.gen.throw(value)\n    ~~~~~~~~~~~~~~^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: All connection attempts failed\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/prisma/engine/query.py\", line 207, in spawn\n    data = await self.request('GET', '/status')\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/prisma/engine/http.py\", line 119, in request\n    resp = await self.session.request(method, url, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/prisma/_async_http.py\", line 28, in request\n    return Response(await self.session.request(method, url, **kwargs))\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpx/_client.py\", line 1540, in request\n    return await self.send(request, auth=auth, follow_redirects=follow_redirects)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpx/_client.py\", line 1629, in send\n    response = await self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/httpx/_client.py\", line 1657, in _send_handling_auth\n    response = await self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<3 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/httpx/_client.py\", line 1694, in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpx/_client.py\", line 1730, in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpx/_transports/default.py\", line 393, in handle_async_request\n    with map_httpcore_exceptions():\n         ~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 162, in __exit__\n    self.gen.throw(value)\n    ~~~~~~~~~~~~~~^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: All connection attempts failed\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/backoff/_async.py\", line 151, in retry\n    ret = await target(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/utils.py\", line 2261, in connect\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/utils.py\", line 2244, in connect\n    await self.db.connect()\n  File \"/usr/lib/python3.13/site-packages/prisma/client.py\", line 358, in connect\n    await self.__engine.connect(\n    ...<2 lines>...\n    )\n  File \"/usr/lib/python3.13/site-packages/prisma/engine/query.py\", line 137, in connect\n    await self.spawn(file, timeout=timeout, datasources=datasources)\n  File \"/usr/lib/python3.13/site-packages/prisma/engine/query.py\", line 230, in spawn\n    raise errors.EngineConnectionError(\n        'Could not connect to the query engine'\n    ) from last_exc\nprisma.engine.errors.EngineConnectionError: Could not connect to the query engine\n\n16:30:52 - LiteLLM Proxy:DEBUG: utils.py:2237 - PrismaClient: connect() called Attempting to Connect to DB\n16:30:52 - LiteLLM Proxy:DEBUG: utils.py:108 - LiteLLM Prisma Client Exception disconnect(): All connection attempts failed\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/usr/lib/python3.13/site-packages/httpx/_transports/default.py\", line 394, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\n    raise exc from None\n  File \"/usr/lib/python3.13/site-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\n    response = await connection.handle_async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        pool_request.request\n        ^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_async/connection.py\", line 101, in handle_async_request\n    raise exc\n  File \"/usr/lib/python3.13/site-packages/httpcore/_async/connection.py\", line 78, in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_async/connection.py\", line 124, in _connect\n    stream = await self._network_backend.connect_tcp(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_backends/auto.py\", line 31, in connect_tcp\n    return await self._backend.connect_tcp(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<5 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_backends/anyio.py\", line 113, in connect_tcp\n    with map_exceptions(exc_map):\n         ~~~~~~~~~~~~~~^^^^^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 162, in __exit__\n    self.gen.throw(value)\n    ~~~~~~~~~~~~~~^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: All connection attempts failed\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/utils.py\", line 2303, in health_check\n    response = await self.db.query_raw(sql_query)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/prisma/client.py\", line 457, in query_raw\n    resp = await self._execute(\n           ^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/prisma/client.py\", line 561, in _execute\n    return await self._engine.query(builder.build(), tx_id=self._tx_id)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/prisma/engine/query.py\", line 244, in query\n    return await self.request(\n           ^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/prisma/engine/http.py\", line 119, in request\n    resp = await self.session.request(method, url, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/prisma/_async_http.py\", line 28, in request\n    return Response(await self.session.request(method, url, **kwargs))\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpx/_client.py\", line 1540, in request\n    return await self.send(request, auth=auth, follow_redirects=follow_redirects)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpx/_client.py\", line 1629, in send\n    response = await self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/httpx/_client.py\", line 1657, in _send_handling_auth\n    response = await self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<3 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/httpx/_client.py\", line 1694, in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpx/_client.py\", line 1730, in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpx/_transports/default.py\", line 393, in handle_async_request\n    with map_httpcore_exceptions():\n         ~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 162, in __exit__\n    self.gen.throw(value)\n    ~~~~~~~~~~~~~~^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: All connection attempts failed\n\nERROR:    Traceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/usr/lib/python3.13/site-packages/httpx/_transports/default.py\", line 394, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\n    raise exc from None\n  File \"/usr/lib/python3.13/site-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\n    response = await connection.handle_async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        pool_request.request\n        ^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_async/connection.py\", line 101, in handle_async_request\n    raise exc\n  File \"/usr/lib/python3.13/site-packages/httpcore/_async/connection.py\", line 78, in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_async/connection.py\", line 124, in _connect\n    stream = await self._network_backend.connect_tcp(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_backends/auto.py\", line 31, in connect_tcp\n    return await self._backend.connect_tcp(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<5 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_backends/anyio.py\", line 113, in connect_tcp\n    with map_exceptions(exc_map):\n         ~~~~~~~~~~~~~~^^^^^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 162, in __exit__\n    self.gen.throw(value)\n    ~~~~~~~~~~~~~~^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: All connection attempts failed\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/starlette/routing.py\", line 693, in lifespan\n    async with self.lifespan_context(app) as maybe_state:\n               ~~~~~~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/proxy_server.py\", line 565, in proxy_startup_event\n    prisma_client = await ProxyStartupEvent._setup_prisma_client(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<3 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/proxy_server.py\", line 3382, in _setup_prisma_client\n    PrismaDBExceptionHandler.handle_db_exception(e)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/db/exception_handler.py\", line 61, in handle_db_exception\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/proxy_server.py\", line 3379, in _setup_prisma_client\n    await prisma_client.health_check()\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/utils.py\", line 2321, in health_check\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/utils.py\", line 2303, in health_check\n    response = await self.db.query_raw(sql_query)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/prisma/client.py\", line 457, in query_raw\n    resp = await self._execute(\n           ^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/prisma/client.py\", line 561, in _execute\n    return await self._engine.query(builder.build(), tx_id=self._tx_id)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/prisma/engine/query.py\", line 244, in query\n    return await self.request(\n           ^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/prisma/engine/http.py\", line 119, in request\n    resp = await self.session.request(method, url, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/prisma/_async_http.py\", line 28, in request\n    return Response(await self.session.request(method, url, **kwargs))\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpx/_client.py\", line 1540, in request\n    return await self.send(request, auth=auth, follow_redirects=follow_redirects)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpx/_client.py\", line 1629, in send\n    response = await self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/httpx/_client.py\", line 1657, in _send_handling_auth\n    response = await self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<3 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/httpx/_client.py\", line 1694, in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpx/_client.py\", line 1730, in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpx/_transports/default.py\", line 393, in handle_async_request\n    with map_httpcore_exceptions():\n         ~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 162, in __exit__\n    self.gen.throw(value)\n    ~~~~~~~~~~~~~~^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: All connection attempts failed\n\nERROR:    Application startup failed. Exiting.\n16:30:52 - LiteLLM Proxy:DEBUG: utils.py:108 - Backing off... this was attempt #1\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/usr/lib/python3.13/site-packages/httpx/_transports/default.py\", line 394, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\n    raise exc from None\n  File \"/usr/lib/python3.13/site-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\n    response = await connection.handle_async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        pool_request.request\n        ^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_async/connection.py\", line 101, in handle_async_request\n    raise exc\n  File \"/usr/lib/python3.13/site-packages/httpcore/_async/connection.py\", line 78, in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_async/connection.py\", line 124, in _connect\n    stream = await self._network_backend.connect_tcp(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_backends/auto.py\", line 31, in connect_tcp\n    return await self._backend.connect_tcp(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<5 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_backends/anyio.py\", line 113, in connect_tcp\n    with map_exceptions(exc_map):\n         ~~~~~~~~~~~~~~^^^^^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 162, in __exit__\n    self.gen.throw(value)\n    ~~~~~~~~~~~~~~^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: All connection attempts failed\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/backoff/_async.py\", line 151, in retry\n    ret = await target(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/utils.py\", line 1281, in check_view_exists\n    ret = await self.db.query_raw(\n          ^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<13 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/prisma/client.py\", line 457, in query_raw\n    resp = await self._execute(\n           ^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/prisma/client.py\", line 561, in _execute\n    return await self._engine.query(builder.build(), tx_id=self._tx_id)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/prisma/engine/query.py\", line 244, in query\n    return await self.request(\n           ^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/prisma/engine/http.py\", line 119, in request\n    resp = await self.session.request(method, url, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/prisma/_async_http.py\", line 28, in request\n    return Response(await self.session.request(method, url, **kwargs))\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpx/_client.py\", line 1540, in request\n    return await self.send(request, auth=auth, follow_redirects=follow_redirects)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpx/_client.py\", line 1629, in send\n    response = await self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/httpx/_client.py\", line 1657, in _send_handling_auth\n    response = await self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<3 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/httpx/_client.py\", line 1694, in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpx/_client.py\", line 1730, in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpx/_transports/default.py\", line 393, in handle_async_request\n    with map_httpcore_exceptions():\n         ~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 162, in __exit__\n    self.gen.throw(value)\n    ~~~~~~~~~~~~~~^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: All connection attempts failed\n\n16:30:52 - LiteLLM Proxy:ERROR: utils.py:2333 - Error getting LiteLLM_SpendLogs row count: All connection attempts failed\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.17.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "mvenkatsriram",
      "author_type": "User",
      "created_at": "2025-06-16T16:43:14Z",
      "updated_at": "2025-06-17T04:28:06Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11766/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11766",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11766",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:46.829733",
      "comments": [
        {
          "author": "burnbrigther",
          "body": "I have this same issue as well.  Any work around?",
          "created_at": "2025-06-17T04:28:06Z"
        }
      ]
    },
    {
      "issue_number": 11789,
      "title": "[Bug]: Anthropic cost calculations are incorrect with streaming and prompt caching",
      "body": "I believe this is the same as bug [#9812](https://github.com/BerriAI/litellm/issues/9812).\n\nInput cache read tokens are being counted as regular input tokens for cost calculations - but only when streaming is enabled.\nThis is using anthropic pass through.\n\n### Streamed response example\nExample LiteLLM for a streamed LLM response with input caching\nLiteLLM cost reported as $0.002059. The true cost should be $0.000292 (roughly 1/10).\n\nLiteLLM usage logs:\n```\n  \"usage_object\": {\n    \"total_tokens\": 2482,\n    \"prompt_tokens\": 2459,\n    \"completion_tokens\": 23,\n    \"prompt_tokens_details\": {\n      \"audio_tokens\": null,\n      \"cached_tokens\": 0\n    },\n    \"cache_read_input_tokens\": 2455,\n    \"completion_tokens_details\": {\n      \"audio_tokens\": null,\n      \"reasoning_tokens\": 0,\n      \"accepted_prediction_tokens\": null,\n      \"rejected_prediction_tokens\": null\n    },\n    \"cache_creation_input_tokens\": 0\n  },\n```\nThe 2455 tokens are being counted as regular input tokens for the cost.\n\n### Non-streamed response example\nFor comparison, the non-streamed case is correct.\nLiteLLM cost reported as $0.000345. This is correct.\nLiteLLM usage logs:\n```\n  \"usage_object\": {\n    \"total_tokens\": 2489,\n    \"prompt_tokens\": 2455,\n    \"completion_tokens\": 34,\n    \"prompt_tokens_details\": {\n      \"audio_tokens\": null,\n      \"cached_tokens\": 2438\n    },\n    \"cache_read_input_tokens\": 2438,\n    \"completion_tokens_details\": null,\n    \"cache_creation_input_tokens\": 0\n  },\n```\n\n\n### Example script to reproduce\n\n```python\nimport anthropic\nimport time\nimport requests\nfrom bs4 import BeautifulSoup\nimport os\n\nclient = anthropic.Anthropic(\n    api_key=LITELLM_API_KEY, \n    base_url=f\"{LITELLM_BASE_URL}/anthropic\")\n# MODEL_NAME = \"claude-sonnet-4-20250514\"\nMODEL_NAME = \"claude-3-5-haiku-20241022\"\n\ndef fetch_article_content(url):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    \n    # Remove script and style elements\n    for script in soup([\"script\", \"style\"]):\n        script.decompose()\n    \n    # Get text\n    text = soup.get_text()\n    \n    # Break into lines and remove leading and trailing space on each\n    lines = (line.strip() for line in text.splitlines())\n    # Break multi-headlines into a line each\n    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n    # Drop blank lines\n    text = '\\n'.join(chunk for chunk in chunks if chunk)\n    \n    return text\n\n# Fetch the content of the article\nbook_url = \"https://www.gutenberg.org/cache/epub/1342/pg1342.txt\"\nbook_content = fetch_article_content(book_url)\nbook_content = book_content[:10000]\n\nprint(f\"Fetched {len(book_content)} characters from the book.\")\nprint(\"First 500 characters:\")\nprint(book_content[:500])\n\ndef make_api_call(name, custom_text, extra_cache, stream):\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"<boook>\" + book_content + \"</boook>\",\n                    \"cache_control\": {\"type\": \"ephemeral\"}\n                },\n                {\n                    \"type\": \"text\",\n                    \"text\": custom_text,\n                    **({\"cache_control\": {\"type\": \"ephemeral\"}} if extra_cache else {})\n                }\n            ]\n        }\n    ]\n\n    start_time = time.time()\n    response = client.messages.create(\n        model=MODEL_NAME,\n        max_tokens=300,\n        messages=messages,\n        extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"},\n        stream=stream\n    )\n    usage = None\n    if stream:\n        for event in response:\n            if hasattr(event, \"usage\"):\n                usage = event.usage\n        content = \"streamed response\"\n    else:\n        usage = response.usage\n        content = response.content\n\n    duration = time.time() - start_time\n\n    print(\"\\n\")\n    print(f\"{name} API call time: {duration:.2f} seconds\")\n    print(f\"{name} API call all usage: {usage}\")\n    print(f\"{name} content: {content}\")\n\n    return response, duration\n\nstream = True\nnon_cached_response, non_cached_time = make_api_call(\"First-Streamed\", \"What is the title of this book? Only output the title.\", False, stream)\ncached_response, cached_time = make_api_call(\"Second-Streamed\", \"Write 1 alternative title for this book. Only output the title.\", True, stream)\n\nstream = False\nnon_cached_response, non_cached_time = make_api_call(\"First-NonStream\", \"What is the title of this book? Only output the title.\", False, stream)\ncached_response, cached_time = make_api_call(\"Second-NonStream\", \"Write 1 alternative title for this book. Only output the title.\", True, stream)\n\n```\n\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "nootreeno",
      "author_type": "User",
      "created_at": "2025-06-17T04:18:43Z",
      "updated_at": "2025-06-17T04:21:51Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11789/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11789",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11789",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:46.975869",
      "comments": []
    },
    {
      "issue_number": 11748,
      "title": "[Bug]: AWS Bedrock Deployed Models Mistral large and mini 2402-v1:0 are not working with converse api",
      "body": "### What happened?\n\nWe're encountering an issue where **Mistral models deployed via AWS Bedrock** are not functioning with the converse API in LiteLLM, despite the official documentation indicating native support.\n\n**Affected Models:**\n\n1. mistral.mistral-large-2402-v1:0\n2. mistral.mistral-small-2402-v1:0\n\n**Expected Behavior:**\nThese models should be compatible with the converse API as per AWS Bedrock's documentation.  https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-supported-models-features.html \n\n**Actual Behavior:**\nAttempts to use the converse API with these models result in errors or no response.\n\n### Relevant log output\n\n```shell\nlitellm.UnsupportedParamsError: bedrock does not support parameters: ['tools'], for model=mistral.mistral-small-2402-v1:0. To drop these, set `litellm.drop_params=True` or for proxy:\n\n`litellm_settings:\n drop_params: true`\n. \n If you want to use these params dynamically send allowed_openai_params=['tools'] in your request.\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.2\n\n### Twitter / LinkedIn details\n\nhttps://www.linkedin.com/in/shagunbansal/",
      "state": "closed",
      "author": "shagunb-acn",
      "author_type": "User",
      "created_at": "2025-06-16T04:15:34Z",
      "updated_at": "2025-06-17T00:34:41Z",
      "closed_at": "2025-06-17T00:34:41Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11748/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11748",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11748",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:46.975892",
      "comments": []
    },
    {
      "issue_number": 11669,
      "title": "[Bug]: Unable to create bedrock model without credentials",
      "body": "### What happened?\n\nWhen adding a new model via the interface, if the provider is Bedrock, then one is required to add a set of AWS credentials. If LiteLLM is running on an AWS instance with an instance profile attached, then a set of static credentials isn't required. This can be demonstrated by clicking the \"Test model\" button with the AWS creds being blank.\n\nTrying to save the model with them being blank results in an error. The fields should not be required.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "mrh-chain",
      "author_type": "User",
      "created_at": "2025-06-12T19:05:46Z",
      "updated_at": "2025-06-17T00:32:11Z",
      "closed_at": "2025-06-17T00:32:11Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11669/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11669",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11669",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:46.975900",
      "comments": [
        {
          "author": "wagnerjt",
          "body": "Same as #8811.\n\nYou can do this programmatically which is what I do and you can see how in the issue",
          "created_at": "2025-06-12T19:54:51Z"
        }
      ]
    },
    {
      "issue_number": 8811,
      "title": "[Feature]: Make `AWS Access Key ID` and `AWS Secret Access Key` optional in the `Add new model` UI",
      "body": "### The Feature\n\nMake AWS Access Key ID and AWS Secret Access Key optional in the Add new model UI\n\n### Motivation, pitch\n\nI deploy Litellm on AWS on both ECS and EKS. The IAM role of my deployment already has access to Bedrock. So, in my config.yaml, I do not specify an `AWS Access Key ID` or `AWS Secret Access Key`, and it \"just works\"\n\nI should not be forced to provide credentials for this\n\nAnd also, the option to provide `os.environ/MY_SECRET_KEY` and `os.environ/MY_SECRET_KEY` does not fix this issue, as ECS/EKS assuming a role also relies on a session token\n\n![Image](https://github.com/user-attachments/assets/5b24533b-80fb-4cd9-b7f7-6b1ec43de4fd)\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "mirodrr2",
      "author_type": "User",
      "created_at": "2025-02-25T18:07:11Z",
      "updated_at": "2025-06-17T00:32:11Z",
      "closed_at": "2025-06-17T00:32:11Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8811/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "ishaan-jaff"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8811",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8811",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:47.161651",
      "comments": [
        {
          "author": "wagnerjt",
          "body": "I also run into this from the frontend, so I do it programmatically and use the `aws_role_name` instead of these two in the body of the model creation.\n\nMaybe the enhancement should be to choosing between ACCESS_KEY + SECRET_KEY or this ROLE_NAME.",
          "created_at": "2025-02-25T18:41:57Z"
        },
        {
          "author": "mirodrr2",
          "body": "> I also run into this from the frontend, so I do it programmatically and use the `aws_role_name` instead of these two in the body of the model creation.\n> \n> Maybe the enhancement should be to choosing between ACCESS_KEY + SECRET_KEY or this ROLE_NAME.\n\nThat would be a okay option, but the ideal fo",
          "created_at": "2025-02-25T18:43:41Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "<img width=\"704\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f34bf5b9-512f-497a-9110-05885c76659f\" />\n\nshould the ui just allow all these params as optional ? @mirodrr2 / @wagnerjt ",
          "created_at": "2025-02-25T18:52:42Z"
        },
        {
          "author": "mirodrr2",
          "body": "Yes, all of them should be optional in my opinion. If you're running on AWS, this is all already taken care of for you via ECS/EKS assumed IAM roles",
          "created_at": "2025-02-25T18:59:06Z"
        },
        {
          "author": "mirodrr2",
          "body": "For reference, this is what my config.yaml looks like for bedrock models, and it works just fine:\n\n```\n- model_name: amazon.nova-pro-v1:0\n    litellm_params:\n      model: bedrock/amazon.nova-pro-v1:0\n  - model_name: amazon.nova-lite-v1:0\n    litellm_params:\n      model: bedrock/amazon.nova-lite-v1:0",
          "created_at": "2025-02-25T19:00:01Z"
        }
      ]
    },
    {
      "issue_number": 11708,
      "title": "[Bug]: aiohttp.ConnectionTimeoutError missing in aiohttp<3.10 ‚Äì crashes litellm despite aiohttp=\"*\" in pyproject",
      "body": "### What happened?\n\nI encountered a runtime error while using `litellm==1.72.4` with `aiohttp==3.9.4`.\n\nThe issue is that `aiohttp.ConnectionTimeoutError` is not available in versions <3.10. However, LiteLLM imports and references it directly in:\n\n`litellm/llms/custom_httpx/aiohttp_transport.py`, [line 18](https://github.com/BerriAI/litellm/blob/07472ce21f6786537047de5a55520aaa4d066478/litellm/llms/custom_httpx/aiohttp_transport.py#L18).\n\nSince the [pyproject.toml](https://github.com/BerriAI/litellm/blob/07472ce21f6786537047de5a55520aaa4d066478/pyproject.toml#L31C1-L31C14) currently lists `aiohttp = \"*\"`, this allowed Poetry to install aiohttp 3.9.4, even though it's incompatible.\n\nThis leads to the following crash at runtime:\n\n### Relevant log output\n\n```shell\nAttributeError: module 'aiohttp' has no attribute 'ConnectionTimeoutError'\n\n  File \".../aiohttp/__init__.py\", line 240, in __getattr__\n    raise AttributeError(f\"module {__name__} has no attribute {name}\")\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.4\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "anujpatel03",
      "author_type": "User",
      "created_at": "2025-06-13T14:28:06Z",
      "updated_at": "2025-06-17T00:31:15Z",
      "closed_at": "2025-06-17T00:31:15Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11708/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11708",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11708",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:47.351681",
      "comments": []
    },
    {
      "issue_number": 7536,
      "title": "[Bug]: migration job only runs if there is a change in values",
      "body": "### What happened?\n\nWe recently updated our helm chart version for the litellm proxy and saw that there was a job added to handle the database migration work. The job was successfully created and ran the first time, but didn't run on subsequent updates because the job already exists. Looking at the migrations-job.yaml template file I see that this checksum annotation exists: `checksum/config: {{ toYaml .Values | sha256sum }}`. The problem is that if nothing changes in the values.yaml file then that checksum doesn't change, so the job is not re-created. Is there a mechanism in place to ensure the migration job will run when required?\n\n### Relevant log output\n\n_No response_\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.55.10\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "mikstew",
      "author_type": "User",
      "created_at": "2025-01-03T20:56:04Z",
      "updated_at": "2025-06-17T00:02:33Z",
      "closed_at": "2025-06-17T00:02:33Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/7536/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/7536",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/7536",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:47.351705",
      "comments": [
        {
          "author": "logg926",
          "body": "# LiteLLM Helm Chart Upgrade Notes\n\n## Known Bug: Migration Job Issues During Upgrades\n\n### Problem Description\n\nThe LiteLLM Helm chart has a design issue with the migration job that can prevent configuration changes from being properly applied during upgrades. This is documented in [Issue #7536](ht",
          "created_at": "2025-03-10T12:30:03Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-09T00:02:15Z"
        }
      ]
    },
    {
      "issue_number": 7584,
      "title": "[Bug]: Model info API with whisper and Dall-e",
      "body": "### What happened?\n\nthe info api did not provide the cost per second for whisper and did not provide image cost for dall-e-3\r\n\n\n### Relevant log output\n\n```shell\n{\r\n  \"data\": {\r\n    \"model_name\": \"openai/whisper-1\",\r\n    \"litellm_params\": {\r\n      \"custom_llm_provider\": null,\r\n      \"tpm\": null,\r\n      \"rpm\": 10000,\r\n      \"api_base\": null,\r\n      \"api_version\": null,\r\n      \"timeout\": null,\r\n      \"stream_timeout\": null,\r\n      \"max_retries\": null,\r\n      \"organization\": null,\r\n      \"configurable_clientside_auth_params\": null,\r\n      \"litellm_trace_id\": null,\r\n      \"region_name\": null,\r\n      \"vertex_project\": null,\r\n      \"vertex_location\": null,\r\n      \"aws_region_name\": null,\r\n      \"watsonx_region_name\": null,\r\n      \"input_cost_per_token\": null,\r\n      \"output_cost_per_token\": null,\r\n      \"input_cost_per_second\": null,\r\n      \"output_cost_per_second\": null,\r\n      \"max_file_size_mb\": null,\r\n      \"max_budget\": null,\r\n      \"budget_duration\": null,\r\n      \"model\": \"openai/whisper-1\"\r\n    },\r\n    \"model_info\": {\r\n      \"id\": \"2841d4444004e878951598c7b37cc8ffc05c295dada0a54eb6cbf55a8303234b\",\r\n      \"db_model\": false,\r\n      \"updated_at\": null,\r\n      \"updated_by\": null,\r\n      \"created_at\": null,\r\n      \"created_by\": null,\r\n      \"base_model\": null,\r\n      \"tier\": null,\r\n      \"team_id\": null,\r\n      \"mode\": \"audio_transcription\"\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n  \"data\": {\r\n    \"model_name\": \"openai/dall-e-3\",\r\n    \"litellm_params\": {\r\n      \"custom_llm_provider\": null,\r\n      \"tpm\": null,\r\n      \"rpm\": null,\r\n      \"api_base\": null,\r\n      \"api_version\": null,\r\n      \"timeout\": null,\r\n      \"stream_timeout\": null,\r\n      \"max_retries\": null,\r\n      \"organization\": null,\r\n      \"configurable_clientside_auth_params\": null,\r\n      \"litellm_trace_id\": null,\r\n      \"region_name\": null,\r\n      \"vertex_project\": null,\r\n      \"vertex_location\": null,\r\n      \"aws_region_name\": null,\r\n      \"watsonx_region_name\": null,\r\n      \"input_cost_per_token\": null,\r\n      \"output_cost_per_token\": null,\r\n      \"input_cost_per_second\": null,\r\n      \"output_cost_per_second\": null,\r\n      \"max_file_size_mb\": null,\r\n      \"max_budget\": null,\r\n      \"budget_duration\": null,\r\n      \"model\": \"openai/dall-e-3\"\r\n    },\r\n    \"model_info\": {\r\n      \"id\": \"55b8aacabc30b32ae6255e7306cc38bbfdaae5148d5c9e712d6601451dffbe32\",\r\n      \"db_model\": false,\r\n      \"updated_at\": null,\r\n      \"updated_by\": null,\r\n      \"created_at\": null,\r\n      \"created_by\": null,\r\n      \"base_model\": null,\r\n      \"tier\": null,\r\n      \"team_id\": null\r\n    }\r\n  }\r\n}\n```\n\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.57.0\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "superpoussin22",
      "author_type": "User",
      "created_at": "2025-01-06T16:27:01Z",
      "updated_at": "2025-06-17T00:02:32Z",
      "closed_at": "2025-06-17T00:02:32Z",
      "labels": [
        "bug",
        "mlops user request",
        "stale",
        "march 2025"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/7584/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/7584",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/7584",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:47.580086",
      "comments": [
        {
          "author": "OrionCodeDev",
          "body": "Any update? @krrishdholakia ",
          "created_at": "2025-03-09T21:37:27Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-09T00:02:14Z"
        }
      ]
    },
    {
      "issue_number": 8240,
      "title": "[Bug]: NoneType object is not iterable",
      "body": "### What happened?\n\n![Image](https://github.com/user-attachments/assets/1a202122-a9f3-4353-b2c0-13e5c26edd7c)\nI get this and in console\n\n```\n11:33:48 - LiteLLM Proxy:ERROR: internal_user_endpoints.py:395 - litellm.proxy.proxy_server.user_info(): Exception occured - 'NoneType' object is not iterable\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/litellm/proxy/management_endpoints/internal_user_endpoints.py\", line 308, in user_info\n    return await _get_user_info_for_proxy_admin()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/litellm/proxy/management_endpoints/internal_user_endpoints.py\", line 431, in _get_user_info_for_proxy_admin\n    for key in _keys_in_db:\nTypeError: 'NoneType' object is not iterable\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nlatest\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "Mte90",
      "author_type": "User",
      "created_at": "2025-02-04T11:34:56Z",
      "updated_at": "2025-06-17T00:02:30Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8240/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8240",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8240",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:47.881070",
      "comments": [
        {
          "author": "jaceyang97",
          "body": "same issue",
          "created_at": "2025-02-05T04:22:12Z"
        },
        {
          "author": "Mte90",
          "body": "https://github.com/BerriAI/litellm/blob/main/litellm/proxy/management_endpoints/internal_user_endpoints.py#L445\n\nLooking here if there is no key there is the error as that case is not handled.\n\n@ishaan-jaff it seems an easy fix but probably require some alerting to the user.",
          "created_at": "2025-02-05T15:41:04Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "can we get help from a contributor PR on this + with testing ? ",
          "created_at": "2025-02-05T15:52:41Z"
        },
        {
          "author": "fredmorais",
          "body": "Same issue here!",
          "created_at": "2025-03-13T20:10:56Z"
        },
        {
          "author": "sarkaramitabh300",
          "body": "Same issue",
          "created_at": "2025-03-14T10:16:52Z"
        }
      ]
    },
    {
      "issue_number": 8510,
      "title": "Ollama Server error '502 Bad Gateway'",
      "body": "I try to invoke a local model deployed with Ollama, always get a 502 connection error.  However, invoke with langchain_community or curl command generated in debug model  is fine.\n- system: ubuntu \n- python:3.12.6\n- lightllm version: 1.61.1\n\n```shell \nresponse = completion(\n...     model=\"ollama/deepseek-r1:1.5b\", \n...     messages=[{ \"content\": \"respond in 20 words. who are you?\",\"role\": \"user\"}], \n...     api_base=\"http://localhost:11434\"\n... )\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nTraceback (most recent call last):\n  File \"/home/will/anaconda3/envs/smol/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 110, in _make_common_sync_call\n    response = sync_httpx_client.post(\n               ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/will/anaconda3/envs/smol/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 557, in post\n    raise e\n  File \"/home/will/anaconda3/envs/smol/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 538, in post\n    response.raise_for_status()\n  File \"/home/will/anaconda3/envs/smol/lib/python3.12/site-packages/httpx/_models.py\", line 829, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Server error '502 Bad Gateway' for url 'http://localhost:11434/api/generate'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/502\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/will/anaconda3/envs/smol/lib/python3.12/site-packages/litellm/main.py\", line 2808, in completion\n    response = base_llm_http_handler.completion(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/will/anaconda3/envs/smol/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 360, in completion\n    response = self._make_common_sync_call(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/will/anaconda3/envs/smol/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 131, in _make_common_sync_call\n    raise self._handle_error(e=e, provider_config=provider_config)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/will/anaconda3/envs/smol/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 959, in _handle_error\n    raise provider_config.get_error_class(\nlitellm.llms.ollama.common_utils.OllamaError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/home/will/anaconda3/envs/smol/lib/python3.12/site-packages/litellm/utils.py\", line 1190, in wrapper\n    raise e\n  File \"/home/will/anaconda3/envs/smol/lib/python3.12/site-packages/litellm/utils.py\", line 1068, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/will/anaconda3/envs/smol/lib/python3.12/site-packages/litellm/main.py\", line 3085, in completion\n    raise exception_type(\n          ^^^^^^^^^^^^^^^\n  File \"/home/will/anaconda3/envs/smol/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2201, in exception_type\n    raise e\n  File \"/home/will/anaconda3/envs/smol/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2170, in exception_type\n    raise APIConnectionError(\nlitellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - \n```",
      "state": "closed",
      "author": "xvshiting",
      "author_type": "User",
      "created_at": "2025-02-13T08:42:59Z",
      "updated_at": "2025-06-17T00:02:28Z",
      "closed_at": "2025-06-17T00:02:28Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8510/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8510",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8510",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:48.193035",
      "comments": [
        {
          "author": "reese-li",
          "body": "I have the same issue",
          "created_at": "2025-03-11T03:18:20Z"
        },
        {
          "author": "xvshiting",
          "body": "I found the reason is that I have a network proxy on my local machine! After turn it off and reboot the problem is solved. Network setting sometime is not work as our wish, it is so weird.",
          "created_at": "2025-03-11T03:22:09Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-10T00:02:00Z"
        }
      ]
    },
    {
      "issue_number": 8539,
      "title": "[Bug]: function_to_dict() fails enums",
      "body": "### What happened?\n\nWhile trying out https://docs.litellm.ai/docs/completion/function_call#using-function_to_dict, i.e. the original example, I noticed that the enum is not rendered as such:\n\n``` py\n    unit : str {'celsius', 'fahrenheit'}\n        Temperature unit\n```\n\nYields:\n\n``` json\n            \"unit\": {\n                \"type\": \"string\",\n                \"description\": \"Temperature unit\"\n            }\n```\n\nWhen I remove the `str`, it is rendered as enum, but weirdly as an array in a string (and OpenAI complains):\n\n``` py\n    unit : {'celsius', 'fahrenheit'}\n        Temperature unit\n```\n\nYields:\n\n``` json\n            \"unit\": {\n                \"type\": \"string\",\n                \"description\": \"Temperature unit\",\n                \"enum\": \"['celsius', 'fahrenheit']\"\n            }\n```\n\nNotice the `\"` around the `[]`, i.e. the array/list is constructed correctly, but instead of remaining an array/list, it became a string in the map/dict.\n\nI need to manually \"fix\" the enum like this to finally succeed:\n\n``` json\n            \"unit\": {\n                \"type\": \"string\",\n                \"description\": \"Temperature unit\",\n                \"enum\": [\"celsius\", \"fahrenheit\"]\n            }\n```\n\nIf I don't do this, OpenAI complains with `is not of type 'array'` and the only reason the example works is because it never really rendered the unit as enum.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.61.3\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "vlerenc",
      "author_type": "User",
      "created_at": "2025-02-14T15:44:06Z",
      "updated_at": "2025-06-17T00:02:27Z",
      "closed_at": null,
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8539/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8539",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8539",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:48.372256",
      "comments": [
        {
          "author": "nickoloss",
          "body": "Upvote",
          "created_at": "2025-02-19T22:24:50Z"
        },
        {
          "author": "Jonarod",
          "body": "@vlerenc @nickoloss given that I did not have time to write tests for my PR, I doubt it will ever be merged. But in case you're in a hurry, don't hesitate to fork [my repo](https://github.com/Jonarod/litellm/tree/function_to_dict) and use the `function_to_dict` branch temporarily on your project as ",
          "created_at": "2025-03-18T15:44:02Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-17T00:02:26Z"
        }
      ]
    },
    {
      "issue_number": 8704,
      "title": "[Bug]: Transitive dependency on tenacity not understood by bazel",
      "body": "### What happened?\n\nHi, we seem to be hitting an issue with our build system (bazel) when using litellm with retries. It looks like it fails to detect the transitive dependency on tenacity, which is needed by litellm.\n\nWe think tenacity should be listed in [poetry dependencies](https://github.com/BerriAI/litellm/blob/main/pyproject.toml#L21) given that it is used at runtime by the litellm package.\n\nIt seems to be already declared in (requirements.txt](https://github.com/BerriAI/litellm/blob/11e9fc7b549116c8267d9c7e3db93d9c084e2837/requirements.txt#L49), so it would probably make sense to be kept in sync?\n\nFor context, most likely this is leading our bazel build to not properly infer dependencies and thus us having to add manual exceptions to declare the dependency on tenacity so that our runtime environment has the proper dependencies installed.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.61.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "regb",
      "author_type": "User",
      "created_at": "2025-02-21T11:05:41Z",
      "updated_at": "2025-06-17T00:02:25Z",
      "closed_at": "2025-06-17T00:02:25Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8704/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8704",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8704",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:48.558393",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Hi @regb our Router uses backoff - https://github.com/BerriAI/litellm/blob/64bbc8508f6a50820e3e69d1efb0bf3966aa0257/pyproject.toml#L39\n\ncan we just migrate the retry in the completion() to use that instead? ",
          "created_at": "2025-03-11T15:16:35Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-10T00:01:57Z"
        }
      ]
    },
    {
      "issue_number": 8744,
      "title": "[Bug]: Embeddings request fails on /v1/embeddings",
      "body": "### What happened?\n\nA bug happened!\n\n```\ncurl --request POST \\\n  --url https://dev-shared-llmgateway.dev.billdot.io/v1/embeddings \\\n  --header 'Authorization: Bearer {{ api_key }}' \\\n  --header 'Content-Type: application/json' \\\n  --header 'User-Agent: insomnia/10.3.0' \\\n  --data '{\n\t\"input\": \"Your text string goes here\",\n\t\"model\": \"text-embedding-3small\"\n}'\n\n```\nThis returns\n```\n{\n\t\"error\": {\n\t\t\"message\": \"litellm.APIError: APIError: OpenAIException - coroutine already executing\\nReceived Model Group=text-embedding-3small\\nAvailable Model Group Fallbacks=None\\nError doing the fallback: list index out of range\",\n\t\t\"type\": null,\n\t\t\"param\": null,\n\t\t\"code\": \"500\"\n\t}\n}\n```\n\nThis is only true for open ai models. it works for bedrock models though.\n\nThe underlying openai key I am using works with the openai endpoint.\n\nMore config\n\n```yml\nlitellm_settings:\n  json_logs: true # if true, logs will be in json format\n  ssl_verify: false # üëà KEY CHANGE\n  success_callback: [\"s3\", \"datadog_llm_observability\"] # logs llm success logs on datadog\n  failure_callback: [\"s3\", \"datadog_llm_observability\"] # logs llm success logs on datadog\n  store_audit_logs: false\n\nmodel_list:\n  - model_name: bedrock/*\n    litellm_params:\n      model: bedrock/*\n  - model_name: gpt-4o-custom\n    litellm_params:\n      model: openai/gpt-4o\n  - model_name: gpt-4o-custom\n    litellm_params:\n      model: openai/gpt-4o-mini\n  - model_name: text-embedding-3large\n    litellm_params:\n      model: openai/text-embedding-3-large\n    model_info:\n      mode: embedding\n  - model_name: text-embedding-3small\n    litellm_params:\n      model: openai/text-embedding-3-small\n    model_info:\n      mode: embedding\n\ngeneral_settings:\n  ssl_verify: false # üëà KEY CHANGE\n  allow_requests_on_db_unavailable: True\n  proxy_batch_write_at: 60 # Batch write spend updates every 60s\n  store_prompts_in_spend_logs: False\n  database_connection_pool_limit: 20 # limit the number of database connections to = MAX Number of DB Connections/Number of instances of litellm proxy (Around 10-20 is good number)\n\nrouter_settings:\n  routing_strategy: simple-shuffle # Literal[\"simple-shuffle\", \"least-busy\", \"usage-based-routing\",\"latency-based-routing\"], default=\"simple-shuffle\"\n  model_group_alias: {\"gpt-4\": \"gpt-4o-custom\"} # all requests with `gpt-4` will be routed to models with `gpt-3.5-turbo`\n  num_retries: 2\n\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nmain-v1.61.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "deepanshululla",
      "author_type": "User",
      "created_at": "2025-02-23T12:03:36Z",
      "updated_at": "2025-06-17T00:02:24Z",
      "closed_at": "2025-06-17T00:02:24Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8744/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8744",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8744",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:48.795084",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "This is not a litellm bug, you need to update your dd-trace version \n\nSee related issue caused because of dd-trace https://github.com/DataDog/dd-trace-py/issues/11994",
          "created_at": "2025-02-23T16:27:29Z"
        },
        {
          "author": "deepanshululla",
          "body": "I am not installing any external dependencies apart from what is already in docker file. This is coming from within the docker file for main-v1.61.1. The bug fix might be to upgrade it within the docker file for that version.\n",
          "created_at": "2025-02-24T12:19:14Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "can you try without dd-trace ? just to confirm dd-trace is causing the issue ",
          "created_at": "2025-02-24T17:15:57Z"
        },
        {
          "author": "deepanshululla",
          "body": "tried it without dd trace. We got the same error without it.\n\n\n\nthis is our entrypoint script\n```\n#!/bin/bash\n\nset -eux\n\n#echo $(pwd)\n\n# Run the Python migration script\npython3 litellm/proxy/prisma_migration.py\n\n#printenv\nexport USE_DDTRACE=false\nif [ \"$USE_DDTRACE\" = \"true\" ]; then\n    exec ddtrace",
          "created_at": "2025-02-25T22:17:30Z"
        },
        {
          "author": "deepanshululla",
          "body": "```\n{\n  \"status\": \"failure\",\n  \"batch_models\": null,\n  \"user_api_key\": \"7e15a39e6425aefdc0a3ab74ce64f26f93c8fddf1162b3bfc021240e9b52d177\",\n  \"error_information\": {\n    \"traceback\": \"  File \\\"/usr/lib/python3.13/site-packages/starlette/_exception_handler.py\\\", line 42, in wrapped_app\\n    await app(s",
          "created_at": "2025-03-10T05:09:06Z"
        }
      ]
    },
    {
      "issue_number": 8771,
      "title": "[Bug]: Inconsistent VertexAI calls due to intermittent auth issues",
      "body": "### What happened?\n\nCalls to VertexAI custom deployed models get intermittent auth errors from VertexAI. LiteLLM's environment and model are correctly set up with VertexAI credentials, so the LiteLLM's completion calls are returning responses if we run few number of requests.\n\nBut when we submit more/frequent requests, LiteLLM's completion calls fails with below error intermittently. \n\n`openai.InternalServerError: Error code: 500 - {'error': {'message': 'litellm.InternalServerError: VertexAIException InternalServerError - 401 Request had invalid authentication credentials. Expected OAuth 2 access token, login cookie or other valid authentication credential. See https://developers.google.com/identity/sign-in/web/devconsole-project.. Received Model Group=pco-llama3-1-8b-ft-icd-l4-predict\\nAvailable Model Group Fallbacks=None', 'type': None, 'param': None, 'code': '500'}}`\n\n### Steps to reproduce\n1. Add a custom deployed model on VertexAI\n2. Configure VertexAI custom model on LiteLLM (with 'model' value as 'vertex_ai/<endpoint-id>')\n3. Fire multiple requests in sequence \n4. Intermittent auth issues will be seen along with successful completion calls.\n\n### Relevant log output\n\n```shell\n[2025-02-24 16:06:34,937] MKK2KJY5Y2DX/ERROR/locust.user.task: litellm.APIError: APIError: Litellm_proxyException - Error code: 500 - {'error': {'message': 'litellm.InternalServerError: VertexAIException InternalServerError - 401 Request had invalid authentication credentials. Expected OAuth 2 access token, login cookie or other valid authentication credential. See https://developers.google.com/identity/sign-in/web/devconsole-project.. Received Model Group=pco-llama3-1-8b-ft-icd-l4-predict\\nAvailable Model Group Fallbacks=None', 'type': None, 'param': None, 'code': '500'}}\nTraceback (most recent call last):\n  File \"/Users/ssiva3/Source/vscode/genai-3/genai-aigateway-litellm-load-test/.venv/lib/python3.12/site-packages/litellm/llms/OpenAI/openai.py\", line 854, in completion\n    raise e\n  File \"/Users/ssiva3/Source/vscode/genai-3/genai-aigateway-litellm-load-test/.venv/lib/python3.12/site-packages/litellm/llms/OpenAI/openai.py\", line 790, in completion\n    self.make_sync_openai_chat_completion_request(\n  File \"/Users/ssiva3/Source/vscode/genai-3/genai-aigateway-litellm-load-test/.venv/lib/python3.12/site-packages/litellm/llms/OpenAI/openai.py\", line 651, in make_sync_openai_chat_completion_request\n    raise e\n  File \"/Users/ssiva3/Source/vscode/genai-3/genai-aigateway-litellm-load-test/.venv/lib/python3.12/site-packages/litellm/llms/OpenAI/openai.py\", line 633, in make_sync_openai_chat_completion_request\n    raw_response = openai_client.chat.completions.with_raw_response.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ssiva3/Source/vscode/genai-3/genai-aigateway-litellm-load-test/.venv/lib/python3.12/site-packages/openai/_legacy_response.py\", line 356, in wrapped\n    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ssiva3/Source/vscode/genai-3/genai-aigateway-litellm-load-test/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 275, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ssiva3/Source/vscode/genai-3/genai-aigateway-litellm-load-test/.venv/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 829, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/Users/ssiva3/Source/vscode/genai-3/genai-aigateway-litellm-load-test/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1280, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ssiva3/Source/vscode/genai-3/genai-aigateway-litellm-load-test/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 957, in request\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File \"/Users/ssiva3/Source/vscode/genai-3/genai-aigateway-litellm-load-test/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1061, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.InternalServerError: Error code: 500 - {'error': {'message': 'litellm.InternalServerError: VertexAIException InternalServerError - 401 Request had invalid authentication credentials. Expected OAuth 2 access token, login cookie or other valid authentication credential. See https://developers.google.com/identity/sign-in/web/devconsole-project.. Received Model Group=pco-llama3-1-8b-ft-icd-l4-predict\\nAvailable Model Group Fallbacks=None', 'type': None, 'param': None, 'code': '500'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/ssiva3/Source/vscode/genai-3/genai-aigateway-litellm-load-test/.venv/lib/python3.12/site-packages/litellm/main.py\", line 1605, in completion\n    raise e\n  File \"/Users/ssiva3/Source/vscode/genai-3/genai-aigateway-litellm-load-test/.venv/lib/python3.12/site-packages/litellm/main.py\", line 1578, in completion\n    response = openai_chat_completions.completion(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ssiva3/Source/vscode/genai-3/genai-aigateway-litellm-load-test/.venv/lib/python3.12/site-packages/litellm/llms/OpenAI/openai.py\", line 864, in completion\n    raise OpenAIError(\nlitellm.llms.OpenAI.openai.OpenAIError: Error code: 500 - {'error': {'message': 'litellm.InternalServerError: VertexAIException InternalServerError - 401 Request had invalid authentication credentials. Expected OAuth 2 access token, login cookie or other valid authentication credential. See https://developers.google.com/identity/sign-in/web/devconsole-project.. Received Model Group=pco-llama3-1-8b-ft-icd-l4-predict\\nAvailable Model Group Fallbacks=None', 'type': None, 'param': None, 'code': '500'}}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/ssiva3/Source/vscode/genai-3/genai-aigateway-litellm-load-test/.venv/lib/python3.12/site-packages/locust/user/task.py\", line 340, in run\n    self.execute_next_task()\n  File \"/Users/ssiva3/Source/vscode/genai-3/genai-aigateway-litellm-load-test/.venv/lib/python3.12/site-packages/locust/user/task.py\", line 373, in execute_next_task\n    self.execute_task(self._task_queue.popleft())\n  File \"/Users/ssiva3/Source/vscode/genai-3/genai-aigateway-litellm-load-test/.venv/lib/python3.12/site-packages/locust/user/task.py\", line 385, in execute_task\n    task(self)\n  File \"/Users/ssiva3/Source/vscode/genai-3/genai-aigateway-litellm-load-test/litellm-load-test/locustfile.py\", line 54, in completion_call\n    response = completion(\n             ^^^^^^^^^^^\n  File \"/Users/ssiva3/Source/vscode/genai-3/genai-aigateway-litellm-load-test/.venv/lib/python3.12/site-packages/litellm/utils.py\", line 960, in wrapper\n    raise e\n  File \"/Users/ssiva3/Source/vscode/genai-3/genai-aigateway-litellm-load-test/.venv/lib/python3.12/site-packages/litellm/utils.py\", line 849, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ssiva3/Source/vscode/genai-3/genai-aigateway-litellm-load-test/.venv/lib/python3.12/site-packages/litellm/main.py\", line 3059, in completion\n    raise exception_type(\n          ^^^^^^^^^^^^^^^\n  File \"/Users/ssiva3/Source/vscode/genai-3/genai-aigateway-litellm-load-test/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2136, in exception_type\n    raise e\n  File \"/Users/ssiva3/Source/vscode/genai-3/genai-aigateway-litellm-load-test/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 404, in exception_type\n    raise APIError(\nlitellm.exceptions.APIError: litellm.APIError: APIError: Litellm_proxyException - Error code: 500 - {'error': {'message': 'litellm.InternalServerError: VertexAIException InternalServerError - 401 Request had invalid authentication credentials. Expected OAuth 2 access token, login cookie or other valid authentication credential. See https://developers.google.com/identity/sign-in/web/devconsole-project.. Received Model Group=pco-llama3-1-8b-ft-icd-l4-predict\\nAvailable Model Group Fallbacks=None', 'type': None, 'param': None, 'code': '500'}}\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nmain-v1.61.15-nightly\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "suresiva",
      "author_type": "User",
      "created_at": "2025-02-24T22:07:52Z",
      "updated_at": "2025-06-17T00:02:23Z",
      "closed_at": "2025-06-17T00:02:23Z",
      "labels": [
        "bug",
        "mlops user request",
        "stale"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8771/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "krrishdholakia"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8771",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8771",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:49.026502",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "is this still active @krrishdholakia @suresiva ? ",
          "created_at": "2025-03-10T15:36:10Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-09T00:02:04Z"
        }
      ]
    },
    {
      "issue_number": 8849,
      "title": "[Bug]: BadRequestError when passing a GCS URI to Gemini models",
      "body": "### What happened?\n\nThe example code from the docs for accessing GCS bucket files is not working. Obviously I put a real gs:// path in my code but it returns this error.\n\nHas anyone gotten this to work?\n\n```\n litellm.BadRequestError: VertexAIException BadRequestError - {\n  \"error\": {\n  \"code\": 400,\n  \"message\": \"Request contains an invalid argument.\",\n   \"status\": \"INVALID_ARGUMENT\"\n  }\n}\n```\n\n```\nimport litellm\nimport os\n\nos.environ[\"GEMINI_API_KEY\"] = \"\" \n\nlitellm.set_verbose = True # üëà See Raw call \n\nmodel = \"gemini/gemini-1.5-flash\"\nresponse = litellm.completion(\n    model=model,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"Please summarize the file.\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": \"gs://...\" # üëà SET THE cloud storage bucket url\n                },\n            ],\n        }\n    ],\n)\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.61.16\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "pjsample",
      "author_type": "User",
      "created_at": "2025-02-26T18:09:35Z",
      "updated_at": "2025-06-17T00:02:22Z",
      "closed_at": "2025-06-17T00:02:22Z",
      "labels": [
        "bug",
        "stale",
        "march 2025"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8849/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8849",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8849",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:49.275437",
      "comments": [
        {
          "author": "pjsample",
          "body": "Below is the LiteLLM debug output. I thought that it might be a permissions issue but it does not work even when I make the bucket public.\n\nPOST Request Sent from LiteLLM:\n```\ncurl -X POST \\\nhttps://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=[my_key] \\\n-H 'C",
          "created_at": "2025-02-26T18:50:59Z"
        },
        {
          "author": "Eunchan24",
          "body": "I have the same problem",
          "created_at": "2025-03-03T13:39:41Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "added to feb 2025, does it work on vertex ai @pjsample ? ",
          "created_at": "2025-03-10T15:40:20Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-09T00:02:01Z"
        }
      ]
    },
    {
      "issue_number": 8867,
      "title": "[Bug]: Floating point rounding errors in x-litellm-response-cost header",
      "body": "### What happened?\n\nThe x-litellm-response-cost header in the response from LiteLLM sometimes contains floating point rounding errors. This isn't ideal since it's containing information around US Dollar financial information. Ideally, this should by a type that's safe from these kind of rounding errors, like `decimal`.\n\nSee screenshot for example of it returning 0.0016150000000000001 as the value instead of 0.001615:\n![Image](https://github.com/user-attachments/assets/b4c3e01d-a7ab-4eb9-a01a-333c8611fd50)\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.60.8\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "portswigger-david",
      "author_type": "User",
      "created_at": "2025-02-27T10:58:13Z",
      "updated_at": "2025-06-17T00:02:21Z",
      "closed_at": "2025-06-17T00:02:21Z",
      "labels": [
        "bug",
        "stale",
        "march 2025"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8867/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8867",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8867",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:49.497076",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "should we just round to 6DP in the response @portswigger-david ? ",
          "created_at": "2025-03-10T15:19:43Z"
        },
        {
          "author": "colesmcintosh",
          "body": "@ishaan-jaff opened  https://github.com/BerriAI/litellm/pull/9118 to solve this",
          "created_at": "2025-03-11T02:15:55Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-10T00:01:53Z"
        }
      ]
    },
    {
      "issue_number": 9011,
      "title": "[Bug]: Unexpected \"values\" wrapper in JSON response with Pydantic models",
      "body": "### What happened?\n\nWhen using LiteLLM with Pydantic models for response formatting, an unexpected \"values\" wrapper is added to the JSON response that isn't part of the defined schema.\n\nThe \"values\" key is not defined in any of the Pydantic models or mentioned in the documentation. This appears to be an unexpected wrapper being added during response processing. Seems to be happening only when `stream=True`\n\n### Reproduction Script\n\n```python\nimport os\n\nfrom litellm import Router\nfrom pydantic import BaseModel\n\n\nclass Recipe(BaseModel):\n    name: str\n    cook_time: str\n\n\nclass RecipeResponse(BaseModel):\n    data: Recipe\n\n\nos.environ[\"ANTHROPIC_API_KEY\"] = \"your-key-here\"  # Replace with your key\n\nsystem_prompt = \"You are a helpful cooking assistant. Respond with recipe details in the specified format.\"\nuser_prompt = \"Give me a quick recipe.\"\n\nsystem_prompt = \"You are a helpful cooking assistant. Respond with recipe details in the specified format.\"\nuser_prompt = \"Give me a quick recipe.\"\n\nrouter = Router(\n    model_list=[\n        {\n            \"model_name\": \"claude-sonnet\",\n            \"litellm_params\": {\n                \"model\": \"anthropic/claude-3-5-sonnet-latest\",\n                \"api_key\": os.environ[\"ANTHROPIC_API_KEY\"],\n            },\n        }\n    ],\n)\n\nresponse = router.completion(\n    model=\"claude-sonnet\",\n    messages=[\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": user_prompt},\n    ],\n    response_format=RecipeResponse,\n    stream=True,\n)\n\nfinal_response = \"\"\nfor chunk in response:\n    text = chunk.choices[0].delta.content\n    if text:\n        final_response += text\n\nprint(\"\\nRaw Response:\")\nprint(final_response)\n```\n\n### Expected Behavior\n\nThe response should match the Pydantic model structure exactly:\n\n```json\n{\n    \"data\": {\n        \"name\": \"Quick Pasta\",\n        \"cook_time\": \"15 minutes\"\n    }\n}\n```\n\n### Actual Behavior\n\nThe response includes an unexpected \"values\" wrapper:\n\n```json\n{\n    \"values\": {\n        \"data\": {\n            \"name\": \"Quick Pasta\",\n            \"cook_time\": \"15 minutes\"\n        }\n    }\n}\n```\n\n### Environment\n\n- LiteLLM Version: 1.62.1\n- Python Version: 3.11.4\n\n### Relevant log output\n\n```shell\nRaw Response:\n{\"values\": {\"data\":{\"name\":\"3-Minute Microwave Scrambled Eggs\",\"cook_time\":\"3 minutes\"}}}\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.62.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "preeteshjain",
      "author_type": "User",
      "created_at": "2025-03-05T16:49:24Z",
      "updated_at": "2025-06-17T00:02:19Z",
      "closed_at": null,
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9011/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9011",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9011",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:49.673725",
      "comments": [
        {
          "author": "Dmarcotrigiano",
          "body": "@preeteshjain Can you check what request is being sent to the model? I'm having the same issue in #9351 and the JSON schema is missing the properties.",
          "created_at": "2025-03-18T21:44:24Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-17T00:02:18Z"
        }
      ]
    },
    {
      "issue_number": 9036,
      "title": "[Feature]: litellm.enable_json_schema_validation be converted to a per-call parameter instead of module level setting.",
      "body": "### The Feature\n\nMove enable_json_schema_validation from litellm.enable_json_schema_validation to a parameter provided to the completion( ) api.\n\n### Motivation, pitch\n\nCurrently, we must set litellm.enable_json_schema_validation = True to enable json schema validation.\nThis is very useful, because if schema validation fails, then LiteLLM will retry or fallback to other models.\n\nWe exploit this property by first trying with Claude, and falling back to GPT Structured Outputs if Claude fails to produce a schema matching the spec.\n\nHowever, there are times when we want to disable this validation (when we have our own error correcting mechanism, or custom parser.) -- Unfortunately, since this isn't a parameter to the completion( ) API, it means we either have it for all requests, or None.\n\n### Are you a ML Ops Team?\n\nYes\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "achpalaman",
      "author_type": "User",
      "created_at": "2025-03-06T22:24:32Z",
      "updated_at": "2025-06-17T00:02:17Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "mlops user request",
        "stale"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9036/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9036",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9036",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:49.873917",
      "comments": [
        {
          "author": "Dmarcotrigiano",
          "body": "Agreed, as a work around you can implement JSON validation similar to their code example:\n\n```python\n    validate_body = False\n    if getattr(body, \"json_output\", False) and getattr(body, \"json_schema\", None):\n        validate_body = True\n        additional_params[\"response_format\"] = {\n            ",
          "created_at": "2025-03-18T16:30:00Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-17T00:02:16Z"
        }
      ]
    },
    {
      "issue_number": 9098,
      "title": "[Feature]: reset budget according to calender month",
      "body": "### The Feature\n\nCurrent reset function allows reset depending on number of days. Litellm should also add functionality to reset budget as per calendar month.  For e.g. If on 10th March a budget is assigned with \"monthly\" reset, then budget would reset on last day of March. \n\n\n### Motivation, pitch\n\nIts easier for end users to have reset budget to be reset according to calendar month. This is also easier for budgeting purpose since cut-off for billing is usually a quarter or a month. \n\nBy the way, UI does have drop down option \"Monthly\" - however its essentially is 30d reset. Which is confusing for users. \n\n### Are you a ML Ops Team?\n\nYes\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "mohittalele",
      "author_type": "User",
      "created_at": "2025-03-10T10:21:54Z",
      "updated_at": "2025-06-17T00:02:16Z",
      "closed_at": "2025-06-17T00:02:16Z",
      "labels": [
        "enhancement",
        "mlops user request",
        "stale",
        "march 2025"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9098/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9098",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9098",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:50.045351",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "good request, added to roadmap ",
          "created_at": "2025-03-11T04:14:46Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-10T00:01:46Z"
        }
      ]
    },
    {
      "issue_number": 9121,
      "title": "litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException -",
      "body": "Server error '502 Bad Gateway' for url 'http://localhost:11434/api/generate'",
      "state": "closed",
      "author": "reese-li",
      "author_type": "User",
      "created_at": "2025-03-11T03:11:26Z",
      "updated_at": "2025-06-17T00:02:14Z",
      "closed_at": "2025-06-17T00:02:14Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9121/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9121",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9121",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:50.236439",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-10T00:01:44Z"
        }
      ]
    },
    {
      "issue_number": 9125,
      "title": "embedding model connection error",
      "body": "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\nwren-ai-service-1  | \nwren-ai-service-1  | 04:15:36 - LiteLLM:DEBUG: exception_mapping_utils.py:2239 - Logging Details: logger_fn - None | callable(logger_fn) - False\nwren-ai-service-1  | 04:15:36 - LiteLLM:DEBUG: litellm_logging.py:1882 - Logging Details LiteLLM-Failure Call: []\nwren-ai-service-1  | \nCalculating embeddings:   0%|          | 0/1 [00:01<?, ?it/s]\nwren-ai-service-1  | \nwren-ai-service-1  | ********************************************************************************\nwren-ai-service-1  | > embedding [src.pipelines.indexing.db_schema.embedding()] encountered an error<\nwren-ai-service-1  | > Node inputs:\nwren-ai-service-1  | {'chunk': \"<Task finished name='Task-10' coro=<AsyncGraphAdap...\",\nwren-ai-service-1  |  'embedder': '<src.providers.embedder.litellm.AsyncDocumentEmbed...'}\nwren-ai-service-1  | ********************************************************************************\nwren-ai-service-1  | Traceback (most recent call last):\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 72, in map_httpcore_exceptions\nwren-ai-service-1  |     yield\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 377, in handle_async_request\nwren-ai-service-1  |     resp = await self._pool.handle_async_request(req)\nwren-ai-service-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\nwren-ai-service-1  |     raise exc from None\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\nwren-ai-service-1  |     response = await connection.handle_async_request(\nwren-ai-service-1  |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 101, in handle_async_request\nwren-ai-service-1  |     raise exc\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 78, in handle_async_request\nwren-ai-service-1  |     stream = await self._connect(request)\nwren-ai-service-1  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 124, in _connect\nwren-ai-service-1  |     stream = await self._network_backend.connect_tcp(**kwargs)\nwren-ai-service-1  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/httpcore/_backends/auto.py\", line 31, in connect_tcp\nwren-ai-service-1  |     return await self._backend.connect_tcp(\nwren-ai-service-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py\", line 113, in connect_tcp\nwren-ai-service-1  |     with map_exceptions(exc_map):\nwren-ai-service-1  |   File \"/usr/local/lib/python3.12/contextlib.py\", line 155, in __exit__\nwren-ai-service-1  |     self.gen.throw(value)\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\nwren-ai-service-1  |     raise to_exc(exc) from exc\nwren-ai-service-1  | httpcore.ConnectError: All connection attempts failed\nwren-ai-service-1  | \nwren-ai-service-1  | The above exception was the direct cause of the following exception:\nwren-ai-service-1  | \nwren-ai-service-1  | Traceback (most recent call last):\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1500, in _request\nwren-ai-service-1  |     response = await self._client.send(\nwren-ai-service-1  |                ^^^^^^^^^^^^^^^^^^^^^^^^\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1674, in send\nwren-ai-service-1  |     response = await self._send_handling_auth(\nwren-ai-service-1  |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1702, in _send_handling_auth\nwren-ai-service-1  |     response = await self._send_handling_redirects(\nwren-ai-service-1  |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1739, in _send_handling_redirects\nwren-ai-service-1  |     response = await self._send_single_request(request)\nwren-ai-service-1  |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1776, in _send_single_request\nwren-ai-service-1  |     response = await transport.handle_async_request(request)\nwren-ai-service-1  |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 376, in handle_async_request\nwren-ai-service-1  |     with map_httpcore_exceptions():\nwren-ai-service-1  |   File \"/usr/local/lib/python3.12/contextlib.py\", line 155, in __exit__\nwren-ai-service-1  |     self.gen.throw(value)\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 89, in map_httpcore_exceptions\nwren-ai-service-1  |     raise mapped_exc(message) from exc\nwren-ai-service-1  | httpx.ConnectError: All connection attempts failed\nwren-ai-service-1  | \nwren-ai-service-1  | The above exception was the direct cause of the following exception:\nwren-ai-service-1  | \nwren-ai-service-1  | Traceback (most recent call last):\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 1085, in aembedding\nwren-ai-service-1  |     headers, response = await self.make_openai_embedding_request(\nwren-ai-service-1  |                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py\", line 131, in async_wrapper\nwren-ai-service-1  |     result = await func(*args, **kwargs)\nwren-ai-service-1  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 1038, in make_openai_embedding_request\nwren-ai-service-1  |     raise e\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 1031, in make_openai_embedding_request\nwren-ai-service-1  |     raw_response = await openai_aclient.embeddings.with_raw_response.create(\nwren-ai-service-1  |                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/openai/_legacy_response.py\", line 381, in wrapped\nwren-ai-service-1  |     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\nwren-ai-service-1  |                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/openai/resources/embeddings.py\", line 243, in create\nwren-ai-service-1  |     return await self._post(\nwren-ai-service-1  |            ^^^^^^^^^^^^^^^^^\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\nwren-ai-service-1  |     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\nwren-ai-service-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\nwren-ai-service-1  |     return await self._request(\nwren-ai-service-1  |            ^^^^^^^^^^^^^^^^^^^^\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1524, in _request\nwren-ai-service-1  |     return await self._retry_request(\nwren-ai-service-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\nwren-ai-service-1  |     return await self._request(\nwren-ai-service-1  |            ^^^^^^^^^^^^^^^^^^^^\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1524, in _request\nwren-ai-service-1  |     return await self._retry_request(\nwren-ai-service-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\nwren-ai-service-1  |     return await self._request(\nwren-ai-service-1  |            ^^^^^^^^^^^^^^^^^^^^\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1534, in _request\nwren-ai-service-1  |     raise APIConnectionError(request=request) from err\nwren-ai-service-1  | openai.APIConnectionError: Connection error.\nwren-ai-service-1  | \nwren-ai-service-1  | During handling of the above exception, another exception occurred:\nwren-ai-service-1  | \nwren-ai-service-1  | Traceback (most recent call last):\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/litellm/main.py\", line 3203, in aembedding\nwren-ai-service-1  |     response = await init_response  # type: ignore\nwren-ai-service-1  |                ^^^^^^^^^^^^^^^^^^^\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 1130, in aembedding\nwren-ai-service-1  |     raise OpenAIError(\nwren-ai-service-1  | litellm.llms.openai.common_utils.OpenAIError: Connection error.\nwren-ai-service-1  | \nwren-ai-service-1  | During handling of the above exception, another exception occurred:\nwren-ai-service-1  | \nwren-ai-service-1  | Traceback (most recent call last):\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/hamilton/async_driver.py\", line 122, in new_fn\nwren-ai-service-1  |     await fn(**fn_kwargs) if asyncio.iscoroutinefunction(fn) else fn(**fn_kwargs)\nwren-ai-service-1  |     ^^^^^^^^^^^^^^^^^^^^^\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/langfuse/decorators/langfuse_decorator.py\", line 219, in async_wrapper\nwren-ai-service-1  |     self._handle_exception(observation, e)\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/langfuse/decorators/langfuse_decorator.py\", line 517, in _handle_exception\nwren-ai-service-1  |     raise e\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/langfuse/decorators/langfuse_decorator.py\", line 217, in async_wrapper\nwren-ai-service-1  |     result = await func(*args, **kwargs)\nwren-ai-service-1  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nwren-ai-service-1  |   File \"/src/pipelines/indexing/db_schema.py\", line 312, in embedding\nwren-ai-service-1  |     return await embedder.run(documents=chunk[\"documents\"])\nwren-ai-service-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/backoff/_async.py\", line 151, in retry\nwren-ai-service-1  |     ret = await target(*args, **kwargs)\nwren-ai-service-1  |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nwren-ai-service-1  |   File \"/src/providers/embedder/litellm.py\", line 143, in run\nwren-ai-service-1  |     embeddings, meta = await self._embed_batch(\nwren-ai-service-1  |                        ^^^^^^^^^^^^^^^^^^^^^^^^\nwren-ai-service-1  |   File \"/src/providers/embedder/litellm.py\", line 105, in _embed_batch\nwren-ai-service-1  |     response = await aembedding(\nwren-ai-service-1  |                ^^^^^^^^^^^^^^^^^\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/litellm/utils.py\", line 1416, in wrapper_async\nwren-ai-service-1  |     raise e\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/litellm/utils.py\", line 1275, in wrapper_async\nwren-ai-service-1  |     result = await original_function(*args, **kwargs)\nwren-ai-service-1  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/litellm/main.py\", line 3219, in aembedding\nwren-ai-service-1  |     raise exception_type(\nwren-ai-service-1  |           ^^^^^^^^^^^^^^^\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2210, in exception_type\nwren-ai-service-1  |     raise e\nwren-ai-service-1  |   File \"/app/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 453, in exception_type\nwren-ai-service-1  |     raise APIError(\nwren-ai-service-1  | litellm.exceptions.APIError: litellm.APIError: APIError: OpenAIException - Connection error.\nwren-ai-service-1  | -------------------------------------------------------------------\nwren-ai-service-1  | Oh no an error! Need help with Hamilton?\nwren-ai-service-1  | Join our slack and ask for help! https://join.slack.com/t/hamilton-opensource/shared_invite/zt-2niepkra8-DGKGf_tTYhXuJWBTXtIs4g\nwren-ai-service-1  | -------------------------------------------------------------------\nwren-ai-service-1  | \nwren-ai-service-1  | INFO:     172.18.0.3:51494 - \"GET /v1/semantics-preparations/6445322602d2ac0472c6688c542e609abe48c287/status HTTP/1.1\" 200 OK\nwren-ui-1          | [2025-03-11T04:15:37.672] [DEBUG] WrenAIAdaptor - Got error in API /v1/semantics-preparations/6445322602d2ac0472c6688c542e609abe48c287/status: [object Object]\nwren-ui-1          | [2025-03-11T04:15:37.672] [DEBUG] WrenAIAdaptor - Got error when deploying to wren AI, hash: 6445322602d2ac0472c6688c542e609abe48c287. Error: [object Object]\nwren-ui-1          | [2025-03-11T04:15:37.686] [INFO] WrenAIAdaptor - Wren AI: Generating recommendation questions\nwren-ai-service-1  | INFO:     172.18.0.3:51502 - \"POST /v1/question-recommendations HTTP/1.1\" 200 OK\nwren-ai-service-1  | I0311 04:15:37.697 8 wren-ai-service:151] Request 42693c7a-0b38-40ec-b9ac-c4f47fc34aa0: Generate Question Recommendation pipeline is running...\nwren-ai-service-1  | I0311 04:15:37.700 8 wren-ai-service:263] Question Recommendation pipeline is running...\nwren-ui-1          | [2025-03-11T04:15:37.701] [INFO] WrenAIAdaptor - Wren AI: Generating recommendation questions, queryId: 42693c7a-0b38-40ec-b9ac-c4f47fc34aa0\ni got this error and my embedding model is nomic-embed-text with ollama and it gives this error \nhow to solve it ",
      "state": "closed",
      "author": "yash199999",
      "author_type": "User",
      "created_at": "2025-03-11T04:25:38Z",
      "updated_at": "2025-06-17T00:02:13Z",
      "closed_at": "2025-06-17T00:02:13Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9125/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9125",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9125",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:50.444963",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-10T00:01:43Z"
        }
      ]
    },
    {
      "issue_number": 9132,
      "title": "[Bug]: Remove token validation in LiteLLM client",
      "body": "### What happened?\n\nWhen sending a token that doesn't start with sk- and is a \"valid\" LiteLLM token, the error message: `Authentication Error, Invalid proxy server token passed. valid_token=None.` shows up.\n\nThere is already an issue similar to this one: https://github.com/BerriAI/litellm/issues/7287\n\nThe reason why I would like to have this removed, is because there could be custom token handling on the backend API server and the LiteLLM proxy client should therefore not be handling API token parsing / validation, but rather just pass the token on.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.63.6\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "jpaodev",
      "author_type": "User",
      "created_at": "2025-03-11T07:28:38Z",
      "updated_at": "2025-06-17T00:02:12Z",
      "closed_at": "2025-06-17T00:02:12Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9132/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9132",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9132",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:50.617667",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-10T00:01:41Z"
        }
      ]
    },
    {
      "issue_number": 9137,
      "title": "Cannot connect to huggingface embedding",
      "body": "There is no connection for huggingface embedding since it always shows me \n\nAPIConnectionError: litellm.APIConnectionError: Expecting value: line 1 column 1 (char 0)\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 3542, in embedding\n    response = huggingface.embedding(\n               ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/huggingface_restapi.py\", line 1142, in embedding\n    data = self._transform_input(\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/huggingface_restapi.py\", line 949, in _transform_input\n    hf_task = get_hf_task_embedding_for_model(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/huggingface_restapi.py\", line 284, in get_hf_task_embedding_for_model\n    model_info_dict = model_info.json()\n                      ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 832, in json\n    return jsonlib.loads(self.content, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/json/__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/json/decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/json/decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)",
      "state": "closed",
      "author": "Mr-Array22",
      "author_type": "User",
      "created_at": "2025-03-11T13:58:53Z",
      "updated_at": "2025-06-17T00:02:11Z",
      "closed_at": "2025-06-17T00:02:11Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9137/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9137",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9137",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:50.795926",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-10T00:01:40Z"
        }
      ]
    },
    {
      "issue_number": 9141,
      "title": "[Bug]: System message is not being logged in Helicone when using Bedrock",
      "body": "### What happened?\n\nNone of my `\\converse` requests' system messages are being logged into Helicone when I'm using Bedrock provider. \nI've been able to trace this to the following line: https://github.com/BerriAI/litellm/blob/d73fcc205244e825b3baa0093e7a33e800026aaa/litellm/litellm_core_utils/litellm_logging.py#L1290\nI'm quite sure that this is due to Bedrock using a separate parameter to pass the system message.\nChanging `self.model_call_details[\"input\"]` to `self.model_call_details[\"messages\"]` fixes this for me but I'm not sure if there are any potential side effects. \n\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\n1.62.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "akamenov",
      "author_type": "User",
      "created_at": "2025-03-11T15:52:43Z",
      "updated_at": "2025-06-17T00:02:10Z",
      "closed_at": "2025-06-17T00:02:10Z",
      "labels": [
        "bug",
        "mlops user request",
        "stale"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9141/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9141",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9141",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:51.035200",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-10T00:01:39Z"
        }
      ]
    },
    {
      "issue_number": 9142,
      "title": "Error Running the Program",
      "body": "\n\n# Agent: Email Categorizer and Prioritizer\n## Using tool: Read a file's content\n## Tool Input:\n\"{\\\"file_path\\\": \\\"output/fetched_emails.json\\\"}\"\n## Tool Output:\n[]\n\n\n# Agent: Email Categorizer and Prioritizer\n## Final Answer:\n```json\n[\n  {\n    \"email_id\": \"1\",\n    \"subject\": \"Update on your Google Pixel Tablet order\",\n    \"sender\": \"Google Store <store-news@google.com>\",\n    \"category\": \"OTHER\",\n    \"priority\": \"LOW\",\n    \"required_action\": \"READ_ONLY\",\n    \"date\": \"2024-07-03\",\n    \"age_days\": 1\n  },\n  {\n    \"email_id\": \"2\",\n    \"subject\": \"??? ???? ?????? ?? ???\",\n    \"sender\": \"YouTube <noreply@youtube.com>\",\n    \"category\": \"YOUTUBE\",\n    \"priority\": \"LOW\",\n    \"required_action\": \"READ_ONLY\",\n    \"date\": \"2024-07-03\",\n    \"age_days\": 1\n  },\n  {\n    \"email_id\": \"3\",\n    \"subject\": \"shutterfly: Enjoy $10 off\",\n    \"sender\": \"Shutterfly <shutterfly@emails.shutterfly.com>\",\n    \"category\": \"PROMOTIONS\",\n    \"priority\": \"LOW\",\n    \"required_action\": \"IGNORE\",\n    \"date\": \"2024-07-03\",\n    \"age_days\": 1\n  },\n  {\n    \"email_id\": \"4\",\n    \"subject\": \"Your report for Test summarization - 2024-07-02\",\n    \"sender\": \"GitHub <noreply@github.com>\",\n    \"category\": \"GITHUB\",\n    \"priority\": \"LOW\",\n    \"required_action\": \"READ_ONLY\",\n    \"date\": \"2024-07-03\",\n    \"age_days\": 1\n  },\n  {\n    \"email_id\": \"5\",\n    \"subject\": \"Comment on your video: Testing again\",\n    \"sender\": \"YouTube <noreply@youtube.com>\",\n    \"category\": \"YOUTUBE\",\n    \"priority\": \"HIGH\",\n    \"required_action\": \"READ_ONLY\",\n    \"date\": \"2024-07-03\",\n    \"age_days\": 1\n  }\n]\n```\n\n\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\n\nProvider List: https://docs.litellm.ai/docs/providers\n\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\n\nProvider List: https://docs.litellm.ai/docs/providers\n\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\n\nProvider List: https://docs.litellm.ai/docs/providers\n\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\n\nProvider List: https://docs.litellm.ai/docs/providers\n\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\n\nProvider List: https://docs.litellm.ai/docs/providers\n\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\n\nProvider List: https://docs.litellm.ai/docs/providers\n\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\n\nProvider List: https://docs.litellm.ai/docs/providers\n\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\n\nProvider List: https://docs.litellm.ai/docs/providers\n\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\n\nProvider List: https://docs.litellm.ai/docs/providers\n\n\nError: Failed to convert text into a Pydantic model due to the following error: litellm.BadRequestError: VertexAIException BadRequestError - {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[0].value': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[1].value': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[2].value': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[3].value': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[4].value': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[5].value': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[6].value': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[7].value': Cannot find field.\",\n    \"status\": \"INVALID_ARGUMENT\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.BadRequest\",\n        \"fieldViolations\": [\n          {\n            \"field\": \"tools[0].function_declarations[0].parameters.properties[0].value\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[0].value': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools[0].function_declarations[0].parameters.properties[1].value\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[1].value': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools[0].function_declarations[0].parameters.properties[2].value\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[2].value': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools[0].function_declarations[0].parameters.properties[3].value\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[3].value': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools[0].function_declarations[0].parameters.properties[4].value\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[4].value': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools[0].function_declarations[0].parameters.properties[5].value\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[5].value': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools[0].function_declarations[0].parameters.properties[6].value\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[6].value': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools[0].function_declarations[0].parameters.properties[7].value\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[7].value': Cannot find field.\"\n          }\n        ]\n      }\n    ]\n  }\n}",
      "state": "closed",
      "author": "meshahan",
      "author_type": "User",
      "created_at": "2025-03-11T17:21:55Z",
      "updated_at": "2025-06-17T00:02:09Z",
      "closed_at": "2025-06-17T00:02:09Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9142/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9142",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9142",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:51.270609",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-10T00:01:37Z"
        }
      ]
    },
    {
      "issue_number": 9189,
      "title": "[Bug]: Retrieving a batch fails with error about incorrect provider",
      "body": "### What happened?\n\nI upload a file to use for the batch API:\n\n```shell\n$ cat file-68dbd7ff214c4f53b0059b6ff59838ed.jsonl\n{\"custom_id\": \"task-0\", \"method\": \"POST\", \"url\": \"/chat/completions\", \"body\": {\"model\": \"gpt-4o-2024-11-20-batch-no-filter\", \"messages\": [{\"role\": \"user\", \"content\": \"what llm are you\"}]}}\n\n$ jq < file-68dbd7ff214c4f53b0059b6ff59838ed.jsonl\n{\n  \"custom_id\": \"task-0\",\n  \"method\": \"POST\",\n  \"url\": \"/chat/completions\",\n  \"body\": {\n    \"model\": \"gpt-4o-2024-11-20-batch-no-filter\",\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": \"what llm are you\"\n      }\n    ]\n  }\n}\n\n$ curl -i -sSL 'http://localhost:4000/v1/files' \\\n    -H \"Authorization: Bearer ${LITELLM_MASTER_KEY}\" \\\n    -F purpose=\"batch\" \\\n    -F file=\"@file-68dbd7ff214c4f53b0059b6ff59838ed.jsonl\"\nHTTP/1.1 200 OK\ndate: Thu, 13 Mar 2025 00:16:33 GMT\nserver: uvicorn\ncontent-length: 210\ncontent-type: application/json\nx-litellm-version: 1.63.7\nx-litellm-key-spend: 0.0\n\n{\"id\":\"file-f2676ac5b0554102b8a3a60ab1c47218\",\"bytes\":188,\"created_at\":1741824996,\"filename\":\"modified_file.jsonl\",\"object\":\"file\",\"purpose\":\"batch\",\"status\":\"processed\",\"expires_at\":null,\"status_details\":null}\n```\n\nand then I create an Azure OpenAI batch:\n\n```shell\n$ curl -i -sSL 'http://localhost:4000/v1/batches' \\\n    -H \"Authorization: Bearer ${LITELLM_MASTER_KEY}\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"input_file_id\": \"file-f2676ac5b0554102b8a3a60ab1c47218\",\n        \"endpoint\": \"/v1/chat/completions\",\n        \"completion_window\": \"24h\",\n        \"model\": \"gpt-4o-2024-11-20-batch-no-filter\"\n    }'\nHTTP/1.1 200 OK\ndate: Thu, 13 Mar 2025 00:53:09 GMT\nserver: uvicorn\ncontent-length: 699\ncontent-type: application/json\nx-litellm-model-id: edfc4f8b120c218e12b7e50f1cba1506ec39c3b8b4db1c0b0c1bab02bb31fc1f\nx-litellm-model-api-base: https://us-w-0008.openai.azure.com\nx-litellm-version: 1.63.7\nx-litellm-key-spend: 0.0\n\n{\"id\":\"batch_990fe845-4ff9-49d8-a3da-a5dcedcc8526\",\"completion_window\":\"24h\",\"created_at\":1741827194,\"endpoint\":\"/chat/completions\",\"input_file_id\":\"file-f2676ac5b0554102b8a3a60ab1c47218\",\"object\":\"batch\",\"status\":\"validating\",\"cancelled_at\":null,\"cancelling_at\":null,\"completed_at\":null,\"error_file_id\":\"\",\"errors\":null,\"expired_at\":null,\"expires_at\":1741913593,\"failed_at\":null,\"finalizing_at\":null,\"in_progress_at\":null,\"metadata\":{\"model_group\":\"gpt-4o-2024-11-20-batch-no-filter\",\"model_group_size\":\"1\",\"deployment\":\"azure/gpt-4o-2024-11-20-batch-no-filter\",\"api_base\":\"https://us-w-0008.openai.azure.com\"},\"output_file_id\":\"\",\"request_counts\":{\"completed\":0,\"failed\":0,\"total\":0},\"usage\":null}\n```\n\nSo now I have an Azure OpenAI batch...\n\n..but then when I try to get the newly created batch, it fails with a strange error about bedrock, which is super weird because we're using Azure OpenAI:\n\n```shell\n$ curl -sSL 'http://localhost:4000/v1/batches/batch_990fe845-4ff9-49d8-a3da-a5dcedcc8526' \\\n    -H \"Authorization: Bearer ${LITELLM_MASTER_KEY}\" \\\n    -H \"Content-Type: application/json\" \\\n    | jq '.'\n{\n  \"error\": {\n    \"message\": \"Internal Server Error, litellm.BadRequestError: LiteLLM doesn't support bedrock for 'create_batch'. Only 'openai' is supported.\",\n    \"type\": \"internal_server_error\",\n    \"param\": null,\n    \"code\": \"500\"\n  }\n}\n```\n\nRelevent config.yaml snippets:\n\n```yaml\nmodel_list:\n  - model_name: gpt-4o-2024-11-20-batch-no-filter\n    litellm_params:\n      model: azure/gpt-4o-2024-11-20-batch-no-filter\n      api_base: os.environ/AOAI_BASE_US_W_0008\n      api_key: os.environ/AOAI_KEY_US_W_0008\n      api_version: 2024-12-01-preview\n    model_info:\n      mode: batch\n```\n\n```yaml\nfiles_settings:\n  - custom_llm_provider: azure\n    api_base: os.environ/AOAI_BASE_US_E_0008\n    api_key: os.environ/AOAI_KEY_US_E_0008\n    api_version: 2024-12-01-preview\n```\n\n```yaml\nlitellm_settings:\n  turn_off_message_logging: True\n  drop_params: True\n  enable_loadbalancing_on_batch_endpoints: true\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nmain\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "msabramo",
      "author_type": "User",
      "created_at": "2025-03-13T03:45:57Z",
      "updated_at": "2025-06-17T00:02:05Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request",
        "stale"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9189/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9189",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9189",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:51.428007",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "> enable_loadbalancing_on_batch_endpoints: true\n\n\nplease remove this and retry @msabramo \n\nloadbalancing batch endpoints is in beta ",
          "created_at": "2025-03-13T05:17:11Z"
        },
        {
          "author": "msabramo",
          "body": "> > enable_loadbalancing_on_batch_endpoints: true\n> \n> please remove this and retry [@msabramo](https://github.com/msabramo)\n> \n> loadbalancing batch endpoints is in beta\n\nThanks! When I remove that I get a different error:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Error code: 500 - {'error': {'messag",
          "created_at": "2025-03-13T13:54:47Z"
        },
        {
          "author": "krrishdholakia",
          "body": "So we're looking at the model name in the batch file -> routing it to that one based on the `model_name`. ",
          "created_at": "2025-03-18T05:05:36Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-17T00:02:05Z"
        }
      ]
    },
    {
      "issue_number": 9337,
      "title": "[Bug]: LiteLLM doesn¬¥t have time to report to Langfuse in a serverless function",
      "body": "### What happened?\n\nLiteLLM Completion does not seem to have enough time to complete the background Langfuse reporting when working in a Lambda environment. The langfuse_context flush method does not have an effect on the LiteLLM completion and the Langfuse records come out halfway done. If time.sleep is used to artificially lengthen the Lambda runtime, then reports come out fine. Also when the Lambda is run a second time the original request is updated with the correct information.\n\nThe Langfuse top level traces are created, but the LiteLLM Completion lags behind, and does not complete at all unless the Lambda is run again. The issue persist regardless of the script structure, and whether LiteLLM is used with Langfuse Observer or without it.\n\nShould the Langfuse flush method also have an effect on LiteLLM Completion? It seems like the LiteLLM Callbacks to Langfuse aren¬¥t sent in time, and/or aren¬¥t effected by the Langfuse flushing.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.63.8\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "EemiSkillwell",
      "author_type": "User",
      "created_at": "2025-03-18T10:24:36Z",
      "updated_at": "2025-06-17T00:01:52Z",
      "closed_at": null,
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9337/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9337",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9337",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:51.626247",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-17T00:01:52Z"
        }
      ]
    },
    {
      "issue_number": 9346,
      "title": "bedrock connection in dspy.",
      "body": "I have few endpoints needs to be tested using bedrock model and it is working fine with openai models.\nI have provided all the configurations in yaml file.\nI'm getting not found error (500 internal error). ",
      "state": "open",
      "author": "jyothi410",
      "author_type": "User",
      "created_at": "2025-03-18T18:18:14Z",
      "updated_at": "2025-06-17T00:01:51Z",
      "closed_at": null,
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9346/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9346",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9346",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:51.802828",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-17T00:01:50Z"
        }
      ]
    },
    {
      "issue_number": 11775,
      "title": "[Bug]: Budget for users created with JWT authentication is not being reset",
      "body": "### What happened?\n\nWe are trying JWT authentication, and with the following configuration, even though the users are created correctly, when the job ResetBudgetJob.reset_budget executes it doesn't reset the budget for any user created through your JWT implementation. Giving one look to the table in the database I can see that the JWT implementation does not populate the `budget_reset_at` column. We tried the following configuration\n\n```yaml\ngeneral_settings:\n  enable_jwt_auth: True\n  litellm_jwtauth:\n    user_email_jwt_field: \"email\"\n    user_id_upsert: true\n\nlitellm_settings:\n  default_internal_user_params:\n    user_role: internal_user\n    max_budget: 20\n    budget_duration: 1m\n```\n\nPreviously we had a a custom auth callback that ended up calling the `NewUserRequest` function with the email and the role `internal_user` with the following configuration. This was the configuration\n\n```yaml\ngeneral_settings:\n  custom_auth: our_auth\n\nlitellm_settings:\n  max_internal_user_budget: 20\n  internal_user_budget_duration: \"1d\"\n```\n\n![Image](https://github.com/user-attachments/assets/facbce8d-2811-4a5f-93f0-b2fede1fd457)\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\n1.72.4\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "abarahonar",
      "author_type": "User",
      "created_at": "2025-06-16T21:57:39Z",
      "updated_at": "2025-06-16T21:58:24Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11775/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11775",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11775",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:52.026069",
      "comments": []
    },
    {
      "issue_number": 11773,
      "title": "[Bug]: Github Action is not checking the black formatting.",
      "body": "### What happened?\n\nThe Github Action is running `black .` which does the formatting. `black . --check` is what would be needed in order to check that the formatting is ok. \n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nlatest\n\n### Twitter / LinkedIn details\n\nhttps://www.linkedin.com/in/njbrake/",
      "state": "open",
      "author": "njbrake",
      "author_type": "User",
      "created_at": "2025-06-16T18:18:54Z",
      "updated_at": "2025-06-16T18:19:21Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11773/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11773",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11773",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:52.026093",
      "comments": [
        {
          "author": "njbrake",
          "body": "Fixed by #11772 ",
          "created_at": "2025-06-16T18:19:21Z"
        }
      ]
    },
    {
      "issue_number": 11762,
      "title": "[Bug]: MCP Server URL Field Validation Fails for Internal Kubernetes Service (short) URLs",
      "body": "### What happened?\n\nI'm encountering an issue with the \"MCP Server URL\" field validation in the Litellm UI. When attempting to input an internal Kubernetes service URL, the UI-side validation fails, displaying an error despite the URL being perfectly valid within my environment.\n\nThe specific URL I am trying to enter is: `http://mcp-server-onyx:8000/sse` (it works with: http://mcp-server-onyx.litellm.svc.cluster.local:8000/sse)\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.6.rc\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "mboret",
      "author_type": "User",
      "created_at": "2025-06-16T13:30:36Z",
      "updated_at": "2025-06-16T15:21:37Z",
      "closed_at": "2025-06-16T13:55:13Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11762/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11762",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11762",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:52.180230",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "Did that work for you ? Curious do you have any feedback on our MCP\r\nintegration. Trying to improve it\r\n\r\nIshaan Jaffer\r\nCo-Founder https://github.com/BerriAI/litellm\r\n\r\n\r\nOn Mon, Jun 16, 2025 at 6:55‚ÄØAM mattboret ***@***.***> wrote:\r\n\r\n> Closed #11762 <https://github.com/BerriAI/litellm/issues/1176",
          "created_at": "2025-06-16T14:21:39Z"
        },
        {
          "author": "mboret",
          "body": "Nop, it doesn't work for the moment. \n\nWith this code (also tested with Roo Code):\n\n```\n\"\"\"MCP Streamable HTTP Client\"\"\"\n\nimport asyncio\nfrom typing import Optional\nfrom contextlib import AsyncExitStack\n\nfrom mcp import ClientSession\nfrom mcp.client.streamable_http import streamablehttp_client\n\n\n\ncl",
          "created_at": "2025-06-16T15:21:37Z"
        }
      ]
    },
    {
      "issue_number": 11758,
      "title": "[Bug]: Request/Response data is not displaying in the LiteLLM UI interface",
      "body": "### What happened?\n\nHello all,\nI'm experiencing an issue where Request/Response data is not displaying in the LiteLLM proxy UI interface, even though I have correctly configured `store_prompts_in_spend_logs: true` in my configuration.\n\nHere is my Kubernetes deployment configuration:\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: litellm-config-file\ndata:\n  config.yaml: |\n      general_settings:\n        master_key: os.environ/LITELLM_MASTER_KEY\n        database_url: os.environ/DATABASE_URL\n        store_model_in_db: true\n        store_prompts_in_spend_logs: true\n        maximum_spend_logs_retention_period: \"7d\"\n        maximum_spend_logs_retention_interval: \"1d\"\n      \n      model_list: \n        - model_name: gpt-3.5-turbo\n          litellm_params:\n            model: azure/gpt-turbo-small-ca\n            api_base: https://my-endpoint-canada-berri992.openai.azure.com/\n            api_key: os.environ/CA_AZURE_OPENAI_API_KEY\n---\napiVersion: v1\nkind: Secret\ntype: Opaque\nmetadata:\n  name: litellm-secrets\ndata:\n  CA_AZURE_OPENAI_API_KEY: bWVvd19pbV9hX2NhdA== # your api key in base64\n  LITELLM_MASTER_KEY: xxxx\n  DATABASE_URL: xxxxxx\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: litellm-deployment\n  labels:\n    app: litellm\nspec:\n  selector:\n    matchLabels:\n      app: litellm\n  template:\n    metadata:\n      labels:\n        app: litellm\n    spec:\n      containers:\n      - name: litellm\n        image: ghcr.io/berriai/litellm:main-stable\n        ports:\n        - containerPort: 4000\n        volumeMounts:\n        - name: config-volume\n          mountPath: /app/proxy_config.yaml\n          subPath: config.yaml\n        envFrom:\n        - secretRef:\n            name: litellm-secrets\n        env:\n        - name: PORT\n          value: \"4000\"\n        - name: STORE_MODEL_IN_DB\n          value: \"True\"\n        - name: STORE_PROMPTS_IN_SPEND_LOGS\n          value: \"True\"\n        - name: MAXIMUM_SPEND_LOGS_RETENTION_PERIOD\n          value: \"7d\"\n        - name: MAXIMUM_SPEND_LOGS_RETENTION_INTERVAL\n          value: \"1d\"\n      volumes:\n        - name: config-volume\n          configMap:\n            name: litellm-config-file\n```\n\n\n**Expected Behavior**:\n\n- Request/Response content should be visible in the LiteLLM proxy UI logs page\n- The store_prompts_in_spend_logs: true setting should enable storage and display of request/response data\n\n**Actual Behavior**:\n- Success logs and error logs are displayed correctly\n- Request/Response content is not showing in the UI logs\n- Only metadata (timestamps, model names, etc.) is visible, but the actual request content and response content are missing\n\n**Configuration Details**:\n- Using Kubernetes deployment\n- PostgreSQL database is properly connected\n- Master key is configured correctly\n- The `store_prompts_in_spend_logs`: true is set in general_settings\n- Environment variable `STORE_PROMPTS_IN_SPEND_LOGS`: \"True\" is also set\n\n### Relevant log output\n\n```shell\nIn the UI Interface it shows:\n\n\nRequest/Response Data Not Available\nTo view request and response details, enable prompt storage in your LiteLLM configuration by adding the following to your proxy_config.yaml file:\n\ngeneral_settings:\n  store_model_in_db: true\n  store_prompts_in_spend_logs: true\nNote: This will only affect new requests after the configuration change.\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nmain-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "codevoyager1984",
      "author_type": "User",
      "created_at": "2025-06-16T12:00:54Z",
      "updated_at": "2025-06-16T12:00:54Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11758/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 1,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11758",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11758",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:52.392647",
      "comments": []
    },
    {
      "issue_number": 11478,
      "title": "[Bug]: Support Mistral OCR from Azure AI foundry",
      "body": "### What happened?\n\n**Problem integrating Mistral OCR with LiteLLM (Azure AI Foundry)**\n\nWe tried to integrate Mistral OCR through the LiteLLM Proxy, but we encountered issues.\n\nWe first attempted this endpoint:\n\n127.0.0.1:34666 - \"POST /mistral/v1/ocr HTTP/1.1\" 404 Not Found\nAnd got the following error:\nhttpx.InvalidURL: /v1/ocr\n\nSince this approach did not work, we tried using the official [pass through endpoint](https://docs.litellm.ai/docs/proxy/pass_through) feature in LiteLLM:\n\nüëâ https://docs.litellm.ai/docs/proxy/pass_through\n\nUsing this method, we managed to bypass the limitation, but this feels like a workaround, not a proper integration.\n\nWhy this matters\nIt looks like we are not the only ones affected ‚Äî see the last comment on this issue:\nhttps://github.com/BerriAI/litellm/issues/9051\n\nClearly, there is growing interest in supporting Mistral OCR natively in LiteLLM, without needing to rely on pass-through hacks.\n\nFeature request\nüëâ Add official support for Mistral OCR in LiteLLM.\nüëâ Possibly make the /v1/ocr route supported out of the box (similar to /v1/chat/completions).\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.0\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "HassanAITBRIK",
      "author_type": "User",
      "created_at": "2025-06-06T08:49:39Z",
      "updated_at": "2025-06-16T11:38:04Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11478/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11478",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11478",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:52.392666",
      "comments": [
        {
          "author": "ladrians",
          "body": "+1 to this request.",
          "created_at": "2025-06-10T09:45:55Z"
        },
        {
          "author": "mohittalele",
          "body": "+1",
          "created_at": "2025-06-16T11:38:04Z"
        }
      ]
    },
    {
      "issue_number": 11756,
      "title": "[Bug]: Async completion are not working with Custom Logger",
      "body": "### What happened?\n\nA bug happened!\n\nHi Team,\n\nWe are using litellm 1.63.14 version and seems like async completion requests with custom logger are not getting logged. They used to work in litellm version 1.53.1, after upgrade it is not working.\n\nWhat could be the issue? \n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.63.14\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "chakreshkolluru",
      "author_type": "User",
      "created_at": "2025-06-16T11:14:43Z",
      "updated_at": "2025-06-16T11:15:40Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11756/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11756",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11756",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:52.597786",
      "comments": []
    },
    {
      "issue_number": 11755,
      "title": "[Bug]: Anthropic v1/messages Request error",
      "body": "### What happened?\n\nAccording to the [release note](https://github.com/BerriAI/litellm/pull/11502), /v1/messages supports all LiteLLM providers, including OpenAI:\n\n\"Support for ALL LiteLLM Providers (OpenAI, Azure, Bedrock, Vertex, DeepSeek, etc.) on /v1/messages API Spec - PR\"\n\nHowever, when I test it using the gpt-4o model on version 1.72.2-stable, I get this error:\n\nLLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gpt-4o-deployment.\n\nAm I missing a required config or flag for the provider? Or is there a bug in the current version?\n\n\n\n```python\nurl = \"http://localhost:4000/v1/messages\"\nheaders = {\n    \"Authorization\": \"Bearer sk-1234\",\n    \"Content-Type\": \"application/json\"\n}\nbody = {\n    \"model\": \"gpt-4o\",\n    \"max_tokens\": 256,\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello, world\"}]\n}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(body))\nprint(response.json())\n```\n\nand got this error message\n\n```\n{'error': {'message': \"Error calling litellm.acompletion for non-Anthropic model: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gpt-4o-deployment\\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\", 'type': 'None', 'param': 'None', 'code': '500'}}\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.2-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "kkc-tonywu",
      "author_type": "User",
      "created_at": "2025-06-16T09:03:35Z",
      "updated_at": "2025-06-16T09:05:14Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11755/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11755",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11755",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:52.597806",
      "comments": []
    },
    {
      "issue_number": 11707,
      "title": "[Bug]: gpt-image-1 started failing for us",
      "body": "### What happened?\n\nYesterday, we noticed that our gpt-image-1 model from Azure started failing. The model returns an image but litellm throws an error because it cannot convert the response from Azure to ImageResponse. See log.\n\n### Relevant log output\n\n```shell\n{\"message\": \"litellm.proxy.proxy_server.image_generation(): Exception occured - litellm.APIConnectionError: AzureException APIConnectionError - Invalid response object Traceback (most recent call last):\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py\\\", line 638, in convert_to_model_response_object\\n    return LiteLLMResponseObjectHandler.convert_to_image_response(\\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\\n        response_object=response_object,\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n        model_response_object=model_response_object,\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n        hidden_params=hidden_params,\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n    )\\n    ^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_response_utils/convert_dict_to_response.py\\\", line 302, in convert_to_image_response\\n    model_response_object = ImageResponse(**model_response_dict)\\nTypeError: ImageResponse.__init__() got an unexpected keyword argument 'background'\\n\\n\\nreceived_args={'response_object': {'created': 1749821792, 'background': 'opaque', 'data': [], 'output_format': 'png', 'quality': 'low', 'size': '1024x1024', 'usage': {'input_tokens': 15, 'input_tokens_details': {'image_tokens': 0, 'text_tokens': 15}, 'output_tokens': 272, 'total_tokens': 287}, 'hidden_params': {'additional_headers': {}}}, 'model_response_object': ImageResponse(created=1749821781, data=[], usage=Usage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None)), 'response_type': 'image_generation', 'stream': False, 'start_time': None, 'end_time': None, 'hidden_params': None, '_response_headers': None, 'convert_tool_call_to_json_mode': None}\\n. Received Model Group=openai_gpt_image_1\\nAvailable Model Group Fallbacks=None\", \"level\": \"ERROR\", \"timestamp\": \"2025-06-13T13:36:37.337619\"}\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nmain-v1.68.0-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "Rasmusafj",
      "author_type": "User",
      "created_at": "2025-06-13T14:25:47Z",
      "updated_at": "2025-06-16T08:22:33Z",
      "closed_at": "2025-06-14T15:08:27Z",
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11707/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11707",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11707",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:52.597813",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "Hi @Rasmusafj did you try upgrading to the latest stable ? this is already fixed on there ",
          "created_at": "2025-06-13T14:58:44Z"
        },
        {
          "author": "Rasmusafj",
          "body": "Can confirm its already fixed! Thanks for the quick response. Will try to upgrade version before creating an issue next time. :D  ",
          "created_at": "2025-06-16T06:56:22Z"
        },
        {
          "author": "Hintic",
          "body": "I also encountered this issue! Could you please share the link to the fix? @ishaan-jaff ",
          "created_at": "2025-06-16T08:22:32Z"
        }
      ]
    },
    {
      "issue_number": 11753,
      "title": "[Feature]: support for all Gemini models under OpenRouter  bro ÔºÅÔºÅÔºÅ",
      "body": "### The Feature\n\nI strongly request support for all Gemini models under OpenRouter. \n\n### Motivation, pitch\n\nCurrently, all my model calls are concentrated on OpenRouter, but LiteLLM does not support the latest OpenRouter Gemini LLM. I would be very grateful for this!\n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "codeugar",
      "author_type": "User",
      "created_at": "2025-06-16T08:16:30Z",
      "updated_at": "2025-06-16T08:16:30Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11753/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11753",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11753",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:52.813277",
      "comments": []
    },
    {
      "issue_number": 7982,
      "title": "[Bug]: logs getting blown up by \"`logging_obj` not found - unable to track `llm_api_duration_ms`\"",
      "body": "### What happened?\n\nWith `litellm==1.59.6` when using `claude-3-5-sonnet-20241022` I am getting blown up by the below logger warning: https://github.com/BerriAI/litellm/blob/v1.59.6/litellm/litellm_core_utils/logging_utils.py#L113-L115\n\nThis is seemingly a new logged message from https://github.com/BerriAI/litellm/pull/7899.\n\nWhy does this warning matter, and can we somehow silence this log?\n\n### Relevant log output\n\n```shell\n2025-01-24 11:52:38,988 - LiteLLM - WARNING - `logging_obj` not found - unable to track `llm_api_duration_ms\n2025-01-24 11:52:39,004 - LiteLLM - WARNING - `logging_obj` not found - unable to track `llm_api_duration_ms\n2025-01-24 11:52:39,066 - LiteLLM - WARNING - `logging_obj` not found - unable to track `llm_api_duration_ms\n2025-01-24 11:52:39,684 - LiteLLM - WARNING - `logging_obj` not found - unable to track `llm_api_duration_ms\n2025-01-24 11:52:40,011 - LiteLLM - WARNING - `logging_obj` not found - unable to track `llm_api_duration_ms\n2025-01-24 11:52:40,097 - LiteLLM - WARNING - `logging_obj` not found - unable to track `llm_api_duration_ms\n2025-01-24 11:52:40,567 - LiteLLM - WARNING - `logging_obj` not found - unable to track `llm_api_duration_ms\n2025-01-24 11:52:41,202 - LiteLLM - WARNING - `logging_obj` not found - unable to track `llm_api_duration_ms\n2025-01-24 11:52:41,702 - LiteLLM - WARNING - `logging_obj` not found - unable to track `llm_api_duration_ms\n2025-01-24 11:52:41,710 - LiteLLM - WARNING - `logging_obj` not found - unable to track `llm_api_duration_ms\n2025-01-24 11:52:41,926 - LiteLLM - WARNING - `logging_obj` not found - unable to track `llm_api_duration_ms\n2025-01-24 11:52:42,022 - LiteLLM - WARNING - `logging_obj` not found - unable to track `llm_api_duration_ms\n2025-01-24 11:52:42,861 - LiteLLM - WARNING - `logging_obj` not found - unable to track `llm_api_duration_ms\n2025-01-24 11:52:43,734 - LiteLLM - WARNING - `logging_obj` not found - unable to track `llm_api_duration_ms\n2025-01-24 11:52:44,039 - LiteLLM - WARNING - `logging_obj` not found - unable to track `llm_api_duration_ms\n2025-01-24 11:52:44,085 - LiteLLM - WARNING - `logging_obj` not found - unable to track `llm_api_duration_ms\n2025-01-24 11:52:44,104 - LiteLLM - WARNING - `logging_obj` not found - unable to track `llm_api_duration_ms\n2025-01-24 11:52:44,590 - LiteLLM - WARNING - `logging_obj` not found - unable to track `llm_api_duration_ms\n2025-01-24 11:52:44,822 - LiteLLM - WARNING - `logging_obj` not found - unable to track `llm_api_duration_ms\n2025-01-24 11:52:44,864 - LiteLLM - WARNING - `logging_obj` not found - unable to track `llm_api_duration_ms\n2025-01-24 11:52:44,985 - LiteLLM - WARNING - `logging_obj` not found - unable to track `llm_api_duration_ms\n2025-01-24 11:52:45,536 - LiteLLM - WARNING - `logging_obj` not found - unable to track `llm_api_duration_ms\n2025-01-24 11:52:46,127 - LiteLLM - WARNING - `logging_obj` not found - unable to track `llm_api_duration_ms\n2025-01-24 11:52:46,453 - LiteLLM - WARNING - `logging_obj` not found - unable to track `llm_api_duration_ms\n2025-01-24 11:52:46,870 - LiteLLM - WARNING - `logging_obj` not found - unable to track `llm_api_duration_ms\n2025-01-24 11:52:47,017 - LiteLLM - WARNING - `logging_obj` not found - unable to track `llm_api_duration_ms\n2025-01-24 11:52:47,088 - LiteLLM - WARNING - `logging_obj` not found - unable to track `llm_api_duration_ms\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.59.6\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "jamesbraza",
      "author_type": "User",
      "created_at": "2025-01-24T19:57:00Z",
      "updated_at": "2025-06-16T07:29:48Z",
      "closed_at": "2025-01-25T22:46:00Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/7982/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "ishaan-jaff"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/7982",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/7982",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:52.813301",
      "comments": [
        {
          "author": "jamesbraza",
          "body": "Should LiteLLM be passing a `logging_obj` here: https://github.com/BerriAI/litellm/blob/v1.59.6/litellm/llms/anthropic/chat/handler.py#L228-L230",
          "created_at": "2025-01-24T20:00:02Z"
        },
        {
          "author": "adrianlyjak",
          "body": "getting the issue with gemini as well",
          "created_at": "2025-01-25T02:54:46Z"
        },
        {
          "author": "adrianlyjak",
          "body": "FWIW, the problem doesn't exist in v1.59.1, but does in v1.59.2: https://github.com/BerriAI/litellm/compare/v1.59.1...v1.59.2",
          "created_at": "2025-01-25T03:49:54Z"
        },
        {
          "author": "krrishdholakia",
          "body": "assigning to @ishaan-jaff ",
          "created_at": "2025-01-25T05:36:44Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Moved this to `.debug` - https://github.com/BerriAI/litellm/blob/a7b3c664d1b05dc6289f956e53b455f40d671bb3/litellm/litellm_core_utils/logging_utils.py#L113\n\nin v1.59.7\n\n\nYes, this is a new response header we added to help track litellm overhead vs. llm api latency overhead. Will gradually instrument ",
          "created_at": "2025-01-25T22:46:00Z"
        }
      ]
    },
    {
      "issue_number": 11752,
      "title": "[Bug]: Inconsistent between calling litellm and azure openai directly.",
      "body": "### What happened?\n\nWe had some users of the litellm proxy that we host report an interesting finding. Calling gpt-4.1 through Azure directly gives different response than calling it through our proxy. \n\nInitially, we thought it was the randomness nature of LLMs but its consistent. \n\n**Prompt:** \"Output a one sentence summary of a match, followed by a markdown table with a tic tac toe game\n\n**Behaviour:** Calling the model through the Litellm proxy consistently don't output the markdown table wheras calling Azure openAI deployment directly consistently outputs the markdown table.  \n\nAny ideas why this is the case?\n\n## Reproducing code:\n```python\nfrom pydantic import BaseModel\n\nfrom openai import OpenAI, AzureOpenAI\n\nAZURE_LLM_API_KEY=\"<API_KEY>\"\nAZURE_LLM_BASE_URL=\"<DEPLOYMENT_URL>\"\nAZURE_LLM_VERSION=\"2024-12-01-preview\" \nAZURE_LLM_MODEL=\"gpt-41\"\n\nLITELLM_PROXY_MODEL= \"openai_gpt41\"\nLITELLM_PROXY_API_KEY= \"<API_KEY>\"\nLITELLM_PROXY_BASE_URL= \"<PROXY_URL>\"\n\nclass Response(BaseModel):\n    answer: str\n\nazure_client = AzureOpenAI(\n    api_key=AZURE_LLM_API_KEY,  \n    api_version=AZURE_LLM_VERSION,\n    azure_endpoint=AZURE_LLM_BASE_URL\n)\n\nlitellm_proxy_client = OpenAI(\n    api_key=LITELLM_PROXY_API_KEY,\n    base_url=LITELLM_PROXY_BASE_URL,\n)\n\nprompt = \"\"\"Output a one sentence summary of a match, followed by a markdown table with a tic tac toe game\"\"\"\n\nlitellm_proxy_result = litellm_proxy_client.beta.chat.completions.parse(\n        temperature=.1,\n        model=LITELLM_PROXY_MODEL,\n        messages=[\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n        response_format=Response,\n   )\n\nprint(\"=\"*20)\nprint(\"### Litellm Proxy - GPT 4.1\")\nprint(\"=\"*20)\nprint(litellm_proxy_result)\nprint(litellm_proxy_result.choices[0].message.parsed.answer)\n\nazure_result = azure_client.beta.chat.completions.parse(\n        temperature=.1,\n        model=AZURE_LLM_MODEL,\n        messages=[\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n        response_format=Response,\n   )\nprint(\"=\"*20)\nprint(\"### Azure - GPT 4.1\")\nprint(\"=\"*20)\nprint(azure_result)\nprint(azure_result.choices[0].message.parsed.answer)\n```\n\nAnd output is:\n\n```text\n### Litellm Proxy - GPT 4.1\n====================\nParsedChatCompletion[Response](id='chatcmpl-BiyQqdISJQ6qP7k16Ynb3e00bqUh8', choices=[ParsedChoice[Response](finish_reason='stop', index=0, logprobs=None, message=ParsedChatCompletionMessage[Response](content='{\"answer\":\"Player X won the match by completing a row across the top.\"}', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, parsed=Response(answer='Player X won the match by completing a row across the top.')))], created=1750058432, model='gpt-4.1-2025-04-14', object='chat.completion', service_tier=None, system_fingerprint='fp_07e970ab25', usage=CompletionUsage(completion_tokens=18, prompt_tokens=74, total_tokens=92, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\nPlayer X won the match by completing a row across the top.\n====================\n### Azure - GPT 4.1\n====================\nParsedChatCompletion[Response](id='chatcmpl-BiyQsEjzIXi0diK4HoVoHFpYNFGqZ', choices=[ParsedChoice[Response](finish_reason='stop', index=0, logprobs=None, message=ParsedChatCompletionMessage[Response](content='{\"answer\":\"Player X won the match by completing a row across the top.\\\\n\\\\n|   |   |   |\\\\n|---|---|---|\\\\n| X | X | X |\\\\n| O | O |   |\\\\n|   |   |   |\"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Response(answer='Player X won the match by completing a row across the top.\\n\\n|   |   |   |\\n|---|---|---|\\n| X | X | X |\\n| O | O |   |\\n|   |   |   |')))], created=1750058434, model='gpt-4.1-2025-04-14', object='chat.completion', service_tier=None, system_fingerprint='fp_07e970ab25', usage=CompletionUsage(completion_tokens=59, prompt_tokens=63, total_tokens=122, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\nPlayer X won the match by completing a row across the top.\n\n|   |   |   |\n|---|---|---|\n| X | X | X |\n| O | O |   |\n|   |   |   |\n\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nmain-v1.72.2-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "Rasmusafj",
      "author_type": "User",
      "created_at": "2025-06-16T07:21:55Z",
      "updated_at": "2025-06-16T07:22:04Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11752/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11752",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11752",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:53.057424",
      "comments": []
    },
    {
      "issue_number": 11750,
      "title": "[Bug]: Invalid tool_choice mapping for models in converse_transformation.py",
      "body": "### What happened?\n\nThere is an incorrect implementation in the map_openai_params method of litellm/llms/bedrock/chat/converse_transformation.py that forcefully injects tool_choice into optional_params, even when it is not required. This causes issues specifically with Mistral models on AWS Bedrock when using the converse API.\n\nAffected Models:\nmistral.mistral-large-2402-v1:0\nmistral.mistral-small-2402-v1:0\n\nProblematic Code:\n`if (\n    litellm.utils.supports_tool_choice(\n        model=model, custom_llm_provider=self.custom_llm_provider\n    )\n    and not is_thinking_enabled\n):\n    optional_params[\"tool_choice\"] = ToolChoiceValuesBlock(\n        tool=SpecificToolChoiceBlock(\n            name=schema_name if schema_name != \"\" else \"json_tool_call\"\n        )\n    )\n`\n\n**Issue**:\nThis code block injects tool_choice even when it is not needed. When using the converse API with Mistral models, this results in the following error:\n\n`litellm.UnsupportedParamsError: bedrock does not support parameters: ['response_format'], for model=mistral.mistral-small-2402-v1:0. To drop these, set `litellm.drop_params=True` or for proxy:\n\nlitellm_settings:\n drop_params: true`\n\n**Workaround**:\nRemoving the above code block allows all AWS Bedrock-supported models to work correctly with function calling and structured output features via the converse API.\n\n**Dependency**:\nThis bug is related to and potentially dependent on the fix for Issue #11748.\n\n\n### Relevant log output\n\n```shell\nlitellm.UnsupportedParamsError: bedrock does not support parameters: ['response_format'], for model=mistral.mistral-small-2402-v1:0. To drop these, set `litellm.drop_params=True` or for proxy:\n\nlitellm_settings:\n drop_params: true\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.2\n\n### Twitter / LinkedIn details\n\nhttps://www.linkedin.com/in/shagunbansal/",
      "state": "open",
      "author": "shagunb-acn",
      "author_type": "User",
      "created_at": "2025-06-16T04:26:12Z",
      "updated_at": "2025-06-16T04:26:12Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11750/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11750",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11750",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:53.057444",
      "comments": []
    },
    {
      "issue_number": 11749,
      "title": "[Bug]: The response headers from LLM provider is not in the LiteLLM response even with return_response_headers set to true",
      "body": "### What happened?\nreturn_response_headers is set to true in the litellm config.\nHowever, the response doesnt contain the headers `llm_provider-` that mentioned in doc for original response headers from the LLM provider.\n\nWe are calling the LiteLLM proxy with Azure OpenAI chat completion endpoint for Claude models from aws bedrock.\n\nlitellm config:\n```\nmodel_list:\n  - model_name: claude-3-opus\n    litellm_params:\n      model: anthropic.claude-3-opus-20240229-v1:0\n      api_base: \"http://litellm-aws\"\n  - model_name: claude-3-5-sonnet-20240620\n    litellm_params:\n      model: anthropic.claude-3-5-sonnet-20240620-v1:0\n      api_base: \"http://litellm-aws\"\n  - model_name: claude-3-5-sonnet-20241022\n    litellm_params:\n      model: anthropic.claude-3-5-sonnet-20241022-v2:0\n      api_base: \"http://litellm-aws\"\n  - model_name: claude-3-sonnet-20240229\n    litellm_params:\n      model: anthropic.claude-3-sonnet-20240229-v1:0\n      api_base: \"http://litellm-aws\"\n  - model_name: claude-3-7-sonnet-20250219\n    litellm_params:\n      model: bedrock/us.anthropic.claude-3-7-sonnet-20250219-v1:0\n      api_base: \"http://litellm-aws\"\n  - model_name: claude-3-5-haiku-20241022\n    litellm_params:\n      model: anthropic.claude-3-5-haiku-20241022-v1:0\n      api_base: \"http://litellm-aws\"\n  - model_name: claude-sonnet-4-20250514\n    litellm_params:\n      model: bedrock/us.anthropic.claude-sonnet-4-20250514-v1:0\n      api_base: \"http://litellm-aws\"\n  - model_name: claude-opus-4-20250514\n    litellm_params:\n      model: bedrock/us.anthropic.claude-opus-4-20250514-v1:0\n      api_base: \"http://litellm-aws\"\n\nlitellm_settings:\n  drop_params: True\n  return_response_headers: true\n\nrouter_settings:\n  num_retries: 0\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.0-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "lyajedi",
      "author_type": "User",
      "created_at": "2025-06-16T04:22:33Z",
      "updated_at": "2025-06-16T04:23:54Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11749/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11749",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11749",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:53.057451",
      "comments": []
    },
    {
      "issue_number": 11689,
      "title": "[Bug]: How do I add a hosted_vllm model in UI?",
      "body": "### What happened?\n\nI checked the normal operation in config.yml.\n```yml\n- model_name: gemma-3-27b\n  litellm_params:\n    model: ISTA-DASLab/gemma-3-27b-it-GPTQ-4b-128g\n    base_url: http://0.0.0.0:8100/v1\n```\n\nHowever, it does not work when I add it as OpenAI-Compatible endpoints in the UI.\n1. \n> model_name: hosted_vllm/ISTA-DASLab/gemma-3-27b-it-GPTQ-4b-128g\n> Not requested due to LiteLLM error\n![Image](https://github.com/user-attachments/assets/21523df7-fdbe-40d1-a299-42a1b2fddbe5)\n![Image](https://github.com/user-attachments/assets/1ec5eb98-b5b0-4158-9d7e-fb11cf427fd9)\n\n2. \n> model_name: hosted_vllm/ISTA-DASLab/gemma-3-27b-it-GPTQ-4b-128g\n> Requested but vllm model name error \n> Because the model name is delivered to **\"host_vllm/ISTA-DASLab/gemma-3-27b-it-GPTQ-4b-128g\"**, not **\"ISTA-DASLab/gemma-3-27b-it-GPTQ-4b-128g\"**\n![Image](https://github.com/user-attachments/assets/3b6b34ed-c768-42d0-858a-335574889d17)\n![Image](https://github.com/user-attachments/assets/1fee4640-41fb-422e-bc6a-8695f253237a)\n```\nERROR 06-13 13:14:14 [serving_chat.py:135] Error with model object='error' message='The model `hosted_vllm/ISTA-DASLab/gemma-3-27b-it-GPTQ-4b-128g` does not exist.' type='NotFoundError' param=None code=404\n```\n\nIs there any other way to solve it?\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.0\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "hanil-jihun",
      "author_type": "User",
      "created_at": "2025-06-13T04:27:46Z",
      "updated_at": "2025-06-16T02:47:39Z",
      "closed_at": "2025-06-16T02:47:39Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11689/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11689",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11689",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:53.057457",
      "comments": [
        {
          "author": "hanil-jihun",
          "body": "I found this is worked\n```\nopenai/ISTA-DASLab/gemma-3-27b-it-GPTQ-4b-128g \n```\nBut this is correct? Is there other way?",
          "created_at": "2025-06-13T04:52:38Z"
        },
        {
          "author": "hanil-jihun",
          "body": "I found second way\nSet custom_model_name:\n```\nhosted_vllm/ISTA-DASLab/gemma-3-27b-it-GPTQ-4b-128g\n```\nAnd set LiteLLM Params like this:\n```\n{\"custom_llm_provider\": \"hosted_vllm\"}\n```\nIt works!\n\nBut is this the best?\nIs there other way?",
          "created_at": "2025-06-13T05:23:37Z"
        }
      ]
    },
    {
      "issue_number": 8448,
      "title": "[Bug]: not handling 429's from OpenRouter",
      "body": "### What happened?\n\nI am getting responses from OpenRouter where:\n\n- `completion.choices[0].message.content` is `None`\n- `completion.error` is present with a value:\n\n```python\n{'message': 'Provider returned error',\n 'code': 429,\n 'metadata': {'raw': 'error code: 1015', 'provider_name': 'Kluster'}}\n```\n\nIt would be nice to handle 429 (rate limit errors) via retrying like other providers.\n\n### Relevant log output\n\n```shell\nModelResponse(id='chatcmpl-<redacted>', created=1739239607, model='openrouter/', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=None, role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None), error={'message': 'Provider returned error', 'code': 429, 'metadata': {'raw': 'error code: 1015', 'provider_name': 'Kluster'}}, user_id='user_<redacted>')\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.60.6\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "jamesbraza",
      "author_type": "User",
      "created_at": "2025-02-11T02:12:07Z",
      "updated_at": "2025-06-16T00:02:05Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8448/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8448",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8448",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:53.251311",
      "comments": [
        {
          "author": "jamesbraza",
          "body": "Full disclaimer, this may also be an OpenRouter bug due to a nonstandard provider Kluster not properly telling OpenRouter they are overloaded",
          "created_at": "2025-02-11T02:12:38Z"
        },
        {
          "author": "kitt1987",
          "body": "Seems litellm doesn't handle the [OpenRouter errors](https://openrouter.ai/docs/api-reference/errors).",
          "created_at": "2025-02-18T03:45:43Z"
        },
        {
          "author": "yoavniran",
          "body": "Facing the same issue while using Crawl4AI (which uses litellm). Getting result.success  True for my LLMExtractionStrategy when in fact openrouter responded with 429 and litellm doesnt propagate that to Crawl4AI. \nThe only thing I see in the console is: \n```\n'NoneType' object has no attribute 'start",
          "created_at": "2025-03-16T22:21:06Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-15T00:02:08Z"
        },
        {
          "author": "jamesbraza",
          "body": "Not stale",
          "created_at": "2025-06-15T03:25:54Z"
        }
      ]
    },
    {
      "issue_number": 8977,
      "title": "[Bug]: `index` field not populated for a chunk with `choices` when calling tools using OpenAI (streaming mode, n>1)",
      "body": "### What happened?\n\nIt seems that (at least when using OpenAI GPT-4o, haven't tested other models) the `index` field in a `choice` object is not populated when a tool is called in streaming mode (with n>1). I have not observed the same happening when not streaming, when no tools are being called nor when using structured outputs. Please take a look at the log below, which (on the left) shows the original OpenAI's integration return for `choices[0].index`, whilst on the right you can see the output of LiteLLM.\n\n```\n0, 0\n0, 0\n0, 0\n1, 0\n1, 0\n2, 0\n2, 0\n0, 0\n1, 0\n2, 0\n0, 0\n1, 0\n2, 0\n0, 0\n1, 0\n2, 0\n0, 0\n1, 0\n2, 0\n0, 0\n1, 0\n2, 0\n0, 0\n1, 0\n2, 0\n0, 0\n1, 0\n2, 0\n0, 0\n1, 0\n2, 0\n0, 0\n1, 0\n2, 0\n0, 0\n1, 0\n2, 0\n0, 0\n1, 0\n2, 0\n1, 0\n2, 0\n0, 0\n1, 0\n```\n\nCode to reproduce:\n```\nclass OutputSchema(BaseModel):\n    message: str\n    title: str\n\n    model_config = dict(extra=\"forbid\")\n\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"strict\": True,\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"unit\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"celsius\", \"fahrenheit\"],\n                    },\n                },\n                \"required\": [\"location\", \"unit\"],\n                \"additionalProperties\": False\n            },\n        },\n\n    }\n]\n\nresponse_oai = openai.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": f\"Use tools to formulate your answer. Later, Please use \"\n                       f\"JSON for the response with the following schema: \"\n            f\"{OutputSchema.model_json_schema()}\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"What is the weather in San Francisco?\",\n        },\n    ],\n    tools=tools,\n    stream=True,\n    n=3\n)\nresponse_litellm = litellm.completion(\n    model=\"gpt-4o\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": f\"Use tools to formulate your answer. Later, Please use \"\n                       f\"JSON for the response with the following schema: \"\n            f\"{OutputSchema.model_json_schema()}\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"What is the weather in San Francisco?\",\n        },\n    ],\n    tools=tools,\n    stream=True,\n    n=3\n)\nindices_oai = [r.choices[0].index for r in response_oai]\nindices_litellm = [r.choices[0].index for r in response_litellm]\nprint(\"\\n\".join(f\"{x}, {y}\" for x, y in zip(indices_oai, indices_litellm)))\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\n1.60.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "andrzej-pomirski-yohana",
      "author_type": "User",
      "created_at": "2025-03-04T12:42:08Z",
      "updated_at": "2025-06-16T00:02:02Z",
      "closed_at": "2025-06-16T00:02:02Z",
      "labels": [
        "bug",
        "mlops user request",
        "stale",
        "march 2025"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8977/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8977",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8977",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:53.508811",
      "comments": [
        {
          "author": "andrzej-pomirski-yohana",
          "body": "As a side-note, this also breaks\n```python\nlitellm.stream_chunk_builder(\n    chunks=chunk_storage, messages=messages\n)\n```\n\nThis is the output, please note the arguments being set to `{}{}{}`, which of course is not a valid JSON value:\n```python\nModelResponse(id='chatcmpl-B7MenXqkPJvEckzxSkWOqeLnugW",
          "created_at": "2025-03-04T13:33:59Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-09T00:01:56Z"
        }
      ]
    },
    {
      "issue_number": 8978,
      "title": "[Bug]: Mistral stream completion request error with litellm proxy server",
      "body": "### What happened?\n\nHi,\n\nI have an error with a Mistral model request using litellm proxy server, that i don't have with other models.\n\nLitellm tag : v1.63.2-stable\nBug reproduced with a free Agent from mistral.ai\n\n\nThe following completion request with Stream=True option ( using openai SDK ) returns a MistralException error.\n\n\n\n> import openai \n> client = openai.OpenAI(api_key=\"xxx\",base_url=\"http://localhost:4001\")\n> \n> response = client.chat.completions.create(model=\"mistral\", messages = [\n>     {\n>         \"role\": \"user\",\n>         \"content\": \"hello\"\n>     }\n> ], stream=True)\n> \n> for chunk in response:\n>     print(chunk)\n> \n\n\n> openai.BadRequestError: Error code: 400 - {'error': {'message': \"litellm.BadRequestError: MistralException - {'detail': [{'type': 'extra_forbidden', 'loc': ['body', 'stream_options', 'include_usage'], 'msg': 'Extra inputs are not permitted', 'input': True, 'url': 'https://errors.pydantic.dev/2.10/v/extra_forbidden'}]}. Received Model Group=mistral\\nAvailable Model Group Fallbacks=None\", 'type': None, 'param': None, 'code': '400'}}\n\n\nIs this a known issue ?\n\n\n### What LiteLLM version are you on ?\n\nv1.63.2-stable",
      "state": "closed",
      "author": "simondutertre",
      "author_type": "User",
      "created_at": "2025-03-04T14:07:14Z",
      "updated_at": "2025-06-16T00:02:01Z",
      "closed_at": "2025-06-16T00:02:01Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8978/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8978",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8978",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:58.676622",
      "comments": [
        {
          "author": "willbelr",
          "body": "The parameters that you use ('content' and or 'role') are invalid.\n\n```\nimport litellm\nimport os\n\n\nos.environ['MISTRAL_API_KEY'] = \"\"\n\nresponse = litellm.completion(\n    model=\"mistral/mistral-tiny\",\n    messages=[{\"role\": \"user\", \"content\": \"Present yourself\"}],\n)\nprint(response)\n```",
          "created_at": "2025-03-08T19:16:21Z"
        },
        {
          "author": "simondutertre",
          "body": "> The parameters that you use ('content' and or 'role') are invalid.\n> \n> ```\n> import litellm\n> import os\n> \n> \n> os.environ['MISTRAL_API_KEY'] = \"\"\n> \n> response = litellm.completion(\n>     model=\"mistral/mistral-tiny\",\n>     messages=[{\"role\": \"user\", \"content\": \"Present yourself\"}],\n> )\n> print(",
          "created_at": "2025-03-08T19:55:36Z"
        },
        {
          "author": "willbelr",
          "body": "Works fine with stream;\n\n```\nimport litellm\nimport os\n\n\nos.environ[\"MISTRAL_API_KEY\"] = \"\"\n\nstream = litellm.completion(\n    model=\"mistral/mistral-tiny\",\n    messages=[{\"role\": \"user\", \"content\": \"Present yourself\"}],\n    stream=True\n)\nfor n, chunk in enumerate(stream):\n    content = chunk[\"choices",
          "created_at": "2025-03-08T20:20:52Z"
        },
        {
          "author": "simondutertre",
          "body": "> Works fine with stream;\n> \n> ```\n> import litellm\n> import os\n> \n> \n> os.environ[\"MISTRAL_API_KEY\"] = \"\"\n> \n> stream = litellm.completion(\n>     model=\"mistral/mistral-tiny\",\n>     messages=[{\"role\": \"user\", \"content\": \"Present yourself\"}],\n>     stream=True\n> )\n> for n, chunk in enumerate(stream)",
          "created_at": "2025-03-10T11:23:22Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-09T00:01:54Z"
        }
      ]
    },
    {
      "issue_number": 8990,
      "title": "[Bug]: Fireworks tool call ids being concatenated from every chunk",
      "body": "### What happened?\n\nReopening https://github.com/BerriAI/litellm/issues/8988 here on second thoughts, in case you guys want to handle it.\n\nI would say this is an issue with Fireworks not following the OpenAI streaming convention precisely, although technically it is within their rights to use their own format and it is an error on the LiteLLM side. This is also an issue in Langchain when streaming from fireworks and combining chunks at the end of the response.\n\nWhen streaming from any Fireworks model with clients such as LiteLLM or LangChain, tool call ids are being concatenated from every chunk into one huge id, which I noticed due to it exceeding the maximum characters allowed by the column's definition in the db. \n\nThis occurs in frameworks such as LiteLLM and Langchain which expect the chunk stream to follow the OpenAI convention of only including the `tool_call_id` in the first chunk and then subsequently relying on the `index` to update the chunk. Fireworks is including the `tool_call_id` in every chunk, which results in it being duplicated * n chunks when merging chunks. \n\n```json\n{\n    \"content\": \"What is the capital of Ireland and how's the weather there?\",\n    \"role\": \"user\"\n},\n{\n    \"content\": \"The capital of Ireland is Dublin. Let me check the weather there for you.\",\n    \"role\": \"assistant\",\n    \"tool_calls\": [\n        {\n            \"type\": \"function\",\n            \"id\": \"call_sp6sdwt7GS7X4QZO1T7npswgcall_sp6sdwt7GS7X4QZO1T7npswgcall_sp6sdwt7GS7X4QZO1T7npswgcall_sp6sdwt7GS7X4QZO1T7npswgcall_sp6sdwt7GS7X4QZO1T7npswgcall_sp6sdwt7GS7X4QZO1T7npswgcall_sp6sdwt7GS7X4QZO1T7npswgcall_sp6sdwt7GS7X4QZO1T7npswgcall_sp6sdwt7GS7X4QZO1T7npswgcall_sp6sdwt7GS7X4QZO1T7npswgcall_sp6sdwt7GS7X4QZO1T7npswgcall_sp6sdwt7GS7X4QZO1T7npswgcall_sp6sdwt7GS7X4QZO1T7npswgcall_sp6sdwt7GS7X4QZO1T7npswgcall_sp6sdwt7GS7X4QZO1T7npswgcall_sp6sdwt7GS7X4QZO1T7npswgcall_sp6sdwt7GS7X4QZO1T7npswgcall_sp6sdwt7GS7X4QZO1T7npswgcall_sp6sdwt7GS7X4QZO1T7npswgcall_sp6sdwt7GS7X4QZO1T7npswgcall_sp6sdwt7GS7X4QZO1T7npswgcall_sp6sdwt7GS7X4QZO1T7npswg\",\n            \"function\": {\n                \"name\": \"get_weather\",\n                \"arguments\": \"{\\\"input_data\\\": {\\\"latitude\\\": 53.3498, \\\"longitude\\\": -6.2603}}\"\n            }\n        }\n    ]\n}\n```\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.61.20-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "Saran33",
      "author_type": "User",
      "created_at": "2025-03-04T22:10:18Z",
      "updated_at": "2025-06-16T00:02:00Z",
      "closed_at": "2025-06-16T00:02:00Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8990/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8990",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8990",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:58.877011",
      "comments": [
        {
          "author": "Saran33",
          "body": "https://discord.com/channels/1137072072808472616/1137125207656628304/threads/1346603771119013963",
          "created_at": "2025-03-04T22:20:38Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "- the id is the same across chunks ? \n- is the code snippet above what you assembled ",
          "created_at": "2025-03-10T15:03:11Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-09T00:01:52Z"
        }
      ]
    },
    {
      "issue_number": 9090,
      "title": "[Feature]: Support for Huggingface models and other Sagemaker Jumpstart Models",
      "body": "### The Feature\n\nAt the moment its only supporting Meta 2 models, can we get support for all the main models that we can use sagemaker to infrence \n\n### Motivation, pitch\n\nLimited ability to use litellm when using sagemaker\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "koushal2018",
      "author_type": "User",
      "created_at": "2025-03-09T16:53:12Z",
      "updated_at": "2025-06-16T00:01:59Z",
      "closed_at": "2025-06-16T00:01:59Z",
      "labels": [
        "enhancement",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9090/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9090",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9090",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:59.077578",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "@koushal2018 can I see a link to the specific sagemaker endpoint / model you are trying to call ? ",
          "created_at": "2025-03-10T14:02:25Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-09T00:01:44Z"
        }
      ]
    },
    {
      "issue_number": 9008,
      "title": "[Bug]: multi instance virtual key tpm rpm setting can not synchronization",
      "body": "### What happened?\n\nIn LiteLLM v1.61.20-stable, when deploying multiple instances, modifying the TPM (transactions per minute) and RPM (requests per minute) of a virtual key on instance A does not reflect the changes on instance B, which still retrieves the original TPM and RPM values.\n\nfirstÔºöinstance A  and  instance B virtual key tpm 70\nsecondÔºöchange instance B virtual key tpm 80\n\nall instance  virtual key settings ui can see virtual key tpm 80Ôºåbut instance A api return tpm 70\n\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.61.20-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "harvardfly",
      "author_type": "User",
      "created_at": "2025-03-05T16:03:20Z",
      "updated_at": "2025-06-16T00:01:59Z",
      "closed_at": "2025-06-16T00:01:59Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9008/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9008",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9008",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:59.293403",
      "comments": [
        {
          "author": "harvardfly",
          "body": "![Image](https://github.com/user-attachments/assets/5817159a-7a57-4fec-bf1e-e08b87db47ee)\n![Image](https://github.com/user-attachments/assets/c544d8c0-3888-4c1d-bf56-6de209d244e2)",
          "created_at": "2025-03-05T16:03:45Z"
        },
        {
          "author": "harvardfly",
          "body": "virtual key update can not synchronization all litellm instance  ?",
          "created_at": "2025-03-06T06:20:39Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "does it eventually get synced after 60 seconds @harvardfly ? ",
          "created_at": "2025-03-10T14:08:07Z"
        },
        {
          "author": "harvardfly",
          "body": "@ishaan-jaff Sometimes it syncs very quickly (right after the modification), but other times it takes more than 20 minutes and still hasn't synced.",
          "created_at": "2025-03-10T16:11:35Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-09T00:01:49Z"
        }
      ]
    },
    {
      "issue_number": 9104,
      "title": "[Bug]: enable_preview_features=True leaks metadata to provider on OpenAI-compatible endpoints (can be security issue too)",
      "body": "### What happened?\n\nWhen `enable_preview_features` is set to `True` in LiteLLM, the proxy unexpectedly adds a large `metadata` block to the JSON request body sent to the upstream provider.  This metadata includes internal LiteLLM information, some of which appears to be security-sensitive (like API key hashes and internal IDs).\n\nThis is **not** happening when `enable_preview_features` is `False`. In that case, the proxy correctly forwards a clean JSON request with only the intended `messages` and `model` parameters.\n\n**Why this is a problem:**\n\n* **Security/Privacy Leak:**  Internal LiteLLM metadata, including potentially sensitive info, is being sent to the LLM provider. This is unexpected and a potential security/privacy concern.\n* **Unexpected Behavior:** The `enable_preview_features` setting is documented to control preview features, not inject metadata into requests.\n* **Bloated Requests:**  The extra metadata makes the requests larger and unnecessary.\n\n**Expected Behavior:**\n\n`enable_preview_features=True` should enable preview features as documented, but **should not** inject this extensive `metadata` block into proxied requests, especially not when using a standard OpenAI-compatible endpoint. The proxy should forward a clean request similar to when `enable_preview_features=False`.\n\n**Reproduction Steps:**\n\n1. Set up LiteLLM proxy.\n2. Send a chat completion request through the proxy with `enable_preview_features=True` and set `openai/` model with custom api base url to see the exact output (use site like webhook.site to see fulk JSON body)\n3. Inspect the JSON request body sent by the proxy to the upstream provider.\n4. Observe the large `metadata` block being added.\n5. Repeat steps 2-4 with `enable_preview_features=False`.\n6. Observe the clean JSON request body without the `metadata` block.\n\n**Impact:**\n\nPotential security and privacy risk, unexpected behavior, increased request size.\n\n**Request:**\n\nPlease investigate why `enable_preview_features=True` is causing this metadata injection and remove it to ensure clean and secure proxying behavior.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.59.9 (latest)\n\n### Twitter / LinkedIn details\n\n@yigitkonur",
      "state": "closed",
      "author": "yigitkonur",
      "author_type": "User",
      "created_at": "2025-03-10T18:26:15Z",
      "updated_at": "2025-06-16T00:01:57Z",
      "closed_at": "2025-06-16T00:01:57Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9104/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9104",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9104",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:30:59.502944",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Hi @yigitkonur this is why it's in preview - because the metadata param is used today by litellm as an internal param (before openai added support for it). \n\nCurious, how would you want us to handle this? ",
          "created_at": "2025-03-10T23:17:22Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-09T00:01:41Z"
        }
      ]
    },
    {
      "issue_number": 9103,
      "title": "[Bug]: `provider_specific_fields` not set when using `stream_chunk_builder`",
      "body": "### What happened?\n\nWhen using the litellm completion with `stream=True`, and assembling the completion from its chunks via `litellm. stream_chunk_builder`, the `provider_specific_fields` are not set when using Anthropic citations.\n\nMWE: \n```python\nimport litellm\n\nmodel = \"claude-3-5-sonnet-latest\"\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"document\",\n                \"source\": {\n                    \"type\": \"text\",\n                    \"media_type\": \"text/plain\",\n                    \"data\": \"The grass is green. The sky is blue.\",\n                },\n                \"title\": \"My Document\",\n                \"context\": \"This is a trustworthy document.\",\n                \"citations\": {\"enabled\": True},\n            },\n            {\n                \"type\": \"text\",\n                \"text\": \"\"\"What's the color of the sky? What's the color of the grass? Are they the same color? Answer each question in one sentence.\"\"\",\n            },\n        ],\n    }\n]\n\nresponse = litellm.completion(\n    model=model,\n    messages=messages,\n    temperature=0.0,\n    max_tokens=1024,\n    stream=False,\n)\nprint(response.choices[0].message.provider_specific_fields)\n```\n\nThis, non-streamed version correctly outputs:\n```\n{'citations': [[{'type': 'char_location', 'cited_text': 'The sky is blue.', 'document_index': 0, 'document_title': 'My Document', 'start_char_index': 20, 'end_char_index': 36}], [{'type': 'char_location', 'cited_text': 'The grass is green. ', 'document_index': 0, 'document_title': 'My Document', 'start_char_index': 0, 'end_char_index': 20}], [{'type': 'char_location', 'cited_text': 'The grass is green. The sky is blue.', 'document_index': 0, 'document_title': 'My Document', 'start_char_index': 0, 'end_char_index': 36}]], 'thinking_blocks': None}\n```\n\nHowever, the streamed version fails:\n```python\nstreamed_response = litellm.completion(\n    model=model,\n    messages=messages,\n    temperature=0.0,\n    max_tokens=1024,\n    stream=True,\n)\nchunks = []\nfor chunk in streamed_response:\n    chunks.append(chunk)\ncompletion = litellm.stream_chunk_builder(chunks)\nprint(completion.choices[0].message.provider_specific_fields)\n```\nOutput:\n```\nNone\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.63.3\n\n### Twitter / LinkedIn details\n\n@gwoelflein / https://www.linkedin.com/in/georg-wolflein/",
      "state": "closed",
      "author": "georg-wolflein",
      "author_type": "User",
      "created_at": "2025-03-10T17:51:32Z",
      "updated_at": "2025-06-16T00:01:57Z",
      "closed_at": "2025-06-16T00:01:57Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9103/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9103",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9103",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:00.112864",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-09T00:01:42Z"
        }
      ]
    },
    {
      "issue_number": 9305,
      "title": "[Bug]: OpenRouter 403 Error Not Support",
      "body": "### What happened?\n\nrequest with code:\n\n![Image](https://github.com/user-attachments/assets/c00a7a80-e865-4307-9ffd-4babe0f0e895)\n\n![Image](https://github.com/user-attachments/assets/bca35df6-8309-4f18-a796-b9685bd548f6)\n\n![Image](https://github.com/user-attachments/assets/52760ba0-4d38-40f3-b941-eeabc7d73c38)\n\n![Image](https://github.com/user-attachments/assets/6e4b7b86-db1f-4290-8bf8-ba5a05d4417d)\n\n![Image](https://github.com/user-attachments/assets/4e185394-c644-4cff-b26d-040201341756)\n\nraw response:\n\n![Image](https://github.com/user-attachments/assets/a7798c77-48ac-48b7-9016-9b68d5894732)\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.63.8\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "EulerBlind",
      "author_type": "User",
      "created_at": "2025-03-17T07:02:55Z",
      "updated_at": "2025-06-16T00:01:48Z",
      "closed_at": null,
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9305/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9305",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9305",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:00.364508",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-16T00:01:48Z"
        }
      ]
    },
    {
      "issue_number": 9309,
      "title": "[Bug]: HuggingFace Embedding Models APIConnectionError",
      "body": "### What happened?\n\nA bug happened! APIConnectionError: litellm.APIConnectionError: Expecting value: line 1 column 1 (char 0)\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.63.11\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "Mr-Array22",
      "author_type": "User",
      "created_at": "2025-03-17T11:54:03Z",
      "updated_at": "2025-06-16T00:01:47Z",
      "closed_at": null,
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9309/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9309",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9309",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:00.587142",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-16T00:01:47Z"
        }
      ]
    },
    {
      "issue_number": 9310,
      "title": "[Bug]: Usage  page spend - ignores records if data has more than 1 record",
      "body": "### What happened?\n\nUsage in Usage tab shows in correct datda\n\n```js\nconst existingDates = new Map(\n      data.map(item => {\n        const standardizedDate = standardizeDate(item.date);\n        return [standardizedDate, {\n          ...item,\n          date: standardizedDate // Store standardized date format\n        }];\n      })\n    );\n```\nthis above function in https://github.com/BerriAI/litellm/blob/main/ui/litellm-dashboard/src/components/usage.tsx\n\nignores the records if data has more than one record of same date.\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nlatest\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "jaswanth8888",
      "author_type": "User",
      "created_at": "2025-03-17T12:08:09Z",
      "updated_at": "2025-06-16T00:01:46Z",
      "closed_at": null,
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9310/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "krrishdholakia"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9310",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9310",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:00.778597",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-16T00:01:45Z"
        }
      ]
    },
    {
      "issue_number": 9312,
      "title": "[Feature]: custom documentation in case of custom authentication header name",
      "body": "### The Feature\n\nLitellm allows for custom authentication header using `litellm_key_header_name`. \n\nwith this setup the the virtual key goes into this header. However, litellm UI API reference tab is not customized as per authentication header. \n\nAPI reference should also include the custom header specific documentation for specific sdks. \n\n\n\n### Motivation, pitch\n\nWith custom header the use different sdk is not straightforward for people who are just getting started. The correct API reference would speed up onboarding of users on litellm. \n\n### Are you a ML Ops Team?\n\nYes\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "mohittalele",
      "author_type": "User",
      "created_at": "2025-03-17T14:43:56Z",
      "updated_at": "2025-06-16T00:01:45Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "mlops user request",
        "stale"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9312/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9312",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9312",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:00.965836",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-16T00:01:44Z"
        }
      ]
    },
    {
      "issue_number": 9313,
      "title": "[Bug]: Pricing is wrong",
      "body": "### What happened?\n\nA bug happened!\n\nFor some reason (presumably after rolling from latest stable version back to v1.63.8-nightly, because of admin user bug in latest) prices increased by several digits.\n\nHere is how it looks in logs:\nGreen highlights seem to be fine, but the last ones are obviously incorrect \n![Image](https://github.com/user-attachments/assets/26173d78-f995-43fe-85f7-e9d319cf5963)\n\nOn the Models page pricing is also very weird:\n\n![Image](https://github.com/user-attachments/assets/95ee737a-41ac-4781-b7aa-07348eef2b26)\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.63.8-nightly\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "ddkedr",
      "author_type": "User",
      "created_at": "2025-03-17T16:14:15Z",
      "updated_at": "2025-06-16T00:01:43Z",
      "closed_at": null,
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9313/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9313",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9313",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:01.168315",
      "comments": [
        {
          "author": "ddkedr",
          "body": "Same problem on the main:latest",
          "created_at": "2025-03-17T17:41:01Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-16T00:01:43Z"
        }
      ]
    },
    {
      "issue_number": 9316,
      "title": "[Bug]: Environment variable not taken into account",
      "body": "### What happened?\n\nWhen environment variable `OTEL_EXPORTER_OTLP_TRACES_HEADERS` is set (see [this example](https://github.com/BerriAI/litellm/blob/41d1a88d059340a4b986501bc960a78a2b33f065/litellm/litellm_core_utils/litellm_logging.py#L2682)), it is not functioning as expected. According to the OpenTelemetry SDK specification, when this variable is set, its value should automatically be used for trace headers without any additional configuration. However, the header is not being sent during trace export.\n\nThe root cause appears to be [here](https://github.com/BerriAI/litellm/blob/41d1a88d059340a4b986501bc960a78a2b33f065/litellm/integrations/opentelemetry.py#L827) where the headers passed directly to the function are overriding the environment variable.\n\nI consistently reproduced this issue using a self-hosted version of `phoenix_arize` with the following environment variables:\n```\nPHOENIX_API_KEY=\"<redacted>\"\nPHOENIX_COLLECTOR_HTTP_ENDPOINT=\"http://phoenix:6006/v1/traces\"\n```\n\nI see two straightforward solutions:\n1. Remove the use of `OTEL_EXPORTER_OTLP_TRACES_HEADERS` in the first file and directly set the header\n2. Modify the `OpenTelemetryConfig` [class](https://github.com/BerriAI/litellm/blob/41d1a88d059340a4b986501bc960a78a2b33f065/litellm/integrations/opentelemetry.py#L47) to handle both environment variables\n\nPlease let me know your thoughts and I'll be happy to open a PR to address this issue.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.63.11-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "mtmvu",
      "author_type": "User",
      "created_at": "2025-03-17T17:02:42Z",
      "updated_at": "2025-06-16T00:01:42Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request",
        "stale"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9316/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9316",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9316",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:01.353510",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-16T00:01:42Z"
        }
      ]
    },
    {
      "issue_number": 9319,
      "title": "[Bug]: Prisma client falling back to WASM mode despite PRISMA_QUERY_ENGINE_TYPE=binary",
      "body": "### What happened?\n\nI'm encountering an issue with my LiteLLM deployment where, despite setting PRISMA_QUERY_ENGINE_TYPE=binary and confirming its presence in the runtime environment, Prisma still attempts to use its WASM mode. During startup, Prisma generates the client and then fails with an error stating:\n`Error: Env var PRISMA_QUERY_ENGINE_BINARY is provided but provided path binary can't be resolved.\n`\nThis is followed by a validation error indicating that the datasource URL isn‚Äôt recognized‚Äîeven though my connection string (e.g., postgresql://llmproxy:dbpassword9090@db:5432/litellm) is correct.\n\nI expected Prisma to use its native binary (as directed by PRISMA_QUERY_ENGINE_TYPE=binary), correctly generate the Prisma client, and connect to our PostgreSQL database without falling back to WASM mode.\n\nAny guidance on how to resolve this conflict between the environment variables, or how to ensure that Prisma consistently uses the native binary instead of falling back to WASM mode, would be greatly appreciated!\n\nHere's the docker-compose.yml:\n`version: \"3.9\"\nservices:\n  db:\n    image: postgres:13\n    restart: always\n    environment:\n      POSTGRES_DB: litellm\n      POSTGRES_USER: llmproxy\n      POSTGRES_PASSWORD: dbpassword9090\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -h 127.0.0.1 -U llmproxy -d litellm\"]\n      interval: 5s\n      timeout: 5s\n      retries: 10\n\n  redis:\n    image: redis:7-alpine\n    restart: always\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"ping\"]\n      interval: 5s\n      timeout: 5s\n      retries: 10\n\n  litellm:\n    build: .\n    restart: unless-stopped\n    ports:\n      - \"4000:4000\"\n    environment:\n      LITELLM_MASTER_KEY: \"...\"\n      LITELLM_SALT_KEY: \"...\"\n      DATABASE_URL: \"postgresql://llmproxy:dbpassword9090@db:5432/litellm\"\n      LITELLM_WORKER_CONFIG: \"/app/config/worker_config.yaml\"\n    depends_on:\n      db:\n        condition: service_healthy\n      redis:\n        condition: service_healthy\n    volumes:\n    - ./config/config.yaml:/app/config/config.yaml:ro\n    - ./config/worker_config.yaml:/app/config/worker_config.yaml:ro`\n\nDockerfile:\n`FROM cgr.dev/chainguard/python:latest-dev\n\nUSER root\nWORKDIR /app\n\nENV HOME=/home/litellm\nENV PATH=\"${HOME}/venv/bin:$PATH\"\n\nRUN apk update && apk add openssl\n\nRUN python -m venv ${HOME}/venv && \\\n    ${HOME}/venv/bin/pip install --upgrade pip\n\nCOPY requirements.txt .\nRUN ${HOME}/venv/bin/pip install --no-cache-dir -r requirements.txt\n\nCOPY . /app\n\nCOPY entrypoint.sh /app/entrypoint.sh\nRUN chmod +x /app/entrypoint.sh\n\n# Expose port 4000\nEXPOSE 4000\n\nENTRYPOINT [\"/app/entrypoint.sh\"]\n\n# Start LiteLLM using our config file\nCMD [\"--port\", \"4000\", \"--config\", \"/app/config/config.yaml\"]\n`\n\nrequirements.yml:\n`litellm[proxy]==1.11.1\nprisma==0.15.0\nfastapi>=0.104.1\nuvicorn>=0.24.0\npydantic>=2.4.2\npython-dotenv>=1.0.0\ntyping-extensions>=4.8.0\nredis>=5.0.1\nsqlalchemy>=2.0.22\npsycopg2-binary>=2.9.9\nprometheus-client>=0.17.1\nbackoff>=2.2.1\nopentelemetry-api>=1.18.0\nopentelemetry-sdk>=1.18.0\nopentelemetry-exporter-otlp>=1.18.0\n`\n\nentrypoint.sh:\n`#!/bin/sh\nif [[ ! \"${DATABASE_URL}\" == postgresql://* ]] && [[ ! \"${DATABASE_URL}\" == postgres://* ]]; then\n  echo \"DATABASE_URL=postgresql://${DATABASE_URL#*://}\" > /app/.env\nelse\n  echo \"DATABASE_URL=${DATABASE_URL}\" > /app/.env\nfi\n\necho \"LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}\" >> /app/.env\necho \"LITELLM_SALT_KEY=${LITELLM_SALT_KEY}\" >> /app/.env\nif [ -n \"${LITELLM_WORKER_CONFIG}\" ]; then\n  echo \"LITELLM_WORKER_CONFIG=${LITELLM_WORKER_CONFIG}\" >> /app/.env\nfi\n\necho \"Created .env file with DATABASE_URL and other environment variables\"\n\nexec litellm \"$@\"`\n\n### Relevant log output\n\n```shell\nRelevant log output:\nDATABASE_URL is: postgresql://llmproxy:dbpassword9090@db:5432/litellm\nPRISMA_QUERY_ENGINE_TYPE is: binary\nPRISMA_QUERY_ENGINE_TYPE=binary\n...\nGenerating Prisma client...\nError: Env var PRISMA_QUERY_ENGINE_BINARY is provided but provided path binary can't be resolved.\n...\nError validating datasource client: the URL must start with the protocol postgresql:// or postgres://.\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.11.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "ecda909",
      "author_type": "User",
      "created_at": "2025-03-17T19:24:08Z",
      "updated_at": "2025-06-16T00:01:41Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request",
        "stale"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9319/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9319",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9319",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:01.531409",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-16T00:01:41Z"
        }
      ]
    },
    {
      "issue_number": 9810,
      "title": "[Feature]: Custom TTL for Gemini Context Caching",
      "body": "### The Feature\n\n[Current Gemini context caching support](https://docs.litellm.ai/docs/providers/gemini#context-caching) (in Anthropic's prompt caching method style) with LiteLLM does not allow a \"TTL\" parameter to be set, resulting in only the default cache duration (1 hour) applicable. Gemini's cache TTL also does not seem to be refreshed like Anthropic's after every use (https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching) too, so being able to define TTLs would be nice.\n\n\n\n### Motivation, pitch\n\n[This issue]( https://github.com/crewAIInc/crewAI/issues/2532) for CrewAI intends on using Gemini and Anthropic's context caching via LiteLLM, so being able to set custom TTLs in Gemini's case would be helpful\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "imperorrp",
      "author_type": "User",
      "created_at": "2025-04-07T23:02:28Z",
      "updated_at": "2025-06-15T21:12:55Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "april 2025"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9810/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9810",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9810",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:01.711633",
      "comments": [
        {
          "author": "ketangangal",
          "body": "Hi @krrishdholakia , I would like to work on this feature !",
          "created_at": "2025-06-04T12:40:55Z"
        },
        {
          "author": "Shakahs",
          "body": "Cache TTL costs money, per the [docs](https://ai.google.dev/gemini-api/docs/pricing) it is $4.50 per 1,000,000 tokens per hour, so not being able to control that TTL is a problem. ",
          "created_at": "2025-06-13T18:20:56Z"
        }
      ]
    },
    {
      "issue_number": 11742,
      "title": "[Bug]: langfuse_otel (python v3 SDK) callback does not nest calls properly",
      "body": "### What happened?\n\nConsider:\n\n```\nfrom langfuse import observe\n\nimport litellm\nfrom litellm import completion\nlitellm.success_callback = [\"langfuse_otel\"]\nlitellm.failure_callback = [\"langfuse_otel\"]\n\n@observe(name=\"testing\")\ndef f(a: int = 3) -> bool:\n    completion(model=\"anthropic/claude-sonnet-4-20250514\", messages=[{\"role\": \"user\", \"content\": \"Hello, how are you?\"}])\n    return a > 2\n\n\nf()\n```\n\nI get these logs:\n```\nOverriding of current TracerProvider is not allowed\n10:48:53 - LiteLLM:WARNING: opentelemetry.py:141 - Proxy Server is not installed. Skipping OpenTelemetry initialization.\n```\n\nand 2 separate traces instead of one where litellm call would be nested underneath (i.e. seems that propagation of span id to litellm doesn't work):\n\nTwo traces:\n![Image](https://github.com/user-attachments/assets/21f9b64b-fcd8-4f3c-9b0b-7a33510dc173)\n\nFirst trace:\n![Image](https://github.com/user-attachments/assets/abe8493b-4d82-4740-986c-cb605d0982df)\n\nSecond trace\n![Image](https://github.com/user-attachments/assets/3c072516-9dc0-42b5-9945-7e83e4fb36d8)\n\nI tried this brutal variant in a hope it will catch up as with the [old integration](https://langfuse.com/docs/integrations/litellm/tracing#use-within-decorated-function) (from [here](https://github.com/langfuse/langfuse/issues/2238)), but nothing:\n```\n@observe(name=\"testing\")\ndef f(a: int = 3) -> bool:\n    with langfuse_client.start_as_current_span(\n        name=\"testing nested span\",\n    ) as span:\n        completion(model=\"anthropic/claude-sonnet-4-20250514\", messages=[{\"role\": \"user\", \"content\": \"Hello, how are you?\"}], metadata={\n            \"langfuse_parent_observation_id\": span.id,\n            \"langfuse_trace_id\": span.trace_id,\n            \"trace_id\": span.trace_id,\n            \"parent_span_id\": span.id,\n            \"existing_trace_id\": span.trace_id,\n            \"parent_observation_id\": span.id,\n            \"generation_id\": span.id\n        })\n    return a > 2\n\n\nf()\n```\n\n**Update**: I made it work only by creating a new child span (because litellm wants to close it) passing parent's `otel_span` through (it had to be the otel one, not langfusespan):\n\n```\n        subspan = langfuse_client.start_span(name=\"litellm_generation_span\")\n        completion(model=\"anthropic/claude-sonnet-4-20250514\", messages=[{\"role\": \"user\", \"content\": \"Hello, how are you?\"}], metadata={\n            \"litellm_parent_otel_span\": supspan._otel_span,\n        })\n```\n\n## Sidenote\n\nThe bottomdown generated gen_ai otel span via callback seems pretty useless:\n\n![Image](https://github.com/user-attachments/assets/b2f4031f-707b-46d7-9546-30a99173f51d)\n\nand the other\n![Image](https://github.com/user-attachments/assets/fc22dcda-a256-4c99-a0c4-16020c1d8cdb)\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nIt's the latest main as of June 15th, 8am UTC.\n\ngit+ssh://git@github.com/BerriAI/litellm.git@48ac5a940fd7b3ad279e0e860a8f1eedd5cdc725\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "hnykda",
      "author_type": "User",
      "created_at": "2025-06-15T08:54:27Z",
      "updated_at": "2025-06-15T20:57:37Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11742/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11742",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11742",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:01.917495",
      "comments": [
        {
          "author": "DevBey",
          "body": "@hnykda also can you please add docs for adding metadata into [langfuse](https://langfuse.com/docs/opentelemetry/get-started) \n\nlangfuse.metadata. \n\n",
          "created_at": "2025-06-15T13:11:33Z"
        },
        {
          "author": "hnykda",
          "body": "Eh, sorry? Not sure what you want me to do",
          "created_at": "2025-06-15T15:33:05Z"
        },
        {
          "author": "DevBey",
          "body": "@hnykda \nin old langfuse v2 metdata used to be given like this, which isn't working in v3 integration. \ncan you please either update docs or check why is this not working ?? \nhttps://langfuse.com/docs/integrations/litellm/tracing#set-custom-trace-id-trace-user-id-and-tags",
          "created_at": "2025-06-15T17:31:07Z"
        }
      ]
    },
    {
      "issue_number": 7717,
      "title": "[Feature]: httpx client initialization should default to system trust store",
      "body": "### The Feature\r\n\r\nWhen no environment variables are specified (`SSL_VERIFY` and `SSL_CERTIFICATE` are not set), the default behavior of any HTTP call should be to use the system trust store.\r\n\r\nThis is not currently the case. Current when those environment variables are left blank, the default behavior of HTTPX is used, which is to use the value of `certifi.where()`: https://www.python-httpx.org/advanced/ssl/\r\n\r\n### Motivation, pitch\r\n\r\nThis is not good default behavior! There are a couple of discussions out there about how `certifi` **behaves differently depending on whether it is installed inside a venv** or not and there is this discussion about how httpx should use the system trust store by default: https://github.com/encode/httpx/issues/302\r\n\r\n---\r\n\r\nAdditionally, it is not trivial to configure `SSL_VERIFY` to work as a string being passed as the `verify` argument to.\r\n\r\nThe [SSL - HTTPX](https://www.python-httpx.org/advanced/ssl/) page shows you how you can use the `SSL_CERT_FILE` and `SSL_CERT_DIR` to provide configuration to users, which should be used over what is currently being done now:\r\n\r\nhttps://github.com/BerriAI/litellm/blob/5af438ed894e270113dd7307d34bfa78d1ccfdd0/litellm/llms/custom_httpx/http_handler.py#L442\r\n\r\n---\r\n\r\nThis affected me yesterday because I was attempting to put a proxy in front of my Ollama endpoint. I happened to secure this proxy with HTTPS and I used a self signed certificate that is trusted by my system trust store. With current `httpx` defaults, I had to first understand why my system trust store was not being trusted by an application I installed via `pipx`. I went down a rabbit hole assuming it was a `pipx` problem, then I thought it was a `litellm` problem, and I now realize it's an `httpx` default problem -- a default we have the ability to fix here.\r\n\r\nI can foresee production use cases where one might want to be able to access an Ollama endpoint over HTTPS with a self-signed certificate. There should be no need to configure environment variables here. We should trust the system trust store by default!\r\n\r\n### Are you a ML Ops Team?\r\n\r\nNo\r\n\r\n### Twitter / LinkedIn details\r\n\r\nhttps://www.linkedin.com/in/lavender-shannon-8ab43b14b/",
      "state": "open",
      "author": "retrodaredevil",
      "author_type": "User",
      "created_at": "2025-01-12T20:55:34Z",
      "updated_at": "2025-06-15T12:08:29Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/7717/reactions",
        "total_count": 5,
        "+1": 5,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/7717",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/7717",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:02.154945",
      "comments": [
        {
          "author": "nicolasesprit",
          "body": "Same here !",
          "created_at": "2025-03-12T09:07:05Z"
        },
        {
          "author": "theobjectivedad",
          "body": "+1",
          "created_at": "2025-04-27T12:49:32Z"
        },
        {
          "author": "Mazyod",
          "body": "This would provide a consistent way to support custom certificates across clients.",
          "created_at": "2025-06-15T12:08:29Z"
        }
      ]
    },
    {
      "issue_number": 10202,
      "title": "[Bug]: LiteLLM example from documentation produces pydantic warnings",
      "body": "### What happened?\n\nWhen running the example from https://docs.litellm.ai/docs/completion/function_call#full-code---parallel-function-calling-with-gpt-35-turbo-1106 with litellm 1.66.0 the following warnings are shown:\n```\n[/home/johann/software/anaconda/envs/llms_wrapper/lib/python3.11/site-packages/pydantic/main.py:463](https://file+.vscode-resource.vscode-cdn.net/home/johann/software/anaconda/envs/llms_wrapper/lib/python3.11/site-packages/pydantic/main.py:463): UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `ChatCompletionMessageToolCall` - serialized value may not be as expected [input_value={'function': {'arguments'...uy', 'type': 'function'}, input_type=dict])\n  PydanticSerializationUnexpectedValue(Expected `ChatCompletionMessageToolCall` - serialized value may not be as expected [input_value={'function': {'arguments'...li', 'type': 'function'}, input_type=dict])\n  PydanticSerializationUnexpectedValue(Expected `ChatCompletionMessageToolCall` - serialized value may not be as expected [input_value={'function': {'arguments'...3N', 'type': 'function'}, input_type=dict])\n  return self.__pydantic_serializer__.to_python(\n```\n\nHere is the complete example as executed:\n\n```\nimport litellm\nimport importlib\nprint(importlib.metadata.version(\"litellm\"))\nimport json\n# set openai api key\nimport os\nos.environ['OPENAI_API_KEY'] = os.environ['MY_OPENAI_API_KEY']\nMODEL = \"openai/gpt-4o\"\n\n# Example dummy function hard coded to return the same weather\n# In production, this could be your backend API or an external API\ndef get_current_weather(location, unit=\"fahrenheit\"):\n    \"\"\"Get the current weather in a given location\"\"\"\n    if \"tokyo\" in location.lower():\n        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": \"celsius\"})\n    elif \"san francisco\" in location.lower():\n        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": \"fahrenheit\"})\n    elif \"paris\" in location.lower():\n        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": \"celsius\"})\n    else:\n        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n\n\ndef test_parallel_function_call():\n    try:\n        # Step 1: send the conversation and available functions to the model\n        messages = [{\"role\": \"user\", \"content\": \"What's the weather like in San Francisco, Tokyo, and Paris?\"}]\n        tools = [\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": \"get_current_weather\",\n                    \"description\": \"Get the current weather in a given location\",\n                    \"parameters\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"location\": {\n                                \"type\": \"string\",\n                                \"description\": \"The city and state, e.g. San Francisco, CA\",\n                            },\n                            \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n                        },\n                        \"required\": [\"location\"],\n                    },\n                },\n            }\n        ]\n        response = litellm.completion(\n            model=MODEL,\n            messages=messages,\n            tools=tools,\n            tool_choice=\"auto\",  # auto is default, but we'll be explicit\n        )\n        print(\"\\nFirst LLM Response:\\n\", response)\n        response_message = response.choices[0].message\n        tool_calls = response_message.tool_calls\n\n        print(\"\\nLength of tool calls\", len(tool_calls))\n\n        # Step 2: check if the model wanted to call a function\n        if tool_calls:\n            # Step 3: call the function\n            # Note: the JSON response may not always be valid; be sure to handle errors\n            available_functions = {\n                \"get_current_weather\": get_current_weather,\n            }  # only one function in this example, but you can have multiple\n            messages.append(response_message)  # extend conversation with assistant's reply\n\n            # Step 4: send the info for each function call and function response to the model\n            for tool_call in tool_calls:\n                function_name = tool_call.function.name\n                function_to_call = available_functions[function_name]\n                function_args = json.loads(tool_call.function.arguments)\n                function_response = function_to_call(\n                    location=function_args.get(\"location\"),\n                    unit=function_args.get(\"unit\"),\n                )\n                messages.append(\n                    {\n                        \"tool_call_id\": tool_call.id,\n                        \"role\": \"tool\",\n                        \"name\": function_name,\n                        \"content\": function_response,\n                    }\n                )  # extend conversation with function response\n            second_response = litellm.completion(\n                model=MODEL,\n                messages=messages,\n            )  # get a new response from the model where it can see the function response\n            print(\"\\nSecond LLM response:\\n\", second_response)\n            return second_response\n    except Exception as e:\n      print(f\"Error occurred: {e}\")\n\ntest_ret1 = test_parallel_function_call()\ntest_ret1\n```\n\nAnd here is the complete output:\n\n```\n1.66.0\n\nFirst LLM Response:\n ModelResponse(id='chatcmpl-BPA3wPIN6GvkAkMIZMHu0EOoki25d', created=1745336580, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_a6889ffe71', choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_current_weather'), id='call_9AjtVQmGvUqZe6YoChInWxuy', type='function'), ChatCompletionMessageToolCall(function=Function(arguments='{\"location\": \"Tokyo, Japan\"}', name='get_current_weather'), id='call_q1li00gf6ci6ErIE5B2lCuli', type='function'), ChatCompletionMessageToolCall(function=Function(arguments='{\"location\": \"Paris, France\"}', name='get_current_weather'), id='call_C2aHqW8JiT6wBwkkELN3Rb3N', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]))], usage=Usage(completion_tokens=69, prompt_tokens=85, total_tokens=154, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n\nLength of tool calls 3\n[/home/johann/software/anaconda/envs/llms_wrapper/lib/python3.11/site-packages/pydantic/main.py:463](https://file+.vscode-resource.vscode-cdn.net/home/johann/software/anaconda/envs/llms_wrapper/lib/python3.11/site-packages/pydantic/main.py:463): UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `ChatCompletionMessageToolCall` - serialized value may not be as expected [input_value={'function': {'arguments'...uy', 'type': 'function'}, input_type=dict])\n  PydanticSerializationUnexpectedValue(Expected `ChatCompletionMessageToolCall` - serialized value may not be as expected [input_value={'function': {'arguments'...li', 'type': 'function'}, input_type=dict])\n  PydanticSerializationUnexpectedValue(Expected `ChatCompletionMessageToolCall` - serialized value may not be as expected [input_value={'function': {'arguments'...3N', 'type': 'function'}, input_type=dict])\n  return self.__pydantic_serializer__.to_python(\n\nSecond LLM response:\n ModelResponse(id='chatcmpl-BPA3xcUSmxyJ6oZgwvu6nhVHRQOjm', created=1745336581, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_90122d973c', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Here's the current weather in the requested cities:\\n\\n- **San Francisco, CA**: 72¬∞F\\n- **Tokyo, Japan**: 10¬∞C\\n- **Paris, France**: 22¬∞C\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]))], usage=Usage(completion_tokens=43, prompt_tokens=158, total_tokens=201, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\nModelResponse(id='chatcmpl-BPA3xcUSmxyJ6oZgwvu6nhVHRQOjm', created=1745336581, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_90122d973c', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Here's the current weather in the requested cities:\\n\\n- **San Francisco, CA**: 72¬∞F\\n- **Tokyo, Japan**: 10¬∞C\\n- **Paris, France**: 22¬∞C\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]))], usage=Usage(completion_tokens=43, prompt_tokens=158, total_tokens=201, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.66.0\n\n### Twitter / LinkedIn details\n\nDefinitely not using Twitter that right-wing-extremists propaganda and disinformation cesspool any more. ",
      "state": "open",
      "author": "johann-petrak",
      "author_type": "User",
      "created_at": "2025-04-22T15:48:16Z",
      "updated_at": "2025-06-15T11:46:00Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10202/reactions",
        "total_count": 4,
        "+1": 4,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10202",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10202",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:02.330148",
      "comments": [
        {
          "author": "cenkalti",
          "body": "I can confirm the bug exists. It is caused by the following piece of code:\n\nhttps://github.com/BerriAI/litellm/blob/1ef17c2f0e075d7fdaf5ca2b376efe02cedb8bcd/litellm/utils.py#L6299-L6300\n\nAssigning a `dict` to a field of `ChatCompletionMessageToolCall` is not correct. That's why Pydantic is raising a",
          "created_at": "2025-04-29T16:27:56Z"
        },
        {
          "author": "SmartManoj",
          "body": "Alternate workaround:\n```python\nimport warnings\nwarnings.filterwarnings(\"ignore\", message=\"Pydantic serializer warnings\")\n```",
          "created_at": "2025-06-15T11:46:00Z"
        }
      ]
    },
    {
      "issue_number": 11325,
      "title": "[Bug]: Groq's distil-whisper-large-v3-en not working with litellm_stable_release_branch-v1.72.0.rc",
      "body": "### What happened?\n\nWas testing `litellm_stable_release_branch-v1.72.0.rc` docker image, because it fixes a problem with gpt-image-1 for us and, when running the standard tests I've started to get this test failing.\n\nThe test is passing with `litellm_stable_release_branch-v1.71.1-stable`.\n\nThis is the configuration:\n```\n  - model_name: \"whisper-distil-groq\"\n    litellm_params:\n      model: \"groq/distil-whisper-large-v3-en\"\n      api_key: \"os.environ/GROQ_API_KEY\"\n      model_info:\n        mode: audio_transcription\n```\n\nAnd this is the test:\n```\ncurl --request POST \\\n  --url http://0.0.0.0:4000/v1/audio/transcriptions \\\n  --header 'authorization: Bearer {{bearer_token}}' \\\n  --header 'content-type: multipart/form-data' \\\n  --form file=@hello-test.m4a \\\n  --form model=whisper-distil-groq\n```\n\nThat, when working, returns:\n```\n{\n  \"text\": \" Hello, how are you?\"\n}\n```\n\n### Relevant log output\n\n```shell\n{\n  \"error\": {\n    \"message\": \"litellm.BadRequestError: GroqException - unknown param `GROQ_TRANSCRIPTION_PARAMS[]`. Received Model Group=whisper-distil-groq\\nAvailable Model Group Fallbacks=None LiteLLM Retried: 1 times, LiteLLM Max Retries: 2\",\n    \"type\": \"None\",\n    \"param\": \"None\",\n    \"code\": \"500\"\n  }\n}\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.0.rc\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "stronk7",
      "author_type": "User",
      "created_at": "2025-06-02T10:08:58Z",
      "updated_at": "2025-06-15T09:54:32Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11325/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11325",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11325",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:04.273453",
      "comments": [
        {
          "author": "jerry-intrii",
          "body": "Same problem here. Downgrade to 1.71.1 works fine\n\n",
          "created_at": "2025-06-11T06:14:20Z"
        },
        {
          "author": "bgeneto",
          "body": "Confirmed. Not only with English language. ",
          "created_at": "2025-06-12T00:07:53Z"
        },
        {
          "author": "stronk7",
          "body": "For the records, I've just tried with `litellm_stable_release_branch-v1.72.2-stable` and it continues failing with same error reported above.\n\nAs a temporal workaround, what I've done is to add a fallback from groq to openai, and that's working till this is fixed:\n\n```\nfallbacks:\n    - ...\n    - ...",
          "created_at": "2025-06-15T09:54:32Z"
        }
      ]
    },
    {
      "issue_number": 11743,
      "title": "[Bug]: Create new user through API send a wrong invitation link",
      "body": "### What happened?\n\nHaving litellm configured to send an email when creating a new user with a valid inivtation link on the UI is working well.\n\nUsing the /user/new api endpoint with send_invite_email=True send a welcome email but with only the base url on the Accept Invite button.\n\n### Relevant log output\n\n```shell\nNo relevant logs observed\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nmain-v1.72.2-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "menardorama",
      "author_type": "User",
      "created_at": "2025-06-15T09:04:37Z",
      "updated_at": "2025-06-15T09:04:44Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11743/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11743",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11743",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:04.451773",
      "comments": []
    },
    {
      "issue_number": 11144,
      "title": "[Bug]: OpenTelemetry callback should not create another TracerProvider",
      "body": "### What happened?\n\nThe init function of `OpenTelemetry` creates a `TracerProvider`.\n\nhttps://github.com/BerriAI/litellm/blob/77d2615185b1949f024e9c93eb9f965e699ac561/litellm/integrations/opentelemetry.py#L105\n\nWhen used in a context where a TracerProvider is already being used (e.g. fastapi application with instrumentation), this causes an error/warning:\n\n`Overriding of current TracerProvider is not allowed`\n\nSee https://opentelemetry.io/docs/concepts/signals/traces/#tracer-provider\n\nIs it possible to change `OpenTelemetry` callback so that the instantiation of the `TracerProvider` becomes optional?\n\n\n\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.70.0\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "bgs4free",
      "author_type": "User",
      "created_at": "2025-05-25T12:08:11Z",
      "updated_at": "2025-06-15T08:57:20Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11144/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11144",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11144",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:04.451800",
      "comments": [
        {
          "author": "hnykda",
          "body": "I am seeing this elsewhere too, and suspect it's an issue: https://github.com/BerriAI/litellm/issues/11742",
          "created_at": "2025-06-15T08:57:20Z"
        }
      ]
    },
    {
      "issue_number": 11453,
      "title": "[Bug]: together_ai/meta-llama/Llama-4-Scout-17B-16E-Instruct not correctly parsing tool response",
      "body": "### What happened?\n\nOn https://models.litellm.ai/ it says that the Model Llama-4-Scout supports tool calling but when used it returns a format which does not work. \n\n### Relevant log output\n\n```shell\nResponse: ModelResponse(id='nwwck5e-2j9zxn-94b225f2c82c7aec', created=1749152495, model='together_ai/meta-llama/Llama-4-Scout-17B-16E-Instruct', object='chat.completion', system_fingerprint=None,         \n            choices=[Choices(finish_reason='stop', index=0,                                                                                                                                                             \n            message=Message(content='<|python_start|>[{\"index\":0,\"function\":{\"arguments\":\"{\\\\\"line_number\\\\\":307}\",\"name\":\"goto\"},\"id\":\"call_1v0k7q2m0kcw3d1p0b3w0\",\"type\":\"function\"}]', role='assistant',             \n            tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}))], usage=Usage(completion_tokens=52, prompt_tokens=14072, total_tokens=14124, completion_tokens_details=None,              \n            prompt_tokens_details=None), service_tier=None, prompt=[])\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.72.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "JarettForzano",
      "author_type": "User",
      "created_at": "2025-06-05T19:43:24Z",
      "updated_at": "2025-06-15T07:19:46Z",
      "closed_at": null,
      "labels": [
        "bug",
        "awaiting: user response"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11453/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11453",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11453",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:04.683538",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Hey @JarettForzano this looks like a together ai bug - how would you want us to handle this? \n\nthe output doesn't look clean to parse given the start tag. \n\nIs this a consistent repro?",
          "created_at": "2025-06-06T06:00:43Z"
        },
        {
          "author": "JarettForzano",
          "body": "Yeah, i believe it was consistent. Would it be possible to parse out the python part so that you can access the necessary values? ",
          "created_at": "2025-06-15T07:19:46Z"
        }
      ]
    },
    {
      "issue_number": 11739,
      "title": "[Bug]: Can not remove/disable Cache Control (through UI)",
      "body": "### What happened?\n\nI've created a Modal and enabled Cache Control through the UI. However when now toggling that off (to remove it) and reload the page it still shows as enabled.\n\n### Relevant log output\n\n```shell\nI assume the issue is that the cache related stuff (`cache_control_injection_points`) is omitted from the PATCH API call. But the system considered committed fields as to be ignored (rather then removed).\n\nProbably need to set to an empty array and/or null when saving via UI?\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.2-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "fkrauthan",
      "author_type": "User",
      "created_at": "2025-06-15T04:07:39Z",
      "updated_at": "2025-06-15T04:07:39Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11739/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11739",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11739",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:04.910492",
      "comments": []
    },
    {
      "issue_number": 4644,
      "title": "[Feature]: Add basic testing for Admin UI ",
      "body": "### The Feature\n\nQOL Improvement for Admin UI, would love help on this issue \r\n\r\n- Test basic sign in flow \r\n- Test Adding a Model \r\n- Test the correct dropdown shows when adding a model\r\n- Test creating an API Key \n\n### Motivation, pitch\n\nbetter QOL UI \n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "ishaan-jaff",
      "author_type": "User",
      "created_at": "2024-07-10T18:17:10Z",
      "updated_at": "2025-06-15T03:51:58Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "good first issue",
        "help wanted"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/4644/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/4644",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/4644",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:04.910508",
      "comments": [
        {
          "author": "mnyamunda-scottlogic",
          "body": "Hi, would it be a good idea to have these as playwright e2e tests?",
          "created_at": "2024-07-11T11:17:07Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-01-28T02:59:22Z"
        },
        {
          "author": "omnisilica",
          "body": "Doesn't look like this has been implemented yet. I'll begin work on this issue.",
          "created_at": "2025-03-13T02:36:59Z"
        },
        {
          "author": "omnisilica",
          "body": "Still working on this issue. Looking forward to completing before the end of this week.",
          "created_at": "2025-03-19T19:46:57Z"
        },
        {
          "author": "Deriverx2",
          "body": "This could be closed right?\n",
          "created_at": "2025-06-02T19:13:15Z"
        }
      ]
    },
    {
      "issue_number": 3163,
      "title": "[Bug]: The term 'litellm' is not recognized as the name of a cmdlet, function, script file, or operable program.",
      "body": "### What happened?\n\nHi, \r\nOnce I installed the litellm package I tried: \r\nlitellm --model [MYMODEL]\r\nbut got this issue:\r\nThe term 'litellm' is not recognized as the name of a cmdlet, function, script file, or operable program.\r\nCheck the spelling of the name, or if a path was included, verify that the path is correct and try again.\r\nAt line:1 char:1\r\nHow can I fix it ? \n\n### Relevant log output\n\n_No response_\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "AvnerAdda",
      "author_type": "User",
      "created_at": "2024-04-19T14:18:55Z",
      "updated_at": "2025-06-15T00:02:15Z",
      "closed_at": "2025-06-15T00:02:15Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/3163/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/3163",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/3163",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:05.107179",
      "comments": [
        {
          "author": "edwinjosegeorge",
          "body": "The is primarily when windows powershell cannot find the program you try to execute. You need to cross check the environment to which the package is installed. Activate your virtual environment and run `pip show litellm` to view the details of litellm package",
          "created_at": "2024-05-15T14:02:30Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-01-29T00:02:00Z"
        },
        {
          "author": "Kilarilla",
          "body": "How to fix",
          "created_at": "2025-03-08T18:16:05Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-07T00:02:13Z"
        }
      ]
    },
    {
      "issue_number": 7109,
      "title": "[Feature]: Create Ruby SDK",
      "body": "### The Feature\n\nWould love a Ruby SDK\n\n### Motivation, pitch\n\nWould be great to have this in other languages to Python\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n@charliecheesma1 / https://www.linkedin.com/in/charliecheesman",
      "state": "closed",
      "author": "Ches-ctrl",
      "author_type": "User",
      "created_at": "2024-12-09T14:40:04Z",
      "updated_at": "2025-06-15T00:02:12Z",
      "closed_at": "2025-06-15T00:02:12Z",
      "labels": [
        "enhancement",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/7109/reactions",
        "total_count": 6,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 6,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/7109",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/7109",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:05.287195",
      "comments": [
        {
          "author": "nimir",
          "body": "Hi,\n\nI just published [`litellm-ruby`](https://github.com/Eptikar/litellm-ruby) ‚Äî a lightweight Ruby client for LiteLLM.  \n\nThis started as an internal library for our Rails app, and I wanted to share it as a contribution to this great project and to the Ruby AI ecosystem. Currently, it focuses on c",
          "created_at": "2025-03-09T09:05:16Z"
        },
        {
          "author": "Ches-ctrl",
          "body": "Nice one :)",
          "created_at": "2025-03-09T19:40:57Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-08T00:02:05Z"
        }
      ]
    },
    {
      "issue_number": 7808,
      "title": "Error 400 when using pydantic objects with default options defined with Google models.",
      "body": "Using google models where a pydantic object has default values set can sometimes fail.   It seems that openai handles this ok.  If i remove these defaults then it works fine.\r\n\r\ne.g.  if you define a model like this (shorted for the example)\r\nclass exampleSpec(BaseModel):\r\n       example1: List[str] **=[]**\r\n       example2: List[str] **=[]**\r\n       example3: List[str] **=[]**\r\n       example4: **str =''**\r\n\r\nwhich is mapped to a structure which looks like this\r\n\r\n'exampleSpec': {'items': \r\n              {'properties':\r\n                  {'example1': {**'default': []**, 'items': {'type': 'string'}, 'type': 'array'},\r\n                  'example2': {**'default': []**, 'items': {'type': 'string'}, 'type': 'array'},\r\n                  'example3': {**'default': []**, 'items': {'type': 'string'}, 'type': 'array'},\r\n                  'example4': {**'default': ''**, 'type': 'string'}}, 'type': 'object'}, 'type': 'array'}, \r\n                \r\n\r\nit seems to cause this 400 error (n.b this error is from a more complex pydantic structure but shows the failure.\r\n\r\nFailed to convert text into a pydantic model due to the following error: litellm.BadRequestError: VertexAIException BadRequestError - {\r\n  \"error\": {\r\n    \"code\": 400,\r\n    \"message\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[3].value.items.properties[0].value': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[3].value.items.properties[1].value': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[3].value.items.properties[2].value': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[3].value.items.properties[3].value': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[4].value.items.properties[0].value': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[4].value.items.properties[1].value': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[4].value.items.properties[2].value': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[4].value.items.properties[3].value': Cannot find field.\",\r\n    \"status\": \"INVALID_ARGUMENT\",\r\n    \"details\": [\r\n      {\r\n        \"@type\": \"type.googleapis.com/google.rpc.BadRequest\",\r\n        \"fieldViolations\": [\r\n          {\r\n            \"field\": \"tools[0].function_declarations[0].parameters.properties[3].value.items.properties[0].value\",\r\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[3].value.items.properties[0].value': Cannot find field.\"\r\n          },\r\n          {\r\n            \"field\": \"tools[0].function_declarations[0].parameters.properties[3].value.items.properties[1].value\",\r\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[3].value.items.properties[1].value': Cannot find field.\"\r\n          },\r\n          {\r\n            \"field\": \"tools[0].function_declarations[0].parameters.properties[3].value.items.properties[2].value\",\r\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[3].value.items.properties[2].value': Cannot find field.\"\r\n          },\r\n          {\r\n            \"field\": \"tools[0].function_declarations[0].parameters.properties[3].value.items.properties[3].value\",\r\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[3].value.items.properties[3].value': Cannot find field.\"\r\n          },\r\n          {\r\n            \"field\": \"tools[0].function_declarations[0].parameters.properties[4].value.items.properties[0].value\",\r\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[4].value.items.properties[0].value': Cannot find field.\"\r\n          },\r\n          {\r\n            \"field\": \"tools[0].function_declarations[0].parameters.properties[4].value.items.properties[1].value\",\r\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[4].value.items.properties[1].value': Cannot find field.\"\r\n          },\r\n          {\r\n            \"field\": \"tools[0].function_declarations[0].parameters.properties[4].value.items.properties[2].value\",\r\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[4].value.items.properties[2].value': Cannot find field.\"\r\n          },\r\n          {\r\n            \"field\": \"tools[0].function_declarations[0].parameters.properties[4].value.items.properties[3].value\",\r\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[4].value.items.properties[3].value': Cannot find field.\"\r\n          }\r\n        ]\r\n      }\r\n    ]\r\n  }\r\n}\r\n Using raw output instead.\r\n",
      "state": "closed",
      "author": "andrewn3",
      "author_type": "User",
      "created_at": "2025-01-16T12:23:07Z",
      "updated_at": "2025-06-15T00:02:10Z",
      "closed_at": "2025-06-15T00:02:10Z",
      "labels": [
        "awaiting: user response",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/7808/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "krrishdholakia"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/7808",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/7808",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:05.512041",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Hey @andrewn3 could you share a script i can run to repro this issue? ",
          "created_at": "2025-01-23T04:09:56Z"
        },
        {
          "author": "JohanBekker",
          "body": "I just ran into the same problem using the `ChatLiteLLM` class in langchain. Here's how to reproduce:\n\n```\nfrom dotenv import load_dotenv\nfrom langchain_community.chat_models import ChatLiteLLM\nfrom langchain_core.messages import HumanMessage\nfrom pydantic import BaseModel, Field\n\nfrom litellm impor",
          "created_at": "2025-03-01T10:55:35Z"
        },
        {
          "author": "aamir-s18",
          "body": "push",
          "created_at": "2025-03-07T04:55:35Z"
        },
        {
          "author": "andrewn3",
          "body": "Sorry I haven't had time to write a demo script but I found the issue was when you define pydantic objects using default and possibly the optional command, and nested pydantic structures e.g here are some examples if this helps.  \n\nclass Example(BaseModel):\n    example_tasks: List[str] = Field(**def",
          "created_at": "2025-03-07T09:48:13Z"
        },
        {
          "author": "winternewt",
          "body": "I ran into same, specifically on Gemini, however I can't confirm it works on Openai. \nIt seems to be an artificial documented schema limitation:\nhttps://platform.openai.com/docs/guides/structured-outputs/supported-schemas#all-fields-must-be-required\n\n@krrishdholakia sharing a repro I have by acciden",
          "created_at": "2025-03-08T02:31:50Z"
        }
      ]
    },
    {
      "issue_number": 9045,
      "title": "[Bug]: Model analytics page showing model retry settings",
      "body": "### What happened?\n\nModel analytics page showing model retry settings\n\n![Image](https://github.com/user-attachments/assets/c579c1b5-cfba-44e2-94e4-dfa3050f7478)\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.61.20\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "jaswanth8888",
      "author_type": "User",
      "created_at": "2025-03-07T06:13:29Z",
      "updated_at": "2025-06-15T00:02:02Z",
      "closed_at": "2025-06-15T00:02:02Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9045/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "ishaan-jaff"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9045",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9045",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:05.776265",
      "comments": [
        {
          "author": "jaswanth8888",
          "body": "I see it removed in https://github.com/BerriAI/litellm/commit/ad450b761696d71a25ea5033313fc966838258f6#diff-2bf81b18a1f9510f2a26b05b93b6bea891c054451df13bd16d696484cbb09f25",
          "created_at": "2025-03-07T06:57:59Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-08T00:01:52Z"
        }
      ]
    },
    {
      "issue_number": 9092,
      "title": "[Feature]: Support FetchAI provider and asi-1-mini model",
      "body": "### The Feature\n\nSupport for the FetchAI provider (https://fetch.ai) and the asi-1-mini model (https://asi1.ai).\n\n### Motivation, pitch\n\nFetchAI is an open marketplace for AI Agents. They released their first own model `asi-1-mini` and I got approached by the team to support it in Promptmetheus, which uses LiteLLM under the hood.\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n@toniengelhardt / [linkedin.com/in/toniengelhardt](https://www.linkedin.com/in/toniengelhardt)",
      "state": "closed",
      "author": "toniengelhardt",
      "author_type": "User",
      "created_at": "2025-03-09T18:59:22Z",
      "updated_at": "2025-06-15T00:02:00Z",
      "closed_at": "2025-06-15T00:02:00Z",
      "labels": [
        "enhancement",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9092/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9092",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9092",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:07.018523",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-08T00:01:47Z"
        }
      ]
    },
    {
      "issue_number": 11733,
      "title": "[Bug]: Endpoint works with Router from SDK but not in Proxy",
      "body": "### What happened?\n\nCurrently LiteLLM Proxy uses the async endpoints (acompletions etc.)\nI am trying to host a model locally which provides only a sync OpenAI-compatible endpoint (it gives something like localhost:8081/v1/chat/completions)\n\nI am able to use my self-hosted LLM with\n- ‚úÖ OpenAI python SDK with `.chat.completions.create(..)`\n- ‚úÖ LiteLLM python SDK with model settings set inside `litellm.router.Router` and then using `router.completion`\n- ‚úÖ Using simple cuRL command on \"http:localhost:8081/v1/chat/completions\"\n\nBut when I run the same configuration via LiteLLM Proxy, it fails and raises an error - since my code doesn't have an async endpoint. \nIt raises the following relevant OpenAI error - `litellm.APIError: APIError: OpenAIException - Connection error`\n\nI suspect its the conflict the LiteLLM Proxy's `Router` is calling the acompletions function which is causing my sync endpoint to not work\n\nSettings I have tried:\n- ssl_verify: False\nCan anyone suggest what could be the fix?\n\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.71.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "siddhantpathakk",
      "author_type": "User",
      "created_at": "2025-06-14T21:19:11Z",
      "updated_at": "2025-06-14T21:20:32Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11733/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11733",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11733",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:07.220181",
      "comments": []
    },
    {
      "issue_number": 9877,
      "title": "[Bug]: How to successfully configure MCP in config.yaml  in Litellm? Please provide a correct example. Thank you very much",
      "body": "### What happened?\n\nA bug happened!\nHello, may I ask if you have successfully called MCP using Litellm? I tried using MCPO to start a service on port 9000 and configured MCP_uservers in config. YAML:\n{\n\"fetch\": {\n\"url\": \" http://localhost:9000/sse \"\n}\n}But it cannot be successfully called. I would like to see how you successfully called it. Thank you very much. Any MCP service is sufficient. Thank you very much\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.65.4\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "bresilin",
      "author_type": "User",
      "created_at": "2025-04-10T08:03:45Z",
      "updated_at": "2025-06-14T19:38:12Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9877/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9877",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9877",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:07.220206",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Hey @bresilin https://docs.litellm.ai/docs/mcp#usage\n\nWhat's unclear from here? \n\n- You need to initiate a client session (`async with sse_client(\"http://localhost:4000/mcp/\") as (read, write):`)\n- Get the mcp tools from proxy - `mcp_tools = await session.list_tools()` \n\n```python\nimport asyncio\nfro",
          "created_at": "2025-04-10T21:48:15Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "Hi @bresilin can you try with the zapier MCP like the example here: https://docs.litellm.ai/docs/mcp#1-define-your-tools-on-under-mcp_servers-in-your-configyaml-file \n\n\nAre you sure your MCP is an sse ? or is it a stdio mcp server ? ",
          "created_at": "2025-04-10T21:55:46Z"
        },
        {
          "author": "bresilin",
          "body": "> Hi [@bresilin](https://github.com/bresilin) can you try with the zapier MCP like the example here: https://docs.litellm.ai/docs/mcp#1-define-your-tools-on-under-mcp_servers-in-your-configyaml-file\n> \n> Are you sure your MCP is an sse ? or is it a stdio mcp server ?\n\nI have successfully connected t",
          "created_at": "2025-04-11T07:55:44Z"
        },
        {
          "author": "bresilin",
          "body": "I have successfully connected to the Zapier MCP service, but I started the fetch service on port 9000 using MCPO http://localhost:9000/stdio But it doesn't take effect. How did you configure it? Does Litellm not support mcpo? Can I see how you configured it? Did you write your own MCP service locall",
          "created_at": "2025-04-11T07:59:45Z"
        },
        {
          "author": "nikhilmaddirala",
          "body": "Hi, I'm also interested in this question! ",
          "created_at": "2025-04-20T22:05:08Z"
        }
      ]
    },
    {
      "issue_number": 11729,
      "title": "[Feature]: Add Resource Constraints for Migration Job Container in Helm Chart",
      "body": "### The Feature\n\nCurrently, the LiteLLM Helm chart does not allow setting resource constraints (CPU/memory requests and limits) for the migration job container. Please add a resources field under migrationJob in values.yaml and update the migration job template to use it.\nExample of the desired addition:\n\n```yaml\n# values.yaml\nmigrationJob:\n  enabled: true\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n    requests:\n      cpu: 250m\n      memory: 512Mi\n```\n\nTemplate update needed:\n```yaml\n# migrations-job.yaml\ncontainers:\n  - name: prisma-migrations\n    image: \"{{ .Values.image.repository }}:{{ .Values.image.tag | default (printf \"main-%s\" .Chart.AppVersion) }}\"\n    imagePullPolicy: {{ .Values.image.pullPolicy }}\n    securityContext:\n      {{- toYaml .Values.securityContext | nindent 12 }}\n    {{- if .Values.migrationJob.resources }}\n    resources:\n      {{- toYaml .Values.migrationJob.resources | nindent 12 }}\n    {{- end }}\n    command: [\"python\", \"litellm/proxy/prisma_migration.py\"]\n```\n\n### Motivation, pitch\n\nResource constraints are essential for stable and predictable Kubernetes deployments, especially in production. This change will ensure the migration job doesn‚Äôt consume excessive resources, aligns with Kubernetes best practices, and matches the resource management already available for the main application container.\n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "Gambit0011",
      "author_type": "User",
      "created_at": "2025-06-14T19:28:50Z",
      "updated_at": "2025-06-14T19:28:50Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11729/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11729",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11729",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:07.415512",
      "comments": []
    },
    {
      "issue_number": 11436,
      "title": "[Bug]: The current mcp tool list interface is too slow",
      "body": "### What happened?\n\nThe current tool acquisition interface synchronizes with the upstream SSE server every time, causing the MCP refresh to be exceptionally slow whenever a tool is added or tested. Is it possible to improve this situation?\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.12.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "sxueck",
      "author_type": "User",
      "created_at": "2025-06-05T10:06:18Z",
      "updated_at": "2025-06-14T18:03:54Z",
      "closed_at": "2025-06-14T18:03:54Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11436/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11436",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11436",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:07.415535",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "cc: @wagnerjt ",
          "created_at": "2025-06-06T06:27:35Z"
        },
        {
          "author": "sxueck",
          "body": "If the upstream SSE server connection is good, the response rate of the `/tool/list` interface is currently acceptable. I feel this issue might not be urgent?",
          "created_at": "2025-06-06T07:43:57Z"
        },
        {
          "author": "wagnerjt",
          "body": "Totally understand that is the behaviour at the moment. Once we add the additional MCP support it will be easier to optimise and improve tool calls like this. Especially once we have support for passing authentication to external MCP servers",
          "created_at": "2025-06-06T14:41:26Z"
        }
      ]
    },
    {
      "issue_number": 10106,
      "title": "[Bug]:Anthropic API throws Bad Request when user_id in metadata is an email address",
      "body": "### Summary\n\nWhen using LiteLLM Proxy with Claude models and passing a `user_id` in the `metadata`, if the `user_id` is an email address, Anthropic responds with a `400 Bad Request` error. This happens because Anthropic **strictly prohibits** including personally identifiable information (PII) like emails or phone numbers in the `user_id` field. [link](https://docs.anthropic.com/en/api/messages#body-metadata-user-id)\n\n### Steps to Reproduce\n\n1. Set up LiteLLM proxy to route requests to Claude 3.7.\n2. Pass a `user_id` in `extra_body.metadata` that is an email address (e.g. `myname@example.com`).\n3. Send a completion request.\n\n```shell \ncurl -X POST \\\nhttps://lite-llm-proxy.example.com \\\n-H 'Authorization: *****' \\\n-d '{'model': 'claude-3.7', 'messages': [{'role': 'user', 'content': 'hey'}], 'stream': True, 'extra_body': {'metadata': {'user_id': 'myname@example.com'}}}'\n```\n\n\n### Expected Behavior\n\nThe request should succeed, and the `user_id` should be included only if it is a valid, opaque identifier (e.g., UUID or hashed value).\n\n\n### Relevant log output\n\n\nThe request fails with:\n\n```shell\nlitellm.BadRequestError: AnthropicException - b'{\"type\":\"error\",\"error\":{\"type\":\"invalid_request_error\",\"message\":\"metadata.user_id: user_id appears to contain an email address. Please send a uuid or hash value instead.\"}}'. Received Model Group=claude-3.7 Available Model Group Fallbacks=None LiteLLM Retried: 1 times, LiteLLM Max Retries: 2\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.66.0-stable\n\n",
      "state": "closed",
      "author": "raz-alon",
      "author_type": "User",
      "created_at": "2025-04-17T13:31:26Z",
      "updated_at": "2025-06-14T15:41:36Z",
      "closed_at": "2025-06-14T15:41:36Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 18,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10106/reactions",
        "total_count": 4,
        "+1": 4,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10106",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10106",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:07.592528",
      "comments": [
        {
          "author": "akorb90",
          "body": "Can confirm this bug.\n\nIn Open-WebUI you can fix that when using Oauth by changing the username claim:\nOAUTH_USERNAME_CLAIM: \"preferred_username\"\nMight differ in other peoples setup though.",
          "created_at": "2025-04-19T04:55:24Z"
        },
        {
          "author": "foegra",
          "body": "i got fixed it by specifying antropic models in config files with skipping params with this: `additional_drop_params: [\"user\"]`\n\nso it looks like this:\n\n```\nmodel_list:\n  - model_name: \"anthropic/*\"\n    litellm_params:\n      model: \"anthropic/*\"\n      api_key: \"api-key\"\n      additional_drop_params:",
          "created_at": "2025-05-31T18:28:26Z"
        },
        {
          "author": "fkrauthan",
          "body": "It would be great if LiteLLM could just have an option to auto hash user param in this type of instance (or for claude specifically if user is an email auto md5 it or something).",
          "created_at": "2025-06-05T00:39:32Z"
        },
        {
          "author": "fkrauthan",
          "body": "@krrishdholakia Is this not yet released in the latest `main-stable`. I am running the latest image but I still get the error: `{\"type\":\"invalid_request_error\",\"message\":\"metadata.user_id: user_id appears to contain an email address. Please send a uuid or hash value instead.\"}` (I am using open webu",
          "created_at": "2025-06-11T03:30:29Z"
        },
        {
          "author": "fkrauthan",
          "body": "I assume the issue is that `map_openai_params` is already writing `metadata.user_id` and that new change does not clear it if it is already in `optional_metadata` and the value is invalid?",
          "created_at": "2025-06-11T03:38:42Z"
        }
      ]
    },
    {
      "issue_number": 11724,
      "title": "[Bug]: #11097 broke HTTP request caching via `vcrpy`",
      "body": "### What happened?\n\nhttps://github.com/BerriAI/litellm/pull/11097 merged in as https://github.com/BerriAI/litellm/commits/86cdb8382b15a470d06f9b3c43a91c96611977a5/ and released as v1.71.0 has broken unit tests that cache HTTP requests using `vcrpy`.\n\n1. Installing `litellm` just before https://github.com/BerriAI/litellm/pull/11097 via `uv pip install git+https://github.com/BerriAI/litellm@2efaa3cf36eaac4eeb21aecbf98bdce715016a6b`: HTTP caching works fine\n2. Installing `litellm` with https://github.com/BerriAI/litellm/pull/11097 via `uv pip install git+https://github.com/BerriAI/litellm@86cdb8382b15a470d06f9b3c43a91c96611977a5`: unit tests break because HTTP requests are now improperly cached and retrieved.\n    - A failure mode I have seen is a completion's `Choice`'s `role` is unexpectedly `None` in streaming mode.\n\nTo reproduce the error:\n\n1. Check out LDP v0.27.0: https://github.com/Future-House/ldp/releases/tag/v0.27.0\n2. Go to `TestLiteLLMModel.test_call_single`: https://github.com/Future-House/ldp/blob/v0.27.0/packages/lmi/tests/test_llms.py#L269\n3. Delete the corresponding VCR cassettes in `cassettes/`\n4. Run the test\n\nI am using Python 3.12.8 with:\n\n```none\nhttpx                     0.28.1\nopenai                    1.86.0\npydantic                  2.11.6\npytest                    8.4.0\npytest-recording          0.13.4\nvcrpy                     7.0.0\n```\n\n---\n\nAlternately, what could have happened is `litellm` developed itself into some known bug of `vcrpy`, I am not sure here.\n\n### Relevant log output\n\n```shell\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for Message\nE       role\nE         Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\nE           For further information visit https://errors.pydantic.dev/2.11/v/string_type\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.71.0\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "jamesbraza",
      "author_type": "User",
      "created_at": "2025-06-14T07:29:34Z",
      "updated_at": "2025-06-14T07:29:34Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11724/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11724",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11724",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:07.785002",
      "comments": []
    },
    {
      "issue_number": 11722,
      "title": "[Bug]: additional_drop_params not work when using /chat/completions -> /responses bridge",
      "body": "### What happened?\n\nWhen using the responses API only model through the chat completions bridge, the `additional_drop_params` parameter does not take effect.\n\nLiteLLM Proxy config\n```yaml\n...\n      - model_name: gpt-o3-pro\n        litellm_params:\n          model: openai/o3-pro\n          api_base: os.environ/OPENAI_API_BASE\n          api_key: os.environ/OPENAI_API_KEY\n          input_cost_per_token: 2e-5\n          output_cost_per_token: 8e-5\n          additional_drop_params: [\"temperature\", \"top_p\", \"top_k\"]\n...\n```\n\nMake a request to litellm chat completions api to use o3 pro model w/ temperature param then:\n\n<code>litellm.BadRequestError: OpenAIException - { \"error\": { \"message\": \"Unsupported parameter: 'temperature' is not supported with this model.\", \"type\": \"invalid_request_error\", \"param\": \"temperature\", \"code\": null } } {\"error\":{\"message\":\"litellm.BadRequestError: OpenAIException - {\\n \\\"error\\\": {\\n \\\"message\\\": \\\"Unsupported parameter: 'temperature' is not supported with this model.\\\",\\n \\\"type\\\": \\\"invalid_request_error\\\",\\n \\\"param\\\": \\\"temperature\\\",\\n \\\"code\\\": null\\n }\\n}\",\"type\":null,\"param\":null,\"code\":\"400\"}}\n</code>\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.72.4 \n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "ZeroClover",
      "author_type": "User",
      "created_at": "2025-06-14T06:30:55Z",
      "updated_at": "2025-06-14T06:30:55Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11722/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11722",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11722",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:07.785017",
      "comments": []
    },
    {
      "issue_number": 11451,
      "title": "[Bug]: Custom root path (still) does not work",
      "body": "### What happened?\n\nBased on #10761, we would expect for the custom root path (as referred to in [docs](https://docs.litellm.ai/docs/proxy/custom_root_ui) to work. Meaning we set `SERVER_ROOT_PATH` as an environment variable when starting the litellm proxy. \n\nHowever, it does not. For example, in setting `SERVER_ROOT_PATH=/proxy`, we see the following:\n- Upon visiting the Swagger UI, the main html loads, and attempts to load its dependent scripts from the bare URL, not using the custom path.\n- Upon visiting the management UI, the main HTML loads, but again the dependent scripts attempt to load from the bare URL, not using the custom path. \n     - This problem may not be immediately obvious if you are directly accessing the container without a proxy in front that will ONLY allow traffic with the custom base path. If litellm receives the request for the js/css at the non-custom base path, it will redirect those requests to the correct custom path, so it appears to work in bench testing. \n\nIf we add in the old setting `PROXY_BASE_URL` that is no longer listed on that docs page:\n- The Swagger UI now loads correctly\n- The management UI's html still refers to the wrong paths\n- The `/litellm/.well-known/litellm-ui-config` API does show the correct base urls\n\nI have tried a number of versions, up to and including the official docker images from [v1.72.1-nightly](https://github.com/BerriAI/litellm/releases/tag/v1.72.1-nightly) to no avail. \n\nI see that @krrishdholakia closed the bug report as completed/fixed back in v1.71.2, but I am curious to here if they actually tested to make sure it worked...\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.1-nightly\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "ak11234",
      "author_type": "User",
      "created_at": "2025-06-05T18:49:44Z",
      "updated_at": "2025-06-13T19:04:41Z",
      "closed_at": "2025-06-05T23:36:48Z",
      "labels": [
        "bug",
        "awaiting: user response"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 19,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11451/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11451",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11451",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:07.785027",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Hi @ak11234 - can you share a video of what you're seeing? ",
          "created_at": "2025-06-05T18:55:52Z"
        },
        {
          "author": "krrishdholakia",
          "body": "we were able to confirm this is working for another user as well ",
          "created_at": "2025-06-05T18:56:13Z"
        },
        {
          "author": "krrishdholakia",
          "body": "The key thing is to reserve `/litellm` route on the base url \n\nso `${BASE_URL}/litellm` -> point to litellm proxy \n\nNot `${BASE_URL}/my-custom-path/litellm` ",
          "created_at": "2025-06-05T18:57:01Z"
        },
        {
          "author": "ak11234",
          "body": "> The key thing is to reserve `/litellm` route on the base url\n> \n> so `${BASE_URL}/litellm` -> point to litellm proxy\n> \n> Not `${BASE_URL}/my-custom-path/litellm`\n\nAhh. That expected behavior does align with what I am seeing (photo below), thank you for clarifying. \n\nHowever, I must say that this ",
          "created_at": "2025-06-05T19:06:40Z"
        },
        {
          "author": "krrishdholakia",
          "body": "The `/litellm` route is also where we serve our `.well-known` to know how to construct the custom url.\n\nThis allows the UI to know how to call the backend. \n\nHow have you seen other products handle this? ",
          "created_at": "2025-06-05T19:44:22Z"
        }
      ]
    },
    {
      "issue_number": 7001,
      "title": "[Bug]: \"timeout\" and \"stream_timeout\" being reset in prod instance - unclear why",
      "body": "### What happened?\n\nI am setting both \"timeout\" and \"stream_timeout\" in my config.yaml like below. \r\n\r\n```yaml\r\n- model_name: \"gpt-4o\"\r\n    litellm_params:\r\n      model: \"azure/gpt-4o\"\r\n      api_key: \"os.environ/AZURE_API_KEY_EU2\"\r\n      api_base: \"os.environ/AZURE_API_BASE_EU2\"\r\n      api_version: \"os.environ/AZURE_API_VERSION\"\r\n      timeout: 300\r\n      stream_timeout: 120\r\n      tpm: 5000000\r\n      tags: [\"East US 2\"]\r\n    model_info:\r\n      mode: \"chat\"\r\n      base_model: \"azure/gpt-4o\"\r\n      <truncated>\r\n\r\n```\r\n\r\nI do not set `request_timeout` under litellm_settings:\r\n```yaml\r\nlitellm_settings:\r\n  num_retries: 0\r\n  callbacks: callback.handler\r\n  drop_params: true\r\n  # request_timeout: 120\r\n  # set_verbose: true\r\n```\r\n\r\nWhat I am observing is that some requests (likely hung for a reason or another) do not get timed out until they reach exactly 6000s, which is the default for request_timeout: https://github.com/BerriAI/litellm/blob/04238cd9a97e802b2637924b8eed46c9012878c6/litellm/__init__.py#L297\r\n\r\nI therefore question whether `timeout` and `stream_timeout` really do what they are supposed to do: https://docs.litellm.ai/docs/proxy/reliability#custom-timeouts-stream-timeouts---per-model\n\n### Relevant log output\n\n_No response_\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.53.1\n\n### Twitter / LinkedIn details\n\nhttps://www.linkedin.com/in/jeromeroussin/",
      "state": "closed",
      "author": "jeromeroussin",
      "author_type": "User",
      "created_at": "2024-12-03T13:05:27Z",
      "updated_at": "2025-06-13T17:38:43Z",
      "closed_at": "2025-03-20T22:06:45Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 13,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/7001/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "krrishdholakia"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/7001",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/7001",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:07.995462",
      "comments": [
        {
          "author": "jeromeroussin",
          "body": "Here is the timeout stacktrace for one of those 6000s (non-streaming) timeouts if that helps:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.12/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\r\n    yield\r\n  File \"/usr/local/lib/python3.12/si",
          "created_at": "2024-12-03T14:28:28Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Thinking through this: \r\n- num_retries for the deployment would be passed into the `.completion()` function\r\n- at the router, for async_function_with_retries -> this the num retries across the model group,  \r\n\r\nso if a user is setting a num retry on a specific model in a list,\r\n\r\neither it means ret",
          "created_at": "2024-12-14T20:19:10Z"
        },
        {
          "author": "krrishdholakia",
          "body": "ignore my comments - i was writing to address num retries, which has a similar issue. ",
          "created_at": "2024-12-14T20:43:33Z"
        },
        {
          "author": "krrishdholakia",
          "body": "I can see the timeout being correctly passed for anthropic. i wonder if this is azure specific. testing now ",
          "created_at": "2024-12-14T20:52:00Z"
        },
        {
          "author": "krrishdholakia",
          "body": "i see the timeout being correctly passed to azure as well ",
          "created_at": "2024-12-14T20:57:52Z"
        }
      ]
    },
    {
      "issue_number": 11709,
      "title": "[Bug]: Call to Action for enterprise in LiteLLM Proxy UI go to a dead page",
      "body": "### What happened?\n\nWhen going on organization page, there is a CTA to send to ask for an enterprise test key:\nhttps://litellm.ai/pricing\n\nAs you can see, it's a dead page, nothing is there.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "sp-aaflalo",
      "author_type": "User",
      "created_at": "2025-06-13T16:06:49Z",
      "updated_at": "2025-06-13T17:09:48Z",
      "closed_at": "2025-06-13T17:09:48Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11709/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11709",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11709",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:08.254635",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "fixed https://github.com/BerriAI/litellm/pull/11711/commits/44aa8db1abc433bd0e69398b21054bc8c82fb531\n\nthanks",
          "created_at": "2025-06-13T17:09:48Z"
        }
      ]
    },
    {
      "issue_number": 11585,
      "title": "[Bug]: Vertex AI pass-through not working with global location",
      "body": "### What happened?\n\nUsing global location with vertex AI pass-through causes a Not Found error, due to the base url not being set correctly. Here's what I get back:\n\n```\n'x-litellm-model-api-base': 'https://global-aiplatform.googleapis.com/v1/...\n```\n\nThe correct base url should be `https://aiplatform.googleapis.com`.\n\nIt looks like this issue has been fixed in several places (https://github.com/BerriAI/litellm/pull/10658, https://github.com/BerriAI/litellm/pull/11194), but it looks like not in the pass-through endpoints URL construction.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.0-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "alvarosevilla95",
      "author_type": "User",
      "created_at": "2025-06-10T11:50:30Z",
      "updated_at": "2025-06-13T16:51:05Z",
      "closed_at": "2025-06-13T16:51:05Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11585/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11585",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11585",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:08.412479",
      "comments": []
    },
    {
      "issue_number": 9233,
      "title": "[Bug]: Internal Server Error (500) when accessing the LiteLLM UI",
      "body": "### What happened?\n\nA bug happened!\n\nThe error message indicates that a 'NoneType' object is not iterable.\n\n![Image](https://github.com/user-attachments/assets/c4b40919-1918-41d9-aa93-0e31d91f3825)\n\n### Relevant log output\n\n```shell\nlitellm-1     | INFO:     172.19.0.1:51180 - \"GET /sso/key/generate HTTP/1.1\" 200 OK\nlitellm-1     | INFO:     172.19.0.1:51180 - \"GET /sso/key/generate HTTP/1.1\" 200 OK\nlitellm-1     | INFO:     172.19.0.3:50130 - \"GET /metrics HTTP/1.1\" 404 Not Found\nlitellm-1     | INFO:     172.19.0.1:51180 - \"POST /login HTTP/1.1\" 303 See Other\nlitellm-1     | INFO:     172.19.0.1:51180 - \"GET /ui/?userID=default_user_id HTTP/1.1\" 304 Not Modified\nlitellm-1     | INFO:     172.19.0.1:51180 - \"GET /models HTTP/1.1\" 200 OK\nlitellm-1     | INFO:     172.19.0.1:51194 - \"GET /sso/get/ui_settings HTTP/1.1\" 200 OK\nlitellm-1     | INFO:     172.19.0.1:51180 - \"GET /models HTTP/1.1\" 200 OK\nlitellm-1     | INFO:     172.19.0.1:51194 - \"GET /sso/get/ui_settings HTTP/1.1\" 200 OK\nlitellm-1     | INFO:     172.19.0.1:51180 - \"GET /models HTTP/1.1\" 200 OK\nlitellm-1     | INFO:     172.19.0.1:51180 - \"GET /models HTTP/1.1\" 200 OK\nlitellm-1     | INFO:     172.19.0.1:51180 - \"GET /ui.txt?_rsc=acgkz HTTP/1.1\" 404 Not Found\nlitellm-1     | 07:07:41 - LiteLLM Proxy:ERROR: internal_user_endpoints.py:395 - litellm.proxy.proxy_server.user_info(): Exception occured - 'NoneType' object is not iterable\nlitellm-1     | Traceback (most recent call last):\nlitellm-1     |   File \"/usr/local/lib/python3.11/site-packages/litellm/proxy/management_endpoints/internal_user_endpoints.py\", line 308, in user_info\nlitellm-1     |     return await _get_user_info_for_proxy_admin()\nlitellm-1     |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlitellm-1     |   File \"/usr/local/lib/python3.11/site-packages/litellm/proxy/management_endpoints/internal_user_endpoints.py\", line 431, in _get_user_info_for_proxy_admin\nlitellm-1     |     for key in _keys_in_db:\nlitellm-1     | TypeError: 'NoneType' object is not iterable\nlitellm-1     | INFO:     172.19.0.1:51194 - \"GET /user/info HTTP/1.1\" 500 Internal Server Error\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nversion = \"1.63.9\"\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "ddm21",
      "author_type": "User",
      "created_at": "2025-03-14T07:08:30Z",
      "updated_at": "2025-06-13T16:18:51Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 14,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9233/reactions",
        "total_count": 10,
        "+1": 10,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9233",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9233",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:08.412501",
      "comments": [
        {
          "author": "tykeal",
          "body": "I see the same thing with version 'main-stable'",
          "created_at": "2025-03-14T14:28:41Z"
        },
        {
          "author": "recklessop",
          "body": "same",
          "created_at": "2025-03-14T22:53:51Z"
        },
        {
          "author": "thelooter",
          "body": "Same",
          "created_at": "2025-03-15T13:05:08Z"
        },
        {
          "author": "manojkollam",
          "body": "I get the same error at login and Virtual Keys i create do not appear on the page",
          "created_at": "2025-03-15T23:20:44Z"
        },
        {
          "author": "StackDev223",
          "body": "> I get the same error at login and Virtual Keys i create do not appear on the page\n\nIm in the same boat. ",
          "created_at": "2025-03-18T23:18:32Z"
        }
      ]
    },
    {
      "issue_number": 11710,
      "title": "[Bug]: disable reasoning_effort for gemini models",
      "body": "### What happened?\n\nIn the [documentation](https://docs.litellm.ai/docs/providers/gemini#usage---thinking--reasoning_content) is says that reasoning_effort parameter can be set to \"disable\" for non reasoning tasks but in the code itsn't allowing it.\n\nhttps://github.com/BerriAI/litellm/blob/620664921902d7a9bfb29897a7b27c1a7ef4ddfb/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py#L362-L381\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.72.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "genrry",
      "author_type": "User",
      "created_at": "2025-06-13T16:08:29Z",
      "updated_at": "2025-06-13T16:14:23Z",
      "closed_at": "2025-06-13T16:14:23Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11710/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11710",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11710",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:08.651570",
      "comments": [
        {
          "author": "genrry",
          "body": "nevermind I updated to current version and is fixed there.",
          "created_at": "2025-06-13T16:14:23Z"
        }
      ]
    },
    {
      "issue_number": 11704,
      "title": "[Bug]: Usage Charts doesn't render properly Y axis Total Tokens when high amount",
      "body": "### What happened?\n\nFor Million scale Total token amounts, the Usage charts does not render properly the Y axis values, as you can see in the screenshots. The Y axis values are trimmed in the right and only show `,XXX,XXX`, leaving out the million digits.\n\n![Image](https://github.com/user-attachments/assets/531d7d07-5c67-47ee-af36-62feb1d9f031)\n![Image](https://github.com/user-attachments/assets/9f018ab7-e963-4996-85da-ec3430be3e2b)\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.71.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "andresC98",
      "author_type": "User",
      "created_at": "2025-06-13T10:30:13Z",
      "updated_at": "2025-06-13T16:01:55Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11704/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11704",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11704",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:08.828792",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "@NANDINI-star ^ ",
          "created_at": "2025-06-13T16:01:55Z"
        }
      ]
    },
    {
      "issue_number": 9754,
      "title": "Add support for using Responses API in /chat/completions spec",
      "body": "> it would be cool to be able to convert to chat api, to use o1 pro on apps that can only use chat \n\n _Originally posted by @tiagoefreitas in [#9146](https://github.com/BerriAI/litellm/issues/9146#issuecomment-2779170259)_",
      "state": "closed",
      "author": "ishaan-jaff",
      "author_type": "User",
      "created_at": "2025-04-04T16:05:30Z",
      "updated_at": "2025-06-13T14:59:53Z",
      "closed_at": "2025-06-12T05:20:19Z",
      "labels": [
        "april 2025"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 17,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9754/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9754",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9754",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:09.018006",
      "comments": [
        {
          "author": "mvrodrig",
          "body": "Cool, I'll be following this too.",
          "created_at": "2025-04-04T19:40:43Z"
        },
        {
          "author": "gavishap",
          "body": "Hi! I'm picking this up, planning to implement internal routing from  /chat/completions to the /responses API. Will aim to open a PR in the next few days. @ishaan-jaff \n",
          "created_at": "2025-04-06T14:08:53Z"
        },
        {
          "author": "mvrodrig",
          "body": "Hi @ishaan-jaff , @gavishap ! What's the status of this?",
          "created_at": "2025-04-10T12:38:26Z"
        },
        {
          "author": "paul-gauthier",
          "body": "It sure would be great to magically map `litellm.completion()` to \"just work\" against models that only support the responses API. But I haven't review the responses API to assess if this is even a reasonable thing to attempt.",
          "created_at": "2025-04-15T00:25:43Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "@paul-gauthier it's a reasonable request and one we plan on supporting, we've added this to our April 2025 roadmap. ",
          "created_at": "2025-04-15T00:35:10Z"
        }
      ]
    },
    {
      "issue_number": 11481,
      "title": "[Bug]: Admin Panel - blank page",
      "body": "### What happened?\n\nI compiled litellm using docker and started it up.  I went to the start page on port 4000 and then clicked on LiteLLM Admin Panel.  A new page popped up and it is blank.  I removed the docker file and started the whole process over again, but the same issue came up.\n\nI had litellm running last week, and I did some tweaking that caused it fail.  My fault for not backing it up before changing some lines.  I wiped my mini server and started fresh.  With this new install, litellm does not load up the login page.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.72.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "EiroaCigarMan",
      "author_type": "User",
      "created_at": "2025-06-06T12:00:23Z",
      "updated_at": "2025-06-13T14:54:47Z",
      "closed_at": "2025-06-13T14:53:35Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11481/reactions",
        "total_count": 5,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 2
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11481",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11481",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:09.219082",
      "comments": [
        {
          "author": "droezel",
          "body": "Same issue here on a fresh install",
          "created_at": "2025-06-06T20:40:25Z"
        },
        {
          "author": "VigneshwarRajasekaran",
          "body": "This issue is due to the inclusion of unwanted files in a recent commit. \nThere were changes for the new ui build [commit](https://github.com/BerriAI/litellm/commit/d408814978b2064c44f65cf1e02c169360c32785)\n\nthis was broken by the later [commit  ](https://github.com/BerriAI/litellm/commit/603bd73a17",
          "created_at": "2025-06-07T06:06:32Z"
        },
        {
          "author": "Rinatum",
          "body": "@krrishdholakia will you merge the PR?\n\nUnreal to use ui service without this fix",
          "created_at": "2025-06-08T05:00:59Z"
        },
        {
          "author": "EiroaCigarMan",
          "body": "I ended troubleshooting this issue with ChatGPT 4o.  It recommended a fresh clean install.  I am still learning how to use Docker.\n\n**Step 1: Wipe all Docker State**\n```\ndocker compose down --volumes --remove-orphans\ndocker system prune -a --volumes -f\n```\n\n**Step 2. Clone a fresh copy of litellm**\n",
          "created_at": "2025-06-12T07:34:00Z"
        },
        {
          "author": "andrewbolster",
          "body": "This is still present with  1.72.2-stable. Unclear how 'stable' having no admin UI is, and how this isn't included in any testing. ",
          "created_at": "2025-06-13T11:35:45Z"
        }
      ]
    },
    {
      "issue_number": 11702,
      "title": "[Bug]: AWS Bedrock vs Pixtral model with multimodal query (image)",
      "body": "### What happened?\n\nHello all,\n\nI'm not 100% sure if this is a \"missing\" feature or a bug but I'm not able to do a multi-modal (text+image) query on Mistral Pixtral model via Litellm on AWS bedrock.\n\n[panda.jpg](https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Grosser_Panda.JPG/1599px-Grosser_Panda.JPG)\n\nHere is my litellm config file:\n\n```bash\nmodel_list:\n  - model_name: pixtral\n    litellm_params:\n      model: bedrock/eu.mistral.pixtral-large-2502-v1:0\n    model_info:\n      supports_vision: True\nenvironment_variables:\n  AWS_REGION: eu-central-1\n```\n\nLitellm version: v1.70.1-stable\n\nTrial code:\n```python\nimport os\nfrom openai import OpenAI\nimport base64\n\nclient = OpenAI(\n  base_url='http://mylitellm/v1',\n  api_key=\n)\n\nmodel = 'pixtral'\n\n# Function to encode the image\ndef encode_image(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n\nimage_path = \"1599px-Grosser_Panda.JPG\"\n# Getting the Base64 string\nbase64_image = encode_image(image_path)\n\nresponse = client.chat.completions.create(\n    model=model,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"Describe the image in detail.\"},\n                {\n                    \"type\": \"image_input\",\n                    \"image_url\": f\"data:image/jpeg;base64,{base64_image}\"\n                },\n            ],\n        }\n    ],\n    max_tokens=1500,\n)\nprint(response[\"choices\"][0][\"message\"][\"content\"])\n\n```\n\nI tried also with \"type\": \"image\" and various other definitions of \"image_url\" but I did not succeeded ;(\n\nThank you for your help!\n\n\n### Relevant log output\n\n```shell\nFile \"/usr/lib/python3.13/site-packages/litellm/proxy/common_request_processing.py\", line 259, in base_process_llm_request\n    responses = await llm_responses\n                ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 968, in acompletion\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 944, in acompletion\n    response = await self.async_function_with_fallbacks(**kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 3492, in async_function_with_fallbacks\n    raise original_exception\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 3306, in async_function_with_fallbacks\n    response = await self.async_function_with_retries(*args, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 3684, in async_function_with_retries\n    raise original_exception\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 3575, in async_function_with_retries\n    response = await self.make_call(original_function, *args, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 3693, in make_call\n    response = await response\n               ^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 1107, in _acompletion\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 1066, in _acompletion\n    response = await _response\n               ^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/utils.py\", line 1490, in wrapper_async\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/utils.py\", line 1351, in wrapper_async\n    result = await original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/main.py\", line 530, in acompletion\n    raise exception_type(\n          ~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<3 lines>...\n        extra_kwargs=kwargs,\n        ^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2232, in exception_type\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2208, in exception_type\n    raise APIConnectionError(\n    ...<8 lines>...\n    )\nlitellm.exceptions.APIConnectionError: litellm.APIConnectionError: Invalid user message={'role': 'user', 'content': [{'type': 'text', 'text': 'Describe the image in detail.'}, {'type': 'image_input', 'image_url': 'data:image/jpeg;base64,/9j/2wBDAAQDAwQDAwQEAwQFBAQFBgoHBgYGBg0JCggKDw0QEA8NDw4RExgUERIXEg4PFRwVFxkZGxsbEBQdHx0aHxgaGxr/2wBDAQQFBQYFBgwHBwwaEQ8RGhoaGhoaGhoaGhoaGhoaGhoaGhoaGhoaGhoaGhoaGhoaGhoaGhoaGhoaGhoaGhoaGhr/wAARCAQqBj8DASIAAhEBAxEB/8QAHQAAAQUBAQEBA\nek6neqsikhthLGY47g/wge2aE9U3MwvLcCaQABTjee/NBul2Ml+EkJZNznaeR2rauqNjfKV/Z//Z'}]} at index 0. Please ensure all user messages are valid OpenAI chat completion messages.\n. Received Model Group=pixtral\nAvailable Model Group Fallbacks=None LiteLLM Retried: 1 times, LiteLLM Max Retries: 2\nInside Max Parallel Request Pre-Call Hook\nInitialized litellm callbacks, Async Success Callbacks: [<bound method Router.deployment_callback_on_success of <litellm.router.Router object at 0x7f9b78027770>>, <litellm.proxy.hooks.model_max_budget_limiter._PROXY_VirtualKeyModelMaxBudgetLimiter object at 0x7f9b78025400>, <litellm.proxy.hooks.max_budget_limiter._PROXY_MaxBudgetLimiter object at 0x7f9b77525e50>, <litellm.proxy.hooks.parallel_request_limiter._PROXY_MaxParallelRequestsHandler object at 0x7f9b77525f90>, <litellm.proxy.hooks.cache_control_check._PROXY_CacheControlCheck object at 0x7f9b77526210>, <enterprise.enterprise_hooks.managed_files._PROXY_LiteLLMManagedFiles object at 0x7f9b775e0050>, <litellm._service_logger.ServiceLogging object at 0x7f9b782beea0>]\nASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nInside Max Parallel Request Failure Hook\nuser_api_key: v5kl7oSUu2k1ASobrM7iwzYfoFwdq\nupdated_value in failure call: {'current_requests': 0, 'current_tpm': 175, 'current_rpm': 4}\nASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nInside Max Parallel Request Failure Hook\nuser_api_key: v5kl7oSUu2k1ASobrM7iwzYfoFwdq\nupdated_value in failure call: {'current_requests': 0, 'current_tpm': 175, 'current_rpm': 4}\nASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nInside Max Parallel Request Failure Hook\nuser_api_key: v5kl7oSUu2k1ASobrM7iwzYfoFwdq\nupdated_value in failure call: {'current_requests': 0, 'current_tpm': 175, 'current_rpm': 4}\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.70.1-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "Extremys",
      "author_type": "User",
      "created_at": "2025-06-13T10:00:35Z",
      "updated_at": "2025-06-13T14:16:43Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11702/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11702",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11702",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:09.437361",
      "comments": [
        {
          "author": "Extremys",
          "body": "Other logs with \"image_url\" value in the query:\n\n```bash\neSagoAJaReQx8g45Jz2o5CivpdkHUMEjJXIztPHb2r6EBNNVkAVvXbkcGjGKWxlptonXto+raOZoMXMVw22WK4cI6ge3mq4dZsem19DS7CaCWR8zBJCzEcgYH9asmiqGilZgCwDEE98881GkiQdZ35CKCLSLHHbKDNCPy0Ly7Y5b6pc22lE2HrNdTZGJ5MbAe3Pv8Aah+nag1zek6neqsikhthLGY47g/wge2aE9U3MwvLcCaQA",
          "created_at": "2025-06-13T10:17:51Z"
        },
        {
          "author": "Extremys",
          "body": "Ok I've finally been able to make it runs there the solution I've found:\n\n```python\nimport os\nfrom openai import OpenAI\nimport base64\n\nclient = OpenAI(\n  base_url='http://mylitellm/v1',\n  api_key=\n)\n\nmodel = 'pixtral'\n\n# Function to encode the image\ndef encode_image(image_path):\n    with open(image_",
          "created_at": "2025-06-13T10:34:11Z"
        },
        {
          "author": "Extremys",
          "body": "With langchain layer the model is highly hallucinating, it seems lol :\n\n```python\nimport base64\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema.messages import HumanMessage\nfrom langchain.schema.document import Document\n \nos.environ[\"OPENAI_API_KEY\"] = \"\"\nos.environ[\"OPENAI_API_BA",
          "created_at": "2025-06-13T11:03:08Z"
        },
        {
          "author": "Extremys",
          "body": "If I change jpg to jpeg I obtain : \n\n```log\nImage Description:\n\n![Image](data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEAYABgAAD/4QBaRXhpZgAATU0AKgAAAAgAAwEAAAMAAAABAAEAAIdpAAQAAAABAAAAJgAAAAAAAqACAAQAAAABAAAApKADAAQAAAABAAAApAAAADIAAAAUAAAAwodpAAQAAAABAAAAugAAAADIAAAAUAAAAwAAAABKADAAQAAAABAAAAwAAAAD/4QC",
          "created_at": "2025-06-13T11:08:28Z"
        },
        {
          "author": "Extremys",
          "body": "Other question does Litellm compatible with document file? It's not clear to me.\n\n[check this aws doc](\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/bedrock-runtime_example_bedrock-runtime_DocumentUnderstanding_Mistral_section.html)\n\n",
          "created_at": "2025-06-13T13:03:58Z"
        }
      ]
    },
    {
      "issue_number": 10399,
      "title": "[Bug]: environment_variables section of config.yaml invalid",
      "body": "### What happened?\n\nLiteLLM is throwing this error in the debug logs. No matter what I set my SALT or MASTER KEY to.\n\n### Relevant log output\n\n```shell\n05:38:16 - LiteLLM Proxy:DEBUG: utils.py:1158 - Creating Prisma Client..\n05:38:17 - LiteLLM Proxy:DEBUG: utils.py:1181 - Success - Created Prisma Client\n05:38:17 - LiteLLM Proxy:DEBUG: utils.py:2227 - PrismaClient: connect() called Attempting to Connect to DB\n05:38:17 - LiteLLM Proxy:DEBUG: utils.py:2231 - PrismaClient: DB not connected, Attempting to Connect to DB\n05:38:17 - LiteLLM Proxy:DEBUG: proxy_server.py:575 - prisma_client: <litellm.proxy.utils.PrismaClient object at 0x7fa899d85010>\n05:38:17 - LiteLLM Proxy:DEBUG: proxy_server.py:2290 - len new_models: 0\n05:38:17 - LiteLLM Proxy:ERROR: encrypt_decrypt_utils.py:60 - Error decrypting value, Did your master_key/salt key change recently? \nError: Invalid base64-encoded string: number of data characters (13) cannot be 1 more than a multiple of 4\nSet permanent salt key - https://docs.litellm.ai/docs/proxy/prod#5-set-litellm-salt-key\n05:38:17 - LiteLLM Proxy:ERROR: encrypt_decrypt_utils.py:60 - Error decrypting value, Did your master_key/salt key change recently? \nError: Incorrect padding\nSet permanent salt key - https://docs.litellm.ai/docs/proxy/prod#5-set-litellm-salt-key\n05:38:17 - LiteLLM Proxy:ERROR: encrypt_decrypt_utils.py:60 - Error decrypting value, Did your master_key/salt key change recently? \nError: Incorrect padding\nSet permanent salt key - https://docs.litellm.ai/docs/proxy/prod#5-set-litellm-salt-key\n05:38:17 - LiteLLM Proxy:ERROR: encrypt_decrypt_utils.py:60 - Error decrypting value, Did your master_key/salt key change recently? \nError: Invalid base64-encoded string: number of data characters (5) cannot be 1 more than a multiple of 4\nSet permanent salt key - https://docs.litellm.ai/docs/proxy/prod#5-set-litellm-salt-key\n05:38:17 - LiteLLM Proxy:DEBUG: proxy_server.py:2443 - Overriding Default 'alerting' values with db 'alerting' values.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:4000 (Press CTRL+C to quit)\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.67.4\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "andrefecto",
      "author_type": "User",
      "created_at": "2025-04-29T05:49:53Z",
      "updated_at": "2025-06-13T14:05:56Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10399/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10399",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10399",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:09.606141",
      "comments": [
        {
          "author": "andrefecto",
          "body": "After more testing, this does seem to happen if I am setting \"LITELLM_MODE: \"PRODUCTION\" and I attempt to use environment_variables inside of the config.yaml.",
          "created_at": "2025-05-02T00:19:05Z"
        },
        {
          "author": "snoyiatk",
          "body": "Faced the same issue, turn out it's a bug, config.yaml environment_variable (almost) never work\n#10547 ",
          "created_at": "2025-05-04T13:29:31Z"
        },
        {
          "author": "ZeroClover",
          "body": "Same issue here, and it's generating a lot of noise in my Sentry. Is there any temporary workaround for this issue?",
          "created_at": "2025-06-13T14:05:56Z"
        }
      ]
    },
    {
      "issue_number": 11703,
      "title": "[Bug]: Langfuse v3 error with liteLLM",
      "body": "### What happened?\n\nGetting the following error when using languse with liteLLM when logging\n\n### Relevant log output\n\n```shell\nERROR:LiteLLM:litellm.utils.py::function_setup() - [Non-Blocking] Error in function_setup\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/site-packages/litellm/utils.py\", line 580, in function_setup\n    set_callbacks(callback_list=callback_list, function_id=function_id)\n  File \"/usr/local/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py\", line 2773, in set_callbacks\n    raise e\n  File \"/usr/local/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py\", line 2749, in set_callbacks\n    langFuseLogger = LangFuseLogger(\n                     ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/litellm/integrations/langfuse/langfuse.py\", line 89, in __init__\n    self.Langfuse: Langfuse = self.safe_init_langfuse_client(parameters)\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/litellm/integrations/langfuse/langfuse.py\", line 142, in safe_init_langfuse_client\n    langfuse_client = Langfuse(**parameters)\n                      ^^^^^^^^^^^^^^^^^^^^^^\nTypeError: Langfuse.__init__() got an unexpected keyword argument 'sdk_integration'\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.17.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "kprubann",
      "author_type": "User",
      "created_at": "2025-06-13T10:09:33Z",
      "updated_at": "2025-06-13T13:48:12Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11703/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11703",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11703",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:09.790969",
      "comments": [
        {
          "author": "ootkin",
          "body": "Same Happening to me, I think we should wait for a patch",
          "created_at": "2025-06-13T13:48:12Z"
        }
      ]
    },
    {
      "issue_number": 11098,
      "title": "[Feature]: Integration with ElevenLabs",
      "body": "### The Feature\n\nHello. When is the expected release date for the integration with ElevenLabs?\n\n### Motivation, pitch\n\n.\n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "wagnerfrana94",
      "author_type": "User",
      "created_at": "2025-05-23T18:17:54Z",
      "updated_at": "2025-06-13T13:22:30Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11098/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11098",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11098",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:09.959208",
      "comments": [
        {
          "author": "marco-ixc",
          "body": "Yeah, would be great to see this coming out soon.",
          "created_at": "2025-05-23T19:28:49Z"
        },
        {
          "author": "teashawn",
          "body": "+1",
          "created_at": "2025-06-13T13:22:30Z"
        }
      ]
    },
    {
      "issue_number": 11623,
      "title": "[Bug]: errors when using database and config models",
      "body": "### What happened?\n\nOur team recently noticed a bug when switching from only sourcing models from the litellm configuration file to using database models. When setting STORE_MODEL_IN_DB='True' using the existing configured models from a config file (1) result in 400 HTTP Bad Request errors and (2) the models intermittently display and disappear in the admin UI. Multiple members of my team are able to confirm both issues.\n\nIt seems like litellm cannot handle both database and config models simultaneously.\n\nTo reproduce the issue:\n1. Add config file with some initial models (e.g. azure gpt-4.1)\n2. test to verify model requests succeed and models display in admin ui \n3. set STORE_MODEL_IN_DB='True'\n4. add a different model from the same LLM provider (e.g. azure gpt-4.1-mini)\n5. test to verify initial (e.g. gpt-4.1) model requests fail and admin ui glitches as mentioned above\n\nWorkaround:\n- duplicate model configuration in database\n\n### Relevant log output\n\n```shell\n\nLevel: High\nTimestamp: 17:25:40\n\nMessage: LLM API call failed: `litellm.BadRequestError: You passed in model=gpt-4.1. There is no 'model_name' with this string . Received Model Group=gpt-4.1\nAvailable Model Group Fallbacks=None`\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.72.0\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "lucinvitae",
      "author_type": "User",
      "created_at": "2025-06-11T13:29:00Z",
      "updated_at": "2025-06-13T10:50:18Z",
      "closed_at": "2025-06-13T10:50:18Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11623/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11623",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11623",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:10.142279",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Hey @lucinvitae, litellm is able to handle both simultaneusly - we run the same logic in our staging and i can see it working. \n\nCan you share your proxy startup logs with `--detailed_debug` enabled? it might show what's happening",
          "created_at": "2025-06-11T16:06:33Z"
        },
        {
          "author": "lucinvitae",
          "body": "> Hey [@lucinvitae](https://github.com/lucinvitae), litellm is able to handle both simultaneusly - we run the same logic in our staging and i can see it working.\n> \n> Can you share your proxy startup logs with `--detailed_debug` enabled? it might show what's happening\n\nThanks for the quick reply! Wi",
          "created_at": "2025-06-11T16:14:02Z"
        },
        {
          "author": "lucinvitae",
          "body": "Hmm, now I'm unable to reproduce the issue in our dev env.\n\nThere was one other thing that may have led to this broken state during the time this issue occurred in our prd env: I tried to create LLM credentials for some of the azure models, and then replaced those credentials. The bad state where co",
          "created_at": "2025-06-13T10:50:18Z"
        }
      ]
    },
    {
      "issue_number": 10314,
      "title": "[Feature]: Add support for OpenAI Image Editing & Inpainting endpoint (/v1/images/edits) - New model gpt-image-1",
      "body": "### The Feature\n\nHello @krrishdholakia , @ishaan-jaff !\n\nI‚Äôd like to request support for the OpenAI Image Editing and Inpainting endpoint (/v1/images/edits) using the `gpt-image-1` model. Below is a minimal Python example demonstrating a direct request to OpenAI:\n\n```\nimport os\nimport requests\nimport base64\n\napi_key = os.getenv(\"OPENAI_API_KEY\")\n\nurl = \"https://api.openai.com/v1/images/edits\"\nheaders = {\n    \"Authorization\": f\"Bearer {api_key}\"\n}\n\n# Prepare the files for image editing/inpainting\nfiles = [\n    (\"image[]\", (\"body-lotion.png\", open(\"body-lotion.png\", \"rb\"), \"image/png\")),\n    (\"image[]\", (\"bath-bomb.png\",   open(\"bath-bomb.png\",   \"rb\"), \"image/png\"))\n]\n\ndata = {\n    \"model\":  \"gpt-image-1\",\n    \"prompt\": \"Create a lovely gift basket with these four items in it\"\n}\n\nresponse = requests.post(url, headers=headers, files=files, data=data)\nresponse.raise_for_status()\n\n# Print the x-request-id\nprint(\"x-request-id:\", response.headers.get(\"x-request-id\"))\n\n# Decode and save the returned image\nresult = response.json()\nb64 = result[\"data\"][0][\"b64_json\"]\nimg_bytes = base64.b64decode(b64)\nwith open(\"gift-basket.png\", \"wb\") as img_file:\n    img_file.write(img_bytes)\n\nprint(\"Image saved as gift-basket.png\")\n```\n\nOpenAI doc: https://platform.openai.com/docs/api-reference/images/createEdit\n\nRelated issues requesting the same endpoint:\n\n-  [#6065](https://github.com/BerriAI/litellm/issues/6065)\n\n- [#6772](https://github.com/BerriAI/litellm/issues/6772)\n\n- [#4922](https://github.com/BerriAI/litellm/issues/4922#issuecomment-2387821722)\n\nThank you for considering this enhancement!\n\n### Motivation, pitch\n\nAdding support for the /v1/images/edits endpoint in Litellm would unlock these advanced editing and inpainting workflows directly in applications, keeping Litellm at the cutting edge of AI-powered creativity.\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "mvrodrig",
      "author_type": "User",
      "created_at": "2025-04-25T14:27:42Z",
      "updated_at": "2025-06-13T09:43:21Z",
      "closed_at": "2025-05-24T17:18:02Z",
      "labels": [
        "enhancement",
        "april 2025"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 24,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10314/reactions",
        "total_count": 16,
        "+1": 16,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "krrishdholakia"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10314",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10314",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:10.347912",
      "comments": []
    },
    {
      "issue_number": 11700,
      "title": "[Bug]: [Bedrock] `us.meta.llama4-scout-17b-instruct-v1:0` model automatically trigger all tools with any prompt",
      "body": "### What happened?\n\n```\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n                },\n                \"required\": [\"location\"],\n            },\n        },\n    }\n]\n\n\nt = time.time()\nresponse = completion(\n    model=\"us.meta.llama4-scout-17b-instruct-v1:0\",\n    temperature=0,\n    messages=[{\"role\": \"user\", \"content\": \"How are you?\"}],\n    tools=tools,\n)\n\nprint(response)\n```\n\n### Relevant log output\n\n```shell\nModelResponse(id='chatcmpl-35b04bf5-0304-4b00-8349-011049c47085', created=1749806018, model='us.meta.llama4-scout-17b-instruct-v1:0', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content='', role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=0, function=Function(arguments='{\"location\": \"San Francisco, CA\", \"unit\": \"fahrenheit\"}', name='get_current_weather'), id='tooluse_UgDJZLJuRmqqpiUJ2FYnIw', type='function')], function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=28, prompt_tokens=254, total_tokens=282, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.72.4\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "dttran-glo",
      "author_type": "User",
      "created_at": "2025-06-13T09:14:28Z",
      "updated_at": "2025-06-13T09:22:39Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11700/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11700",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11700",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:10.347932",
      "comments": []
    },
    {
      "issue_number": 11666,
      "title": "[Bug]: no attribute 'choices' when calling Azure OpenAI via SDK",
      "body": "### What happened?\n\nTrying to integrate Azure OpenAI into our chat app using a gpt-4.1-mini model adn the LiteLLM SDK.\n\nWhen i try to call it in my code\n\n```\nlitellm._turn_on_debug()\nresult = litellm.completion(\n      model=\"azure/gpt-4.1-mini\",\n      messages=[{\"role\": \"user\", \"content\": \"good morning\"}],\n      stream=False\n)\n```\n\ni get the attached log output (with error caught in my application's try-catch at the last line)\n\nI confirmed, this curl command works:\n```\ncurl -X POST 'https://***.openai.azure.com/openai/deployments/gpt-4.1-mini/chat/completions?api-version=2024-02-15-preview' -H 'api-key: ***' -H 'Content-Type: application/json' -d '{\"model\": \"gpt-4.1-mini\", \"messages\": [{\"role\": \"user\", \"content\": \"good morning\"}]}'\n```\n\nThe curl commands in the debug output do NOT work. But i also see a POST request with a http 200 - that seems good?\n\nAny advice on what i need to fix or where to drill into this to find the root cause?\n\n_Update:_\nI also tried different API versions and models with no luck.\n\n### Relevant log output\n\n```shell\n18:28:11 - LiteLLM:DEBUG: litellm_logging.py:461 - self.optional_params: {}\n2025-06-12 18:28:11 - self.optional_params: {}\n18:28:11 - LiteLLM:DEBUG: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n2025-06-12 18:28:11 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n18:28:11 - LiteLLM:INFO: utils.py:3101 - \nLiteLLM completion() model= gpt-4.1-mini; provider = azure\n2025-06-12 18:28:11 - \nLiteLLM completion() model= gpt-4.1-mini; provider = azure\n18:28:11 - LiteLLM:DEBUG: utils.py:3104 - \nLiteLLM: Params passed to completion() {'model': 'gpt-4.1-mini', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'azure', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'good morning'}], 'thinking': None, 'web_search_options': None}\n2025-06-12 18:28:11 - \nLiteLLM: Params passed to completion() {'model': 'gpt-4.1-mini', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'azure', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'good morning'}], 'thinking': None, 'web_search_options': None}\n18:28:11 - LiteLLM:DEBUG: utils.py:3107 - \nLiteLLM: Non-Default params passed to completion() {'stream': False}\n2025-06-12 18:28:11 - \nLiteLLM: Non-Default params passed to completion() {'stream': False}\n18:28:11 - LiteLLM:DEBUG: utils.py:3736 - Azure optional params - api_version: api_version=None, litellm.api_version=None, os.environ['AZURE_API_VERSION']=2024-02-15-preview\n2025-06-12 18:28:11 - Azure optional params - api_version: api_version=None, litellm.api_version=None, os.environ['AZURE_API_VERSION']=2024-02-15-preview\n18:28:11 - LiteLLM:DEBUG: utils.py:340 - Final returned optional params: {'stream': False, 'extra_body': {}}\n2025-06-12 18:28:11 - Final returned optional params: {'stream': False, 'extra_body': {}}\n18:28:11 - LiteLLM:DEBUG: litellm_logging.py:461 - self.optional_params: {'stream': False, 'extra_body': {}}\n2025-06-12 18:28:11 - self.optional_params: {'stream': False, 'extra_body': {}}\n18:28:11 - LiteLLM:DEBUG: litellm_logging.py:908 - \n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://***.openai.azure.com \\\n-H 'api_key: ****' -H 'azure_ad_token: *****' \\\n-d '{'model': 'gpt-4.1-mini', 'messages': [{'role': 'user', 'content': 'good morning'}], 'stream': False, 'extra_body': {}}'\n\n\n2025-06-12 18:28:11 - \n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://***.openai.azure.com \\\n-H 'api_key: ****' -H 'azure_ad_token: *****' \\\n-d '{'model': 'gpt-4.1-mini', 'messages': [{'role': 'user', 'content': 'good morning'}], 'stream': False, 'extra_body': {}}'\n\n\n18:28:11 - LiteLLM:DEBUG: common_utils.py:389 - Initializing Azure OpenAI Client for gpt-4.1-mini, Api Base: https://***.openai.azure.com, Api Key:***************\n2025-06-12 18:28:11 - Initializing Azure OpenAI Client for gpt-4.1-mini, Api Base: https://***.openai.azure.com, Api Key:***************\n2025-06-12 18:28:12 - HTTP Request: POST https://***.openai.azure.com/openai/deployments/gpt-4.1-mini/chat/completions?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\n18:28:12 - LiteLLM:DEBUG: exception_mapping_utils.py:2300 - Logging Details: logger_fn - None | callable(logger_fn) - False\n2025-06-12 18:28:12 - Logging Details: logger_fn - None | callable(logger_fn) - False\n18:28:12 - LiteLLM:DEBUG: litellm_logging.py:2165 - Logging Details LiteLLM-Failure Call: []\n2025-06-12 18:28:12 - Logging Details LiteLLM-Failure Call: []\n2025-06-12 18:28:12 - ERROR - Error prompting the model: litellm.APIError: AzureException APIError - 'LegacyAPIResponse' object has no attribute 'choices\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.72.4\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "leakb",
      "author_type": "User",
      "created_at": "2025-06-12T17:01:57Z",
      "updated_at": "2025-06-13T09:07:40Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11666/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11666",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11666",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:10.347939",
      "comments": []
    },
    {
      "issue_number": 9251,
      "title": "[Feature]: Codestral from provider azure_ai",
      "body": "### The Feature\n\nHi! \n\nI would love support for the /fim/completions endpoint when hosting Codestral through Azure AI. The endpoint like this: https://Codestral-2501-xxxxxx.models.ai.azure.com/v1/fim/completions\n\nThere seems to be hacks around it using model provider _text-completion-codestral_ but I haven't gotten it to work. \n\nI've tried both of these setups:\n\n```\n  - model_name: codestral-2501\n    litellm_params:\n      model: text-completion-codestral/codestral-2501\n      api_key: os.environ/AZURE_AI_API_KEY\n      api_base: https://Codestral-2501-xxxx.models.ai.azure.com/v1/fim/completions\n```\nand\n```\n  - model_name: codestral-2501\n    litellm_params:\n      model: azure_ai/codestral-2501\n      api_key: os.environ/AZURE_AI_API_KEY\n      api_base: https://Codestral-2501-xxxx.models.ai.azure.com/v1/fim/completions\n```\n\nPlease advise on if this should work or if this is indeed a feature we need!\n\n### Motivation, pitch\n\nThis would enable usage of code autocompletion for Codestral hosted from Azure AI. A pretty common use case \n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "gustavhertz",
      "author_type": "User",
      "created_at": "2025-03-14T15:27:28Z",
      "updated_at": "2025-06-13T08:47:33Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 16,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9251/reactions",
        "total_count": 3,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9251",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9251",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:10.347944",
      "comments": [
        {
          "author": "FabianHertwig",
          "body": "This seems to be the workaround https://github.com/BerriAI/litellm/issues/5502\n\nLinking this for reference. I am also interested in a solution.",
          "created_at": "2025-03-17T14:35:17Z"
        },
        {
          "author": "gustavhertz",
          "body": "@FabianHertwig Thanks for the link. The suggested workaround seems to work for `codestral-latest` and `codestral-2407` but not for `codestral-2501` which is the model I'm interested in. I tried the linked suggestion without any success.\n\nI am interested in seeing if anyone got a workaround to work :",
          "created_at": "2025-03-17T14:45:51Z"
        },
        {
          "author": "FabianHertwig",
          "body": "For me it works with this config and sending requests through Python, but I want to make it work with continue.dev, which fails so far.\n\nConfig: \n\n```yaml\n  - model_name: codestral-latest\n    litellm_params:\n      model: text-completion-codestral/codestral-2501\n      api_base: \"https://Codestral-250",
          "created_at": "2025-03-17T15:01:42Z"
        },
        {
          "author": "gustavhertz",
          "body": "I also want it to work for Continue, so let's try to make it work together! Ill try your script too.",
          "created_at": "2025-03-17T15:04:26Z"
        },
        {
          "author": "gustavhertz",
          "body": "Continue calls `\"    url = f\"{api_base.rstrip('/')}/fim/completions\"` not `url = f\"{api_base.rstrip('/')}/v1/completions\"` for autocomplete. (a bit depending on what provider you report in continue, but if you set it to Mistral it does)\n\nThat doesn't work with my setup. \n",
          "created_at": "2025-03-17T15:13:19Z"
        }
      ]
    },
    {
      "issue_number": 11231,
      "title": "[Bug]: embedding over proxy for an OpenAI-compatible endpoint not working",
      "body": "### What happened?\n\nI'm trying to use an embedding model through LiteLLM but am getting 400 errors. \n\nI've set up the embedding model as shown here:\n```json\n{\n  \"model_name\": \"text-embedding-v3\",\n  \"litellm_params\": {\n    \"input_cost_per_token\": 0,\n    \"output_cost_per_token\": 0,\n    \"custom_llm_provider\": \"openai\",\n    \"litellm_credential_name\": \"ÈòøÈáå‰∫ë-prod\",\n    \"use_in_pass_through\": false,\n    \"use_litellm_proxy\": false,\n    \"merge_reasoning_content_in_choices\": false,\n    \"model\": \"text-embedding-v3\"\n  },\n  \"model_info\": {\n    \"id\": \"11ad1dcc-dbb2-4aca-914d-f3c5878e34d9\",\n    \"db_model\": true,\n    \"input_cost_per_token\": 0,\n    \"output_cost_per_token\": 0\n  },\n  \"provider\": \"openai\",\n  \"input_cost\": 0,\n  \"output_cost\": 0,\n  \"litellm_model_name\": \"text-embedding-v3\",\n  \"cleanedLitellmParams\": {\n    \"input_cost_per_token\": 0,\n    \"output_cost_per_token\": 0,\n    \"custom_llm_provider\": \"openai\",\n    \"litellm_credential_name\": \"ÈòøÈáå‰∫ë-prod\",\n    \"use_in_pass_through\": false,\n    \"use_litellm_proxy\": false,\n    \"merge_reasoning_content_in_choices\": false\n  }\n}\n```\n\nCode I'm using to invoke LLM:\n```python\nfrom openai import OpenAI\n\n# Initialize with custom endpoint\n# client = OpenAI(\n#     api_key=\"*\",  # Replace with your actual API key\n#     base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n# )\nclient = OpenAI(\n    api_key=\"*\",  # Replace with your actual API key\n    base_url=\"https://litellm.*.net/v1\"  # Your custom endpoint\n)\n\ndef get_embeddings(text, model):\n    response = client.embeddings.create(\n        input=text,\n        model=model\n    )\n    return response.data[0].embedding  # For single text input\n\nembedding = get_embeddings(\"Sample text to embed\", model=\"text-embedding-v3\")\nprint(f\"First 5 dimensions: {embedding[:5]}\")\n```\n\nChat models with the same credentials work fine over LiteLLM, and changing the key and base url to directly call the provider works fine as well. \n\n### Relevant log output\n\n```shell\n08:52:21 - LiteLLM Proxy:ERROR: proxy_server.py:4000 - litellm.proxy.proxy_server.embeddings(): Exception occured - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=text-embedding-v3\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers. Received Model Group=text-embedding-v3\nAvailable Model Group Fallbacks=None LiteLLM Retried: 1 times, LiteLLM Max Retries: 2\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/proxy_server.py\", line 3952, in embeddings\n    responses = await llm_responses\n                ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 2675, in aembedding\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 2664, in aembedding\n    response = await self.async_function_with_fallbacks(**kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 3536, in async_function_with_fallbacks\n    raise original_exception\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 3350, in async_function_with_fallbacks\n    response = await self.async_function_with_retries(*args, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 3728, in async_function_with_retries\n    raise original_exception\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 3619, in async_function_with_retries\n    response = await self.make_call(original_function, *args, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 3737, in make_call\n    response = await response\n               ^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 2745, in _aembedding\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 2732, in _aembedding\n    response = await response\n               ^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/utils.py\", line 1492, in wrapper_async\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/utils.py\", line 1353, in wrapper_async\n    result = await original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/main.py\", line 3366, in aembedding\n    raise exception_type(\n    ...<5 lines>...\n    )\n  File \"/usr/lib/python3.13/site-packages/litellm/main.py\", line 3338, in aembedding\n    _, custom_llm_provider, _, _ = get_llm_provider(\n                                   ~~~~~~~~~~~~~~~~^\n        model=model, api_base=kwargs.get(\"api_base\", None)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\", line 373, in get_llm_provider\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\", line 350, in get_llm_provider\n    raise litellm.exceptions.BadRequestError(  # type: ignore\n    ...<8 lines>...\n    )\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=text-embedding-v3\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers. Received Model Group=text-embedding-v3\nAvailable Model Group Fallbacks=None LiteLLM Retried: 1 times, LiteLLM Max Retries: 2\nINFO:     10.20.21.250:35864 - \"POST /v1/embeddings HTTP/1.0\" 400 Bad Request\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.71.1-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "michaelmyc",
      "author_type": "User",
      "created_at": "2025-05-29T09:53:36Z",
      "updated_at": "2025-06-13T08:37:41Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11231/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11231",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11231",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:10.561585",
      "comments": [
        {
          "author": "vriesdemichael",
          "body": "I had a similiar experience with embedding models not being found, but chat completion models working just fine. I am using hosted vLLM models.\n\nAfter debugging for a while, I found that the provider part of the name was not stripped when trying to connect to the vllm server.\nSo when trying to confi",
          "created_at": "2025-06-11T13:33:19Z"
        },
        {
          "author": "michaelmyc",
          "body": "That's interesting insight. I won't be able to change the model name for a cloud provider, so that workaround seems to be busted. I haven't had the time to actually debug this, but I think this should be an easy bug to fix. ",
          "created_at": "2025-06-13T08:37:41Z"
        }
      ]
    },
    {
      "issue_number": 11699,
      "title": "[Bug]: The model gemini-2.5-flash with the merge_reasoning_content_in_choices parameter does not work in version v1.72.2-stable",
      "body": "### What happened?\n\nWhen I select gemini-2.5-flash in the stream with a merge_reasoning_content_in_choices parameter, I get this error!\n\n### Relevant log output\n\n```shell\nType:\nAPIConnectionError\nMessage:\nlitellm.APIConnectionError: unsupported operand type(s) for +=: 'NoneType' and 'str'\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/streaming_handler.py\", line 1377, in chunk_creator\n    return self.return_processed_chunk_logic(\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        completion_obj=completion_obj,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model_response=model_response,  # type: ignore\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        response_obj=response_obj,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/streaming_handler.py\", line 812, in return_processed_chunk_logic\n    self._optional_combine_thinking_block_in_choices(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        model_response=model_response\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/streaming_handler.py\", line 890, in _optional_combine_thinking_block_in_choices\n    model_response.choices[0].delta.content += (\n        \"<think>\" + reasoning_content\n    )\nTypeError: unsupported operand type(s) for +=: 'NoneType' and 'str'\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.72.2-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "YuriyTW",
      "author_type": "User",
      "created_at": "2025-06-13T08:37:02Z",
      "updated_at": "2025-06-13T08:37:09Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11699/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11699",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11699",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:10.768226",
      "comments": []
    },
    {
      "issue_number": 11698,
      "title": "[Bug]: Swagger UI doesn't load in offline environment",
      "body": "### What happened?\n\nThis looks like a regression as it was fixed (and working) with #5737.\nMaybe broken by commit 00be76abf48d5854f87ab62218b1e16d9ba384e2 ?\n\n![Image](https://github.com/user-attachments/assets/7747b3c7-5a13-490b-b727-bf8840d836a5)\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "wizche",
      "author_type": "User",
      "created_at": "2025-06-13T07:49:42Z",
      "updated_at": "2025-06-13T07:49:42Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11698/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11698",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11698",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:10.768246",
      "comments": []
    },
    {
      "issue_number": 11694,
      "title": "[Bug]: ImageResponse missing 'id' attribute causing AttributeError in image editing",
      "body": "### What happened?\n\nImage editing requests fail with AttributeError: 'ImageResponse' object has no attribute 'id' while image generation works fine. This occurs when certain logging callbacks (like Braintrust, Langfuse) try to access the id attribute from the response object.\n\nStack trace points to:\nlitellm/utils.py:6214 in get_logging_id() function\nresponse_obj.get(\"id\") fails because ImageResponse doesn't have an id attribute\nRoot Cause Analysis\nThe issue stems from different response processing paths between image generation and image editing:\nImage Generation (‚úÖ Works)\nImage Editing (‚ùå Fails)\n\nTechnical Details\nImage generation calls convert_to_model_response_object() with response_type=\"image_generation\", which routes to LiteLLMResponseObjectHandler.convert_to_image_response()\nImage editing directly creates ImageResponse(**raw_response_json) in OpenAIImageEditConfig.transform_image_edit_response()\nThe ImageResponse class (in litellm/types/utils.py) inherits from OpenAI's ImagesResponse but doesn't define an id attribute\nLogging callbacks in get_logging_id() function expect all response objects to have an id attribute.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "dhgate-group",
      "author_type": "User",
      "created_at": "2025-06-13T06:22:44Z",
      "updated_at": "2025-06-13T06:22:44Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11694/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11694",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11694",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:10.768254",
      "comments": []
    },
    {
      "issue_number": 11690,
      "title": "[Bug]: `get_formatted_prompt` should be able to handle `acompletion` request",
      "body": "### What happened?\n\nThe function `litellm.utils.get_formatted_prompt(...)` returns an empty string for `acompletion` requests because the `acompletion` type is missing from the if statement.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.72.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "QwertyJack",
      "author_type": "User",
      "created_at": "2025-06-13T05:27:24Z",
      "updated_at": "2025-06-13T05:30:33Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11690/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11690",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11690",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:10.768261",
      "comments": [
        {
          "author": "QwertyJack",
          "body": "The fix is simple: add `acompletion` to the first clause.\n\nBtw, I've found many similar issues in the codebase. To avoid repeatedly handling both sync and async types, I suggest creating a new enum class for `call_type`.",
          "created_at": "2025-06-13T05:30:33Z"
        }
      ]
    },
    {
      "issue_number": 11660,
      "title": "[Bug]: \"Resource not found\" occurred when call /images/edits with azure/gpt-image-1",
      "body": "### What happened?\n\n  curl -X POST \"http://litellm.example.com/v1/images/edits\" \\\n  -H \"Authorization: Bearer sk-xxxxxxx\" \\\n  -F \"model=azure/gpt-image-1\" \\\n  -F \"image=@otter.png\" \\\n  -F \"prompt=change it to black and white\" \\\n  -F \"n=1\" \\\n  -F \"size=1024x1024\" \\\n  -F \"response_format=url\"\n\nnot working but return \n\n{\"error\":{\"message\":\"litellm.APIError: AzureException APIError - {\\\"error\\\":{\\\"code\\\":\\\"404\\\",\\\"message\\\": \\\"Resource not found\\\"}}\",\"type\":null,\"param\":null,\"code\":\"404\"}}\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "goodosoft",
      "author_type": "User",
      "created_at": "2025-06-12T12:33:45Z",
      "updated_at": "2025-06-13T03:05:23Z",
      "closed_at": "2025-06-13T03:05:23Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11660/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11660",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11660",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:10.941837",
      "comments": [
        {
          "author": "goodosoft",
          "body": " must use \"gpt-image-1\" as model nameÔºåand must specify \"api version\"„ÄÇ",
          "created_at": "2025-06-13T03:05:23Z"
        }
      ]
    },
    {
      "issue_number": 9197,
      "title": "[Bug]: How to add response id and reasoning_content with CustomLLM?",
      "body": "### What happened?\n\nWe have a custom llm api with custom response format. We want to use CustomLLM to provide openai compatible api. How to return our response id and reasoning_cotent, cause I notice that GenericStreamingChunk only has properties blow\n```\nclass GenericStreamingChunk(TypedDict, total=False):\n    text: Required[str]\n    tool_use: Optional[ChatCompletionToolCallChunk]\n    is_finished: Required[bool]\n    finish_reason: Required[str]\n    usage: Required[Optional[ChatCompletionUsageBlock]]\n    index: int\n\n    # use this dict if you want to return any provider specific fields in the response\n    provider_specific_fields: Optional[Dict[str, Any]]\n```\nAs revelant log shows, id is created by `_generate_id()`, not by my customAPI.\nAlso, I have no idea how to return reasoning_cotent.\nIs there any examples?\n\n### Relevant log output\n\n```shell\n{\n    \"id\": \"chatcmpl-19641983-6b09-4170-b0b8-a32914e77d82\",\n    \"created\": 1741775427,\n    \"model\": \"deepseek-r1\",\n    \"object\": \"chat.completion.chunk\",\n    \"choices\": [\n        {\n            \"index\": 0,\n            \"delta\": {\n                \"content\": \"\",\n            }\n        }\n    ]\n}\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv.1.63.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "KyleZhang0536",
      "author_type": "User",
      "created_at": "2025-03-13T07:21:18Z",
      "updated_at": "2025-06-13T02:27:39Z",
      "closed_at": "2025-06-13T02:27:39Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9197/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9197",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9197",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:11.139003",
      "comments": [
        {
          "author": "James4Ever0",
          "body": "https://github.com/BerriAI/litellm/issues/9171",
          "created_at": "2025-03-13T10:02:26Z"
        },
        {
          "author": "KyleZhang0536",
          "body": "Our deepseek-r1 api is not openai compatible. So I can't use `deepseek` as provider directly\nThe incoming calls like\n```\ncurl --location 'http://xxxx:8080/v1/biz/completion' \\\n--header 'Accept: text/stream-event' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"model\": \"deepseek-r1\",\n   ",
          "created_at": "2025-03-13T12:10:16Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-12T00:01:45Z"
        },
        {
          "author": "KyleZhang0536",
          "body": "As a temporary solution, return ModelResponseStream in the custom_handler, and comment out these three lines in streaming_handler.py to be processed by the \"OpenAI chat model\".\nhttps://github.com/BerriAI/litellm/blob/61838bbbfcfa810ae39c84f4b3c75bea08df1f5a/litellm/litellm_core_utils/streaming_handl",
          "created_at": "2025-06-13T02:27:32Z"
        }
      ]
    },
    {
      "issue_number": 11681,
      "title": "[Bug]: When using Image edit with Azure OpenAI Service, the API key is set in the Authorization header.",
      "body": "### What happened?\n\nWhen using Image edit with Azure OpenAI Service, if We are using API key authentication, We need to set it in the api-key header.\nHowever, the current implementation appears to set the API key in the Authorization header.\nSetting authentication information in the Authorization header is for when using Entra ID authentication, so We believe there should be conditional logic based on the authentication method being used.\nhttps://github.com/BerriAI/litellm/blob/main/litellm/llms/azure/image_edit/transformation.py#L11-L31\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.5.dev1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "yamada-kazuhiro-tm6",
      "author_type": "User",
      "created_at": "2025-06-13T00:07:41Z",
      "updated_at": "2025-06-13T00:08:56Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11681/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11681",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11681",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:12.970732",
      "comments": []
    },
    {
      "issue_number": 3332,
      "title": "[Feature]: Proxy - Support passing through OpenAI, anthropic API Keys from request headers",
      "body": "### The Feature\n\n- User wants to pass their OpenAI, Anthropic API Key in the request header\r\n- LiteLLM should use the api key passed in the header to make the LLM API Call\r\n- This should be opt in, `litellm.use_llm_key_in_header=True` \n\n### Motivation, pitch\n\nuser request \n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "ishaan-jaff",
      "author_type": "User",
      "created_at": "2024-04-27T18:17:21Z",
      "updated_at": "2025-06-13T00:02:18Z",
      "closed_at": "2025-06-13T00:02:18Z",
      "labels": [
        "enhancement",
        "good first issue",
        "help wanted",
        "stale"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/3332/reactions",
        "total_count": 9,
        "+1": 9,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/3332",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/3332",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:12.970760",
      "comments": [
        {
          "author": "paneru-rajan",
          "body": "If we add this, then request header key can be added to the data, which then goes to the usual flow.\r\n\r\nFile: proxy_server.py\r\n\r\n```python\r\n        # Reading API KEY From header\r\n        if req_header_api_key := request.headers.get('X-API-KEY'):\r\n            data[\"api_key\"] = req_header_api_key\r\n```",
          "created_at": "2024-05-03T04:42:17Z"
        },
        {
          "author": "paneru-rajan",
          "body": "After looking into the Pull-Request feedback, i spend quite sometime figuring things out, here's is my findings:\r\n\r\n-----\r\nOpenAPI and Anthropic usage SDK and HTTP requests to serve the responses, here is how it looks like:\r\n\r\n### OpenAI Key format\r\nFollowing is rest api call, api key is being passe",
          "created_at": "2024-05-05T13:58:44Z"
        },
        {
          "author": "paneru-rajan",
          "body": "Following up, let me know feedback if there are any.\r\n\r\nCC: @krrishdholakia / @ishaan-jaff ",
          "created_at": "2024-05-17T13:29:47Z"
        },
        {
          "author": "dnetguru",
          "body": "This would be very useful for us as well.",
          "created_at": "2025-01-06T02:43:38Z"
        },
        {
          "author": "A-D-I-T-Y-A",
          "body": "Any updates on this? I think this would be a common use-case in SaaS.",
          "created_at": "2025-03-03T14:31:56Z"
        }
      ]
    },
    {
      "issue_number": 7185,
      "title": "[Bug]: Infinite loop when checking `get_openai_supported_params`",
      "body": "### What happened?\n\nWhen using `model=together_ai/meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo` and `custom_provider=None`, there is an infinite loop when trying to call `get_supported_openai_params`. [Here](https://github.com/BerriAI/litellm/blob/main/litellm/litellm_core_utils/get_supported_openai_params.py#L102), get_supported_openai_params calls through to `get_model_info` [here](https://github.com/BerriAI/litellm/blob/main/litellm/llms/together_ai/chat.py#L25), which calls back to `get_supported_openai_params` [here](https://github.com/BerriAI/litellm/blob/main/litellm/utils.py#L4548). \r\n\r\nI think it might only be triggered because of [this issue](https://github.com/BerriAI/litellm/issues/7183), but I'm not sure. \n\n### Relevant log output\n\n_No response_\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.54.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "adampauls",
      "author_type": "User",
      "created_at": "2024-12-11T22:26:29Z",
      "updated_at": "2025-06-13T00:02:15Z",
      "closed_at": "2025-06-13T00:02:15Z",
      "labels": [
        "bug",
        "stale",
        "feb 2025",
        "service availability"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/7185/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/7185",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/7185",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:13.171491",
      "comments": [
        {
          "author": "madhukar01",
          "body": "I tried reproducing this today with v1.54.1 and the model, custom provider mentioned in the issue. \nCalls to `completion`, `get_supported_openai_params` and `get_model_info` runs fine without issues.\n\nIf the issue still persists, we need additional info to reproduce this behavior.",
          "created_at": "2025-02-13T09:49:53Z"
        },
        {
          "author": "SunnyWan59",
          "body": "I think I see the infinite loop you are talking about. For some reason, it doesn't seem to affect the response, but there is an infinite loop nonetheless.  To reproduce it, just track the three functions `completion`, `get_supported_openai_params`  and `get_model_info`by putting break points there i",
          "created_at": "2025-03-06T23:23:54Z"
        },
        {
          "author": "SunnyWan59",
          "body": "However when you just run the code above normally, it works as expected, although perhaps slower. ",
          "created_at": "2025-03-06T23:24:35Z"
        },
        {
          "author": "SunnyWan59",
          "body": "Ok, the offender probably [here](https://github.com/BerriAI/litellm/blob/main/litellm/llms/together_ai/chat.py#L25).  I don't understand why get_model_info is being called here. Other `get_supported_openai_params` implementations simply return a list of params. ",
          "created_at": "2025-03-06T23:42:51Z"
        },
        {
          "author": "SunnyWan59",
          "body": "Just wrote a pr for this #9037",
          "created_at": "2025-03-07T00:28:59Z"
        }
      ]
    },
    {
      "issue_number": 9027,
      "title": "[Bug]: Admin UI Redirects Endlessly with SSO Login",
      "body": "### What happened?\n\nHi there! After setting up a completely new instance of LiteLLM with SSO authentication, I can not login to the admin UI (the page refreshes endlessly). \n\n### Steps to reproduce\n\n1. deploy the latest litellm docker image\n2. setup environment variables for SSO login\n3. navigate to `<ip_addr>:4000/ui`\n4. get redirected to SSO authorization page\n5. redirected back\n6. 4-5 repeats endlessly\n\n\n\n### Relevant log output\n\n```shell\nlitellm-1     | INFO:     XX.XX.XX.XX:61269 - \"GET /sso/key/generate HTTP/1.1\" 303 See Other\nlitellm-1     | INFO:     XX.XX.XX.XX:61269 - \"GET /get_image HTTP/1.1\" 200 OK\nlitellm-1     | INFO:     XX.XX.XX.XX:61269 - \"GET /ui.txt?_rsc=144pr HTTP/1.1\" 404 Not Found\nlitellm-1     | 09:29:22 - LiteLLM Proxy:INFO: ui_sso.py:671 - user_defined_values for creating ui key: {'models': [], 'user_id': 'MASKED@MASKED.COM', 'user_email': 'MASKED@MASKED.COM', 'user_role': 'internal_user_viewer', 'max_budget': None, 'budget_duration': None}\nlitellm-1     | 09:29:22 - LiteLLM Proxy:INFO: utils.py:1678 - Data Inserted into Keys Table\nlitellm-1     | LiteLLM Proxy: PrismaClient: Before upsert into litellm_verificationtoken\nlitellm-1     | INFO:     XX.XX.XX.XX:61338 - \"GET /sso/callbackcode=6io6qfn6bwYSrO2Ja5R7bJjclMPCIIMF&state=litellm HTTP/1.1\" 303 See Other\nlitellm-1     | INFO:     XX.XX.XX.XX:61338 - \"GET /ui/?userID=MASKED@MASKED.COM HTTP/1.1\" 200 OK\nlitellm-1     | INFO:     XX.XX.XX.XX:61338 - \"GET /sso/key/generate HTTP/1.1\" 303 See Other\nlitellm-1     | INFO:     XX.XX.XX.XX:61360 - \"GET /ui.txt?_rsc=ri3na HTTP/1.1\" 404 Not Found\nlitellm-1     | 09:29:42 - LiteLLM Proxy:INFO: ui_sso.py:671 - user_defined_values for creating ui key: {'models': [], 'user_id': 'MASKED@MASKED.COM, 'user_email': 'MASKED@MASKED.COM', 'user_role': 'internal_user_viewer', 'max_budget': None, 'budget_duration': None}\nlitellm-1     | 09:29:42 - LiteLLM Proxy:INFO: utils.py:1678 - Data Inserted into Keys Table\nlitellm-1     | LiteLLM Proxy: PrismaClient: Before upsert into litellm_verificationtoken\nlitellm-1     | INFO:     XX.XX.XX.XX:61445 - \"GET /sso/callback?code=nnFkSOkiakyHjxjAAkFONav1E1rdC3Of&state=litellm HTTP/1.1\" 303 See Other\nlitellm-1     | INFO:     XX.XX.XX.XX:61445 - \"GET /ui/_next/static/chunks/webpack-75a5453f51d60261.js HTTP/1.1\" 304 Not Modified\nlitellm-1     | INFO:     XX.XX.XX.XX:61457 - \"GET /ui/_next/static/chunks/fd9d1056-524b80e1a6b8bb06.js HTTP/1.1\" 304 Not Modified\nlitellm-1     | INFO:     XX.XX.XX.XX:61461 - \"GET /ui/_next/static/chunks/main-app-475d6efe4080647d.js HTTP/1.1\" 304 Not Modified\nlitellm-1     | INFO:     XX.XX.XX.XX:61460 - \"GET /ui/_next/static/chunks/117-883150efc583d711.js HTTP/1.1\" 304 Not Modified\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.63.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "hrz6976",
      "author_type": "User",
      "created_at": "2025-03-06T11:22:28Z",
      "updated_at": "2025-06-13T00:02:04Z",
      "closed_at": "2025-06-13T00:02:04Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9027/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9027",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9027",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:13.349108",
      "comments": [
        {
          "author": "hrz6976",
          "body": "Possibly related issue: https://github.com/BerriAI/litellm/issues/8317",
          "created_at": "2025-03-06T11:23:02Z"
        },
        {
          "author": "hrz6976",
          "body": "Believe I've located the root cause of this issue and #8317. In both cases, we used http://IP:PORT to access the admin UI, where:\nhttps://github.com/BerriAI/litellm/blob/1de33f3443a14c623797317c11e01ed71eb0a382/litellm/proxy/management_endpoints/ui_sso.py#L692-L696\n\nSecure cookies won't be sent over",
          "created_at": "2025-03-06T11:57:42Z"
        },
        {
          "author": "hrz6976",
          "body": "<img width=\"236\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/fa5a1077-96e8-41c2-bfb6-6575824e65c0\" />\n\nYep it worked with secure=False. The best option to me is to make it opt-in by adding a config option UI_SECURE_COOKIE and let the default be False.\n\nSome recommendations to make it",
          "created_at": "2025-03-06T12:05:40Z"
        },
        {
          "author": "krrishdholakia",
          "body": "cc: @ishaan-jaff i think you're working on this? ",
          "created_at": "2025-03-07T07:34:47Z"
        },
        {
          "author": "krrishdholakia",
          "body": "> [Fix admin ui build scripts](https://github.com/hrz6976/litellm/commit/9ac991b3b65240334b54765a673de9e5de2a5a31)\n\ni don't follow your fix on the build script, what was the key change? ",
          "created_at": "2025-03-07T07:36:37Z"
        }
      ]
    },
    {
      "issue_number": 9024,
      "title": "[Bug]: Got exception from REDIS Connection closed by server",
      "body": "### What happened?\n\n```yaml\ngeneral_settings:\n  disable_master_key_return: true\n  disable_reset_budget: true\n  disable_retry_on_max_parallel_request_limit_error: true\n  master_key: \"sk-123\"\n  alerting: [\"slack\"]\n  alert_types: [\"llm_exceptions\", \"llm_requests_hanging\"]\n  database_url: \"\"\n  database_connection_pool_limit: 5\n  database_connection_timeout: 60\n  proxy_batch_write_at: 60\n  store_model_in_db: true\nlitellm_settings:\n  ssl_verify: false\n  json_logs: true\n  cache: True\n  cache_params:\n    type: redis\n    namespace: \"litellm_caching\"\n    host: \"redis.db\"\n    port: 6379\n    ttl: 86400\n    supported_call_types: [\"acompletion\", \"atext_completion\", \"aembedding\", \"atranscription\"]\n  drop_params: true\n  num_retries: 0\n  redact_user_api_key_info: true\n```\n\n\n### Relevant log output\n\n```shell\n{\"message\": \"LiteLLM Redis Caching: async set_cache_pipeline() - Got exception from REDIS Connection closed by server., Writing value=None\", \"level\": \"ERROR\", \"timestamp\": \"2025-03-06T09:31:49.040450\"}\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.61.3\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "langgg0511",
      "author_type": "User",
      "created_at": "2025-03-06T09:35:52Z",
      "updated_at": "2025-06-13T00:02:04Z",
      "closed_at": "2025-06-13T00:02:04Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9024/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9024",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9024",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:13.596359",
      "comments": [
        {
          "author": "PRANJALRANA11",
          "body": "hey",
          "created_at": "2025-03-07T08:21:10Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-06T00:01:49Z"
        }
      ]
    },
    {
      "issue_number": 9043,
      "title": "[Feature]: ChatLiteLLMRouter to support json_schema",
      "body": "### The Feature\n\nCurrently ChatLiteLLMRouter does not support json_schema method which is the gold standard for Structured Output using OpenAI.\n\nLangchain's ChatOpenAI supports `chat.with_structured_output(pydantic_model, method=\"json_schema\")` but if chat is ChatLiteLLMRouter the following error is received:\n```\nlangchain_core/language_models/chat_models.py\", line 1303, in with_structured_output\n    raise ValueError(msg)\nValueError: Received unsupported arguments {'method': 'json_schema'}\n```\n\n### Motivation, pitch\n\nLangchain is a standard in the industry and supporting this functionality will not just improve user's reliability since Structured Output using json_mode, as opposed to json_schema, is frequently faulty and causes issues in production. \n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "eilam-stream",
      "author_type": "User",
      "created_at": "2025-03-07T05:23:20Z",
      "updated_at": "2025-06-13T00:02:03Z",
      "closed_at": "2025-06-13T00:02:03Z",
      "labels": [
        "enhancement",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9043/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9043",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9043",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:13.787424",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Hi @eilam-stream litellm supports `json_schema` - https://docs.litellm.ai/docs/completion/json_mode#2-check-if-model-supports-json_schema. \n\nI believe chatlitellmrouter is a community langfuse integration - we would welcome any PR to https://github.com/langchain-ai/langchain",
          "created_at": "2025-03-07T07:27:27Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-06T00:01:46Z"
        }
      ]
    },
    {
      "issue_number": 9049,
      "title": "[Bug]: cached tokens are not priced when using custom handler",
      "body": "### What happened?\n\nonly `prompt_tokens` and `completion_tokens` tokens are priced, `cached_tokens` are not calculated\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.61.20\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "yeahyung",
      "author_type": "User",
      "created_at": "2025-03-07T09:35:04Z",
      "updated_at": "2025-06-13T00:02:02Z",
      "closed_at": "2025-06-13T00:02:02Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9049/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9049",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9049",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:13.960068",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-06T00:01:45Z"
        }
      ]
    },
    {
      "issue_number": 9053,
      "title": "[Feature]: make function calling work with azure openai o3-mini",
      "body": "### The Feature\n\nHey, it looks like o3-mini via azure openai doesn't support function calling through litellm yet. Although the model itself does support it! \n\nLet me know if you cant reproduce this. My request body has valid tools object but still the response returned through litellm says function call not recognised. \n\nrequest - \n`{\n    \"provider\": \"azure\",\n    \"model\": \"o3-mini\",\n    \"messages\": [{ \"content\": \"What's the weather like in San Francisco, Tokyo, and Paris?\",\"role\": \"user\"}],\n    \"tools\": [\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": \"get_current_weather\",\n                    \"description\": \"Get the current weather in a given location\",\n                    \"parameters\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"location\": {\n                                \"type\": \"string\",\n                                \"description\": \"The city and state, e.g. San Francisco, CA\"\n                            },\n                            \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\n                        },\n                        \"required\": [\"location\"]\n                    }\n                }\n            }\n        ]\n}`\n\nresponse - \n`{\"id\":\"chatcmpl-B8SIaaHiSRxvmA2nsLrblbqgDno3i\",\"created\":1741355104,\"model\":\"azure/o3-mini-2025-01-31\",\"object\":\"chat.completion\",\"system_fingerprint\":\"fp_db33bdfc5e\",\"choices\":[{\"finish_reason\":\"stop\",\"index\":0,\"message\":{\"content\":\"I don‚Äôt have real-time weather data at my disposal. However, I can offer a brief overview of the typical weather patterns for each city and suggest how you can get current forecasts:\\n\\n‚Ä¢ San Francisco: The city is known for its mild, often cool climate. Thanks to its coastal location and marine layer, you might experience fog and cooler temperatures even during summer mornings, with temperatures generally staying moderate throughout the day.\\n\\n‚Ä¢ Tokyo: Tokyo has a humid subtropical climate. Summers tend to be hot and humid with the possibility of occasional heat waves, while winters are relatively mild but can feel chilly. The city also has a rainy season (tsuyu) in early summer.\\n\\n‚Ä¢ Paris: Paris generally experiences an oceanic climate with moderate seasonal variations. Summers are typically warm but not excessively hot, and winters are cool with occasional chilly spells. Rain can occur throughout the year but rarely is it consistently heavy.\\n\\nFor the most accurate, up-to-date weather conditions, I recommend checking a trusted weather website or app (such as Weather.com, AccuWeather, or your preferred local service).\",\"role\":\"assistant\",\"tool_calls\":null,\"function_call\":null,\"refusal\":null}}],\"usage\":{\"completion_tokens\":1187,\"prompt_tokens\":19,\"total_tokens\":1206,\"completion_tokens_details\":{\"accepted_prediction_tokens\":0,\"audio_tokens\":0,\"reasoning_tokens\":960,\"rejected_prediction_tokens\":0},\"prompt_tokens_details\":{\"audio_tokens\":0,\"cached_tokens\":0}},\"service_tier\":null,\"prompt_filter_results\":[{\"prompt_index\":0,\"content_filter_results\":{\"hate\":{\"filtered\":false,\"severity\":\"safe\"},\"jailbreak\":{\"filtered\":false,\"detected\":false},\"self_harm\":{\"filtered\":false,\"severity\":\"safe\"},\"sexual\":{\"filtered\":false,\"severity\":\"safe\"},\"violence\":{\"filtered\":false,\"severity\":\"safe\"}}}]}`\n\n### Motivation, pitch\n\nIt would be nice if reasoning models support function calling in litellm. \n\n### Are you a ML Ops Team?\n\nYes\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "anmolbhatia05",
      "author_type": "User",
      "created_at": "2025-03-07T13:54:46Z",
      "updated_at": "2025-06-13T00:02:01Z",
      "closed_at": "2025-06-13T00:02:01Z",
      "labels": [
        "enhancement",
        "mlops user request",
        "stale"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9053/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9053",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9053",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:14.137752",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-06T00:01:44Z"
        }
      ]
    },
    {
      "issue_number": 11603,
      "title": "[Feature]: Upgrade to the MCP streamable http transport",
      "body": "### The Feature\n\nUpgrade allowing SSE and the newer streamable http support\n\n### Motivation, pitch\n\nNo lag behind\n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "wagnerjt",
      "author_type": "User",
      "created_at": "2025-06-10T22:17:42Z",
      "updated_at": "2025-06-12T23:32:05Z",
      "closed_at": "2025-06-12T23:32:05Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11603/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11603",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11603",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:14.330980",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "What is the ETA on this? @wagnerjt @ishaan-jaff ",
          "created_at": "2025-06-11T16:07:28Z"
        }
      ]
    },
    {
      "issue_number": 11680,
      "title": "[Feature]: Support `think` parameter for Ollama models",
      "body": "### The Feature\n\nOllama now [supports thinking](https://ollama.com/blog/thinking) with the new `think` parameter in the `ollama.generate` and `ollama.chat` functions. This doesn't seem to be implemented in LiteLLM yet, however.\n\n### Motivation, pitch\n\nSupport for reasoning Ollama models.\n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "saattrupdan",
      "author_type": "User",
      "created_at": "2025-06-12T23:18:07Z",
      "updated_at": "2025-06-12T23:18:07Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11680/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11680",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11680",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:14.510970",
      "comments": []
    },
    {
      "issue_number": 11491,
      "title": "[Bug]: Not able to get annotations from gpt4o-search-preview",
      "body": "### What happened?\n\nWhy am I not able to get annotations which include url_citations when I use gpt4o-search-preview?\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.51.0\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "srushti98-qurrent",
      "author_type": "User",
      "created_at": "2025-06-06T17:02:17Z",
      "updated_at": "2025-06-12T22:39:12Z",
      "closed_at": "2025-06-12T22:39:12Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11491/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11491",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11491",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:14.510989",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "Have you tried upgrading your litellm version üòÖ ? ",
          "created_at": "2025-06-06T18:02:15Z"
        }
      ]
    },
    {
      "issue_number": 10849,
      "title": "[Bug]: LiteLLM Team Table expanding makes /v1/models slow",
      "body": "### What happened?\n\nGET /v1/models takes minimum 2s and very often it goes up to 10s. I'm using Open WebUI which will call that endpoint on each prompt. All responses are now regularly waited for at least 5s. Looking at the Langfuse Time to first token is less than a second.\n\nI can observe the same thing when I call that endpoint from Postman and when opening list of models on LiteLLM Dashboard. I can't say when this started to happen, but it is weeks. These are the models that I have:\n\ndall-e-3  \nopenai/chatgpt-4o-latest  \ngemini/gemini-2.0-flash  \ngpt-4.1  \nopenai/text-embedding-3-large  \ngemini/gemini-2.5-flash-preview-04-17  \ngpt-4.1-nano  \ngemini/gemini-2.5-pro-preview-05-06  \nopenai/o3-mini  \nopenai/gpt-4o-mini  \nlocal/llama3.2:1b  \nopenai/gpt-4o  \nlocal/gemma2:2b  \nopenai/gpt-4o-latest  \ngemini/gemini-2.5-pro-preview-03-25  \no3  \no4-mini  \ngpt-4.1-mini  \nmistral/mistral-small-latest\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.69.0\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "ante-maric",
      "author_type": "User",
      "created_at": "2025-05-15T07:57:06Z",
      "updated_at": "2025-06-12T20:19:53Z",
      "closed_at": null,
      "labels": [
        "bug",
        "awaiting: user response"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10849/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10849",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10849",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:14.749143",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Do you have `check_provider_endpoints` enabled? \n\n```yaml\nlitellm_settings:\n  check_provider_endpoints: true\n```\n",
          "created_at": "2025-05-15T15:25:43Z"
        },
        {
          "author": "ante-maric",
          "body": "> Do you have `check_provider_endpoints` enabled?\n> \n> litellm_settings:\n>   check_provider_endpoints: true\n\nNo, these are my settings:\n\n```yaml\n      general_settings:\n          alerting: [\"email\"]\n      litellm_settings:\n          turn_off_message_logging: True\n          log_raw_request_response: ",
          "created_at": "2025-05-15T15:28:35Z"
        },
        {
          "author": "ante-maric",
          "body": "@krrishdholakia Any other suggestion? I tried setting  check_provider_endpoint explicitly to false, but there is no effect.",
          "created_at": "2025-05-19T13:50:32Z"
        },
        {
          "author": "ante-maric",
          "body": "@krrishdholakia You haven't helped much on this issue, I figure it out how to solve it and I'll leave it to you to figure out the root cause.\n\nMetadata column in LiteLLM_TeamTable for my team has grown to over 15MB in size containing this gibberish:\n\n```json\n{\n  \"0\": \"{\",\n  \"1\": \"\\n\",\n  \"2\": \" \",\n  ",
          "created_at": "2025-06-12T14:05:27Z"
        }
      ]
    },
    {
      "issue_number": 6770,
      "title": "[Bug]: Tag Based Routing redirecting to default model",
      "body": "### What happened?\n\nI was trying to do tag based routing in LiteLLM, but it redirects to \"default\" tag more than the tag that i am passing in the request:\r\n\r\nExample Config \r\n```\r\n  - model_name: mock-llm\r\n    litellm_params:\r\n      model: openai/kunal-llm1\r\n      api_key: kunal-123\r\n      api_base: http://random-ip:4001/v1/\r\n      tags: [\"medium\"]\r\n  - model_name: mock-llm\r\n    litellm_params:\r\n      model: openai/kunal-llm2\r\n      api_key: kunal-123\r\n      api_base: http://another-random-ip:4001/v1/\r\n      tags: [\"default\"]      \r\n```      \r\nCan you tell what i am doing wrong, as i can see i am not hitting tpm/rpm limits of any \"medium\" tagged model.      \n\n### Relevant log output\n\n_No response_\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "kunalchamoli",
      "author_type": "User",
      "created_at": "2024-11-16T20:27:32Z",
      "updated_at": "2025-06-12T20:18:43Z",
      "closed_at": "2025-06-12T20:18:43Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/6770/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/6770",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/6770",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:14.961938",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "@kunalchamoli - what tags are you sending in your request. Can I see a sample request you're sending ",
          "created_at": "2024-11-17T03:18:07Z"
        },
        {
          "author": "kunalchamoli",
          "body": "@ishaan-jaff sample curl\r\nMy curl request is working fine as i can see **medium** tagged as well as **default** tagged model are receiving request, my question is why is default tag getting more requests(or even any request) than medium, if i am not hitting TPM/RPM limits of medium tagged model.\r\n\r\n",
          "created_at": "2024-11-17T18:32:09Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "are you expected default to get no requests when tags = medium @kunalchamoli  ? ",
          "created_at": "2024-11-19T03:38:17Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "the original behavior was if a deployment is tagged as default it can be used for all tags + when no tags are sent ",
          "created_at": "2024-11-19T03:38:52Z"
        },
        {
          "author": "kunalchamoli",
          "body": "I expect default tag to get some requests, probably less than medium tagged model. But as i can see from my dashboard(model) default tag is consistently getting more requests than medium tagged.\r\nAnd i can't post dashboard photos or screenshot as it is restricted. ",
          "created_at": "2024-11-19T08:40:33Z"
        }
      ]
    },
    {
      "issue_number": 11673,
      "title": "[Bug]: Model-Discovery not working when using a different frontend-name",
      "body": "### What happened?\n\nGiven the following config:\n\n```\n  - model_name: \"google-gemini/*\"\n    litellm_params:\n      model: \"gemini/*\"\n      api_key: \"\"\n\nlitellm_settings:\n  check_provider_endpoint: true \n```\n\n\nWill not work properly and return the google models with the google-gemini prefix. In fact it will just return the wildcard model.\n\nWhy? Because here we have the wrong 'provider' name, the logic seems to be broken, instead of model-names tuples need to be used, since we need frontend and backend information, one to determine the provider (if wildcard) and one to return on the response. Because I named it `google-gemini` the `get_known_models_from_wildcard` will not work correctly.\n\n![Image](https://github.com/user-attachments/assets/ce3c5988-3ae9-4f39-8d9d-237be2bb99f0)\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nmain branch\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "matthid",
      "author_type": "User",
      "created_at": "2025-06-12T20:14:07Z",
      "updated_at": "2025-06-12T20:14:07Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11673/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11673",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11673",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:15.133933",
      "comments": []
    },
    {
      "issue_number": 11671,
      "title": "[Feature]: Support logging for multiple guardrails",
      "body": "### The Feature\n\nIn the standard logging payload, log a list of `StandardLoggingGuardrailInformation` for `guardrail_information` instead of single instance.\n\n### Motivation, pitch\n\nIf multiple guardrails are engaged and they fail their checks and these are logged, the last guardrail registered overwrites the information from the previous guardrails. Litellm already logs how many guardrails are enabled for that request in `applied_guardrails`. Makes sense to log multiple guardrail information.\n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "thetonus",
      "author_type": "User",
      "created_at": "2025-06-12T20:04:48Z",
      "updated_at": "2025-06-12T20:04:48Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11671/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11671",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11671",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:15.133957",
      "comments": []
    },
    {
      "issue_number": 11587,
      "title": "[Bug]: Unable to pass image in request to Mistral models via Async Client",
      "body": "### What happened?\n\nStarting with 1.69.1, I'm unable to include an image input into a request for Mistral models. This works on previous versions without any issues and I've confirmed the issue exists in 1.72.2. It also doesn't seem to extend to other model families. Would appreciate any help on this, thanks!\n\n```\nreq = {\n        'messages': \n        [\n            {\n                'content': \n                [\n                      {\n                          'text': 'Was an image passed in, yes or no?',\n                          'type': 'text'\n                      },\n                      {\n                          'image_url': {\n                              'url': 'data:image/png;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7',\n                              'detail': 'auto'\n                          },\n                          'type': 'image_url'\n                      }\n                    ],\n                'role': 'user'\n            }\n        ],\n        'model': 'mistral-medium-2505',\n        'n': 1,\n        'custom_llm_provider': 'mistral',\n        'max_completion_tokens': 256,\n        'api_key': 'XXXXXX'\n    }\n\nawait litellm.acompletion(**req)\n```\n\n### Relevant log output\n\n```shell\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nFile .../litellm/main.py:508, in acompletion(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, base_url, api_version, api_key, model_list, extra_headers, thinking, **kwargs)\n    507 elif asyncio.iscoroutine(init_response):\n--> 508     response = await init_response\n    509 else:\n\nFile .../litellm/llms/openai/openai.py:765, in OpenAIChatCompletion.acompletion(self, messages, optional_params, litellm_params, provider_config, model, model_response, logging_obj, timeout, api_key, api_base, api_version, organization, client, max_retries, headers, drop_params, stream_options, fake_stream)\n    764 response = None\n--> 765 data = await provider_config.async_transform_request(\n    766     model=model,\n    767     messages=messages,\n    768     optional_params=optional_params,\n    769     litellm_params=litellm_params,\n    770     headers=headers or {},\n    771 )\n    772 for _ in range(\n    773     2\n    774 ):  # if call fails due to alternating messages, retry with reformatted message\n\n.../litellm/llms/openai/chat/gpt_transformation.py:400, in OpenAIGPTConfig.async_transform_request(self, model, messages, optional_params, litellm_params, headers)\n    392 async def async_transform_request(\n    393     self,\n    394     model: str,\n   (...)    398     headers: dict,\n    399 ) -> dict:\n--> 400     transformed_messages = await self._transform_messages(\n    401         messages=messages, model=model, is_async=True\n    402     )\n    404     return {\n    405         \"model\": model,\n    406         \"messages\": transformed_messages,\n    407         **optional_params,\n    408     }\n\nTypeError: object list can't be used in 'await' expression\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.72.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "ParthivNaresh",
      "author_type": "User",
      "created_at": "2025-06-10T14:25:06Z",
      "updated_at": "2025-06-12T19:36:32Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11587/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11587",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11587",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:15.133965",
      "comments": [
        {
          "author": "AmineDjeghri",
          "body": "I am facing a similar issue with async mistral with text? Does it work for you using sync client instead of async ?\n\nhttps://github.com/BerriAI/litellm/issues/11591",
          "created_at": "2025-06-10T15:02:53Z"
        },
        {
          "author": "ParthivNaresh",
          "body": "I'm not seeing the error with async mistral with text, but it works with the sync client (text and image)",
          "created_at": "2025-06-10T15:30:38Z"
        },
        {
          "author": "AmineDjeghri",
          "body": "okey, so i have a similar issue i think, i am using azure ai models (mistral, deepseek) and async calls don't work while sync calls do work",
          "created_at": "2025-06-12T19:36:09Z"
        }
      ]
    },
    {
      "issue_number": 9347,
      "title": "[Bug]: Budget UI - Should show Budget Created",
      "body": "### What happened?\n\nOne the Budget create page, once the Budget record is created, the UI shows \"API Key\" created.\nIt should be Budget Created\n\n![Image](https://github.com/user-attachments/assets/fdbdb1bb-5a67-4ac7-8d94-af7dae9e184e)\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.63.11-stable.patch1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "metalshanked",
      "author_type": "User",
      "created_at": "2025-03-18T18:51:11Z",
      "updated_at": "2025-06-12T19:28:34Z",
      "closed_at": "2025-06-12T19:28:34Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9347/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9347",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9347",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:15.330535",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "@metalshanked can you file a PR for this ? ",
          "created_at": "2025-03-18T18:52:46Z"
        },
        {
          "author": "jtong99",
          "body": "Hey @vuanhtu52, this may relate to what you're doing. If yes, hope you can fix this as well.",
          "created_at": "2025-06-10T13:52:40Z"
        },
        {
          "author": "vuanhtu52",
          "body": "@ishaan-jaff Hi, I just tested on the latest version and it seems like the bug still persists. I made a PR to fix this problem: https://github.com/BerriAI/litellm/pull/11608. When you have some time please have a look and let me know. Much appreciated!",
          "created_at": "2025-06-11T02:10:06Z"
        }
      ]
    },
    {
      "issue_number": 11670,
      "title": "[Bug]: Adding a Vertex model doesn't allow referencing credentials in the environment",
      "body": "### What happened?\n\nAs you try to add a vertex model, you are requested to upload the credentials for a GCP SA or use an existing credential.\n\nYou should be allowed to reference a path on the local filesystem where process can find the file, just as you are able to do when adding models via the config.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "mrh-chain",
      "author_type": "User",
      "created_at": "2025-06-12T19:20:47Z",
      "updated_at": "2025-06-12T19:20:47Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11670/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11670",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11670",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:15.550615",
      "comments": []
    },
    {
      "issue_number": 9858,
      "title": "[Bug]: OpenRouter models missing from LiteLLM",
      "body": "### What happened?\n\nI just installed LiteLLM according to the \"docker getting started\" guide and added my openrouter key, however I cannot see all models in the test key chat interface. Even though I can see deepseek-chat and gpt4, gemini-2.5 is not on there. I can interact with the other models, so it is not a problem of connectivity. I enabled the wildcard all models, is there anything else I should do that is not in the docs?\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.65.4\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "freddyholms",
      "author_type": "User",
      "created_at": "2025-04-09T17:22:11Z",
      "updated_at": "2025-06-12T18:51:58Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9858/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9858",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9858",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:15.550635",
      "comments": [
        {
          "author": "mrexodia",
          "body": "This is not documented anywhere, but LiteLLM does not call `/models` for registered models. It _only_ uses https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json to enumerate the wildcards. You can provide an override for the cost map to add more models.",
          "created_at": "2025-05-17T15:09:46Z"
        },
        {
          "author": "kmac",
          "body": "Thanks @mrexodia, I just started using litellm and ran into this - confusing. \n\nI was thinking that it is odd that this cost model file is being updated manually, since openrouter publishes all of this info in its API.  But it looks like openai doesn't support querying the model pricing: https://com",
          "created_at": "2025-05-19T14:04:42Z"
        },
        {
          "author": "mrexodia",
          "body": "Locally I started writing a script, but it‚Äôs not fully trivial to convert all the values unfortunately‚Ä¶",
          "created_at": "2025-05-19T17:05:56Z"
        },
        {
          "author": "dannykorpan",
          "body": "Same issue here and so LiteLLM is unusable in combination with Cline/Roo Code due to missing context window.",
          "created_at": "2025-06-02T19:58:46Z"
        },
        {
          "author": "dannykellett",
          "body": "Hi @dannykorpan I want to use roo with litellm and openrouter and came across your comment. Litellm is new to me and therefore excuse the noobie question but I've added: `openrouter/anthropic/claude-3.7-sonnet` because that's the one I use most, are you saying there is an issue with context window e",
          "created_at": "2025-06-11T20:30:15Z"
        }
      ]
    },
    {
      "issue_number": 9805,
      "title": "[Feature]: enable prompt caching by default in model configuration for bedrock claude models",
      "body": "### The Feature\n\n# Feature Request: Enable Prompt Caching by Default for Specific Models\n\n## Description\nLiteLLM supports prompt caching for models like Bedrock Claude, but currently requires users to explicitly set parameters in their messages. We need the ability to enable prompt caching by default for specific models, particularly Bedrock Claude models, so users don't have to modify their messages for every request.\n\n## Current Behavior\n- Users must manually add `cache_control` parameters to their messages\n- No way to globally enable prompt caching by default for specific models\n- Caching capability exists but is opt-in for every request\n\n## Desired Behavior\n- Allow setting prompt caching as a default for specific models in configuration\n- Automatically inject cache control parameters for Bedrock Claude models when not explicitly specified\n- Provide a simple toggle like `enable_prompt_caching=True` in model configuration\n\n## Proposed Implementation\n1. Add a `supports_prompt_caching` flag in model configuration\n2. Enhance the message transformation pipeline to automatically add cache control\n3. Add a preprocessing step in `BedrockLLM.completion()` that modifies messages for models with default caching enabled\n\n### Example Configuration\n```python\nlitellm.register_model(\n    model_cost={\n        \"bedrock/anthropic.claude-3-sonnet-20240229-v1:0\": {\n            \"supports_prompt_caching\": True,\n            \"default_prompt_caching\": True,  #  enforce prompt cachine always\n            \"litellm_provider\": \"bedrock\",\n            \"mode\": \"chat\"\n        }\n    }\n)\n```\n\n## Are you a ML Ops Team?\n\nYes\n\n## Twitter / LinkedIn details\n\nhttp://linkedin.com/in/hmaya",
      "state": "closed",
      "author": "HaithamMaya",
      "author_type": "User",
      "created_at": "2025-04-07T18:21:33Z",
      "updated_at": "2025-06-12T16:50:43Z",
      "closed_at": "2025-06-12T16:50:43Z",
      "labels": [
        "enhancement",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9805/reactions",
        "total_count": 7,
        "+1": 7,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9805",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9805",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:15.789856",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "supported here: https://docs.litellm.ai/docs/tutorials/prompt_caching \n\nwe support prompt cache injection points ",
          "created_at": "2025-06-12T16:50:43Z"
        }
      ]
    },
    {
      "issue_number": 11665,
      "title": "[Bug]: Errors with perplexity and anthropic calls with Tools",
      "body": "### What happened?\n\nReopening with python example.\n\nI try to call different models with tool calls. Working on openai but not on some other ( sonar / claude )\n\nUsing : litellm 1.71.1\n\nIs this planned to be supported ?\n\n\n\n\n```\n\nimport openai\n\nclient = openai.OpenAI(api_key=\"TOKEN\",base_url=\"LITELLM-SERVER\")\n\nresponse = client.chat.completions.create(model=\"PERPLEXITYMODEL\", messages = [\n      {\n        \"role\": \"assistant\",\n        \"tool_calls\": [\n          {\n            \"function\": {\n              \"arguments\": '{\"query\":\"foobar\"}',\n              \"name\": \"Search\",\n            },\n            \"id\": \"call_ShUM9uxonziXPYjU9a4uoy4s\",\n            \"type\": \"function\",\n          },\n        ],\n      },\n      {\n        \"content\": \"this is a tool response\",\n        \"role\": \"tool\",\n        \"tool_call_id\": \"call_ShUM9uxonziXPYjU9a4uoy4s\",\n      },\n      {\n        \"content\": \"hi\",\n        \"role\": \"user\",\n      },\n    ])\n\n```\n\n\n> 'litellm.BadRequestError: PerplexityException - [\"At body -> messages -> 0 -> content: Field required\", \"At body -> messages -> 1 -> role: Input should be \\'system\\', \\'user\\' or \\'assistant\\'\"].\n\n\nThank you\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.71.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "simonljn",
      "author_type": "User",
      "created_at": "2025-06-12T16:42:55Z",
      "updated_at": "2025-06-12T16:42:55Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11665/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11665",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11665",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:15.995630",
      "comments": []
    },
    {
      "issue_number": 11196,
      "title": "[Bug]: Anthropic model calls with Tools :  Input should be a valid dictionary",
      "body": "### What happened?\n\nUsing litelllm image v1.71.1-stable for litellm proxy\n\nI have an error with Anthropic model calls with Tools that i don't have on openai calls for the same request : \n\n> 'litellm.BadRequestError: AnthropicException - {\"type\":\"error\",\"error\":{\"type\":\"invalid_request_error\",\"message\":\"messages.1.content.0.tool_use.input: Input should be a valid dictionary\"}}. Received Model Group=x-anthropic-x\\n'\n\n\nSample code :\n\n```\nimport openai from \"openai\";\n\nconst client = new openai({\n  apiKey: \"xx\",\n  baseURL: \"https://litellm-xx\",\n});\n\nasync function main() {\n  const out = await client.chat.completions.create({\n    messages: [\n      {\n        content: \"search : foo\",\n        role: \"user\",\n      },\n      {\n        role: \"assistant\",\n        tool_calls: [\n          {\n            function: {\n              arguments: '\"{\\\\\"query\\\\\": \\\\\"foo\\\\\"}\"',\n              name: \"WebSearch\",\n            },\n            id: \"toolu_01LaSrYM7jQ4snmb3h4cb2A2\",\n            type: \"function\",\n          },\n        ],\n      },\n      {\n        content: \"...  foo search response ....\",\n        role: \"tool\",\n        tool_call_id: \"toolu_01LaSrYM7jQ4snmb3h4cb2A2\",\n      },\n      {\n        content: \"search : bar\",\n        role: \"user\",\n      },\n    ],\n    model: \"x-anthropic-x\",\n    tool_choice: \"auto\",\n    tools: [\n      {\n        function: {\n          description: \"Make a search\",\n          name: \"Search\",\n          parameters: {\n            properties: {\n              query: {\n                description: \"The search query\",\n                type: \"string\",\n              },\n            },\n            required: [\"query\"],\n            type: \"object\",\n          },\n        },\n        type: \"function\",\n      },\n    ],\n  });\n\n  console.log(out);\n}\n\nmain();\n\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.71.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "simonljn",
      "author_type": "User",
      "created_at": "2025-05-28T07:49:52Z",
      "updated_at": "2025-06-12T16:33:16Z",
      "closed_at": "2025-06-12T16:33:16Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11196/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11196",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11196",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:15.995653",
      "comments": []
    },
    {
      "issue_number": 10389,
      "title": "[Bug]: metadata.api_base metric no longer emitted by otel after v1.49.0",
      "body": "### What happened?\n\nWe have been running v1.49.0-stable for many months and have otel enabled sending metrics to honeycomb.  We have been using metadata.api_base metrics to help us differentiate endpoints/regions for latency comparison.  After upgrading to v1.50.4 we noticed that this metrics was missing so we rolled back and remained on v1.49.0 hoping that this was a bug that would be fixed in later versions. Last week we had to update to v1.67.0-stable but the metadata.api_base metric is still missing.  We did some digging through the code changes but were not able to identify a change that would disable this metric or should prevent it from being emitted.\n\nExpected behavior:\nmetadata.api_base metric should continue to be emitted on newer versions\n\nAction behavior:\nmetadata.api_base metric is no longer emitted\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.67.0-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "Tom-MarrLabs",
      "author_type": "User",
      "created_at": "2025-04-28T19:32:08Z",
      "updated_at": "2025-06-12T15:40:00Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request",
        "may 2025"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10389/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10389",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10389",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:15.995662",
      "comments": [
        {
          "author": "Tom-MarrLabs",
          "body": "Any ideas or progress on this?  I am surprised that others have not reported the same issue as this metrics is critical to our ability to monitor and create reports on latency for load balancing our end points.",
          "created_at": "2025-05-09T02:28:56Z"
        },
        {
          "author": "Tom-MarrLabs",
          "body": "Pinging on this.  Any ideas or investigation going on?",
          "created_at": "2025-05-26T23:46:57Z"
        },
        {
          "author": "Tom-MarrLabs",
          "body": "We have since updated to main-v1.71.1-stable and the problem is still present",
          "created_at": "2025-06-12T15:39:59Z"
        }
      ]
    },
    {
      "issue_number": 10937,
      "title": "[Bug]: Adding a selfhosted TGI endpoint to config.yaml makes LiteLLM stuck on pod startup",
      "body": "### What happened?\n\n> **TLDR**: When adding a [HF TGI ](https://github.com/huggingface/text-generation-inference)selfhosted model endpoint to LiteLLM config.yaml using the documented way, pod gets stuck on Application startup\n\n\nI have the following config.yaml\n\n```yaml\nmodel_list:\n  - model_name: qwen3-32b\n    litellm_params:\n      model: hosted_vllm/qwen3-32b\n      api_base: https://my-qwen3-local-vllm-endpoint/v1\n      api_key: none\n      rpm: 1440\n    model_info:\n      supports_reasoning: True\n  - model_name: huggingface/mistral7b\n    litellm_params:\n      model: huggingface/mistral7b\n      api_base: https://my-mistral-local-tgi-endpoint/v1\n      api_key: os.environ/MISTRAL_7B\ngeneral_settings:\n    master_key: os.environ/PROXY_MASTER_KEY\n    background_health_checks: False\nlitellm_settings:\n    ssl_verify: false\n```\n\nI have also tried with `model: huggingface/tgi` without any luck either, as well as removing `/v1` suffix from api base, etc.\n\nIf I am only using the vLLM model, it works fine. However, **the moment I add the TGI inference endpoint model**, LiteLLM is **unable** to start (gets stuck on Uvicorn Waiting for Application startup). This also happens if I only keep the TGI model in the list (removing the vLLM one).\n\nI have validated that there is **existing** connectivity between the LiteLLM proxy pod and the vLLM and TGI pods. \n\nTesting this **via UI** (trying to add new model but checking connection, as I cannot add models via UI since **I am deploying LiteLLM via Helm chart**) **works**, as the model connection check succeeds.\n\nHowever, when doing the model addition via yaml, the LiteLLM proxy pod is stuck on `INFO: Waiting for application startup.`. DEBUG logs also doesn't show anything else.\n\n### Relevant log output\n\n```shell\nPrisma schema loaded from schema.prisma\nDatasource \"client\": PostgreSQL database \"litellm\", schema \"public\" at \"genai-gateway-db\"\nThe database is already in sync with the Prisma schema.\nRunning generate... (Use --skip-generate to skip the generators)\nRunning generate... - Prisma Client Python (v0.11.0)\nSome types are disabled by default due to being incompatible with Mypy, it is highly recommended\nto use Pyright instead and configure Prisma Python to use recursive types. To re-enable certain types:\ngenerator client {\nprovider = \"prisma-client-py\"\nrecursive_type_depth = -1\n}\nIf you need to use Mypy, you can also disable this message by explicitly setting the default value:\ngenerator client {\nprovider = \"prisma-client-py\"\nrecursive_type_depth = 5\n}\nFor more information see: https://prisma-client-py.readthedocs.io/en/stable/reference/limitations/#default-type-limitations\n‚úî Generated Prisma Client Python (v0.11.0) to ./../../prisma in 370ms\nINFO: Started server process [1]\nINFO: Waiting for application startup.\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\n1.69.0\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "andresC98",
      "author_type": "User",
      "created_at": "2025-05-19T09:33:41Z",
      "updated_at": "2025-06-12T10:37:12Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10937/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10937",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10937",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:16.171070",
      "comments": [
        {
          "author": "andresC98",
          "body": "**Update:** seems that the only way I managed to get it working is to **use vLLM setup** but for the TGI endpoint, as follows\n\n```\n...\n  - model_name: mistral7b\n    litellm_params:\n      model: hosted_vllm/mistral7b\n...\n```\n\nwhich is surprising to me since the recommended setup for the Hugginface TG",
          "created_at": "2025-05-19T09:46:30Z"
        },
        {
          "author": "andresC98",
          "body": "As a note, specifying the suggested config in the docs for TGI also doesnt work\n\n```\nproxy_config:\n  model_list:\n  - model_name: mistral7b\n    litellm_params:\n      model: huggingface/tgi\n      api_base: https://selfhosted-tgI-endpoint.com\n```\n\nAdding log verbosity and log level DEBUG to litellm pro",
          "created_at": "2025-06-12T10:23:25Z"
        }
      ]
    },
    {
      "issue_number": 10335,
      "title": "[Bug]: Azure gpt-image-1 cost tracking not working when size/quality set to auto or left default",
      "body": "### What happened?\n\nWhen configuring azure gpt-image-1 with Azure in proxy:\n\n```yaml\n  - model_name: 'openai.gpt-image-1'\n    litellm_params:\n      model: 'azure/gpt-image-1'\n      api_base: 'https://example.openai.azure.com/'\n      api_version: '2025-03-01-preview'\n      api_key: 'secret'\n```\n\nThe following seem to be true:\n\n* Costs are tracked when both size & quality are explicitly set\n* Costs are tracked when quality is explicitly set but size is set to `None` (or not set by user)\n* Costs are *not* tracked when either or both of size or quality are set explicitly to `auto`\n* Costs are *not* tracked when both of size and quality are set to `None` (or not set by user)\n* Costs are *not* tracked whenever quality is set to `None` (or not set by user)\n\nRunning some code to test all variations:\n\n```python\nfrom base64 import b64decode\nfrom concurrent.futures import ThreadPoolExecutor\nfrom io import BytesIO\nfrom openai import OpenAI\nfrom openai.types import ImagesResponse\nfrom PIL import Image\n\nclient = OpenAI()\n\nmodel_name = 'openai.gpt-image-1'\n\nprompt = \"Create a painted portrait of a man, with a blue background and a red hat\"\n\nquality_options = ['low', 'medium', 'high', 'auto', None]\nsize_options = ['1024x1024', '1024x1536', '1536x1024', 'auto', None]\n\nimages: list[Image.Image] = []\n\nwith ThreadPoolExecutor(max_workers=4) as executor:\n    futures = []\n    for quality in quality_options:\n        for size in size_options:\n            future = executor.submit(\n                client.images.generate,\n                model=model_name,\n                prompt=prompt,\n                quality=quality,\n                size=size,\n                extra_body={\n                    'metadata': {\n                        'tags': [f'size:{size}', f'quality:{quality}'],\n                    }\n                }\n            )\n            futures.append(future)\n\n    for future in futures:\n        try:\n            response: ImagesResponse = future.result()\n            \n            image_b64 = response.data[0].b64_json\n            image_data = b64decode(image_b64)\n            image = Image.open(BytesIO(image_data))\n            images.append(image)\n\n        except Exception as e:\n            print(f\"Error generating image: {e}\")\n\nfor i, image in enumerate(images):\n    image.show(title=f\"Image {i+1}\")\n```\nLiteLLM Logs for the 25 possible combinations:\n\n<img width=\"1437\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e3cebca4-4508-47f6-9f96-ca187227f167\" />\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.67.3\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "marty-sullivan",
      "author_type": "User",
      "created_at": "2025-04-26T17:36:29Z",
      "updated_at": "2025-06-12T10:07:49Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10335/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "krrishdholakia"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10335",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10335",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:16.342517",
      "comments": [
        {
          "author": "marty-sullivan",
          "body": "@krrishdholakia as of the latest version this is working a lot better. It still seems that cost is not tracked when `size='auto'` though.",
          "created_at": "2025-05-05T06:21:51Z"
        }
      ]
    },
    {
      "issue_number": 9159,
      "title": "[Bug]: Add Bedrock Support for /v1/messages API ",
      "body": "### What happened?\n\nPreviously I had been using the v1/messages endpoint to access the Claude model on bedrock.\n\nYesterday I upgraded litellm to 1.63.6 and realized that the v1/messages endpoint is no longer available.\n\n```\n{\n    \"model\": \"claude-3-5-haiku-20241022\",\n    \"max_tokens\": 1024,\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"hi\"\n                }\n            ]\n        }\n    ]\n}\n```\n\n\n\n### Relevant log output\n\n```shell\n{\n    \"error\": {\n        \"message\": \"anthropic_messages() missing 1 required positional argument: 'api_key'\",\n        \"type\": \"None\",\n        \"param\": \"None\",\n        \"code\": \"500\"\n    }\n}\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\n1.63.6\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "huangyafei",
      "author_type": "User",
      "created_at": "2025-03-12T03:01:11Z",
      "updated_at": "2025-06-12T10:00:52Z",
      "closed_at": "2025-06-12T10:00:52Z",
      "labels": [
        "bug",
        "mlops user request",
        "stale",
        "march 2025"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9159/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "ishaan-jaff"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9159",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9159",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:16.536603",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "I'll be DRI on this, we recently migrated this to support thinking ",
          "created_at": "2025-03-12T03:17:20Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "- @huangyafei are you calling bedrock api and anthropic api ? \n- can I see how you defined it on your config.yaml ",
          "created_at": "2025-03-12T03:20:16Z"
        },
        {
          "author": "huangyafei",
          "body": "> * [@huangyafei](https://github.com/huangyafei) are you calling bedrock api and anthropic api ?‰Ω†Âú®Ë∞ÉÁî® Bedrock API Âíå Anthropic API ÂêóÔºü\n> * can I see how you defined it on your config.yamlËÉΩÂê¶ËÆ©ÊàëÁúãÁúã‰Ω†Âú®‰Ω†ÁöÑ config.yaml Êñá‰ª∂‰∏≠ÊòØÂ¶Ç‰ΩïÂÆö‰πâÂÆÉÁöÑÔºü\n\nHere's my config.yaml file:\n\n```\nmodel_list:\n  - model_name: gpt-4o-mini\n    lit",
          "created_at": "2025-03-12T03:28:42Z"
        },
        {
          "author": "huangyafei",
          "body": "Supplement:\n\nI made the attempt:\n\n1. using the /v1/chat/completions endpoint, the bedrock/claude model can be accessed normally.\n2. replace the bedrock/claude model in config.yaml with the official Anthropic one, and you can access the Claude model via the /v1/messages endpoint.\n\nSo the current prob",
          "created_at": "2025-03-12T03:31:41Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Hi @huangyafei the `/v1/messages` endpoint is beta, as we try to figure out the best approach here. \n\nHow could we have communicated the change better? ",
          "created_at": "2025-03-13T06:00:11Z"
        }
      ]
    },
    {
      "issue_number": 11276,
      "title": "[Feature]: LiteLLM Java client SDK",
      "body": "### The Feature\n\nLiteLLM is awesome!\n\nBut I would need a Java client to be able to use it in my project (https://docs.enola.dev/use/chat/#ai).\n\n### Motivation, pitch\n\nRequired to (more easily) use & integrate with LiteLLM from Java.\n\nUseful specifically for https://docs.langchain4j.dev, Spring AI and Google's (new!) Agent Development Kit (ADK) in Java.\n\nWould you like me to write one for you? I would be happy to!\n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\nhttps://ch.linkedin.com/in/vorburger",
      "state": "open",
      "author": "vorburger",
      "author_type": "User",
      "created_at": "2025-05-30T19:52:57Z",
      "updated_at": "2025-06-12T09:01:45Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11276/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11276",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11276",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:16.771461",
      "comments": [
        {
          "author": "wagnerjt",
          "body": "Are you looking for a client to interact with the litellm proxy? If so, you can use any client and we have people using OpenAI's java client to our proxy instance",
          "created_at": "2025-06-12T09:01:27Z"
        }
      ]
    },
    {
      "issue_number": 11552,
      "title": "[Bug]: POST /key/generate requires Enterprise",
      "body": "### What happened?\n\nHi, we tried to create keys via API request and it returns error which says Enterprise key is required. Sales team told that this is unattended behavior and asked to create issue on github.\n\n`POST /key/generate`\n\nRequest\n```\n{\n  \"provider\": \"litellm.main_1_llm_tier-1\",\n  \"models\": [\n    \"LLM Tier 1\"\n  ],\n  \"max_budget\": 100.0,\n  \"user_id\" : \"39\",\n  \"team_id\": \"foobar\",\n  \"max_parallel_requests\": 2,\n  \"metadata\" : {\n    \"resource_id\": \"65\"\n  },\n  \"tpm_limit\": 1000,\n  \"rpm_limit\": 60,\n  \"key_alias\": \"prod-key-1\",\n  \"permissions\": {\n    \"can_create_keys\": \"false\"\n  },\n  \"guardrails\": [\n    \"content_filter\",\n    \"token_limit\"\n  ],\n  \"blocked\": false,\n  \"duration\": \"P0D\",\n  \"budget_duration\": \"P0D\"\n}\n```\n\nResponse\n\n```\n{\n    \"error\": {\n        \"message\": \"{'error': 'This feature is only available for LiteLLM Enterprise users. You must be a LiteLLM Enterprise user to use this feature. If you have a license please set `LITELLM_LICENSE` in your env. Get a 7 day trial key here: https://www.litellm.ai/#trial. \\\\nPricing: https://www.litellm.ai/#pricing'}\",\n        \"type\": \"internal_server_error\",\n        \"param\": \"None\",\n        \"code\": \"403\"\n    }\n}\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nDocker litellm/litellm-database:v1.69.0-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "i-makashev",
      "author_type": "User",
      "created_at": "2025-06-09T12:04:13Z",
      "updated_at": "2025-06-12T07:44:26Z",
      "closed_at": "2025-06-12T07:44:26Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11552/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11552",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11552",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:16.979345",
      "comments": [
        {
          "author": "cavazzo",
          "body": "Hello, seems like you are setting guardrails for yor key, that feature is only available in enterprise plan, you need to remove it in order to create the key.\n\nRefers: https://docs.litellm.ai/docs/proxy/guardrails/quick_start#-control-guardrails-per-api-key",
          "created_at": "2025-06-09T14:10:19Z"
        },
        {
          "author": "i-makashev",
          "body": "Hi, Thank you, that was helpful",
          "created_at": "2025-06-12T07:44:26Z"
        }
      ]
    },
    {
      "issue_number": 11656,
      "title": "[Bug]: Unknown Premium error",
      "body": "### What happened?\n\nI am trying to add a model to Litellm ui.\n\nWhen I try to add a team, I get a premium warning.\n\nThe warning is very superficial. It is not clear what exactly it is caused by. But when I try to add a team, I get this error. Why should adding a team be Premium? How can users be tracked if it is not one of the most basic functional features? The interesting thing is that we can add models while adding a Team.\n\n![Image](https://github.com/user-attachments/assets/28488eb4-01fb-4020-ae93-1d49594c81c2)\n\nIf premium errors are to be given, the input fields should turn red or be \"unclickable\" until premium is activated, just like in form controls.\n\n### Relevant log output\n\n```shell\n\"error\":{\"message\":\"{'error': 'You must be a LiteLLM Enterprise user to use this feature. If you have a license please set `LITELLM_LICENSE` in your env. Get a 7 day trial key here: https://www.litellm.ai/enterprise#trial. \\\\nPricing: https://www.litellm.ai/#pricing'}\",\"type\":\"auth_error\",\"param\":\"None\",\"code\":\"403\"}}\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.72.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "MustafaDUT",
      "author_type": "User",
      "created_at": "2025-06-12T07:22:46Z",
      "updated_at": "2025-06-12T07:22:55Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11656/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11656",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11656",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:17.141538",
      "comments": []
    },
    {
      "issue_number": 11262,
      "title": "[Bug]: ollama_chat streaming tool calls cannot be reconstructed, due to missing tool call id from ollama",
      "body": "### What happened?\n\n‚è≥ Context: Streaming tool calls with Ollama was not supported until recently (cf. https://github.com/BerriAI/litellm/issues/11104 and https://github.com/BerriAI/litellm/pull/11171).\n\n‚ö†Ô∏è Current situation: Streaming tool calls technically work in that you can see them in the streamed chunks, but it is currently not possible to reconstruct the full model response including the tool call with `response = litellm.stream_chunk_builder(chunks)`.\n\nüîç The root cause is that `ChunkProcessor.get_combined_tool_content` silently drops all tool calls because Ollama does not return tool call ids as part of its response (cf. line 154 below):\nhttps://github.com/BerriAI/litellm/blob/44a69421ead8dcde0f2a5c9be995c49eaa9a1fea/litellm/litellm_core_utils/streaming_chunk_builder_utils.py#L151-L167\n\nHere is a unit test that reproduces the issue:\n```python\n\"\"\"Test LiteLLM's support for tool use with Ollama.\"\"\"\n\nimport asyncio\n\nimport litellm\nimport pytest\nfrom litellm import stream_chunk_builder\n\n\n@pytest.mark.parametrize(\n    \"stream\", [pytest.param(False, id=\"stream=False\"), pytest.param(True, id=\"stream=True\")]\n)\n@pytest.mark.parametrize(\n    \"sync\", [pytest.param(False, id=\"sync=False\"), pytest.param(True, id=\"sync=True\")]\n)\ndef test_litellm_ollama_tool_use(sync: bool, stream: bool) -> None:  # noqa: FBT001\n    \"\"\"Test LiteLLM's support for tool use with Ollama.\"\"\"\n    tools = [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"get_weather\",\n                \"description\": \"Get the weather for a location\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\"location\": {\"type\": \"string\"}},\n                    \"required\": [\"location\"],\n                },\n            },\n        }\n    ]\n    completion_kwargs = {\n        \"model\": \"ollama_chat/qwen3:4b\",  # Important: use `ollama_chat` instead of `ollama`\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"What's the weather like in New York?\"},\n            {\n                \"role\": \"assistant\",\n                \"content\": (\n                    \"'<think>\\nOkay, the user is asking about the weather in New York. \"\n                    \"Let me check the tools available. \"\n                    \"There's a function called get_weather that takes a location parameter. \"\n                    \"So I need to call that function with 'New York' as the location. \"\n                    \"I should make sure the arguments are correctly formatted in JSON. \"\n                    \"Let me structure the tool call accordingly.\\n</think>\\n\\n\"\n                ),\n            },\n        ],\n        \"tools\": tools,\n        \"stream\": stream,\n    }\n    if sync:\n        # Synchronous completion\n        response = litellm.completion(**completion_kwargs)\n        if stream:\n            response = stream_chunk_builder(list(response))\n    else:\n        # Asynchronous completion\n        async def _acompletion(**kwargs):  # type: ignore[no-untyped-def]\n            response = await litellm.acompletion(**kwargs)\n            if kwargs.get(\"stream\"):\n                return [chunk async for chunk in response]\n            return response\n\n        response = asyncio.run(_acompletion(**completion_kwargs))  # type: ignore[no-untyped-call]\n        if stream:\n            response = stream_chunk_builder(response)\n    assert response.choices[0].message.tool_calls, \"No tool call detected\"\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.71.2.dev2\n\n### Twitter / LinkedIn details\n\n@LaurentSorber",
      "state": "closed",
      "author": "lsorber",
      "author_type": "User",
      "created_at": "2025-05-30T10:04:01Z",
      "updated_at": "2025-06-12T06:50:51Z",
      "closed_at": "2025-06-08T03:50:08Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11262/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "krrishdholakia"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11262",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11262",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:17.141558",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Thanks - I should have qa'ed it better. I'll try and have this out for today's stable release. ",
          "created_at": "2025-05-31T15:45:44Z"
        },
        {
          "author": "lsorber",
          "body": "Any progress on this yet @krrishdholakia? Don't mean to push, just curious!",
          "created_at": "2025-06-06T20:36:07Z"
        },
        {
          "author": "krrishdholakia",
          "body": "able to repro - sorry for the delay",
          "created_at": "2025-06-08T01:28:00Z"
        },
        {
          "author": "lsorber",
          "body": "I confirm that that 1.72.3 fixes the issue!",
          "created_at": "2025-06-11T13:44:45Z"
        },
        {
          "author": "douo",
          "body": "The issue still exists. In version 1.72.3, the `get_weather` test case passes, but when I change the `get_weather` tool to `get_time`, the issue reappears.\n\n```python\n\"\"\"Test LiteLLM's support for tool use with Ollama.\"\"\"\n\nimport asyncio\n\nimport litellm\nimport pytest\nfrom litellm import stream_chunk",
          "created_at": "2025-06-12T03:05:39Z"
        }
      ]
    },
    {
      "issue_number": 11434,
      "title": "[Bug]: `input_type` not supported on proxy for azure_ai cohere-embed-v-4",
      "body": "### What happened?\n\nDirect call to Azure:\n```\ncurl --request POST \\\n  --url https://cohere-embed-v4.eastus2.models.ai.azure.com/v2/embed \\\n  --header 'accept: application/json' \\\n  --header 'content-type: application/json' \\\n  --header \"Authorization: Bearer xxx\" \\\n  --data '{\n    \"model\": \"cohere-embed-v4-0\",\n    \"input_type\": \"image\",\n    \"embedding_types\": [\"float\"],\n    \"images\": [\"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD//gAfQ29tcHJlc3NlZCBieSBqcGVnLXJlY29tcHJlc3P/2wCEAAQEBAQEBAQEBAQGBgUGBggHBwcHCAwJCQkJCQwTDA4MDA4MExEUEA8QFBEeFxUVFx4iHRsdIiolJSo0MjRERFwBBAQEBAQEBAQEBAYGBQYGCAcHBwcIDAkJCQkJDBMMDgwMDgwTERQQDxAUER4XFRUXHiIdGx0iKiUlKjQyNEREXP/CABEIAZABkAMBIgACEQEDEQH/xAAdAAEAAQQDAQAAAAAAAAAAAAAABwEFBggCAwQJ/9oACAEBAAAAAN/gAAAAAAAAAAAAAAAAAAAAAAAAAAHTg9j6agAAp23/ADjsAAAPFrlAUYeagAAArdZ12uzcAAKax6jWUAAAAO/bna+oAC1aBxAAAAAAbM7rVABYvnRgYAAAAAbwbIABw+cMYAAAAAAvH1CuwA091RAAAAAAbpbPAGJfMXzAAAAAAJk+hdQGlmsQAAAAABk31JqBx+V1iAAAAAALp9W6gRp826AAAAAAGS/UqoGuGjwAAAAAAl76I1A1K1EAAAAAAG5G1ADUHU0AAAAAAu/1Cu4DVbTgAAAAAA3n2JAIG0IAAAAAArt3toAMV+XfEAAAAAL1uzPlQBT5qR2AAAAAenZDbm/AAa06SgAAAAerYra/LQADp+YmIAAAAC77J7Q5KAACIPnjwAAAAzbZzY24gAAGq+m4AAA7Zo2cmaoAAANWdOOAAAMl2N2TysAAAApEOj2HgAOyYtl5w5jw4zZPJyuGQ5H2AAAdes+suDUAVyfYbZTLajG8HxjgD153n3IAABH8QxxiVo4XPKpGlyTKjowvCbUAF4mD3AAACgqCzYPiPQAA900XAACmN4favRk+a9wB0xdiNAAAvU1cgAxeDcUoPdL0s1B44atQAACSs8AEewD0gM72I5jjDFiAAAPfO1QGL6z9IAlGdRgkaAAABMmRANZsSADls7k6kFW8AAAJIz4DHtW6AAk+d1jhUAAAGdyWBFcGgAX/AGnYZFgAAAM4k4CF4hAA9u3FcKi4AAAEiSEBCsRgAe3biuGxWAAACXsoAiKFgALttgs0J0AAAHpnvkBhOt4AGebE1pBtsAAAGeySA4an2wAGwEjGFxaAAAe+c+wAjKBgAyfZ3kUh3HAAAO6Yb+AKQLGgBctmb2HXDNjAAD1yzkQAENRF1gyvYG9AcI2wjgAByyuSveAAWWMcQtnoyOQs8qAPFhVh8HADt999y65gAAKKgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAf/8QAGgEBAAMBAQEAAAAAAAAAAAAAAAEFBgIEA//aAAgBAhAAAAAAAAAAAAABEAAJkBEAAB0CIAABMhyAAA6EQAAA6EQAABMiIAAAmREAAAmQiAABMgOQAEyAHIATIACIBMu7H3fT419eACEnps7DoPFQch889Wd3V2TeWIBV0o+eF8I0OrXVoAIyvBm8uDe2Wp6ADO+Mw9WDV6rSgAzvjMNWA1Op1AARlvmZbOA3NnpfSAK6iHnwfnFttZ9Wh7AeXPcB5cxWd3Wk7Pvb+uR8q+rgAAAAAAAAP//EABsBAQABBQEAAAAAAAAAAAAAAAAEAQIDBQYH/9oACAEDEAAAAAAAAAC20AL6gCNDxAArnn3gpro4AAv2l4QIgAAJWwGLVAAAX7cQYYAAFdyNZgAAAy7UazAAABsZI18UAAE6YEfWgACRNygavCACsmZkALNZjAMkqVcAC2FFoKyJWe+fMyYoMAAUw2L8t0jYzqhE0dAzd70eHj+PK7mcAa7UDN7VvBwXmDb7EAU5uw9C9KCnh2n6WoAaKIey9ODy/jN+ADRRD2fpQeY8P0QAU5zGel+gg8V53oc4AgaYTfcJ45Tx5I31wCPobQ2PpPRYuP8APMZm2kqoxQddQAAAAAAAAP/EAFMQAAEDAgIDCQkMBwUIAwAAAAECAwQFEQAGBzFREhMhMEBBYXGBCBQYIjJCRlDSFSBSVGJygpGTobHREDRDc6LBwiMzU3CyFiQlNVVkdISSlLP/2gAIAQEAAT8A/wAo74nVaBAb32bNYitfDfcS2PrURiZpU0dwVFMjN1OVY8O8u7//APkFYc076LmfSVSvmQpB/ox4QGjH/r7v/wBGR7OPCA0YH0ge7IMj2ceEBowPpA92QZHs48IDRgfSB7sgyPZx4QGjA+kD3ZBkezjwgNGB9IHuyDI9nHhAaMD6QPdkGR7OPCA0YH0ge7IMj2ceEBowPpA92QZHs48IDRgfSB7sgyPZx4QGjA+kD3ZBkezjwgNGB9IHuyDI9nHhAaMD6QPdkGR7OPCA0YH0ge7IMj2ceEBowPpA92QZHs48IDRgfSB7sgyPZx4QGjA+kD3ZBkezjwgNGB9IHuyDI9nHhAaMD6QPdkGR7OPCA0Y89fd7IMj2cN6e9GDpCTmRaOuFI9nEDSlo9qakpj5upoJNgH3d4+50JxGlxpbSH4r7bzSvJW0sLSeop5NWsw0fL8RU2rVGPDjJ4C6+4EAnYnaegYzV3StDhFcfK1LdqDuoSZBLDHWlPlqxXtNmkOulaVVxcFg3/sYA73A+kLrxKnTJrpfmSXX3jrcdWVqPWVYudvJ7nbil16s0R7vikVSVDduCVR3lNk9e5IvjKfdG5rpKmo+Yo7NXi8ALlgxJH0kiysZL0l5Uzsz/AMFn2l7m7kJ8BuSj6PnAbU8ieeZitOPPuoQ22krWtZCUpSkXJJOoDGkHui4MBT1MyW2ibITdJnuA97o/dJ1uHFczFXMyzV1Gu1N+bJV57yr7kbEjUkdA5dGlSYb7UqJIcZfaUFtuNLKFoUNRSocIONF3dBb6tih58eSCQEM1PUOqT7eELS4lK0KCkkAgg3BB4/M2Z6NlKlSKtWJiI8VoWueFS1nUhA85ZxpJ0v13Pj7kNorg0NC7tw0K4XNi3yPKPRqHqLQnpkeoD8XKmZZJVSHCG4klw/qijqQs/wCF/pwDfjc1ZqpOUKNLrVXf3qMyLJSLFbrh8ltA51qxn7P9az9V1z6istxWypMSIhRLbCD+Kj5yvUYJHCMdz7pLXWoByfWJBXUILV4bizwvRk+Z0qa4yoTodKgyZ859DEWO0t11xZslCEC5UrGlHSNOz/XVvBa26RFKkQY+xHO4v5a/UtArU3LlZptbpzm4lQ30ut7DbWk9ChwHGXq5EzHQ6ZWoCv8AdpsdDyRrIKtaFdKTwHi+6I0hrffGRKU/ZloodqSkngW5rQz1I1n1P3M2ZzJpFYyvIXdUJ0SowP8AhP8AAtI6AvitIWbWclZVqlbWElxpvcRmz+0kOcDaf5nEyXJnypM2Y8p2Q+6t11xRupa1m6lHpJ9T6B6uaVpHo7alEMz0PQnepxN0/wASRgauJ7pTNZmVynZTjuXZpzYkSRtkPDgB6UI9UZMlrgZsy1MQqxZqkRy/QHRfA4iZIaiRX5D6ghpptTi1bEIFycZmrL2YcwVitvk7ubLdfsfNClcCewcHqiiX91qbbX3yz/rGBxGmKse4ujnMz6F2dfjiGj/2VBs/ccE3J9UZOirm5ry3EQm5eqkRu3Qp0YHEd01PLGUqPT0mxk1QLV0oZaPteqdBtKNV0kUIkXah77Md6mkcH8RGBq4jupH7JyXG/wDPcP1tj1T3MuWVMQK5mt9FjJWmDGO1tHjuHqJ4nupEnvrJa+beZ4/jR6ooNGnZhrFOotNa3yXMeS02OvWo9CRwk4ytQIeWKDS6HC/V4TCWgq1itWtSz0rPCeJ7qKNenZSl2/upEtonpcShXqcC+NA+jFeW4H+1NbYKatOaswysWMaOrbscc4rujaYZuj/vzccMCpR3yehwFn+r1MAVGwGNDOhVbK4ubc4xLLFnYMB1PCNjrw/BHF58opzDk7MlHSndOSID28ja6gbtH3jChZRHqShZerOZag1S6JT3pcpzUhsahtUTwJTtJxow0G0vKRYreYS1PrIAUhNrx4yvkA+WsfCONXFnGlTLZytnqvU5KLRlvmTG2Fl/xwB0J1eookOXPkNRYUZ1991W5baaQVrWdiUi5JxkbudKzVCzOzg+abE196NWXKWOnWlvGW8p0DKMEU6g01qKzwFe5F1uEDynFnhUeO7pTJ5n0aBmyK3d+mneJVtZjOnxVfQX6ghwZtRktQ4EV6RJcNkNMoK1qOwJTcnGTe5yr9V3qXmuSKXFNj3uizkpY/0oxlbIOVslRt6oVKaZdIst9XjyHPnOK4ezkFVgw6vAmU2ewHYsllbDiFaloWNyoYz1lKZknMtRoEu6gyvdMO8zrC/IXy2j0Cs5glpg0WmyJkk+YwgrIG1WwdJxk7uap75amZyqQit6zChkLe6lueSnGWcl5ayjGEegUliKCAFuAbp5z57irqPI9NOjVOdqB31T2x7tU5KlxNryNa2CenWnDra2XFtOoUhaFFKkqFiCOAgg8qyro7zdnJwCh0Z5xi9lSVje46etarA22DGUe5spEPe5ebqgue78Ui3aj9Sl+WvFIodHoMREGj02PDjJ1NMNhAJ2m2s8m07aIHJi5WdMsxSZFiuoxG08LoGt9sDz/hjGrkzLD0hxDLDSluLISlKQSpRPMAMZU0C54zFvcidHTR4Sv2k24dI+SyPG+u2MqaBskZc3qRLimrzEftZoBaB+S0PFw0y2y2hppCUIQAEpSAAAOYAauU6XtBJmuycy5LjASVXcl05sWDu1bGxe1GHWnGXFtOoUhxCilSVAghSTYgg6iOR5eyfmXNT/AHvQKNJmKBspTaLNo+es2SntOMq9zNIc3uTm+sBoazEgWWvtdWLDGWchZTyk2E0KiR4zlrKkEbt9XW4u6uW6SNDNAzwHZ7BTTq3YkSm0XS7sS+ka/na8ZuyJmbJMwxK9T1NJJs1IR47D3S2vj2mXXlobabUtaiAlKRcknUAMZV0F56zJvT8iEKVCVY77PuhZHyWvLxlTuesl0Te3qqlysy08JMnxI4PQ0n+onEWDFhMNxokdphhsWQ20gIQkbEpFgPeyqnBg/rMhCCBfc3ur6hw4lZ1hNbpMdlbpGokhKT+OHs7zVf3EdpHzgVfzGDnGqnnbHUkYGcqqOZo/OT+VsMZ5eBG/w0K2lJKPaxDzfTJBCXFLZUTbxk3+q2GJTEhAcYdQtB1KSoEckqdLp1ThvQqnEZkxXU7lbLyAtCusKxnPubKVNU9NyhOMB03Pekm7kfsXwqRjM+jfOWUVLNZochEcapLY31gj56LgduLHZxNjjL+TM0ZpcDdCokuWL2LiEWaSflOKskYyt3M8t0tSM31hLCNZiwbLc7XVCwxljR9lHKDaRQ6Kww6BZUlQ32Qr6a7nAAHvFLSkEqUAAMT81UyGClDm/r2N6u1WKhm2oywpDKt4bPMjX/8ALC3HHCVLWSSbm+338adLhuB2O+tChzg4pOdOFDVRRbm31A/EflhiQ1IbS6y4laFaik3HJCkKBBAII4RjMOibIOYCtc/LkZD6tb0W8Zy+0luwVisdzDRX925RMyS4uxMtlD46gUFGKj3NWdY11wajSpbf71bS/qUnErQTpPjXIy2Xk7WZLCv68L0R6R2/KylO+ikK/A4Tom0jL1ZRqHa3bEXQjpPlkBGVXkDa48yj8V4p/c358lEGW/TIaOcOSCtfYG0qxSO5gp6AldczQ+9tbhsBr+NwqxRNDWjygFDjGXmpL4N99nEyVH6K/FGGmGY7SGm20oQgAJSkAJAHMAPeyJ8WEjfJD6EX1XP4DWTioZ1ZRdEBndnmWvgT2DE6tVCoE98SFFPMgGyR2DBN+E8XSq3MpToUyu7ZIK0HUcUmsRapGK46wlfBuknWnk5AOsY3I2YsNmLAagPf1HMFNp+6S68FOD9mjhV+QxUM5THrohJDKNutWHpL8halvOqWo6yokk8fT58inSESI6ylST2EbDtGKRU49VitvtkJI8tOsg7OOJA1nFSzhQKaVIkT21OA23DV3Fdu51Yk6VICCREpzznS4pKPw3WDpXk34KOgD9+fZwxpWB4JNIIG1D1/xTinaSMvylJDy3YyjwDfUXH1pviFPhTGw/FkNuoOpbagofdxU2fHhMqekOBDadus4q+bJcwqahkssfxnrOFKKjckk8iodWcpUxDySS2rgcTfWMMPtvstvNKCkLSFJI5weMzFm6mZfQUvL32UQCiOg+N1q2DFbzlWa2paXHyzGOplolKbfKOtWLnb72FUp9NeD8GU4y4OdBtfr2jGW9JTbqm4tdQlCr2D6fIPzxzYadbdQhxpYUlQBBBuCD7+pVKPTIq5D6uAcCUjWpWwYqtWlVV9Tr6yE6kIHkpHJcl1cqS5TXjfc+O3f7xxedc6IoqTAgEKnqHCdYZB5ztVsGH5D0p5x+Q6px1ZKlKUbknico5zk0J5EWWtTtPWeFOstdKejaMR5TMxhuQw4lbTiQpKkm4UD7151thtbriwlCElSidQAxXaw7VZalXsyglLadg/M8mpstcKbHko1oWDbb0duGXEOtIcQbpUkKB2g8Tm3MSMv0xbySDJduhhB+FtPQMSJD0p5yRIcK3XFFSlK1kni9HealU+UijzFjvZ5X9iVHyHDzdSve5yqqm2kU5pViuynCNnMOUZVld80lgKsVNEtns4QPqPEKNgTjOdbVWq0+tC7xmCWmRzWTrV2njEqUhQUkkEG4Ixk6ue7dFjPuuXeau08Plp5+0cP6VrS22pSiAACSdgGKpMXPnSJK/PWSBsHMOzlGRX/EmsW8koWOs3B4jONTNNoNQkIUUr3ve27awpzxb4PCTxujGpKYqkinKV4klvdJ+e3+nMkjvakS1DWtIb7FcB+7BNyTyjI67S5CDzsqP1EcRpUkqRTqfFBtvr6l9iE2/nx2V5XeeYKS9/3CEdizuD+OEm4/RnVak0+OhJtd256gm38+U5JTeY+rYyofeniNKyjv8AR0c24f8AxTx1NJTUYKhrD7Z/iGEeSP0Z63Pe8Xc6hur9dxynI7JtNeOqyAO0m/EaVv1mj/Mf/FPHU7/mEL98j8cI8gfozq2pdOZWnmdseopJ5TlKIWKShZFi8tSz2eL/AC4jSsx/Y0qR8FbqD9IA8dQmFSK1S2UjypTQ7N0L4SLJ/RmOOJVIloSk+Ijdjb4nCcEWJB5PDjrlSWWGxdS1hI7TiHHRGjsso8htCUDqSLcRpDppl5ckLABXHUl8DYBwH7jx2juAZeYmXyk7iM2t07L23I/HA/QtIWkpULggjFXgqp8+RHINkrO5O0axyfJlLK3l1F1Pit3S3cecRr7BxMqM3IjusOpCkOoKVjakixGKzTXaTU5cB4HdNOEAnzk6we0cbo3o5g0hU91FnZhCh+7T5PvM6UjfWkTmE3W0LObSnmPZyanQHqjKajMjhUeE2uANpxAhNQYzTDabNtpsOk85PXxWkjLJmRk1mGjdPR0WdA85rb9HjMqUByv1Rtgg97N2W+vYjZ1qww02y2htCQlCEhKUjUAPeLQlxCkLAUlQsQdRBxmKiOUqWopSox1m6FHht0HkjDDsl1DLKCpajYAYoFFRSYw3dlSF8K1bPkji1JCgUkXBxnjJTlJecqVOZvCWbrQn9kT/AEniqVSplYmNQoTRW4s9iRzqUeYDGXaBFoFPbiMC6/KdctYrVt/Ie+qECNMjKjyE7oLHaOkYrVEkUl8hQKmVE7hY1HkUOFInPoYjtla1bMUDLzNKb3xyy5KvKXzDoTxrjaHEKQ4gKSoWIIuCDzYzTo5WlTk2ggEG6lxr6vmH+WHmXWHFtPNqQ4k2UlQIIOwg+/y/lCq19xKm2yzFv4z7g8X6I844oOXoFBiiPDb4TYuOny1kbTxEmOxKaVHebS4hXlA4rWTpEdSnqfdxu5JR5w6tuFtONKKXEFJBsQeOShSzZIvilZTnTShySCwyfhDxj1DFPpcSmtBuM0B8JR4VK6zyCr5apFaQROiJWsCwdT4qx1KGKloseG7XSp4UnmQ+LfxJxJyLmaMoj3OU4n4TakqwrLVfSbGjy/sV4ZyhmN/yKRI+kncf6rYhaM64+QZa2YyOk7tQ7E4o+jyiU0h2SgzHhzu+R2I/PCEIbASgAJAsAOLqFFp84HvphKlkCyhwK4OnZiXkcElUKV9Fz2hh/KdZataPuwfOSoEYXQqog2MJ49Taj/LHuNVPiEj7Jf5Y9xqp8QkfZL/LHuNVPiEj7Jf5Y9xqp8QkfZL/ACx7jVT4hI+yX+WPcaqfEJH2S/yx7jVT4hI+yX+WEUCquaoTw+chQ/EYYyjWHQSpgN9K1C33XOIuR0+VMlfRbH8ziFRKdTwksRkhY89XjK+/VyWwxYf5ef/EADgRAAIBAgMDCQUHBQAAAAAAAAECAwQRAAUgMUFhEhMhIjBAUXGREDJQU6EGFDNCYoGSUnKiwdH/2gAIAQIBAT8A+L37e/wE9zHfj3k90Gk90Gk9ztqPcbd3t3e3b2129qRySGyIScRZY56ZXtwGFoKZfyX8zj7rT/JX0w+X0zbFKngcTZdLHdozyx9cbOg9pbFtENJPNYqlh4nEOWxJYykufQYVFQWRQBw1VVGk4LKAJPHxwysjFWFiNUsscKGSVwqjecVOfgErSxX/AFNhs5r2P4oHkoxHndchHKZXHFf+YpM7gnISYc0/+J0KpYhVFycUtCkQDygM/huHZZjThl59R1l97iNMsqQxvLIbKoucV1dLWykkkRg9VdOUZmyOtLO10PQhO4+Hty6mCrz7jpPu+XZsoZSp2EEYkQxyOh/KSNGf1JAipVO3rNq2EHGW1P3mkikJ6w6reYxGpd0QbyBhVCqFGwC3aV4tUycbHRnLFq+UeAUfTX9nmJhqE3BwfUYoxeqi8+1ryDVPwA0ZwCMwm4hT9Nf2eB5qobcWUfTFM3Inib9Q7QkAEnYMSvzkrv4knRn8BEkVQB0Ecg+Y15RTmCij5Qsz9c/v7KWYTQo28dDefZ5hUBI+aU9Z9vAaamnSqheF9jD0OKmmlpZWilFiNh3Eacqy9quUSSLaFDc8T4YAt7KWpNPJfap94YR1kUOhuD2NTVJTr4vuGHdpHZ3NydVVSQVaciZfIjaMVOR1URJhtKvocNSVSmzU8gP9pxHQVkhASnf9xbFJkJuHq2Fv6F/2cIiRoqIoVQLADRBUSwG6Ho3g7DiLMYX6Huh9RgTwtslT1GOdi+YnqMc7F8xP5DHOxfMT+Qxz0XzE9Rh6ymTbKD5dOJsyY3WFbcThmZiWYkk7z8W//8QAOREAAgECAgYHBwMDBQAAAAAAAQIDAAQFERITICExkQYwQVFSYXEQFCJAQlOBMlChI4KSYnJzsbL/2gAIAQMBAT8A/YCyjiwFa2PxjnWtj8Y51rY/GOda2PxjnWtj8Y51rY/GOda2PxjnWtj8Y51rY/GOda2PxjnWtj8YoMp4EHq5LlV3LvNPNI/FuXW5kcDUdw6cd4pJFkGanbJABJqacvmq7l+RR2Rgy0jiRQw2rmXM6CncOPydq+T6B4HZmfQjJ7eA+UQ6LqfMbN229V/Pyg4j1GzcnOVvlIV0pFH52bgZSt8pbRaC6TcTs3YycHvHyQBJAFQ2+WTyfgbVymlHmOI+Rjt3fe3wio4kj4Df39RNGY38jw60AscgMzSWrHe5yFJEkfBd/f1UiLIpU1JG0ZyPVJE7/pWktRxc/gUqKgyVQOtZVcZMMxUlqw3pvHdRBU5EEbIBO4CktpG3t8IpLeNOzM+fsSN5DkikmosPY75Wy8hS2duv0Z+te7wfaXlT2Nu3BSvoalsJE3xnTH81vG49UVVtzAGjbRH6cq90TxGvdE8RoW0Q7M6Cqu5VA9kVrNLvC5DvNRWEa75CWPIUqqgyVQB5bVzarMCy7n7++mUoxVhkRtW9tPdypBbRNJI3BVFYf0FdlWTErnQP24uP5JqLojgUYyNqznvZ2q46GYLKDq0khPejk/8ArOsU6HX1irTWre8xDeQBk4/FHduPtALEKozJq3skjAaQaT/wOqv4NJdco3jj6bNtby3c8VtAulJIwVRWCYJb4PbKqqGnYDWSdpPcPLZ6V9HEmikxOxjAlQaUqL9Q7x5+2xgCrrmG8/p9OrIDAg8CKkTQd07iRsdBcPV3ucSkX9H9KP1O8naIBBBG410gsBh2K3MCDKNjrE/2tSLpuqDtIFKAqhRwA6y9GVw/mAdjohEEwK2I4u0jH/Lb6exgXljL2tEwP9pq0GdzF69bfHO4fyAGx0ScPgVpl9JkB/yO309cG6w9O0ROeZq3bQnib/UOsJyBJqV9ZI7952Ogl8DDdYezfEra1B5HcdvpTfC+xicoc44QIl/t4/z7LaUTRK3bwPr1d9PoJqlPxN/A2cOvpsNvIbyA/Eh3jvHaDWHYjbYnapdWzgg/qHap7js9JseTDLZreBwbuVSAB9AP1GiSSSeJ9ltcGB8/pPEUjq6hlOYPU3FykC97dgp3aRi7HMnaw3FbzCptdaSZeJDvVh5isO6aYdcqq3gNvJ25705ikxXDJAGS/gI/5FqfHMIt10pb+H0DBjyGdYr03XRaLCojnw1sg/6FTTSzyPNNIXkc5szHMnYhuJIDmh3doPCo7+F9z5oaE0R4SrzrWR/cXnWsj+4vOtZH9xeYrWx/cXmKe6gTjID6b6lxAnMQrl5mmYsSzEkn92//2Q==\"]\n  }'\n```\nworks fine as expected!\n\nadded model to config.yaml:\n```\n- model_name: cohere-embed-v4-0\n    litellm_params:\n      model: azure_ai/cohere-embed-v4-0\n      api_base: https://cohere-embed-v4.eastus2.models.ai.azure.com\n      api_key: os.environ/AZURE_AI_KEY_COHERE_EMBED_V4\n    model_info:\n      mode: embedding\n      base_model: azure_ai/embed-v-4-0\n      region: \"East US 2\"\n```\n\nNow, same call via litellm proxy:\n```\ncurl --request POST \\\n  --url https://localhost/v2/embed \\\n  --header 'accept: application/json' \\\n  --header 'content-type: application/json' \\\n  --header \"Authorization: Bearer xxx\" \\\n  --data '{\n    \"model\": \"cohere-embed-v4-0\",\n    \"input_type\": \"image\",\n    \"embedding_types\": [\"float\"],\n    \"images\": [\"data:image/jpeg;base64,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\"]\n  }'\n```\nresponse: `{\"detail\":\"Not Found\"}`\n\neven tried same request with this base url: `https://localhost/embeddings`\nbut got error: `{\"error\":{\"message\":\"Router.aembedding() missing 1 required positional argument: 'input'\",\"type\":\"None\",\"param\":\"None\",\"code\":\"500\"}}`\n\nI believe https://github.com/BerriAI/litellm/pull/11346/commits/5cc416f9e4c908082fe82a9349535f727e0cf4bc#diff-39e2a0167b06d30a355b0708bb7be6674837443436b66b2f91257d4ac0d5ee58 only fixed it for the sdk, not the proxy\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\n1.72.1-nightly\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "taralika",
      "author_type": "User",
      "created_at": "2025-06-05T09:14:03Z",
      "updated_at": "2025-06-12T06:18:04Z",
      "closed_at": "2025-06-06T06:30:56Z",
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11434/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11434",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11434",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:17.333275",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "> localhost/v2/embed\n\nThis is correct, there is no `/v2/embed` on litellm",
          "created_at": "2025-06-06T06:28:55Z"
        },
        {
          "author": "krrishdholakia",
          "body": "> but got error: {\"error\":{\"message\":\"Router.aembedding() missing 1 required positional argument: 'input'\",\"type\":\"None\",\"param\":\"None\",\"code\":\"500\"}}\n\nyou need to pass the `images` as `input` @taralika \n\nOur api - `/v1/embeddings` is in the OpenAI format - and so expects the input to be in the `inp",
          "created_at": "2025-06-06T06:30:28Z"
        },
        {
          "author": "krrishdholakia",
          "body": "This error looks due to user error - incorrect field used. \n\nLet me know if this persists after changing the field from images -> input ",
          "created_at": "2025-06-06T06:30:54Z"
        },
        {
          "author": "taralika",
          "body": ">you need to pass the images as input @taralika\n> Our api - /v1/embeddings is in the OpenAI format - and so expects the input to be in the input field\n\n@krrishdholakia can you provide an example? I do not see any sample of how to send images to /v1/embeddings here https://platform.openai.com/docs/ap",
          "created_at": "2025-06-11T19:54:25Z"
        },
        {
          "author": "krrishdholakia",
          "body": "@taralika https://docs.litellm.ai/docs/embedding/supported_embedding#image-embeddings",
          "created_at": "2025-06-11T20:30:40Z"
        }
      ]
    },
    {
      "issue_number": 11648,
      "title": "[Feature]: support extra containers in migrations-job.yaml helm template",
      "body": "### The Feature\n\nSupport `extraContainers` for `migrations-job.yaml `, analogous to the `deployment.yaml` template.\n\n### Motivation, pitch\n\nI'm using CloudSQL proxy sidecars to connect to Postgres. The deployment is happy but the migration job fails without the proxy.\n\nI'm planning on submitting a PR for this but wanted an issue tracker ticket to reference.\n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "stevenaldinger",
      "author_type": "User",
      "created_at": "2025-06-12T02:32:53Z",
      "updated_at": "2025-06-12T06:16:18Z",
      "closed_at": "2025-06-12T06:16:18Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11648/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11648",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11648",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:17.622548",
      "comments": []
    },
    {
      "issue_number": 11266,
      "title": "[Bug]: LiteLLM Proxy $0 Cost Tracking for Codestral",
      "body": "### What happened?\n\nHey all, in our LiteLLM Proxy set-up under `v1.71.2-nightly` all requests to `text-completion-codestral/codestral-latest` are tracked with $0.\n\nWe've previously had this problem with `azure/gpt-4.1` as well on `v1.71.1-stable` but this is fixed with the new version. Thanks for that!!!\n\nOur `values.yaml` looks like\n```yaml\n...\nmodel_list:\n  - model_name: codestral-latest\n    litellm_params:\n      model: text-completion-codestral/codestral-latest\n      api_base: <base-url>\n      api_key: <api-key>\n    model_info:\n      input_cost_per_token: 0.0000003\n      output_cost_per_token: 0.0000009\n...\nlitellm_settings:\n  set_verbose: false\n  json_logs: true\n  cache: true\n  cache_params:\n    type: redis\n    supported_call_types: []\n    port: 6379\n  redact_messages_in_exceptions: True\n```\nhowever, this is the only model, that displays `cost_per_token` in exponential-notation in the LiteLLM metadata logs of a request:\n```\n...\n  \"input_cost_per_token\": \"3e-07\",\n  \"output_cost_per_token\": \"9e-07\",\n...\n```\nCould this be the cause? Happy to provide more information.\n\nCheers, Andreas ‚úåÔ∏è\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.71.2-nightly\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "ndrsfel",
      "author_type": "User",
      "created_at": "2025-05-30T13:42:09Z",
      "updated_at": "2025-06-12T06:07:24Z",
      "closed_at": "2025-06-12T06:07:23Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11266/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11266",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11266",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:17.622571",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Hey @ndrsfel can you share the complete debug log? and the request used to trigger this (is it on `/chat/completion` or `/completion`, etc.) ",
          "created_at": "2025-05-31T15:43:39Z"
        },
        {
          "author": "ndrsfel",
          "body": "Hey @krrishdholakia we've figured it out what went wrong. Sharing our solution might be helpful for you as well.\n\nAs stated in the description above, we define \n```yaml\nmodel_info:\n  input_cost_per_token: 0.0000003\n  output_cost_per_token: 0.0000009\n```\nin our yaml, which then gets converted to  \n``",
          "created_at": "2025-06-11T15:30:59Z"
        },
        {
          "author": "krrishdholakia",
          "body": "That's helpful - we should fix that. Thanks for the work on this @ndrsfel ",
          "created_at": "2025-06-11T15:47:06Z"
        }
      ]
    },
    {
      "issue_number": 11087,
      "title": "[Feature]: Support Gemini imagen",
      "body": "### The Feature\n\n```\nresponse = image_generation(model='gemini/imagen-3.0-generate-002', prompt='futuristic cityscape at sunset', n=1)\n```\nreturns\n```\nImageResponse(created=1787925156, data=[], usage=Usage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\n```\nPlease add support for imagen.\n\n### Motivation, pitch\n\nImage generation\n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "boosh",
      "author_type": "User",
      "created_at": "2025-05-23T10:14:08Z",
      "updated_at": "2025-06-12T03:43:11Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11087/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11087",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11087",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:17.827148",
      "comments": [
        {
          "author": "Classic298",
          "body": "Imagen 3 works for me (proxy).",
          "created_at": "2025-05-23T10:18:43Z"
        },
        {
          "author": "boosh",
          "body": "Strange. Can you share some sample code please?",
          "created_at": "2025-05-23T10:20:26Z"
        },
        {
          "author": "Classic298",
          "body": "```\n### Image Generation\n\n  - model_name: imagen-3.0-generate-002\n    litellm_params:\n      model: vertex_ai/imagen-3.0-generate-002\n      aspectRatio: \"16:9\"\n      person_generation: allow_all\n      safety_filter_level: block_none\n    model_info:\n      output_cost_per_image: 0.04\n```\n\n(btw litellm ",
          "created_at": "2025-05-23T10:32:20Z"
        },
        {
          "author": "boosh",
          "body": "Thanks. Maybe the issue is I'm trying to hit the Gemini API directly, not through Vertex",
          "created_at": "2025-05-23T10:37:29Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "@boosh iirc we only support Vertex AI for imagen ",
          "created_at": "2025-05-28T23:49:30Z"
        }
      ]
    },
    {
      "issue_number": 10465,
      "title": "[Bug]: Response Format should be supported for OpenRouter",
      "body": "### What happened?\n\nResponse_format is supported for some models hosted at open-router.\nThe same is not reflected at : from litellm.utils import supports_response_schema\n\nHere are the docs: \nList of models which are currently supported with response schema with open-router, can be founded here:\nhttps://openrouter.ai/docs/features/structured-outputs,https://openrouter.ai/models?order=newest&supported_parameters=structured_outputs\n\n\n### Relevant log output\n\n```shell\n<>\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nLatest\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "Vidit-Ostwal",
      "author_type": "User",
      "created_at": "2025-05-01T05:17:30Z",
      "updated_at": "2025-06-12T03:34:26Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10465/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10465",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10465",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:23.013661",
      "comments": [
        {
          "author": "Vidit-Ostwal",
          "body": "https://github.com/crewAIInc/crewAI/issues/2729",
          "created_at": "2025-05-01T05:17:48Z"
        },
        {
          "author": "ahmed0x77",
          "body": "## Hey everyone! üöÄ I ran into this exact issue and found **three working solutions**\n\n**The Problem:** OpenRouter models DO support structured outputs, but LiteLLM's `supports_response_schema()` returns `False` for them, causing:\n- CrewAI and other frameworks to skip sending `response_format` \n- \"Un",
          "created_at": "2025-06-12T03:34:26Z"
        }
      ]
    },
    {
      "issue_number": 11104,
      "title": "[Bug]: Ollama models are unable to do tool calling via LiteLLM",
      "body": "### What happened?\n\nA bug happened!\n\n@krrishdholakia and @ishaan-jaff I think I may have found a bug with Ollama models and tool calling, for which I am not quite sure how to fix this.\n\nTo begin, I have a minimal reproducible example here: https://gist.github.com/ericmjl/289be4c6b46adb525175c4b1db2f97f1\n\nI tried using 3 Ollama models that are known to be capable of tool calling.\n\nWhen I use the Completions API to call out to `ollama_chat/` models, I find that I am unable to get a tool call response from the models. However, when I switch to using the Ollama API directly, almost always consistently I am able to get a tool call response.\n\nI'm not sure what's happening here, might there be something in the translation layer to Ollama?\n\n### Relevant log output\n\nTo view the notebook, run:\n\n```shell\nuvx marimo edit https://gist.githubusercontent.com/ericmjl/289be4c6b46adb525175c4b1db2f97f1/raw/c10288114b3b3d965c5d5f8d28770d5da0b83b52/reprex_ollama_tools_failing.py\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.70.4\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "ericmjl",
      "author_type": "User",
      "created_at": "2025-05-23T21:28:31Z",
      "updated_at": "2025-06-12T03:01:27Z",
      "closed_at": "2025-05-27T23:14:50Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 11,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11104/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "krrishdholakia"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11104",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11104",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:23.237640",
      "comments": [
        {
          "author": "ericmjl",
          "body": "May be related to https://github.com/BerriAI/litellm/issues/5617.",
          "created_at": "2025-05-23T21:31:02Z"
        },
        {
          "author": "krrishdholakia",
          "body": "ack",
          "created_at": "2025-05-23T22:22:13Z"
        },
        {
          "author": "krrishdholakia",
          "body": "What do you see when debug is enabled - `litellm._turn_on_debug()` ? ",
          "created_at": "2025-05-23T22:22:47Z"
        },
        {
          "author": "ericmjl",
          "body": "@krrishdholakia this is an example of what I see for `llama3.1`:\n\n```text\n22:12:03 - LiteLLM:DEBUG: utils.py:337 - \n\n22:12:03 - LiteLLM:DEBUG: utils.py:337 - Request to litellm:\n22:12:03 - LiteLLM:DEBUG: utils.py:337 - litellm.completion(model='ollama_chat/llama3.1', tools=[{'name': 'write_and_execu",
          "created_at": "2025-05-24T02:18:22Z"
        },
        {
          "author": "ericmjl",
          "body": "@krrishdholakia I'm not sure if this might add noise or signal to this thread, but I just forked and cloned litellm + ollama-python into a single workspace and got cursor to try to diagnose this issue. What I have is the following:\n\n---\n\nI'll help you investigate this issue thoroughly. Let's start b",
          "created_at": "2025-05-24T15:29:35Z"
        }
      ]
    },
    {
      "issue_number": 11651,
      "title": "[Feature]:",
      "body": "### The Feature\n\nSQS logs producer\n\n### Motivation, pitch\n\nWe want to enable custom kms encryption on s3 and PII anonymization on datadog.\n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "deepanshululla",
      "author_type": "User",
      "created_at": "2025-06-12T02:59:14Z",
      "updated_at": "2025-06-12T02:59:14Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11651/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11651",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11651",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:23.533505",
      "comments": []
    },
    {
      "issue_number": 8984,
      "title": "[Bug]: Sonnet 3.7 got capped at 64k instead of 128k",
      "body": "### What happened?\n\nlitellm.BadRequestError: AnthropicException - b'\n```json\n{\n  \"type\": \"error\",\n  \"error\": {\n    \"type\": \"invalid_request_error\",\n    \"message\": \"max_tokens: 128000 > 64000, which is the maximum allowed number of output tokens for claude-3-7-sonnet-20250219\"\n  }\n}\n```\n'\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.61.16 via [Aider](https://github.com/Aider-AI/aider/blob/cecfbc7e207eba1961f2cfad32ff544242e3e9aa/requirements.txt#L104)\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "lavantien",
      "author_type": "User",
      "created_at": "2025-03-04T19:16:56Z",
      "updated_at": "2025-06-12T00:01:57Z",
      "closed_at": "2025-06-12T00:01:57Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8984/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8984",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8984",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:23.533529",
      "comments": [
        {
          "author": "johann-petrak",
          "body": "I think this is a restriction of claude37, not a litellm bug\nas claude37 has a default max output of 8192, and  64000 with extended thinking. \n\nWhy do you think 128k should be possible?\n\nSee https://docs.anthropic.com/en/docs/about-claude/models/all-models#model-comparison-table\n",
          "created_at": "2025-03-05T09:55:53Z"
        },
        {
          "author": "lavantien",
          "body": "> I think this is a restriction of claude37, not a litellm bug as claude37 has a default max output of 8192, and 64000 with extended thinking.\n> \n> Why do you think 128k should be possible?\n> \n> See https://docs.anthropic.com/en/docs/about-claude/models/all-models#model-comparison-table\n\nFrom the do",
          "created_at": "2025-03-06T19:28:28Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-05T00:01:48Z"
        }
      ]
    },
    {
      "issue_number": 9030,
      "title": "[Bug]: Documented custom SSO handler raises an exception",
      "body": "### What happened?\n\nHello folks. We have recently enabled SSO and are attempting to limit logins to existing users in the database. I am using the Custom SSO Handler from the [documentation](https://docs.litellm.ai/docs/proxy/custom_sso), however I get the attached exception and a 500 error, even when trying to log in with an existing user id. Is there a better or different way I should be doing this? Thanks!\n\n### Relevant log output\n\n```shell\nlitellm-1  | INFO:     172.18.0.1:57272 - \"GET /sso/key/generate HTTP/1.1\" 303 See Other\nlitellm-1  | INFO:     172.18.0.1:57272 - \"GET /ui.txt?_rsc=144pr HTTP/1.1\" 404 Not Found\nlitellm-1  | 15:18:32 - LiteLLM Proxy:ERROR: internal_user_endpoints.py:454 - litellm.proxy.proxy_server.user_info(): Exception occured - 'Depends' object has no attribute 'user_role'\nlitellm-1  | Traceback (most recent call last):\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/litellm/proxy/management_endpoints/internal_user_endpoints.py\", line 382, in user_info\nlitellm-1  |     teams_1 = await list_team(\nlitellm-1  |               ^^^^^^^^^^^^^^^^\nlitellm-1  |     ...<5 lines>...\nlitellm-1  |     )\nlitellm-1  |     ^\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/litellm/proxy/management_helpers/utils.py\", line 372, in wrapper\nlitellm-1  |     raise e\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/litellm/proxy/management_helpers/utils.py\", line 288, in wrapper\nlitellm-1  |     result = await func(*args, **kwargs)\nlitellm-1  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/litellm/proxy/management_endpoints/team_endpoints.py\", line 1532, in list_team\nlitellm-1  |     if not allowed_route_check_inside_route(\nlitellm-1  |            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\nlitellm-1  |         user_api_key_dict=user_api_key_dict, requested_user_id=user_id\nlitellm-1  |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlitellm-1  |     ):\nlitellm-1  |     ^\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/litellm/proxy/auth/auth_checks.py\", line 260, in allowed_route_check_inside_route\nlitellm-1  |     user_api_key_dict.user_role != LitellmUserRoles.PROXY_ADMIN\nlitellm-1  |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlitellm-1  | AttributeError: 'Depends' object has no attribute 'user_role'\nlitellm-1  | inside custom sso handler\nlitellm-1  | userIDPInfo: id='drews-p@duke.edu' email='drews-p@duke.edu' first_name=None last_name=None display_name='drews-p@duke.edu' picture=None provider=None team_ids=[]\nlitellm-1  | INFO:     172.18.0.1:57272 - \"GET /sso/callback?code=KlZZ5Y HTTP/1.1\" 500 Internal Server Error\nlitellm-1  | ERROR:    Exception in ASGI application\nlitellm-1  | Traceback (most recent call last):\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/litellm/proxy/management_endpoints/internal_user_endpoints.py\", line 382, in user_info\nlitellm-1  |     teams_1 = await list_team(\nlitellm-1  |               ^^^^^^^^^^^^^^^^\nlitellm-1  |     ...<5 lines>...\nlitellm-1  |     )\nlitellm-1  |     ^\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/litellm/proxy/management_helpers/utils.py\", line 372, in wrapper\nlitellm-1  |     raise e\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/litellm/proxy/management_helpers/utils.py\", line 288, in wrapper\nlitellm-1  |     result = await func(*args, **kwargs)\nlitellm-1  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/litellm/proxy/management_endpoints/team_endpoints.py\", line 1532, in list_team\nlitellm-1  |     if not allowed_route_check_inside_route(\nlitellm-1  |            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\nlitellm-1  |         user_api_key_dict=user_api_key_dict, requested_user_id=user_id\nlitellm-1  |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlitellm-1  |     ):\nlitellm-1  |     ^\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/litellm/proxy/auth/auth_checks.py\", line 260, in allowed_route_check_inside_route\nlitellm-1  |     user_api_key_dict.user_role != LitellmUserRoles.PROXY_ADMIN\nlitellm-1  |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlitellm-1  | AttributeError: 'Depends' object has no attribute 'user_role'\nlitellm-1  |\nlitellm-1  | During handling of the above exception, another exception occurred:\nlitellm-1  |\nlitellm-1  | Traceback (most recent call last):\nlitellm-1  |   File \"/app/custom_sso.py\", line 25, in custom_sso_handler\nlitellm-1  |     _user_info = await user_info(user_id=userIDPInfo.id)\nlitellm-1  |                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/litellm/proxy/management_helpers/utils.py\", line 372, in wrapper\nlitellm-1  |     raise e\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/litellm/proxy/management_helpers/utils.py\", line 288, in wrapper\nlitellm-1  |     result = await func(*args, **kwargs)\nlitellm-1  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/litellm/proxy/management_endpoints/internal_user_endpoints.py\", line 459, in user_info\nlitellm-1  |     raise handle_exception_on_proxy(e)\nlitellm-1  | litellm.proxy._types.ProxyException\nlitellm-1  |\nlitellm-1  | During handling of the above exception, another exception occurred:\nlitellm-1  |\nlitellm-1  | Traceback (most recent call last):\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/uvicorn/protocols/http/h11_impl.py\", line 407, in run_asgi\nlitellm-1  |     result = await app(  # type: ignore[func-returns-value]\nlitellm-1  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlitellm-1  |         self.scope, self.receive, self.send\nlitellm-1  |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlitellm-1  |     )\nlitellm-1  |     ^\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/uvicorn/middleware/proxy_headers.py\", line 69, in __call__\nlitellm-1  |     return await self.app(scope, receive, send)\nlitellm-1  |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlitellm-1  |     )\nlitellm-1  |     ^\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/uvicorn/middleware/proxy_headers.py\", line 69, in __call__\nlitellm-1  |     return await self.app(scope, receive, send)\nlitellm-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/fastapi/applications.py\", line 1054, in __call__\nlitellm-1  |     await super().__call__(scope, receive, send)\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/starlette/applications.py\", line 113, in __call__\nlitellm-1  |     await self.middleware_stack(scope, receive, send)\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/starlette/middleware/errors.py\", line 187, in __call__\nlitellm-1  |     raise exc\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/starlette/middleware/errors.py\", line 165, in __call__\nlitellm-1  |     await self.app(scope, receive, _send)\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/starlette/middleware/cors.py\", line 85, in __call__\nlitellm-1  |     await self.app(scope, receive, send)\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\nlitellm-1  |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\nlitellm-1  |     raise exc\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\nlitellm-1  |     await app(scope, receive, sender)\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/starlette/routing.py\", line 715, in __call__\nlitellm-1  |     await self.middleware_stack(scope, receive, send)\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/starlette/routing.py\", line 735, in app\nlitellm-1  |     await route.handle(scope, receive, send)\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/starlette/routing.py\", line 288, in handle\nlitellm-1  |     await self.app(scope, receive, send)\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/starlette/routing.py\", line 76, in app\nlitellm-1  |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\nlitellm-1  |     raise exc\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\nlitellm-1  |     await app(scope, receive, sender)\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/starlette/routing.py\", line 73, in app\nlitellm-1  |     response = await f(request)\nlitellm-1  |                ^^^^^^^^^^^^^^^^\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 301, in app\nlitellm-1  |     raw_response = await run_endpoint_function(\nlitellm-1  |                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlitellm-1  |     ...<3 lines>...\nlitellm-1  |     )\nlitellm-1  |     ^\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\nlitellm-1  |     return await dependant.call(**values)\nlitellm-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/litellm/proxy/management_endpoints/ui_sso.py\", line 549, in auth_callback\nlitellm-1  |     user_defined_values = await user_custom_sso(result)  # type: ignore\nlitellm-1  |                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlitellm-1  |   File \"/app/custom_sso.py\", line 40, in custom_sso_handler\nlitellm-1  |     raise Exception(\"Failed custom auth\")\nlitellm-1  | Exception: Failed custom auth\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.61.20-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "drewstinnett",
      "author_type": "User",
      "created_at": "2025-03-06T15:28:31Z",
      "updated_at": "2025-06-12T00:01:55Z",
      "closed_at": "2025-06-12T00:01:55Z",
      "labels": [
        "bug",
        "mlops user request",
        "stale"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9030/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9030",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9030",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:23.776684",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-05T00:01:45Z"
        }
      ]
    },
    {
      "issue_number": 11639,
      "title": "[Bug]: Unable to find token in cache or `LiteLLM_VerificationTokenTable`",
      "body": "### What happened?\n\nGet this error when using the `/chat/completions` endpoint with only a few of the virtual keys\n\n`HTTP 401: {\"error\":{\"message\":\"Authentication Error, Invalid proxy server token passed. Received API Key = sk-...ITlA, Key Hash (Token) =5ea43e256ed93ce3082d9e89be60b7b2ff5709391df2b35b2eca0e493eeb4c72. Unable to find token in cache or `LiteLLM_VerificationTokenTable`\",\"type\":\"token_not_found_in_db\",\"param\":\"key\",\"code\":\"401\"}}`\n\n```\nI can see the key info using the hash here with the `key/info` endpoint\ncurl -X 'GET' \\\n  'https://wordsmith-proxy-litellm.uat.includedhealth.com/key/info?key=5ea43e256ed93ce3082d9e89be60b7b2ff5709391df2b35b2eca0e493eeb4c72' \\\n  -H 'accept: application/json' \\\n  -H 'x-litellm-api-key: sk-...'\n```\n\nreturns\n\n```\n{\n  \"key\": \"5ea43e256ed93ce3082d9e89be60b7b2ff5709391df2b35b2eca0e493eeb4c72\",\n  \"info\": {\n    \"key_name\": \"sk-...ITlA\",\n    \"key_alias\": null,\n    \"soft_budget_cooldown\": false,\n    \"spend\": 0,\n    \"expires\": null,\n    \"models\": [],\n    \"aliases\": {},\n    \"config\": {},\n    \"user_id\": \"<user_id>\",\n    \"team_id\": \"<team_id>\",\n    \"permissions\": {},\n    \"max_parallel_requests\": null,\n    \"metadata\": {},\n    \"blocked\": null,\n    \"tpm_limit\": null,\n    \"rpm_limit\": null,\n    \"max_budget\": null,\n    \"budget_duration\": null,\n    \"budget_reset_at\": null,\n    \"allowed_cache_controls\": [],\n    \"allowed_routes\": [],\n    \"model_spend\": {},\n    \"model_max_budget\": {},\n    \"budget_id\": null,\n    \"organization_id\": null,\n    \"object_permission_id\": null,\n    \"created_at\": \"2025-06-11T19:38:46.895000+00:00\",\n    \"created_by\": \"default_user_id\",\n    \"updated_at\": \"2025-06-11T19:38:46.895000+00:00\",\n    \"updated_by\": \"default_user_id\",\n    \"litellm_budget_table\": null,\n    \"litellm_organization_table\": null,\n    \"object_permission\": null\n  }\n}\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.72.4\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "amoghnalwaya",
      "author_type": "User",
      "created_at": "2025-06-11T20:19:07Z",
      "updated_at": "2025-06-11T23:41:45Z",
      "closed_at": "2025-06-11T23:41:45Z",
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11639/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11639",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11639",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:24.005109",
      "comments": []
    },
    {
      "issue_number": 11637,
      "title": "[Bug]: disk cache fails on docker image",
      "body": "### What happened?\n\nDear LiteLLM,\n\nKudos for the great work your are doing! I am really enjoy every new version!\n\nPlease note the following:\n\nEnabling the disk cache on docker image `ghcr.io/berriai/litellm:main-latest` fails with the following error:\n\n```\nPlease install litellm with `litellm[caching]` to use disk caching\n```\n\nThe following setting was enabled on yml:\n```\nlitellm_settings:\n  cache: true\n  cache_params:\n    type: disk\n    disk_cache_dir: /tmp/litellm-cache\n```\n\nI am using latest available version (but issue, exists on previous versions too).\n\n```\ndocker image ls ghcr.io/berriai/litellm\nREPOSITORY                TAG           IMAGE ID       CREATED        SIZE\nghcr.io/berriai/litellm   main-latest   5de7889127f4   13 hours ago   1.98GB\n```\n\nI guess the caching dependencies should be installed by default on docker image, to use that feature.\n\nThank you!\n\n### Relevant log output\n\n```shell\n....\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/proxy_server.py\", line 572, in proxy_startup_event\n    await initialize(**worker_config)\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/proxy_server.py\", line 3003, in initialize\n    ) = await proxy_config.load_config(router=llm_router, config_file_path=config)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/proxy_server.py\", line 1715, in load_config\n    self._init_cache(cache_params=cache_params)\n    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/proxy_server.py\", line 1523, in _init_cache\n    litellm.cache = Cache(**cache_params)\n                    ~~~~~^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/caching/caching.py\", line 205, in __init__\n    self.cache = DiskCache(disk_cache_dir=disk_cache_dir)\n                 ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/caching/disk_cache.py\", line 19, in __init__\n    raise ModuleNotFoundError(\n        \"Please install litellm with `litellm[caching]` to use disk caching.\"\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.4\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "athoik",
      "author_type": "User",
      "created_at": "2025-06-11T19:19:13Z",
      "updated_at": "2025-06-11T22:53:22Z",
      "closed_at": "2025-06-11T22:53:21Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11637/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11637",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11637",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:24.005133",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "it's not on the default docker, we want to reduce bloat. You'll need to add it to requirements.txt ",
          "created_at": "2025-06-11T22:53:22Z"
        }
      ]
    },
    {
      "issue_number": 11308,
      "title": "[Feature]: Background mode in the Responses API",
      "body": "### The Feature\n\nIs any plans to support this from proxy?\n\nIt will be nice to have support of polling background responses in general\nhttps://platform.openai.com/docs/guides/background\n\n### Motivation, pitch\n\nSupport for polling background responses can be critial for using LiteLLM with API gateways with limited response timeouts\n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "semyonc",
      "author_type": "User",
      "created_at": "2025-06-01T15:38:32Z",
      "updated_at": "2025-06-11T22:41:45Z",
      "closed_at": "2025-06-11T22:41:45Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11308/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11308",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11308",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:24.227313",
      "comments": [
        {
          "author": "marty-sullivan",
          "body": "This does seem like it should be supported. I'd be concerned with how LiteLLM would be able to track costs though and that would have to be factored in to implementing it (perhaps LiteLLM itself would periodically poll until the response is complete to collect usage data). Or at least a way to disab",
          "created_at": "2025-06-11T21:07:29Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "added here: https://github.com/BerriAI/litellm/pull/11640\n\nfor cost tracking, we plan on storing async jobs in a DB table and having a background job to get the cost every 24 hrs ",
          "created_at": "2025-06-11T22:41:29Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "closing as completed ",
          "created_at": "2025-06-11T22:41:45Z"
        }
      ]
    },
    {
      "issue_number": 11633,
      "title": "[Bug]: team models not visible to an internal user",
      "body": "### What happened?\n\nafter creating an user assigned to a team, the user can not see the team available models (model hub) and when creating a new virtual key this key has not access to their team models \n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv 1.72.4\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "snova-jorgep",
      "author_type": "User",
      "created_at": "2025-06-11T17:44:23Z",
      "updated_at": "2025-06-11T20:43:58Z",
      "closed_at": "2025-06-11T20:43:58Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11633/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11633",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11633",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:24.416163",
      "comments": [
        {
          "author": "snova-jorgep",
          "body": "issue ws not giving permission to the user to add team keys so the key was not a team key ",
          "created_at": "2025-06-11T20:43:58Z"
        }
      ]
    },
    {
      "issue_number": 8648,
      "title": "[Feature]: Improving Retry Mechanism Consistency and Logging for Streamed Responses in LiteLLM Proxy",
      "body": "### The Feature\n\nI would greatly appreciate it if the following improvements could be considered:\n\n1.  **Improved Logging for Streamed Errors:**  For errors encountered in streaming mode, could the logging be made more user-friendly, similar to the non-streaming case?  Displaying a clear error message and an indication of whether a retry will be attempted (like \"Retrying request with num_retries: X\") would significantly improve the debugging experience, instead of a full Python stack trace.\n\n2.  **Consistent Retry Behavior:**  If an LLM call fails with a retryable error (like a 429) *before* any data has been streamed to the client, would it be possible for LiteLLM to initiate a retry, just as it does for non-streaming requests?  This would provide a more consistent and robust user experience.\n\nThank you again for your time and consideration. I believe these changes would make LiteLLM even more resilient and easier to use, especially when working with models that have strict rate limits.\n\n### Motivation, pitch\n\nHello LiteLLM team,\n\nFirst of all, thank you for developing and maintaining this useful library!\n\nI'm currently using LiteLLM Proxy with the Gemini model (`gemini/gemini-2.0-pro-exp-02-05`).  Due to the low rate limits and experimental nature of this model on Google's Vertex AI, I frequently encounter 429 errors.  I've configured retries in LiteLLM, but I've observed inconsistent behavior in how retries are handled, specifically when dealing with streaming responses.\n\n**Observed Behavior:**\n\n*   **Successful Retry (Non-streaming):** When a non-streaming request encounters a 429 error, LiteLLM correctly initiates retries, as shown in the logs:\n\n    ```\n    09:51:13 - LiteLLM Router:INFO: router.py:983 - litellm.acompletion(model=gemini/gemini-2.0-pro-exp-02-05) Exception litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n      \"error\": {\n        \"code\": 429,\n        \"message\": \"Resource has been exhausted (e.g. check quota).\",\n        \"status\": \"RESOURCE_EXHAUSTED\"\n      }\n    }\n\n    09:51:13 - LiteLLM Router:INFO: router.py:3151 - Retrying request with num_retries: 3\n    ```\n\n*   **No Retry (Streaming):**  When a *streaming* request encounters a 429 error *before* any data has been sent to the client, the retry mechanism does *not* seem to be triggered.  Instead, a lengthy Python stack trace is logged, making it difficult to quickly identify the issue:\n\n    ```\n    09:49:32 - LiteLLM Proxy:ERROR: proxy_server.py:3038 - litellm.proxy.proxy_server.async_data_generator(): Exception occured - litellm.APIConnectionError: APIConnectionError: OpenAIException - litellm.RateLimitError: litellm.RateLimitError: VertexAIException - b'{\\n  \"error\": {\\n    \"code\": 429,\\n    \"message\": \"Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/error-code-429 for more details.\",\\n    \"status\": \"RESOURCE_EXHAUSTED\"\\n  }\\n}\\n'\n    Traceback (most recent call last):\n      File \"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/streaming_handler.py\", line 1545, in __anext__\n        async for chunk in self.completion_stream:\n        ...<50 lines>...\n            return processed_chunk\n      File \"/usr/lib/python3.13/site-packages/openai/_streaming.py\", line 147, in __aiter__\n        async for item in self._iterator:\n            yield item\n      File \"/usr/lib/python3.13/site-packages/openai/_streaming.py\", line 174, in __stream__\n        raise APIError(\n        ...<3 lines>...\n        )\n    openai.APIError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - b'{\\n  \"error\": {\\n    \"code\": 429,\\n    \"message\": \"Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/error-code-429 for more details.\",\\n    \"status\": \"RESOURCE_EXHAUSTED\"\\n  }\\n}\\n'\n\n    During handling of the above exception, another exception occurred:\n\n    Traceback (most recent call last):\n      File \"/usr/lib/python3.13/site-packages/litellm/proxy/proxy_server.py\", line 3017, in async_data_generator\n        async for chunk in response:\n        ...<14 lines>...\n                yield f\"data: {str(e)}\\n\\n\"\n      File \"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/streaming_handler.py\", line 1700, in __anext__\n        raise exception_type(\n              ~~~~~~~~~~~~~~^\n            model=self.model,\n            ^^^^^^^^^^^^^^^^^\n        ...<3 lines>...\n            extra_kwargs={},\n            ^^^^^^^^^^^^^^^^\n        )\n        ^\n      File \"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2206, in exception_type\n        raise e  # it's already mapped\n        ^^^^^^^\n      File \"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 462, in exception_type\n        raise APIConnectionError(\n        ...<7 lines>...\n        )\n    litellm.exceptions.APIConnectionError: litellm.APIConnectionError: APIConnectionError: OpenAIException - litellm.RateLimitError: litellm.RateLimitError: VertexAIException - b'{\\n  \"error\": {\\n    \"code\": 429,\\n    \"message\": \"Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/error-code-429 for more details.\",\\n    \"status\": \"RESOURCE_EXHAUSTED\"\\n  }\\n}\\n'\n    ```\n    It took me some time (partially due to my limited familiarity with Python) to realize that the difference between successful and unsuccessful retries was related to whether the request was streaming or not.\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "fengjiajie",
      "author_type": "User",
      "created_at": "2025-02-19T04:01:12Z",
      "updated_at": "2025-06-11T20:36:10Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8648/reactions",
        "total_count": 4,
        "+1": 4,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8648",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8648",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:24.631123",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-05-21T00:01:41Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Hey @fengjiajie does this happen on the first chunk of the stream? ",
          "created_at": "2025-06-01T01:46:00Z"
        },
        {
          "author": "fengjiajie",
          "body": "> Hey [@fengjiajie](https://github.com/fengjiajie) does this happen on the first chunk of the stream?\n\nThis issue is a bit old, and if I recall correctly, the problem was with the first chunk. At that time, gemini-exp was free, and it frequently happened that requests would hang for a long time befo",
          "created_at": "2025-06-01T12:31:54Z"
        },
        {
          "author": "maxjacu",
          "body": "@krrishdholakia yes its before the stream begins. My solution I experimented with was to handle this error within the streaming function before the first token was streamed. \n\nWe aren't handling errors like this in streaming mode atm. I understand why, but rate limit errors etc can be safely handled",
          "created_at": "2025-06-11T20:35:52Z"
        }
      ]
    },
    {
      "issue_number": 9792,
      "title": "[Bug]: logs blowing up with `Cannot add callback - would exceed MAX_CALLBACKS limit of 30.`",
      "body": "### What happened?\n\nRunning an evaluation script made a few months ago, with modern `litellm` my logs get immediately demolished with many lines saying:\n\n```none\nLiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n```\n\nI don't know what this is, the script does not configure 30 callbacks. This seems to be a new behavior added sometime in the past 3 months (https://github.com/BerriAI/litellm/pull/8112).\n\nSome possible resolutions include:\n\n1. Adding context to this message: what is the callback related to, why this error matters, possible resolutions\n1. Removing this warning, or designing control flows to not have this edge case\n\nIn the meantime, this goes away as of `litellm==1.58.4` (`pip install \"litellm<1.59\"`).\n\n### Relevant log output\n\n```shell\n2025-04-06 17:21:37,193 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,193 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,194 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,194 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,194 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,194 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,194 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,194 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,194 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,194 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,195 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,195 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,195 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,195 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,195 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,196 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,196 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,196 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,196 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,196 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,196 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,196 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,197 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,197 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,197 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,197 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,197 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,197 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,198 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,198 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,198 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,198 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,198 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,198 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n2025-04-06 17:21:37,199 - LiteLLM - WARNING - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.65.4.post1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "jamesbraza",
      "author_type": "User",
      "created_at": "2025-04-07T00:28:07Z",
      "updated_at": "2025-06-11T20:13:20Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9792/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9792",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9792",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:24.842891",
      "comments": [
        {
          "author": "daaain",
          "body": "I'm also getting this after updating the version, not sure why?\n\nI'm only setting 2 callbacks on the top level: \n``` \nlitellm.success_callback = [\"langfuse\"]\nlitellm.failure_callback = [\"langfuse\"] \n```\n\nAnother thing I changed is to create a new LiteLLM `Router` instance before each call, because I",
          "created_at": "2025-04-24T11:05:47Z"
        },
        {
          "author": "jamesbraza",
          "body": "With `litellm==1.67.4.post1`, I disabled this message via:\n\n```python\nimport litellm\n\n\ndef update_litellm_max_callbacks(value: int = 1000) -> None:\n    \"\"\"Update litellm's MAX_CALLBACKS limit, can call with default to defeat this limit.\n\n    SEE: https://github.com/BerriAI/litellm/issues/9792\n    \"\"",
          "created_at": "2025-06-11T20:13:20Z"
        }
      ]
    },
    {
      "issue_number": 11156,
      "title": "[Bug]: Wrong Gemini 2.5 cost calculation",
      "body": "### What happened?\n\nGemini 2.5 supports implicit context caching, but litellm does not take this into account while calculating the cost.\n\nExample:\n\n```\nfrom litellm import ModelResponse\nimport litellm\nfrom dotenv import load_dotenv\n\nload_dotenv() # Gemini & langfuse API key's\n\nlitellm.success_callback = [\"langfuse\"]\nlitellm.failure_callback = [\"langfuse\"]\nlitellm._turn_on_debug()\n\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"\\n\".join([\"This is a test message to check the response from the Gemini model.\"]*1000)\n    },\n]\n\nresponse: ModelResponse = litellm.completion(\n    model=\"gemini/gemini-2.5-flash-preview-04-17\",\n    messages=messages,\n    thinking={\"type\": \"disabled\", \"budget_tokens\": 0} # turn off thinking\n)\n\nmessages.extend(\n    [\n        {\n            \"role\": \"assistant\",\n            \"content\": response.choices[0].message.content\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"This is a test message to check the response from the Gemini model.\"\n        }\n    ]\n)\n\nresponse: ModelResponse = litellm.completion(\n    model=\"gemini/gemini-2.5-flash-preview-04-17\",\n    messages=messages,\n    thinking={\"type\": \"disabled\", \"budget_tokens\": 0}\n)\n```\n\nThis gives the following debug log:\n\n```\nLiteLLM:DEBUG: utils.py:338 - RAW RESPONSE:\n{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"parts\": [\n          {\n            \"text\": \"Understood. This is a test message to check the response from the Gemini model.\"\n          }\n        ],\n        \"role\": \"model\"\n      },\n      \"finishReason\": \"STOP\",\n      \"index\": 0\n    }\n  ],\n  \"usageMetadata\": {\n    \"promptTokenCount\": 15033,\n    \"candidatesTokenCount\": 17,\n    \"totalTokenCount\": 15050,\n    \"cachedContentTokenCount\": 14316,\n    \"promptTokensDetails\": [\n      {\n        \"modality\": \"TEXT\",\n        \"tokenCount\": 15033\n      }\n    ],\n    \"cacheTokensDetails\": [\n      {\n        \"modality\": \"TEXT\",\n        \"tokenCount\": 14316\n      }\n    ]\n  },\n  \"modelVersion\": \"models/gemini-2.5-flash-preview-04-17\",\n  \"responseId\": \"VFg0aK_YCv64nsEPgLKoqA4\"\n}\n```\n\nAnd litellm reports the following cost:\n\n```\nlitellm_logging.py:1130 - response_cost: 0.00226515\n```\n\nGemini pricing:\ninput: $0.15 / 1000000 tokens\noutput: $0.60 / 1000000 tokens\n\nWhich corresponds to `0.15*(15033/1000000) + 0.6*(17/1000000) = 0.00226515`.\n\nHowever, implicit caching should give a 75% discount for cached input tokens:\nhttps://developers.googleblog.com/en/gemini-2-5-models-now-support-implicit-caching/\n\nWhich would come down to:\n\n`0.15*0.25*(14316/1000000)+0.15*((15033-14316)/1000000)+0.6*(17/1000000) = 0.0006546`\n\nThe difference is quite substantial, around 3.5x, so it would be great to get this to work properly. \n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\n1.71.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "JohanBekker",
      "author_type": "User",
      "created_at": "2025-05-26T12:49:47Z",
      "updated_at": "2025-06-11T19:50:52Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11156/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11156",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11156",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:25.001774",
      "comments": [
        {
          "author": "recrudesce",
          "body": "Seems none of the Gemini models have cache pricing configured in https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\nYou can override it in your own config, though - example here shown for Claude, but it gives you the syntax:\n\n```\nmodel_list:\n  - model_name: \"prod/clau",
          "created_at": "2025-05-26T13:44:55Z"
        },
        {
          "author": "awesie",
          "body": "Related to #10667",
          "created_at": "2025-05-27T17:30:41Z"
        },
        {
          "author": "arunbugkiller",
          "body": "@recrudesce need help if possible on how to enable thinking using litellm config so that the thinking is shown on the dashboard also. I am using OpenWebUI with LiteLLM ",
          "created_at": "2025-06-03T05:17:27Z"
        },
        {
          "author": "clarity99",
          "body": "since #10667 is now complete and cached tokens are reported accurately, I guess this could now be fixed? It's not just cached tokens, I think also reasoning tokens are not included properly in the calculation",
          "created_at": "2025-06-11T19:50:52Z"
        }
      ]
    },
    {
      "issue_number": 10024,
      "title": "[Bug]: Prisma Migrate fails with a custom install",
      "body": "### What happened?\n\nI am running litellm in a custom docker image. I have `--use_prisma_migrate` set. The migrations are successful, but the proxy server fails to start because the prisma client was not generate. However, when I do not use this flag, prisma client is able to be generated and push changes to the db. I want to use migrations in my deployment as they are safer, but the prisma client is not being generated.\n\nAm I misunderstanding how `--use_prisma_migrate` works: should it be run before the proxy server in a separate task or should it run in the same process before the proxy server? If the former, will data loss occur because prisma will still push changes regardless? If the latter, seems like the prisma client is never being generated for this code path.\n\n### Relevant log output\n\n```shell\nlitellm-1  | Setting Cache on Proxy\nlitellm-1  | LiteLLM: Proxy initialized with Config, Set models:\nlitellm-1  |     SG161222/RealVisXL_V5.0\nlitellm-1  |     google/gemini-1.5-pro-002\nlitellm-1  |     google/imagen-3.0-fast-generate-001\nlitellm-1  | 16:44:14 - LiteLLM Router:INFO: router.py:649 - Routing strategy: simple-shuffle\nlitellm-1  | ERROR:    Traceback (most recent call last):\nlitellm-1  |   File \"/opt/genai-gateway/.venv/lib/python3.13/site-packages/litellm/proxy/utils.py\", line 1160, in __init__\nlitellm-1  |     from prisma import Prisma  # type: ignore\nlitellm-1  |     ^^^^^^^^^^^^^^^^^^^^^^^^^\nlitellm-1  |   File \"<frozen importlib._bootstrap>\", line 1412, in _handle_fromlist\nlitellm-1  |   File \"/opt/genai-gateway/.venv/lib/python3.13/site-packages/prisma/__init__.py\", line 50, in __getattr__\nlitellm-1  |     raise RuntimeError(\nlitellm-1  |     ...<3 lines>...\nlitellm-1  |     ) from None\nlitellm-1  | RuntimeError: The Client hasn't been generated yet, you must run `prisma generate` before you can use the client.\nlitellm-1  | See https://prisma-client-py.readthedocs.io/en/stable/reference/troubleshooting/#client-has-not-been-generated-yet\nlitellm-1  |\n\n\n...\n\nlitellm-1  |   File \"/opt/genai-gateway/.venv/lib/python3.13/site-packages/litellm/proxy/proxy_server.py\", line 3241, in _setup_prisma_client\nlitellm-1  |     PrismaDBExceptionHandler.handle_db_exception(e)\nlitellm-1  |     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^\nlitellm-1  |   File \"/opt/genai-gateway/.venv/lib/python3.13/site-packages/litellm/proxy/db/exception_handler.py\", line 61, in handle_db_exception\nlitellm-1  |     raise e\nlitellm-1  |   File \"/opt/genai-gateway/.venv/lib/python3.13/site-packages/litellm/proxy/proxy_server.py\", line 3220, in _setup_prisma_client\nlitellm-1  |     raise e\nlitellm-1  |   File \"/opt/genai-gateway/.venv/lib/python3.13/site-packages/litellm/proxy/proxy_server.py\", line 3216, in _setup_prisma_client\nlitellm-1  |     prisma_client = PrismaClient(\nlitellm-1  |         database_url=database_url, proxy_logging_obj=proxy_logging_obj\nlitellm-1  |     )\nlitellm-1  |   File \"/opt/genai-gateway/.venv/lib/python3.13/site-packages/litellm/proxy/utils.py\", line 1162, in __init__\nlitellm-1  |     raise Exception(\"Unable to find Prisma binaries.\")\nlitellm-1  | Exception: Unable to find Prisma binaries.\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.66.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "thetonus",
      "author_type": "User",
      "created_at": "2025-04-15T16:53:18Z",
      "updated_at": "2025-06-11T19:33:08Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10024/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10024",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10024",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:25.196526",
      "comments": [
        {
          "author": "thedevd",
          "body": "I am struggling with this, since past 24 hours, I have to manually run prisma generate command. \nThis has to be handled by litellm itself when i run litellm server using command - litellm ",
          "created_at": "2025-06-11T19:33:08Z"
        }
      ]
    },
    {
      "issue_number": 9794,
      "title": "[Bug]: Documentation error - Incorrect AWS environment variable names",
      "body": "### Description:\n\nThere's a documentation error regarding AWS environment variable names in the LiteLLM proxy configuration. \n\nCurrently, the docs suggest using `AWS_PROFILE_NAME` and `AWS_REGION_NAME` as environment variables:\n- From [LiteLLM documentation](https://docs.litellm.ai/docs/proxy/config_settings#environment-variables---reference)\n- In various examples showing `aws_profile_name` and `aws_region_name` parameters\n\nHowever, these don't align with the actual environment variables that `boto3` recognizes, which according to [AWS CLI documentation](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-envvars.html) should be:\n- `AWS_PROFILE` (not AWS_PROFILE_NAME)\n- `AWS_REGION` (not AWS_REGION_NAME)\n\n### Context:\nIn my deployment setup, I'm using Docker Compose to run the LiteLLM proxy, and it's more convenient to define environment variables directly in the `docker-compose.override.yml` file rather than in the `litellm.config.yaml` file:\n\n```yaml\nservices:\n  litellm:\n    volumes:\n      - ~/.aws:/root/.aws\n    environment:\n      AWS_PROFILE: 7t           # Correct variable name\n      AWS_REGION: us-east-1     # Correct variable name\n```\n\nWhen following the documentation and using `AWS_PROFILE_NAME` and `AWS_REGION_NAME` instead, `boto3` fails to detect the credentials, resulting in authentication errors.\n\n### Steps to reproduce:\n1. Set up LiteLLM proxy with AWS Bedrock using Docker Compose\n2. Configure environment variables as per documentation using `AWS_PROFILE_NAME` and `AWS_REGION_NAME`\n3. Observe \"Unable to locate credentials\" error from boto3\n4. Change to the correct variables (`AWS_PROFILE` and `AWS_REGION`) and verify that it works\n\n### Fix required:\nUpdate the documentation to use the correct AWS environment variable names that boto3 actually uses:\n- `AWS_PROFILE` instead of `AWS_PROFILE_NAME`\n- `AWS_REGION` instead of `AWS_REGION_NAME`\n\nThere are other AWS environment variable names, in the LiteLLM docs, that need to be checked out against the AWS CLI docs, as well.\n\nThis would help other users deploying LiteLLM with AWS services, especially in containerized environments.\n\n### Relevant log output\n\n```shell\nlitellm.exceptions.AuthenticationError: litellm.AuthenticationError: BedrockException Invalid Authentication - Unable to locate credentials.\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nmain-v1.65.4-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "spenpal",
      "author_type": "User",
      "created_at": "2025-04-07T01:48:40Z",
      "updated_at": "2025-06-11T19:03:49Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9794/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9794",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9794",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:25.413383",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "@spenpal this isn't a bug - we do support those environment variables as documented - https://github.com/BerriAI/litellm/blob/e94eb4ec70615b5a053aaf5f04e41f9580fa857a/litellm/llms/bedrock/base_aws_llm.py#L91",
          "created_at": "2025-04-15T05:44:29Z"
        },
        {
          "author": "leezu",
          "body": "@krrishdholakia following https://docs.litellm.ai/docs/providers/bedrock#sso-login-aws-profile with `AWS_PROFILE` in the environment does not work however. `AWS_PROFILE_NAME` in the environment does not work either. Manually passing `aws_profile_name` to `completion()` works.",
          "created_at": "2025-06-11T19:03:49Z"
        }
      ]
    },
    {
      "issue_number": 11597,
      "title": "[Feature]: add o3-pro",
      "body": "### The Feature\n\no3-pro just released today \nhttps://x.com/openai/status/1932483131363504334?s=46&t=C3j3v9UayPcyYV6ZRHXzIw\n\ncould you add it?\nhttps://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json\ni cant see in this list\n\n### Motivation, pitch\n\nadd new model\n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "ali-coplanet",
      "author_type": "User",
      "created_at": "2025-06-10T17:38:08Z",
      "updated_at": "2025-06-11T18:58:15Z",
      "closed_at": "2025-06-11T16:09:07Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11597/reactions",
        "total_count": 4,
        "+1": 4,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11597",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11597",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:25.612885",
      "comments": [
        {
          "author": "LyoSU",
          "body": "\nThis model works exclusively through the responses API. Will this be a problem?\n\n`This model is only supported in v1/responses and not in v1/chat/completions.`",
          "created_at": "2025-06-11T08:01:41Z"
        },
        {
          "author": "ali-coplanet",
          "body": "yeah i'm facing this \n\n`2025-06-11 11:36:43,630 - root - ERROR - LiteLLM call failed: litellm.BadRequestError: OpenAIException - This model is only supported in v1/responses and not in v1/chat/completions.`",
          "created_at": "2025-06-11T09:37:52Z"
        },
        {
          "author": "ali-coplanet",
          "body": "@krrishdholakia ",
          "created_at": "2025-06-11T09:41:54Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Added here - https://github.com/BerriAI/litellm/blob/c002e4cd4eb0155ec6fac88dfb74b525b32204fd/model_prices_and_context_window.json#L743",
          "created_at": "2025-06-11T16:09:07Z"
        },
        {
          "author": "krrishdholakia",
          "body": "We needed to fix the `mode` to `responses` - which i just pushed a fix for, let me know if you still see this issue in a few minutes",
          "created_at": "2025-06-11T16:09:33Z"
        }
      ]
    },
    {
      "issue_number": 11636,
      "title": "[Bug]: Budget reset for users does not work through the UI where default budgets are present",
      "body": "### What happened?\n\nResetting a Users budget to 'blank' in their User profile does not reset their budget to 'unlimited'. The UI says it does but this is not honoured, and after a few refreshes, does back to the previous budget. \n\nIn this case, there's a default budget of $X; emptying this field in the User Details portion appears to be reflected in the UI but doesn't last and the budget resets to $X\n\nRemediation is to give user higher budget. Have not tested resetting per-user budgets through the API directly yet. Attempting values for new budget of null|None through the UI also didn't work. \n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.72.2\n\n### Twitter / LinkedIn details\n\n@bolster / @andrewbolster",
      "state": "open",
      "author": "andrewbolster",
      "author_type": "User",
      "created_at": "2025-06-11T18:41:15Z",
      "updated_at": "2025-06-11T18:44:18Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11636/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11636",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11636",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:25.881630",
      "comments": []
    },
    {
      "issue_number": 5737,
      "title": "[Bug]: Swagger UI doesn't load in offline environment",
      "body": "### What happened?\r\n\r\nWhen hosting LiteLLM in an offline environment, Swagger doesn't load as it tries to fetch `swagger-ui.css` and other related swagger files from https://cdn.jsdelivr.net.\r\n\r\nAs a result of not being able to fetch the assets, a blank page is displayed.\r\n\r\nLiteLLM should be configured to ask Swagger to use a locally held version of the Swagger assets.\r\n\r\n### Relevant log output\r\n\r\n_No response_\r\n\r\n### Twitter / LinkedIn details\r\n\r\n_No response_",
      "state": "closed",
      "author": "cardin",
      "author_type": "User",
      "created_at": "2024-09-17T10:48:47Z",
      "updated_at": "2025-06-11T18:37:46Z",
      "closed_at": "2025-04-06T20:55:08Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/5737/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/5737",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/5737",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:25.881653",
      "comments": [
        {
          "author": "nicolasesprit",
          "body": "Same here.\r\n\r\n- cdn.jsdelivr.net\r\n- fastapi.tiangolo.com\r\n\r\nA global check is needed to remove every internet dependencies for offline deployement",
          "created_at": "2024-10-02T13:03:19Z"
        },
        {
          "author": "cln-io",
          "body": "Same here, \r\n\r\nIn an airgapped network it tries to connect to cdn.jsdelivr.net, resulting in a blank page",
          "created_at": "2025-01-09T09:47:51Z"
        },
        {
          "author": "thedevd",
          "body": "What is the exact configuration to enable this if I am running litellm using command line \n`litellm`",
          "created_at": "2025-06-11T18:37:46Z"
        }
      ]
    },
    {
      "issue_number": 9340,
      "title": "[Bug]: ssl_verify=false has no effect anymore",
      "body": "### What happened?\n\n(At least) since [v1.63.11-stable](https://github.com/BerriAI/litellm/releases/tag/v1.63.11-stable.patch1) connections to selfsigned backend servers fail despite `ssl_verify: False` in `litellm_settings`.\n\nUsing 'http://' in the API URLs immediately resolves the issue.\n\n### Relevant log output\n\n```shell\n15:03:35 - LiteLLM Router:DEBUG: cooldown_handlers.py:326 - retrieve cooldown models: [('b4d9e7905ba2c0be134e60a8864fc38097ebe9cec327a1c8485f6e3fdff915e2', {'exception_received': 'litellm.APIError: APIError: OpenAIException - Connection error.', 'status_code': '500', 'timestamp': 1742310215.9349358, 'cooldown_time': 5}), ('6153cef9064bcc1eb6bcf36bda86279d020d00b955bbb6a4627e7e81d54ceb1c', {'exception_received': 'litellm.APIError: APIError: OpenAIException - Connection error.', 'status_code': '500', 'timestamp': 1742310215.7337375, 'cooldown_time': 5}), ('309dd32880097fbd7801280198465047e45302adae05bfac41f7999f672a6ca0', {'exception_received': 'litellm.APIError: APIError: OpenAIException - Connection error.', 'status_code': '500', 'timestamp': 1742310215.836235, 'cooldown_time': 5})]\n15:03:35 - LiteLLM:DEBUG: cooldown_callbacks.py:33 - In router_cooldown_event_callback - updating prometheus\n15:03:35 - LiteLLM Router:DEBUG: router.py:3169 - TracebackTraceback (most recent call last):\n  File \"/usr/local/lib/python3.13/site-packages/httpx/_transports/default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"/usr/local/lib/python3.13/site-packages/httpx/_transports/default.py\", line 373, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.13/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\n    raise exc from None\n  File \"/usr/local/lib/python3.13/site-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\n    response = await connection.handle_async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        pool_request.request\n        ^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/usr/local/lib/python3.13/site-packages/httpcore/_async/connection.py\", line 101, in handle_async_request\n    raise exc\n  File \"/usr/local/lib/python3.13/site-packages/httpcore/_async/connection.py\", line 78, in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.13/site-packages/httpcore/_async/connection.py\", line 156, in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.13/site-packages/httpcore/_backends/anyio.py\", line 67, in start_tls\n    with map_exceptions(exc_map):\n         ~~~~~~~~~~~~~~^^^^^^^^^\n  File \"/usr/local/lib/python3.13/contextlib.py\", line 162, in __exit__\n    self.gen.throw(value)\n    ~~~~~~~~~~~~~~^^^^^^^\n  File \"/usr/local/lib/python3.13/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate (_ssl.c:1018)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.13/site-packages/openai/_base_client.py\", line 1500, in _request\n    response = await self._client.send(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<3 lines>...\n    )\n    ^\n  File \"/usr/local/lib/python3.13/site-packages/httpx/_client.py\", line 1661, in send\n    response = await self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/local/lib/python3.13/site-packages/httpx/_client.py\", line 1689, in _send_handling_auth\n    response = await self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<3 lines>...\n    )\n    ^\n  File \"/usr/local/lib/python3.13/site-packages/httpx/_client.py\", line 1726, in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.13/site-packages/httpx/_client.py\", line 1763, in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.13/site-packages/httpx/_transports/default.py\", line 372, in handle_async_request\n    with map_httpcore_exceptions():\n         ~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/usr/local/lib/python3.13/contextlib.py\", line 162, in __exit__\n    self.gen.throw(value)\n    ~~~~~~~~~~~~~~^^^^^^^\n  File \"/usr/local/lib/python3.13/site-packages/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate (_ssl.c:1018)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.13/site-packages/litellm/llms/openai/openai.py\", line 952, in async_streaming\n    headers, response = await self.make_openai_chat_completion_request(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/local/lib/python3.13/site-packages/litellm/litellm_core_utils/logging_utils.py\", line 131, in async_wrapper\n    result = await func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.13/site-packages/litellm/llms/openai/openai.py\", line 438, in make_openai_chat_completion_request\n    raise e\n  File \"/usr/local/lib/python3.13/site-packages/litellm/llms/openai/openai.py\", line 420, in make_openai_chat_completion_request\n    await openai_aclient.chat.completions.with_raw_response.create(\n        **data, timeout=timeout\n    )\n  File \"/usr/local/lib/python3.13/site-packages/openai/_legacy_response.py\", line 381, in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2000, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n    ...<43 lines>...\n    )\n    ^\n  File \"/usr/local/lib/python3.13/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.13/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n    ...<5 lines>...\n    )\n    ^\n  File \"/usr/local/lib/python3.13/site-packages/openai/_base_client.py\", line 1534, in _request\n    raise APIConnectionError(request=request) from err\nopenai.APIConnectionError: Connection error.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.13/site-packages/litellm/main.py\", line 471, in acompletion\n    response = await init_response\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.13/site-packages/litellm/llms/openai/openai.py\", line 1010, in async_streaming\n    raise OpenAIError(\n    ...<4 lines>...\n    )\nlitellm.llms.openai.common_utils.OpenAIError: Connection error.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.13/site-packages/litellm/router.py\", line 3161, in async_function_with_fallbacks\n    response = await self.async_function_with_retries(*args, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.13/site-packages/litellm/router.py\", line 3537, in async_function_with_retries\n    raise original_exception\n  File \"/usr/local/lib/python3.13/site-packages/litellm/router.py\", line 3430, in async_function_with_retries\n    response = await self.make_call(original_function, *args, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.13/site-packages/litellm/router.py\", line 3546, in make_call\n    response = await response\n               ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.13/site-packages/litellm/router.py\", line 1077, in _acompletion\n    raise e\n  File \"/usr/local/lib/python3.13/site-packages/litellm/router.py\", line 1036, in _acompletion\n    response = await _response\n               ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.13/site-packages/litellm/utils.py\", line 1441, in wrapper_async\n    raise e\n  File \"/usr/local/lib/python3.13/site-packages/litellm/utils.py\", line 1300, in wrapper_async\n    result = await original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.13/site-packages/litellm/main.py\", line 490, in acompletion\n    raise exception_type(\n          ~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<3 lines>...\n        extra_kwargs=kwargs,\n        ^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/usr/local/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2214, in exception_type\n    raise e\n  File \"/usr/local/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 455, in exception_type\n    raise APIError(\n    ...<6 lines>...\n    )\nlitellm.exceptions.APIError: litellm.APIError: APIError: OpenAIException - Connection error. LiteLLM Retried: 1 times, LiteLLM Max Retries: 2\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.63.11\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "Ithanil",
      "author_type": "User",
      "created_at": "2025-03-18T15:19:27Z",
      "updated_at": "2025-06-11T17:14:45Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9340/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9340",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9340",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:26.069144",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "Acknowledged and working on a fix ",
          "created_at": "2025-03-18T15:27:29Z"
        },
        {
          "author": "matheusgs-ciandt",
          "body": "Hey Folks, is there any update or workaround this this BUG?",
          "created_at": "2025-05-13T20:46:37Z"
        },
        {
          "author": "avmturo",
          "body": "Bumping this, would be great to have it working\n\n> Hey Folks, is there any update or workaround this this BUG?\n\n",
          "created_at": "2025-05-16T23:47:30Z"
        },
        {
          "author": "Akearee",
          "body": "only solution is to change https to http, but it's better to be able to fix it",
          "created_at": "2025-06-11T17:14:44Z"
        }
      ]
    },
    {
      "issue_number": 11592,
      "title": "[Bug]: UI not accessible",
      "body": "### What happened?\n\nA bug happened!\n\nRelated to the closed case - https://github.com/BerriAI/litellm/issues/11531\n\nI was using the latest image which then I faced 404's on the logs:\n```Ruby\nlitellm  | INFO:     192.168.1.x:58284 - \"GET /litellm2/_next/static/chunks/webpack-a426aae3231a8df1.js HTTP/1.1\" 404 Not Found\nlitellm  | INFO:     192.168.1.x:58285 - \"GET /litellm2/_next/static/chunks/fd9d1056-205af899b895cbac.js HTTP/1.1\" 404 Not Found\nlitellm  | INFO:     192.168.1.x:58284 - \"GET /litellm2/_next/static/chunks/117-c4922b1dd81b62ce.js HTTP/1.1\" 404 Not Found\nlitellm  | INFO:     192.168.1.x:58286 - \"GET /litellm2/_next/static/chunks/main-app-4f7318ae681a6d94.js HTTP/1.1\" 404 Not Found\n```\n\nDid as per the comment here - https://github.com/BerriAI/litellm/issues/11531#issuecomment-2955082599, right now i'm facing a new one. \n\n<img width=\"1103\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/5debc599-587d-4509-92f8-f7d656adcc71\" />\n\nNo logs on litellm regarding the above error. \n\nLet me know what can be done, thank you\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.2.rc\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "decipher27",
      "author_type": "User",
      "created_at": "2025-06-10T15:21:06Z",
      "updated_at": "2025-06-11T16:11:16Z",
      "closed_at": "2025-06-11T16:11:16Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11592/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11592",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11592",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:26.280660",
      "comments": [
        {
          "author": "azdolinski",
          "body": "![Image](https://github.com/user-attachments/assets/87837fd0-16ea-4959-905c-f77af744a5e8)\n\nSame problem - 1.72.2\nlooks like URL is incorrect...  **GET /litellm2/...** not exist...",
          "created_at": "2025-06-10T15:51:55Z"
        },
        {
          "author": "ErcinDedeoglu",
          "body": "image: ghcr.io/berriai/litellm:main-latest\n\n**Container logs:**\n```\ngenerator client {\n  provider             = \"prisma-client-py\"\n  recursive_type_depth = -1\n}\nIf you need to use Mypy, you can also disable this message by explicitly setting the default value:\ngenerator client {\n  provider          ",
          "created_at": "2025-06-10T16:10:45Z"
        },
        {
          "author": "ErcinDedeoglu",
          "body": "APIs are not working also.",
          "created_at": "2025-06-10T16:15:47Z"
        },
        {
          "author": "syntax-shaman88",
          "body": "After two days of debugging my application with the LiteLLM UI, I encountered issues similar to those experienced by others. I tried various fixes, such as modifying paths, restoring older versions, and using different browsers, but the problem persisted in Docker Compose.\n\nHowever, UI somehow works",
          "created_at": "2025-06-10T17:53:24Z"
        },
        {
          "author": "mckenna654",
          "body": "White screen here, cannot load UI at all.",
          "created_at": "2025-06-11T00:00:02Z"
        }
      ]
    },
    {
      "issue_number": 11617,
      "title": "[Bug]: MCP not functional as expected",
      "body": "### What happened?\n\nA bug happened!\nim using MCP server i created, i tested it using python client (sdk) and Curser.AI IDE client, it show the tools and able to use them.\n\nwhen add the same thing in Litellm, no tools appears, i tried to add the litellm mcp using the Curser.AI but no luck.\nNote, that i do not see any call from litellm to my MCP server, litellm do not do any call, then i saw the exception attached.\n\nHere is the MCP Server repo: https://github.com/Joseph1977/py-mcp-server\nIn the repo there is aclient test as well as MCP Flow (may help in implementing the client side): [link](https://github.com/Joseph1977/py-mcp-server/blob/master/mcp-flow.md)\n\nhere is the MCP Server setting in Curser IDE  (1st directly to my mcp server - working, 2nd through litellm not working):\n```\n{\n  \"mcpServers\": {\n    \"websearch and weather mcp example\": {\n      \"description\": \"Web search and weather MCP server\",\n      \"url\": \"http://localhost:8001/mcp\",\n      \"transport\": \"sse\"\n    },\n    \"benrazthroghtlitellm\": {\n      \"description\": \"benrazthroghtlitellm\",\n      \"url\": \"http://localhost:4000/mcp\",\n      \"transport\": \"sse\"\n    }\n  }\n}\n```\n\n![Image](https://github.com/user-attachments/assets/f4ed4693-3cac-46c2-bc31-bfcbaedaf454)\n\n\nin litellm :\n\n![Image](https://github.com/user-attachments/assets/3322fcfc-e3c3-40d3-80bc-3d6ec8666216)\n\n\n\nif i set the MCP transport in Litellm as http, i get the following call in my MCP (in litellm no exception but no tools)\n\n```\nINFO:     172.20.0.1:38302 - \"POST /mcp HTTP/1.1\" 307 Temporary Redirect\nINFO:     172.20.0.1:38304 - \"POST /mcp/ HTTP/1.1\" 200 OK\n2025-06-11 11:51:55,828 - mcp.server.lowlevel.server - INFO - Processing request of type ListToolsRequest\n```\n\n\nthe logs if directly from Curser IDE (hope this help):\n```\nINFO:     172.20.0.1:57164 - \"POST /mcp HTTP/1.1\" 307 Temporary Redirect\n2025-06-11 11:52:49,751 - mcp.server.streamable_http_manager - INFO - Created new transport with session ID: 99214bfd82bc452085071d9b5ce6f13b\nINFO:     172.20.0.1:57170 - \"POST /mcp/ HTTP/1.1\" 200 OK\nINFO:     172.20.0.1:57176 - \"POST /mcp HTTP/1.1\" 307 Temporary Redirect\nINFO:     172.20.0.1:57182 - \"POST /mcp/ HTTP/1.1\" 202 Accepted\nINFO:     172.20.0.1:57186 - \"GET /mcp HTTP/1.1\" 307 Temporary Redirect\nINFO:     172.20.0.1:57196 - \"POST /mcp HTTP/1.1\" 307 Temporary Redirect\nINFO:     172.20.0.1:57210 - \"GET /mcp/ HTTP/1.1\" 200 OK\nINFO:     172.20.0.1:57216 - \"POST /mcp/ HTTP/1.1\" 200 OK\n2025-06-11 11:52:49,820 - mcp.server.lowlevel.server - INFO - Processing request of type ListToolsRequest\n```\n\n\n\n\n### Relevant log output\n\n```shell\n|                ~~~~~~~~~~~~^\n\n    |         client,\n\n    |         ^^^^^^^\n\n    |     ...<2 lines>...\n\n    |         timeout=httpx.Timeout(timeout, read=sse_read_timeout),\n\n    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n    |     ) as event_source:\n\n    |     ^\n\n    |   File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n\n    |     return await anext(self.gen)\n\n    |            ^^^^^^^^^^^^^^^^^^^^^\n\n    |   File \"/usr/lib/python3.13/site-packages/httpx_sse/_api.py\", line 69, in aconnect_sse\n\n    |     async with client.stream(method, url, headers=headers, **kwargs) as response:\n\n    |                ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n    |   File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n\n    |     return await anext(self.gen)\n\n    |            ^^^^^^^^^^^^^^^^^^^^^\n\n    |   File \"/usr/lib/python3.13/site-packages/httpx/_client.py\", line 1617, in stream\n\n    |     response = await self.send(\n\n    |                ^^^^^^^^^^^^^^^^\n\n    |     ...<4 lines>...\n\n    |     )\n\n    |     ^\n\n    |   File \"/usr/lib/python3.13/site-packages/httpx/_client.py\", line 1661, in send\n\n    |     response = await self._send_handling_auth(\n\n    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n    |     ...<4 lines>...\n\n    |     )\n\n    |     ^\n\n    |   File \"/usr/lib/python3.13/site-packages/httpx/_client.py\", line 1689, in _send_handling_auth\n\n    |     response = await self._send_handling_redirects(\n\n    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n    |     ...<3 lines>...\n\n    |     )\n\n    |     ^\n\n    |   File \"/usr/lib/python3.13/site-packages/httpx/_client.py\", line 1726, in _send_handling_redirects\n\n    |     response = await self._send_single_request(request)\n\n    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n    |   File \"/usr/lib/python3.13/site-packages/httpx/_client.py\", line 1763, in _send_single_request\n\n    |     response = await transport.handle_async_request(request)\n\n    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n    |   File \"/usr/lib/python3.13/site-packages/httpx/_transports/default.py\", line 372, in handle_async_request\n\n    |     with map_httpcore_exceptions():\n\n    |          ~~~~~~~~~~~~~~~~~~~~~~~^^\n\n    |   File \"/usr/lib/python3.13/contextlib.py\", line 162, in __exit__\n\n    |     self.gen.throw(value)\n\n    |     ~~~~~~~~~~~~~~^^^^^^^\n\n    |   File \"/usr/lib/python3.13/site-packages/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\n\n    |     raise mapped_exc(message) from exc\n\n    | httpx.ConnectError: All connection attempts failed\n\n    +------------------------------------\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.2   - commit: 3a8c4dabd91adc4d497c8772c19f1869c55a5edf , branch (litellm_stable_release_branch)\n\n### Twitter / LinkedIn details\n\n@JosephBenraz / https://www.linkedin.com/in/josephbenraz/",
      "state": "closed",
      "author": "Joseph1977",
      "author_type": "User",
      "created_at": "2025-06-11T11:57:08Z",
      "updated_at": "2025-06-11T16:07:12Z",
      "closed_at": "2025-06-11T16:07:12Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11617/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11617",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11617",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:26.533922",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "That's because this is pending: https://github.com/BerriAI/litellm/issues/11603 ",
          "created_at": "2025-06-11T15:00:48Z"
        }
      ]
    },
    {
      "issue_number": 11626,
      "title": "[Bug]: Openrouter streaming Doesn't Return 'cost' and 'is_byok' from openrouter",
      "body": "### What happened?\n\nWhen querying the openrouter api directly with \"usage\":{\"include\":True} we see the usage come back like this:\n\n`Usage object: {'prompt_tokens': 28, 'completion_tokens': 38, 'total_tokens': 66, 'cost': 7.1e-05, 'is_byok': False, 'prompt_tokens_details': {'cached_tokens': 0}, 'cost_details': {'upstream_inference_cost': None}, 'completion_tokens_details': {'reasoning_tokens': 0}}\n`\n\nyet when we are querying openrouter using stream with stream_options={\"include_usage\":True} this is the usage object we get back:\n`Usage information found:\nRaw usage object: Usage(completion_tokens=34, prompt_tokens=13, total_tokens=47, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None)\n`\n\nscript to replicate on latest main state:\n```\n\nimport os\nimport sys\nimport litellm\n\n# Set your OpenRouter API key\nos.environ[\"OPENROUTER_API_KEY\"] = \"insert_key_here\"\n\n# Define a simple message\nmessages = [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n\n# Enable verbose logging\nos.environ[\"LITELLM_VERBOSE\"] = \"1\"\n\ndef test_streaming_with_final_check():\n    print(\"\\\\n\\\\nTesting model: openai/gpt-3.5-turbo (stream=True with final check)\")\n    print(\"-\" * 50)\n\n    response_stream = litellm.completion(\n        model=\"openrouter/openai/gpt-3.5-turbo\",\n        messages=messages,\n        stream=True,\n        stream_options={\"include_usage\": True}\n    )\n\n    # Iterate through the stream and collect chunks\n    chunks = []\n    final_usage = None\n    for chunk in response_stream:\n        chunks.append(chunk)\n        # The final usage object is expected in the last chunk\n        if hasattr(chunk, \"usage\") and chunk.usage is not None:\n            final_usage = chunk.usage\n\n    print(\"Streaming finished.\")\n\n    # Manually build the final response from chunks\n    # This simulates what happens internally after the stream ends.\n    # The key is that the *last* chunk should contain the complete usage.\n    # Let's check the final_usage object we captured.\n\n    print(\"\\\\nChecking for usage information in the final captured usage object:\")\n    if final_usage:\n        print(\"Usage information found:\")\n        print(f\"Raw usage object: {final_usage}\")\n        print(f\"Prompt tokens: {final_usage.prompt_tokens}\")\n        print(f\"Completion tokens: {final_usage.completion_tokens}\")\n        print(f\"Total tokens: {final_usage.total_tokens}\")\n\n        # Check for cost information\n        if hasattr(final_usage, \"cost\") and final_usage.cost is not None:\n            print(f\"Cost: {final_usage.cost}\")\n            assert final_usage.cost > 0\n        else:\n            print(\"No cost information found in the final usage object.\")\n            assert False, \"Cost not found\"\n\n        # Check for is_byok\n        if hasattr(final_usage, \"is_byok\") and final_usage.is_byok is not None:\n            print(f\"Is BYOK: {final_usage.is_byok}\")\n        else:\n            print(\"No is_byok information found in the final usage object.\")\n            assert False, \"is_byok not found\"\n    else:\n        print(\"No usage information found in any chunk.\")\n        assert False, \"Usage object not found\"\n\ntry:\n    test_streaming_with_final_check()\n    print(\"\\\\nAll tests passed!\")\nexcept Exception as e:\n    print(f\"\\\\nTest failed: {e}\")\n    sys.exit(1)\n\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.3\n\n### Twitter / LinkedIn details\n\nhttps://www.linkedin.com/in/dagokogos/",
      "state": "open",
      "author": "daarko10",
      "author_type": "User",
      "created_at": "2025-06-11T14:45:32Z",
      "updated_at": "2025-06-11T16:04:23Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11626/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11626",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11626",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:26.710139",
      "comments": []
    },
    {
      "issue_number": 10932,
      "title": "[Bug]: Why Nova-pro-v1(bedrock) generates reasoning block?",
      "body": "### What happened?\n\nI'm confused about why there will be a thinking block while using Nova-pro-v1(bedrock)? As I know and I checked the model information, it does not support thinking.\n\n<img width=\"1117\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/d9f6d843-8d54-4cb0-bb9a-cca46c02bca8\" />\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.69.3\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "Snow9666",
      "author_type": "User",
      "created_at": "2025-05-19T03:29:39Z",
      "updated_at": "2025-06-11T15:57:19Z",
      "closed_at": "2025-06-11T15:53:59Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10932/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10932",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10932",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:26.710364",
      "comments": [
        {
          "author": "zachvida",
          "body": "Hi. I can share that Nova is a thinking model. AWS Documentation states that through prompt engineering you can suggest this behavior.  https://docs.aws.amazon.com/nova/latest/userguide/prompting-chain-of-thought.html\n\nHowever the unlike the Sonnet 3.7+ models it does not have an API field to flag i",
          "created_at": "2025-06-11T15:52:07Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "thanks @zachvida ",
          "created_at": "2025-06-11T15:53:59Z"
        },
        {
          "author": "zachvida",
          "body": "quick example for those that come after:\n```python\n\nimport boto3\nfrom botocore.exceptions import ClientError\n\n# Create a Bedrock Runtime client in the AWS Region you want to use.\nclient = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n\n#Set the model ID, e.g. Amazon Nova Lite.\nmodel_id = \"",
          "created_at": "2025-06-11T15:55:40Z"
        }
      ]
    },
    {
      "issue_number": 11274,
      "title": "[Feature]: enable Cohere Embed v4 model in `embedding`",
      "body": "### The Feature\n\nCohere Embed v4 model\n\n### Motivation, pitch\n\nIt's the latest and best multi-modal Embedding model!\n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "anuar12",
      "author_type": "User",
      "created_at": "2025-05-30T17:53:33Z",
      "updated_at": "2025-06-11T15:23:32Z",
      "closed_at": "2025-06-02T22:55:00Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11274/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11274",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11274",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:26.926690",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Hey @anuar12 this should already work - what's the error you're seeing? ",
          "created_at": "2025-05-31T15:38:09Z"
        },
        {
          "author": "colesmcintosh",
          "body": "Hey @anuar12! üëã\n\nCohere Embed v4 is already implemented and working in LiteLLM\n\n```python\nfrom litellm import embedding\n\n# Works with text and images (multimodal)\nresponse = embedding(model=\"cohere/embed-v4.0\", input=\"Hello world!\", dimensions=512)\n```\n\nI just added more tests to ensure robust suppo",
          "created_at": "2025-06-02T17:59:59Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Thanks @colesmcintosh ",
          "created_at": "2025-06-02T22:55:04Z"
        },
        {
          "author": "anuar12",
          "body": "@colesmcintosh \nAwesome, thanks a lot! Just tested and it works!\n\nOn a separate note, there doesn't seem to be `num_retries` on `embedding()`. Sometimes Cohere api fails with 500 status code, retry would be nice ",
          "created_at": "2025-06-11T15:23:12Z"
        }
      ]
    },
    {
      "issue_number": 11593,
      "title": "[Bug]: Huggingface dedicated inference endpoints no longer works",
      "body": "### What happened?\n\nHi there! My team and I were working on an older version of litellm (1.65) using Huggingface's inference server endpoints. Upgrading to the most recent version causes our model inference to break. Even using the provided example code in the docs doesn't work: \n\n```\nimport os\nfrom litellm import completion\n\nos.environ[\"HF_TOKEN\"] = \"hf_xxxxxx\"\n\nresponse = completion(\n    model=\"huggingface/tgi\",\n    messages=[{\"content\": \"Hello, how are you?\", \"role\": \"user\"}],\n    api_base=\"https://my-endpoint.endpoints.huggingface.cloud/v1/\"\n)\nprint(response)\n```\n\nThe error message we get is: \n\n```\nlitellm.exceptions.APIConnectionError: litellm.APIConnectionError: not enough values to unpack (expected 2, got 1)\nTraceback (most recent call last):\n  File \"/Users/steveli/anaconda3/envs/py313/lib/python3.13/site-packages/litellm/main.py\", line 2254, in completion\n    response = base_llm_http_handler.completion(\n        model=model,\n    ...<13 lines>...\n        stream=stream,\n    )\n  File \"/Users/steveli/anaconda3/envs/py313/lib/python3.13/site-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 314, in completion\n    data = provider_config.transform_request(\n        model=model,\n    ...<3 lines>...\n        headers=headers,\n    )\n  File \"/Users/steveli/anaconda3/envs/py313/lib/python3.13/site-packages/litellm/llms/huggingface/chat/transformation.py\", line 124, in transform_request\n    first_part, remaining = model.split(\"/\", 1)\n    ^^^^^^^^^^^^^^^^^^^^^\nValueError: not enough values to unpack (expected 2, got 1)\n```\n\nWe've narrowed the issue down to the changes introduced in this pr: https://github.com/BerriAI/litellm/pull/8258. From my understanding the changes were made to support the serverless endpoints by using the `base_llm_http_handler`, which rely on the model id/base url to pass into the openai API format. However, this removes support for the inference server endpoints, as the model_id that's passed in is simply `huggingface/tgi.` \n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.72\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "lithafnium",
      "author_type": "User",
      "created_at": "2025-06-10T15:58:55Z",
      "updated_at": "2025-06-11T14:37:01Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11593/reactions",
        "total_count": 2,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 1,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11593",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11593",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:27.098399",
      "comments": [
        {
          "author": "andresC98",
          "body": "This issue also affects self-hosted [TGI](https://github.com/huggingface/text-generation-inference) endpoints, we are facing the same issue.",
          "created_at": "2025-06-11T14:37:01Z"
        }
      ]
    },
    {
      "issue_number": 9847,
      "title": "[Feature]: Make Azure AD scope a configurable parameter in get_azure_ad_token_from_entrata_id",
      "body": "### The Feature\n\nThe scope parameter in the function get_azure_ad_token_from_entrata_id (defined in common_utils.py) is currently hardcoded to:\n\n`scope: str = \"https://cognitiveservices.azure.com/.default\"\n`\n\nThis default scope is tied to Microsoft Cognitive Services, and while it works for some use cases, it may not apply across different tenants or organizational needs.\n\nProblem:\n\nIn many enterprise environments, the Azure AD scope may vary depending on:\n\nThe specific resource being accessed.\n\nThe tenant-specific configuration.\n\nCustom applications or APIs that use a different base URL.\n\nAs a result, the hardcoded default scope can lead to token authentication failures when used outside of the intended Microsoft endpoint.\n\nSuggested Change:\n\nUpdate the function so that scope becomes a required or properly configurable parameter (instead of defaulting silently):\n\n`def get_azure_ad_token_from_entrata_id(\n    tenant_id: str,\n    client_id: str,\n    client_secret: str,\n    scope: str = \"https://cognitiveservices.azure.com/.default\",  # make optional or remove default\n) -> Callable[[], str]:\n    ...\n`\n\n\n\n### Motivation, pitch\n\nI'm currently integrating Azure AD authentication for different resources across multiple tenants, and I've encountered an issue where the hardcoded scope value (https://cognitiveservices.azure.com/.default) does not apply to our use case.\n\nSince the Azure scope can vary based on the resource URI or tenant configuration, having a configurable scope parameter would make this function far more flexible and reusable. Right now, we're forced to fork or modify the codebase to pass a custom scope, which isn't ideal.\n\nMaking scope explicitly configurable (and not tied to a single default) would significantly improve developer experience and make the function suitable for a broader set of real-world scenarios.\n\n### Are you a ML Ops Team?\n\nYes\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "kjoth",
      "author_type": "User",
      "created_at": "2025-04-09T07:15:21Z",
      "updated_at": "2025-06-11T12:45:41Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9847/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9847",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9847",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:27.299300",
      "comments": [
        {
          "author": "kjoth",
          "body": "@krrishdholakia @ishaan-jaff  Tagging this for the above feature request. The current implementation does not support Azure authentication across different organizations, particularly when their scope variables are derived from their respective Managed Applications.",
          "created_at": "2025-05-14T06:18:34Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Hey @kjoth a PR here is welcome! \n",
          "created_at": "2025-05-14T06:25:07Z"
        },
        {
          "author": "kjoth",
          "body": "> Hey [@kjoth](https://github.com/kjoth) a PR here is welcome!\n\nSure, let me try it out. ",
          "created_at": "2025-05-26T07:19:35Z"
        },
        {
          "author": "kjoth",
          "body": "Hi @krrishdholakia , PR has been created for the above request - https://github.com/BerriAI/litellm/pull/11621 . Kindly review.",
          "created_at": "2025-06-11T12:45:41Z"
        }
      ]
    },
    {
      "issue_number": 8536,
      "title": "[Bug]: Model Not Mapped Yet - Unable to Use",
      "body": "### What happened?\n\nThe document shows that the GitHub model is available, but when I try to use it, I am told \"This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\"\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n0\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "Okeanos404",
      "author_type": "User",
      "created_at": "2025-02-14T12:50:48Z",
      "updated_at": "2025-06-11T11:00:49Z",
      "closed_at": null,
      "labels": [
        "bug",
        "awaiting: user response"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8536/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8536",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8536",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:29.140762",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Hi @Henrik404 the model cost map shouldn't prevent a call from happening. It just means we can't track the cost. \n\nCan you share your client code and the error you see? \n",
          "created_at": "2025-02-15T00:43:11Z"
        },
        {
          "author": "mbanerjeepalmer",
          "body": "What's the workaround here? There's nothing obvious in the docs:https://docs.litellm.ai/search?q=Not%20mapped\n\nLooks like I can pass all the costs etc. in as kwargs. But until I do, the logging is pretty obtrusive. I would like a 'Don't bother tracking costs' option.",
          "created_at": "2025-05-05T13:28:14Z"
        },
        {
          "author": "mbanerjeepalmer",
          "body": "@Okeanos404 What was the solution?",
          "created_at": "2025-06-10T02:32:45Z"
        }
      ]
    },
    {
      "issue_number": 10626,
      "title": "[Bug]: Swagger and Ui issue with kubernetes",
      "body": "### What happened?\n\nI have a lite llm service running on k8s behind the path /llm.\nI built the image with the UI_BASE_PATH env var like the doc and set the env var SERVER_ROOT_PATH to /llm\nUnfortunately I dont have any access to the UI or the swagger\n\nhere is my Dockerfile\n```Dockerfile\nFROM ghcr.io/berriai/litellm:main-v1.68.0-stable\n\nWORKDIR /app\n\nRUN apk upgrade --no-cache && apk add --update npm curl\n\nENV NVM_DIR=/root/.nvm\n\nRUN curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.38.0/install.sh | bash\n\nENV NODE_VERSION=18.17.0\n\nENV PATH=$NVM_DIR/versions/node/v$NODE_VERSION/bin:$PATH\n\nRUN mkdir -p $NVM_DIR && \\\n    curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.7/install.sh | bash && \\\n    . $NVM_DIR/nvm.sh && \\\n    nvm install $NODE_VERSION && nvm use default\n\n\nRUN mkdir -p config\n\nCOPY ./config.yaml ./config\nCOPY ./google_credentials.json ./config\n\nWORKDIR /app/ui/litellm-dashboard\nRUN npm install\n\nENV UI_BASE_PATH=/llm/ui\n\nRUN UI_BASE_PATH=$UI_BASE_PATH npm run build\n\nRUN mkdir -p /app/litellm/proxy/_experimental/out\n\nRUN rm -rf /app/litellm/proxy/_experimental/out/* && \\\n    mv ./out/* /app/litellm/proxy/_experimental/out/\n\nWORKDIR /app\n\nRUN chmod +x ./docker/entrypoint.sh\nRUN chmod +x docker/prod_entrypoint.sh\n\nEXPOSE 4000/tcp\n\nCMD [\"--port\", \"4000\", \"--config\", \"./config/config.yaml\"]\n```\n\nAnd my ingress config:\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: lite-llm\n  namespace: application\n\nspec:\n  ingressClassName: nginx\n  rules:\n  - http:\n      paths:\n      - path: /llm\n        pathType: Prefix\n        backend:\n          service:\n            name: lite-llm\n            port:\n              number: 4000```\n\n### Relevant log output\n\n```shell\nOn /llm/ui:\nAll the request are made on https://mydns.com/ui/_next/*\n\nOn /llm:\nI have this error on the console:\nUncaught ReferenceError: SwaggerUIBundle is not defined\n    <anonymous> https://mydns.com/llm/:15\n\nand all the request are made in /swagger/* instead of /llm/swagger/* which is accessible with curl.\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.68.0\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "kbencherif",
      "author_type": "User",
      "created_at": "2025-05-07T15:22:54Z",
      "updated_at": "2025-06-11T09:10:22Z",
      "closed_at": "2025-06-11T09:10:22Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10626/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10626",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10626",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:29.325352",
      "comments": [
        {
          "author": "kbencherif",
          "body": "Fixed in v1.72.4",
          "created_at": "2025-06-11T09:10:22Z"
        }
      ]
    },
    {
      "issue_number": 11613,
      "title": "[Bug]: impossible to update budget on teams",
      "body": "### What happened?\n\nA bug happened!\n\n### Relevant log output\n\n```shell\n+ Exception Group Traceback (most recent call last):\n  |   File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/_utils.py\", line 76, in collapse_excgroups\n  |     yield\n  |   File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/middleware/base.py\", line 186, in __call__\n  |     async with anyio.create_task_group() as task_group:\n  |                ~~~~~~~~~~~~~~~~~~~~~~~^^\n  |   File \"/opt/bitnami/python/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 685, in __aexit__\n  |     raise BaseExceptionGroup(\n  |         \"unhandled errors in a TaskGroup\", self._exceptions\n  |     )\n  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n  +-+---------------- 1 ----------------\n    | Traceback (most recent call last):\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/uvicorn/protocols/http/h11_impl.py\", line 407, in run_asgi\n    |     result = await app(  # type: ignore[func-returns-value]\n    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |         self.scope, self.receive, self.send\n    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |     )\n    |     ^\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/uvicorn/middleware/proxy_headers.py\", line 69, in __call__\n    |     return await self.app(scope, receive, send)\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/fastapi/applications.py\", line 1054, in __call__\n    |     await super().__call__(scope, receive, send)\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/applications.py\", line 113, in __call__\n    |     await self.middleware_stack(scope, receive, send)\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/middleware/errors.py\", line 187, in __call__\n    |     raise exc\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    |     await self.app(scope, receive, _send)\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/middleware/base.py\", line 185, in __call__\n    |     with collapse_excgroups():\n    |          ~~~~~~~~~~~~~~~~~~^^\n    |   File \"/opt/bitnami/python/lib/python3.13/contextlib.py\", line 162, in __exit__\n    |     self.gen.throw(value)\n    |     ~~~~~~~~~~~~~~^^^^^^^\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/_utils.py\", line 82, in collapse_excgroups\n    |     raise exc\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/middleware/base.py\", line 187, in __call__\n    |     response = await self.dispatch_func(request, call_next)\n    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/litellm/proxy/middleware/prometheus_auth_middleware.py\", line 47, in dispatch\n    |     response = await call_next(request)\n    |                ^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/middleware/base.py\", line 163, in call_next\n    |     raise app_exc\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/middleware/base.py\", line 149, in coro\n    |     await self.app(scope, receive_or_disconnect, send_no_error)\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/middleware/cors.py\", line 93, in __call__\n    |     await self.simple_response(scope, receive, send, request_headers=headers)\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\n    |     await self.app(scope, receive, send)\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    |     raise exc\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    |     await app(scope, receive, sender)\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/routing.py\", line 715, in __call__\n    |     await self.middleware_stack(scope, receive, send)\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/routing.py\", line 735, in app\n    |     await route.handle(scope, receive, send)\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/routing.py\", line 288, in handle\n    |     await self.app(scope, receive, send)\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/routing.py\", line 76, in app\n    |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    |     raise exc\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    |     await app(scope, receive, sender)\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/routing.py\", line 73, in app\n    |     response = await f(request)\n    |                ^^^^^^^^^^^^^^^^\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/fastapi/routing.py\", line 301, in app\n    |     raw_response = await run_endpoint_function(\n    |                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |     ...<3 lines>...\n    |     )\n    |     ^\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n    |     return await dependant.call(**values)\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/litellm/proxy/management_helpers/utils.py\", line 368, in wrapper\n    |     raise e\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/litellm/proxy/management_helpers/utils.py\", line 284, in wrapper\n    |     result = await func(*args, **kwargs)\n    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/litellm/proxy/management_endpoints/team_endpoints.py\", line 675, in update_team\n    |     updated_kv = await handle_update_object_permission(\n    |                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |     ...<2 lines>...\n    |     )\n    |     ^\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/litellm/proxy/management_endpoints/team_endpoints.py\", line 763, in handle_update_object_permission\n    |     object_permission_id = await handle_update_object_permission_common(\n    |                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |     ...<3 lines>...\n    |     )\n    |     ^\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/litellm/proxy/management_helpers/object_permission_utils.py\", line 58, in handle_update_object_permission_common\n    |     await prisma_client.db.litellm_objectpermissiontable.find_unique(\n    |         where={\"object_permission_id\": object_permission_id_to_use},\n    |     )\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/prisma/actions.py\", line 7553, in find_unique\n    |     resp = await self._client._execute(\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |     ...<6 lines>...\n    |     )\n    |     ^\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/prisma/client.py\", line 561, in _execute\n    |     return await self._engine.query(builder.build(), tx_id=self._tx_id)\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/prisma/engine/query.py\", line 244, in query\n    |     return await self.request(\n    |            ^^^^^^^^^^^^^^^^^^^\n    |     ...<4 lines>...\n    |     )\n    |     ^\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/prisma/engine/http.py\", line 141, in request\n    |     return utils.handle_response_errors(resp, errors_data)\n    |            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n    |   File \"/opt/bitnami/python/lib/python3.13/site-packages/prisma/engine/utils.py\", line 200, in handle_response_errors\n    |     raise prisma_errors.DataError(data[0])\n    | prisma.errors.DataError: The column `LiteLLM_ObjectPermissionTable.vector_stores` does not exist in the current database.\n    +------------------------------------\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/uvicorn/protocols/http/h11_impl.py\", line 407, in run_asgi\n    result = await app(  # type: ignore[func-returns-value]\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self.scope, self.receive, self.send\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/uvicorn/middleware/proxy_headers.py\", line 69, in __call__\n    return await self.app(scope, receive, send)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/fastapi/applications.py\", line 1054, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/applications.py\", line 113, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/middleware/errors.py\", line 187, in __call__\n    raise exc\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    await self.app(scope, receive, _send)\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/middleware/base.py\", line 185, in __call__\n    with collapse_excgroups():\n         ~~~~~~~~~~~~~~~~~~^^\n  File \"/opt/bitnami/python/lib/python3.13/contextlib.py\", line 162, in __exit__\n    self.gen.throw(value)\n    ~~~~~~~~~~~~~~^^^^^^^\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/_utils.py\", line 82, in collapse_excgroups\n    raise exc\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/middleware/base.py\", line 187, in __call__\n    response = await self.dispatch_func(request, call_next)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/litellm/proxy/middleware/prometheus_auth_middleware.py\", line 47, in dispatch\n    response = await call_next(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/middleware/base.py\", line 163, in call_next\n    raise app_exc\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/middleware/base.py\", line 149, in coro\n    await self.app(scope, receive_or_disconnect, send_no_error)\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/middleware/cors.py\", line 93, in __call__\n    await self.simple_response(scope, receive, send, request_headers=headers)\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\n    await self.app(scope, receive, send)\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/routing.py\", line 715, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/routing.py\", line 735, in app\n    await route.handle(scope, receive, send)\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/routing.py\", line 288, in handle\n    await self.app(scope, receive, send)\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/routing.py\", line 76, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/starlette/routing.py\", line 73, in app\n    response = await f(request)\n               ^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/fastapi/routing.py\", line 301, in app\n    raw_response = await run_endpoint_function(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<3 lines>...\n    )\n    ^\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n    return await dependant.call(**values)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/litellm/proxy/management_helpers/utils.py\", line 368, in wrapper\n    raise e\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/litellm/proxy/management_helpers/utils.py\", line 284, in wrapper\n    result = await func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/litellm/proxy/management_endpoints/team_endpoints.py\", line 675, in update_team\n    updated_kv = await handle_update_object_permission(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<2 lines>...\n    )\n    ^\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/litellm/proxy/management_endpoints/team_endpoints.py\", line 763, in handle_update_object_permission\n    object_permission_id = await handle_update_object_permission_common(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<3 lines>...\n    )\n    ^\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/litellm/proxy/management_helpers/object_permission_utils.py\", line 58, in handle_update_object_permission_common\n    await prisma_client.db.litellm_objectpermissiontable.find_unique(\n        where={\"object_permission_id\": object_permission_id_to_use},\n    )\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/prisma/actions.py\", line 7553, in find_unique\n    resp = await self._client._execute(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/prisma/client.py\", line 561, in _execute\n    return await self._engine.query(builder.build(), tx_id=self._tx_id)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/prisma/engine/query.py\", line 244, in query\n    return await self.request(\n           ^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/prisma/engine/http.py\", line 141, in request\n    return utils.handle_response_errors(resp, errors_data)\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/python/lib/python3.13/site-packages/prisma/engine/utils.py\", line 200, in handle_response_errors\n    raise prisma_errors.DataError(data[0])\nprisma.errors.DataError: The column `LiteLLM_ObjectPermissionTable.vector_stores` does not exist in the current database.\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nlast main\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "superpoussin22",
      "author_type": "User",
      "created_at": "2025-06-11T06:38:02Z",
      "updated_at": "2025-06-11T06:38:09Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11613/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11613",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11613",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:29.569398",
      "comments": []
    },
    {
      "issue_number": 11590,
      "title": "[Feature]: Add a close method to LangFuseLogger class",
      "body": "### The Feature\n\nHi, we have been an early adopter and a frequent user of litellm at Joveo.\n\nWe have had to extend your Langfuse logger and use it in our custom logger method to suit our usecase but lately due to changes to the MAX_LANGFUSE_LOGGER limits we are facing issues where our custom logger stops working. I would to request adding a close method to that class so that when our custom logger's method hits the closure we terminate the client\n\n\n\n### Motivation, pitch\n\nWe have been using litellm with little to no issues. But the recent upgrade has disrupted our operations. We had to extend the LangFuseLogger class and use it in our custom logger for a niche PII usecase. due to the limits placed on its instances we have been facing errors, increasing the limits hasnt helped us. So if a close method is added to this class which updates the litellm count internally we would like to handle the closure manually\n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "bhaveshnarra",
      "author_type": "User",
      "created_at": "2025-06-10T15:01:32Z",
      "updated_at": "2025-06-11T03:51:17Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11590/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11590",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11590",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:29.569420",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "the root cause is langfuse clients init 1 thread on startup which was impacting cpu% of our users",
          "created_at": "2025-06-10T22:55:01Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Hey @bhaveshnarra to understand this correctly - were you using litellm's native langfuse integration -> hit the max client issue -> move to a custom implementation? \n",
          "created_at": "2025-06-10T22:56:21Z"
        },
        {
          "author": "bhaveshnarra",
          "body": "hi @krrishdholakia I was using the custom integration from the start but after a recent push I started getting the max clients error and logs stopped showing up. if these a close method available on your class. I will call it from my custom logger when I need to close the client ",
          "created_at": "2025-06-11T03:51:17Z"
        }
      ]
    },
    {
      "issue_number": 11365,
      "title": "[Feature]: Support Qdrant vector store",
      "body": "### The Feature\n\nit looks like litellm only supports bedrock knowledgebase, but it would be great if it could also support qdrant as well.\n\n### Motivation, pitch\n\nreduce vendor lock-in, self deploy options, etc...\n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "schwifty-brows",
      "author_type": "User",
      "created_at": "2025-06-03T13:30:02Z",
      "updated_at": "2025-06-11T02:04:43Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11365/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11365",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11365",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:29.829857",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "done here: https://github.com/BerriAI/litellm/issues/11365 ",
          "created_at": "2025-06-06T01:28:49Z"
        },
        {
          "author": "browsified",
          "body": "Can you check it out, it looks like the build failed ",
          "created_at": "2025-06-11T02:04:43Z"
        }
      ]
    },
    {
      "issue_number": 11543,
      "title": "[Feature]: reasoning_effort for Perplexity api",
      "body": "### The Feature\n\nHi, firstly thank you for the effort in keeping litellm updated. Greatly appreciate this. \n\nhttps://docs.perplexity.ai/models/models/sonar-deep-research\n\nThere is a new reasoning_effort param for perplexity api which allows us to select a higher amount of reasoning effort for the param. \n\n### Motivation, pitch\n\nThe reasoning_effort works using curl but not the litellm perplexity call. \n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "lppier",
      "author_type": "User",
      "created_at": "2025-06-09T02:23:02Z",
      "updated_at": "2025-06-11T01:21:38Z",
      "closed_at": "2025-06-10T00:07:32Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11543/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11543",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11543",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:30.107190",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "done here: https://github.com/BerriAI/litellm/pull/11562 ",
          "created_at": "2025-06-09T22:35:03Z"
        },
        {
          "author": "lppier",
          "body": "Thank you!!",
          "created_at": "2025-06-11T01:21:38Z"
        }
      ]
    },
    {
      "issue_number": 8578,
      "title": "An error occurred during question recommendation generation: litellm.APIError: APIError: OpenAIException - Connection error.",
      "body": "100it [00:00, 9756.92it/s]           \nI0217 06:51:32.295 8 wren-ai-service:154] Request ac107217-f0f3-4dbd-bbc0-b4b140a9ba4d: Generate Question Recommendation pipeline is running...\nI0217 06:51:32.296 8 wren-ai-service:263] Question Recommendation pipeline is running...\n\n********************************************************************************\n> generate [src.pipelines.generation.question_recommendation.generate()] encountered an error<\n> Node inputs:\n{'generator': '<function LitellmLLMProvider.get_generator.<locals...',\n 'prompt': \"<Task finished name='Task-111' coro=<AsyncGraphAda...\"}\n********************************************************************************\nTraceback (most recent call last):\n  File \"/app/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 72, in map_httpcore_exceptions\n    yield\n  File \"/app/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 377, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\n    raise exc from None\n  File \"/app/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\n    response = await connection.handle_async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 101, in handle_async_request\n    raise exc\n  File \"/app/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 78, in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 124, in _connect\n    stream = await self._network_backend.connect_tcp(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/httpcore/_backends/auto.py\", line 31, in connect_tcp\n    return await self._backend.connect_tcp(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py\", line 113, in connect_tcp\n    with map_exceptions(exc_map):\n  File \"/usr/local/lib/python3.12/contextlib.py\", line 155, in __exit__\n    self.gen.throw(value)\n  File \"/app/.venv/lib/python3.12/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: All connection attempts failed\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/app/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1582, in _request\n    response = await self._client.send(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1674, in send\n    response = await self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1702, in _send_handling_auth\n    response = await self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1739, in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1776, in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 376, in handle_async_request\n    with map_httpcore_exceptions():\n  File \"/usr/local/lib/python3.12/contextlib.py\", line 155, in __exit__\n    self.gen.throw(value)\n  File \"/app/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 89, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: All connection attempts failed\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/app/.venv/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 771, in acompletion\n    headers, response = await self.make_openai_chat_completion_request(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py\", line 131, in async_wrapper\n    result = await func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 419, in make_openai_chat_completion_request\n    raise e\n  File \"/app/.venv/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 401, in make_openai_chat_completion_request\n    await openai_aclient.chat.completions.with_raw_response.create(\n  File \"/app/.venv/lib/python3.12/site-packages/openai/_legacy_response.py\", line 381, in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/langfuse/openai.py\", line 759, in _wrap_async\n    raise ex\n  File \"/app/.venv/lib/python3.12/site-packages/langfuse/openai.py\", line 715, in _wrap_async\n    openai_response = await wrapped(**arg_extractor.get_openai_args())\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1727, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1849, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1543, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1606, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1606, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1676, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1616, in _request\n    raise APIConnectionError(request=request) from err\nopenai.APIConnectionError: Connection error.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/app/.venv/lib/python3.12/site-packages/litellm/main.py\", line 463, in acompletion\n    response = await init_response\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/litellm/llms/openai/openai.py\", line 817, in acompletion\n    raise OpenAIError(\nlitellm.llms.openai.common_utils.OpenAIError: Connection error.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/app/.venv/lib/python3.12/site-packages/hamilton/async_driver.py\", line 122, in new_fn\n    await fn(**fn_kwargs) if asyncio.iscoroutinefunction(fn) else fn(**fn_kwargs)\n    ^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/langfuse/decorators/langfuse_decorator.py\", line 219, in async_wrapper\n    self._handle_exception(observation, e)\n  File \"/app/.venv/lib/python3.12/site-packages/langfuse/decorators/langfuse_decorator.py\", line 517, in _handle_exception\n    raise e\n  File \"/app/.venv/lib/python3.12/site-packages/langfuse/decorators/langfuse_decorator.py\", line 217, in async_wrapper\n    result = await func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/src/pipelines/generation/question_recommendation.py\", line 48, in generate\n    return await generator(prompt=prompt.get(\"prompt\"))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/src/providers/llm/litellm.py\", line 71, in _run\n    completion: Union[ModelResponse] = await acompletion(\n                                       ^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/litellm/utils.py\", line 1358, in wrapper_async\n    raise e\n  File \"/app/.venv/lib/python3.12/site-packages/litellm/utils.py\", line 1217, in wrapper_async\n    result = await original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/litellm/main.py\", line 482, in acompletion\n    raise exception_type(\n          ^^^^^^^^^^^^^^^\n  File \"/app/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2190, in exception_type\n    raise e\n  File \"/app/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 450, in exception_type\n    raise APIError(\nlitellm.exceptions.APIError: litellm.APIError: APIError: OpenAIException - Connection error.\n-------------------------------------------------------------------\nOh no an error! Need help with Hamilton?\nJoin our slack and ask for help! https://join.slack.com/t/hamilton-opensource/shared_invite/zt-2niepkra8-DGKGf_tTYhXuJWBTXtIs4g\n-------------------------------------------------------------------\n\nE0217 06:51:41.734 8 wren-ai-service:60] An error occurred during question recommendation generation: litellm.APIError: APIError: OpenAIException - Connection error.\nthis is my error cannot able to send the request to backend server running on local machine with openaicompitable custom llm \n\n\ntype: llm\nprovider: litellm_llm\ntimeout: 900\nmodels:\n- model: openai/root/.cache/huggingface/hub/Mistral-Small-24B-Instruct-2501-Q4_K_M.gguf\n  api_base: https://IP ADDRESS /v1   Different machine IP address\n  api_key_name: LLM_OPENAI_API_KEY\n  api_key: abc123\n  kwargs:\n    temperature: 0\n    n: 1\n    # for better consistency of llm response, refer: https://platform.openai.com/docs/api-reference/chat/create#chat-create-seed\n    seed: 0\n    max_tokens: 1024\n# - model: /root/.cache/huggingface/hub/Mistral-Small-24B-Instruct-2501-Q4_K_M.gguf\n#   api_base: https://IPADDRESS.com/v1\n#   api_key_name: LLM_OPENAI_API_KEY\n#   kwargs:\n#     temperature: 0\n#     n: 1\n#     # for better consistency of llm response, refer: https://platform.openai.com/docs/api-reference/chat/create#chat-create-seed\n#     seed: 0\n#     max_tokens: 4096\n\n# - model: o3-mini-2025-01-31\n#   api_base: https://api.openai.com/v1\n#   api_key_name: abc123\n#   kwargs:\n#     n: 1\n#     seed: 0\n#     max_completion_tokens: 4096\n#     reasoning_effort: low\n# ---\n# type: embedder\n# provider: litellm_embedder\n# models:\n# - model: openai/nomic-embed-text:latest\n#   api_base: https://localhost:11434/v1\n#   api_key_name: LLM_OPENAI_API_KEY\n#   dimenssion: 768\n#   timeout: 120\n---\ntype: embedder\nprovider: litellm_embedder \nmodels:\n# put OPENAI_API_KEY=OPENAI_API_KEY<random_string> in ~/.wrenai/.env\n- model: openai/nomic-embed-text:latest  # put your ollama embedder model name here, openai/<ollama_model_name>\n  api_base: http://host.docker.internal:11434/v1  # change this to your ollama host, api_base should be <ollama_url>/v1\n  api_key_name: LLM_OPENAI_API_KEY\n  # api_key: abc123\n  timeout: 900\n---\ntype: document_store\nprovider: qdrant\nlocation: http://qdrant:6333\nembedding_model_dim: 768  # put your embedding model dimension here\ntimeout: 900\nrecreate_index: true\n---\ntype: engine\nprovider: wren_ui\nendpoint: http://wren-ui:3000\n\n---\ntype: pipeline\npipes:\n  - name: db_schema_indexing\n    embedder: litellm_embedder.openai/nomic-embed-text:latest\n    document_store: qdrant\n\n  - name: historical_question_indexing\n    embedder: litellm_embedder.openai/nomic-embed-text:latest\n    document_store: qdrant\n\n  - name: table_description_indexing\n    embedder: litellm_embedder.openai/nomic-embed-text:latest\n    document_store: qdrant\n\n  - name: db_schema_retrieval\n    llm: litellm_llm.openai/root/.cache/huggingface/hub/Mistral-Small-24B-Instruct-2501-Q4_K_M.gguf\n\n    embedder: litellm_embedder.openai/nomic-embed-text:latest\n    document_store: qdrant\n\n  - name: historical_question_retrieval\n    embedder: litellm_embedder.openai/nomic-embed-text:latest\n    document_store: qdrant\n\n  - name: sql_generation\n    llm: litellm_llm.openai/root/.cache/huggingface/hub/Mistral-Small-24B-Instruct-2501-Q4_K_M.gguf\n    engine: wren_ui\n\n  - name: sql_correction\n    llm: litellm_llm.openai/root/.cache/huggingface/hub/Mistral-Small-24B-Instruct-2501-Q4_K_M.gguf\n    engine: wren_ui\n\n  - name: followup_sql_generation\n    llm: litellm_llm.openai/root/.cache/huggingface/hub/Mistral-Small-24B-Instruct-2501-Q4_K_M.gguf\n    engine: wren_ui\n\n  - name: sql_summary\n    llm: litellm_llm.openai/root/.cache/huggingface/hub/Mistral-Small-24B-Instruct-2501-Q4_K_M.gguf\n\n  - name: sql_answer\n    llm: litellm_llm.openai/root/.cache/huggingface/hub/Mistral-Small-24B-Instruct-2501-Q4_K_M.gguf\n    engine: wren_ui\n\n  - name: sql_breakdown\n    llm: litellm_llm.openai/root/.cache/huggingface/hub/Mistral-Small-24B-Instruct-2501-Q4_K_M.gguf\n    engine: wren_ui\n\n  - name: sql_expansion\n    llm: litellm_llm.openai/root/.cache/huggingface/hub/Mistral-Small-24B-Instruct-2501-Q4_K_M.gguf\n    engine: wren_ui\n\n  - name: sql_explanation\n    llm: litellm_llm.openai/root/.cache/huggingface/hub/Mistral-Small-24B-Instruct-2501-Q4_K_M.gguf\n\n  - name: sql_regeneration\n    llm: litellm_llm.openai/root/.cache/huggingface/hub/Mistral-Small-24B-Instruct-2501-Q4_K_M.gguf\n    engine: wren_ui\n\n  - name: semantics_description\n    llm: litellm_llm.openai/root/.cache/huggingface/hub/Mistral-Small-24B-Instruct-2501-Q4_K_M.gguf\n\n  - name: relationship_recommendation\n    llm: litellm_llm.openai/root/.cache/huggingface/hub/Mistral-Small-24B-Instruct-2501-Q4_K_M.gguf\n    engine: wren_ui\n\n  - name: question_recommendation\n    llm: litellm_llm.openai/root/.cache/huggingface/hub/Mistral-Small-24B-Instruct-2501-Q4_K_M.gguf\n\n  - name: question_recommendation_db_schema_retrieval\n    llm: litellm_llm.openai/root/.cache/huggingface/hub/Mistral-Small-24B-Instruct-2501-Q4_K_M.gguf\n    embedder: litellm_embedder.openai/nomic-embed-text:latest\n    document_store: qdrant\n\n  - name: question_recommendation_sql_generation\n    llm: litellm_llm.openai/root/.cache/huggingface/hub/Mistral-Small-24B-Instruct-2501-Q4_K_M.gguf\n    engine: wren_ui\n\n  - name: data_assistance\n    llm: litellm_llm.openai/root/.cache/huggingface/hub/Mistral-Small-24B-Instruct-2501-Q4_K_M.gguf\n\n  - name: intent_classification\n    llm: litellm_llm.openai/root/.cache/huggingface/hub/Mistral-Small-24B-Instruct-2501-Q4_K_M.gguf\n    embedder: litellm_embedder.openai/nomic-embed-text:latest\n    document_store: qdrant\n\n  - name: sql_pairs_indexing\n    document_store: qdrant\n    embedder: litellm_embedder.openai/nomic-embed-text:latest\n\n  - name: sql_pairs_deletion\n    document_store: qdrant\n    embedder: litellm_embedder.openai/nomic-embed-text:latest\n\n  - name: sql_pairs_retrieval\n    document_store: qdrant\n    embedder: litellm_embedder.openai/nomic-embed-text:latest\n    llm: litellm_llm.openai/root/.cache/huggingface/hub/Mistral-Small-24B-Instruct-2501-Q4_K_M.gguf\n\n  - name: preprocess_sql_data\n    llm: litellm_llm.openai/root/.cache/huggingface/hub/Mistral-Small-24B-Instruct-2501-Q4_K_M.gguf\n\n  - name: sql_executor\n    engine: wren_ui\n\n  - name: chart_generation\n    llm: litellm_llm.openai/root/.cache/huggingface/hub/Mistral-Small-24B-Instruct-2501-Q4_K_M.gguf\n\n  - name: chart_adjustment\n    llm: litellm_llm.openai/root/.cache/huggingface/hub/Mistral-Small-24B-Instruct-2501-Q4_K_M.gguf\n\n  - name: sql_question_generation\n    llm: litellm_llm.openai/root/.cache/huggingface/hub/Mistral-Small-24B-Instruct-2501-Q4_K_M.gguf\n\n  - name: sql_generation_reasoning\n    llm: litellm_llm.openai/root/.cache/huggingface/hub/Mistral-Small-24B-Instruct-2501-Q4_K_M.gguf\n---\nsettings:\n  column_indexing_batch_size: 50\n  table_retrieval_size: 10\n  table_column_retrieval_size: 100\n  allow_using_db_schemas_without_pruning: false\n  litellm_turn_on_debug: true\n  query_cache_maxsize: 1000\n  query_cache_ttl: 3600\n  langfuse_host: https://cloud.langfuse.com\n  langfuse_enable: false\n  logging_level: DEBUG\n  development: false\nother than wren ai calls and response are working and also where i can see the request call send to backend llm so where i can see the request format hows it going \n",
      "state": "closed",
      "author": "yash199999",
      "author_type": "User",
      "created_at": "2025-02-17T07:12:26Z",
      "updated_at": "2025-06-11T00:02:00Z",
      "closed_at": "2025-06-11T00:02:00Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8578/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8578",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8578",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:30.325468",
      "comments": [
        {
          "author": "leohash",
          "body": "I have the same error. Help please.",
          "created_at": "2025-02-18T00:48:42Z"
        },
        {
          "author": "yash199999",
          "body": "what happens can anyone help here related to the issue\n",
          "created_at": "2025-03-05T05:36:28Z"
        },
        {
          "author": "yash199999",
          "body": "the get request is going to the backend llm server that is not allowed only post allowed but the curl is created correct \n",
          "created_at": "2025-03-05T05:37:14Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-04T00:01:58Z"
        }
      ]
    },
    {
      "issue_number": 8999,
      "title": "[Bug]: incorrect completion_start_time for streaming request",
      "body": "### What happened?\n\nPull request #8156 removed the call to `logging_obj.success_handler` in the file:\n`litellm/litellm_core_utils/streaming_handler.py`\n\nDue to this removal, the following function from `litellm/litellm_core_utils/litellm_logging.py` is no longer invoked during streaming responses:\n\n```\ndef _success_handler_helper_fn(\n    self,\n    result=None,\n    start_time=None,\n    end_time=None,\n    cache_hit=None,\n    standard_logging_object: Optional[StandardLoggingPayload] = None,\n):\n    try:\n        if start_time is None:\n            start_time = self.start_time\n        if end_time is None:\n            end_time = datetime.datetime.now()\n        if self.completion_start_time is None:\n            self.completion_start_time = end_time\n            self.model_call_details[\"completion_start_time\"] = (\n                self.completion_start_time\n            )\n```\n\nAs a result, `completion_start_time` is not accurately recorded, impacting the calculation of the \"time to first token\" metric.\n\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.61.20\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "ljk53",
      "author_type": "User",
      "created_at": "2025-03-05T08:50:13Z",
      "updated_at": "2025-06-11T00:01:57Z",
      "closed_at": "2025-06-11T00:01:57Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8999/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8999",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8999",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:30.552583",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-04T00:01:47Z"
        }
      ]
    },
    {
      "issue_number": 9006,
      "title": "OpenMeter Integration: \"Error at \\\"/subject\\\": value must be a string",
      "body": "> ## OpenMeter Integration: \"Error at \\\"/subject\\\": value must be a string\"\n> \n> **Description:**\n> \n> I'm encountering an issue with the OpenMeter integration in LiteLLM.  I consistently receive a `400 Bad Request` error from OpenMeter with the message: `\"Error at \\\"/subject\\\": value must be a string\"`. This indicates that the value being sent for the `subject` field in the OpenMeter event is not a string, even though I'm explicitly setting it as a string.\n> \n> **Steps to Reproduce:**\n> \n> 1.  Configure the OpenMeter integration in LiteLLM.\n> 2.  Send a request to the LiteLLM proxy (e.g., `/chat/completions`) including the `\"user\"` parameter in the JSON payload.  \n> \n> **some logs:**\n> \n> ```\n> File \"/usr/lib/python3.13/site-packages/litellm/integrations/openmeter.py\", line 130, in async_log_success_event\n> raise Exception(f\"OpenMeter logging error: {e.response.text}\")\n> Exception: OpenMeter logging error: {\"type\":\"about:blank\",\"title\":\"Bad Request\",\"status\":400,\"detail\":\"request body has an error: doesn't match schema #/components/schemas/Event: Error at \\\"/subject\\\": value must be a string\"}\n> ``` \n\n _Originally posted by @mkagit in [#7569](https://github.com/BerriAI/litellm/issues/7569#issuecomment-2701312216)_",
      "state": "closed",
      "author": "mkagit",
      "author_type": "User",
      "created_at": "2025-03-05T15:36:52Z",
      "updated_at": "2025-06-11T00:01:56Z",
      "closed_at": "2025-06-11T00:01:56Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9006/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9006",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9006",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:30.743053",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-04T00:01:46Z"
        }
      ]
    },
    {
      "issue_number": 9857,
      "title": "[Bug]: litellm fails to process title request from librechat",
      "body": "### What happened?\n\nWhat happened?\nwhen running litellm, everything appears to work EXCEPT the titlemodel functionality from librechat.\n\nrecorded request from librechat\n\n{\n\"model\": \"\",\n\"user\": \"\",\n\"temperature\": 0.2,\n\"presence_penalty\": 0,\n\"frequency_penalty\": 0,\n\"max_tokens\": 16,\n\"messages\": [\n{\n\"role\": \"system\",\n\"content\": \"Please generate a concise, 5-word-or-less title for the conversation, using its same language, with no punctuation. Apply title case conventions appropriate for the language. Never directly mention the language name or the word \"title\"\\n\\n||>User:\\n\"test title\"\\n||>Response:\\n\"\"I don't see any text to check beyond \\\"test title.\\\" Could you please share the full text you'd like me to look at? I can help evaluate titles or review any other text you provide.\"\"\\n\\n||>Title:\"\n}\n]\n}\n\nand the response from litellm\n\nlitellm.BadRequestError: Invalid Message bedrock requires at least one non-system message\n\nno matter what title model we pick, its the same result\n\nVersion Information\nlatest\n\nSteps to Reproduce\nroute traffic to litellm proxy\nobserve logs\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.65.4-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "browsified",
      "author_type": "User",
      "created_at": "2025-04-09T17:14:39Z",
      "updated_at": "2025-06-10T23:42:23Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9857/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9857",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9857",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:30.933412",
      "comments": [
        {
          "author": "browsified",
          "body": "posted here and librechat is telling us this is an issue with litellm\n\nhttps://github.com/danny-avila/LibreChat/issues/6804",
          "created_at": "2025-04-09T18:41:06Z"
        },
        {
          "author": "igoichuk",
          "body": "I see the same issue on 1.68-stable and 1.69-stable and LibreChat 0.7.8 ",
          "created_at": "2025-05-12T16:57:04Z"
        },
        {
          "author": "alexhafner",
          "body": "One combination that works is calling the invoke endpoint explicitly ie `model: bedrock/invoke/anthropic.claude-3-5-haiku-20241022-v1:0` in combination with the `config.yaml` setting `modify_params` to `true`\n\n```yaml\nlitellm_settings:\n  modify_params: true\n```",
          "created_at": "2025-06-10T23:42:23Z"
        }
      ]
    },
    {
      "issue_number": 9146,
      "title": "[Feature]: OpenAI Responses API Support",
      "body": "### The Feature\n\nParent ticket to track support for new OpenAI responses API \n\nBased on discord, this will be supported as a separate api spec, instead of trying to translate into `/chat/completions`. https://discord.com/channels/1123360753068540065/1139937429588021289/1349086515665305671\n\nChecklist\n\n#### Checklist for create responses endpoint \n- [x] Non-streaming Async\n- [x] Non-streaming Sync \n- [x] Streaming Async \n- [x] Streaming Sync \n- [x] Non-streaming logging + cost tracking \n- [x] Streaming logging + cost tracking \n- [x] litellm.router support\n\n#### Proxy Checklist \n- [x] post /responses OpenAI non-streaming \n- [x] post /responses OpenAI non-streaming - logging + cost tracking \n- [x] post /responses OpenAI streaming\n- [x] post /responses OpenAI streaming - logging + cost tracking \n- [x] Get Response `get https://api.openai.com/v1/responses/{response_id}`\n- [x] Delete Response `delete https://api.openai.com/v1/responses/{response_id}`\n- [x] List Input items `get https://api.openai.com/v1/responses/{response_id}/input_items`\n\n### Motivation, pitch\n\nMake it easy to give devs llm access\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "krrishdholakia",
      "author_type": "User",
      "created_at": "2025-03-11T18:36:19Z",
      "updated_at": "2025-06-10T22:48:49Z",
      "closed_at": "2025-06-10T22:48:49Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 39,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9146/reactions",
        "total_count": 20,
        "+1": 13,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 7,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9146",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9146",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:31.113577",
      "comments": []
    },
    {
      "issue_number": 11503,
      "title": "[Bug]: `No module named 'diskcache'`",
      "body": "### What happened?\n\nWith Python 3.11 and after `pip install litellm==1.72.1`:\n\n```python\nfrom litellm.caching import Cache\n\nCache(type=\"disk\")\n```\n\nCan `litellm` make an extra for `diskcache`, to avoid the `ModuleNotFoundError`\n\n### Relevant log output\n\n```shell\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/path/to/.venv/lib/python3.11/site-packages/litellm/caching/caching.py\", line 205, in __init__\n    self.cache = DiskCache(disk_cache_dir=disk_cache_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/path/to/.venv/lib/python3.11/site-packages/litellm/caching/disk_cache.py\", line 16, in __init__\n    import diskcache as dc\nModuleNotFoundError: No module named 'diskcache'\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.72.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "jamesbraza",
      "author_type": "User",
      "created_at": "2025-06-06T23:49:04Z",
      "updated_at": "2025-06-10T21:54:12Z",
      "closed_at": "2025-06-10T21:54:12Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11503/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11503",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11503",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:31.113599",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "It's not a required dep, how would you suggest we handle this. We don't want to add bloat to the core SDK ",
          "created_at": "2025-06-07T00:52:40Z"
        },
        {
          "author": "jamesbraza",
          "body": "Thanks for asking, yeah the way to do it is:\n\n1. Add an optional extra such as `caching` (see how we do it [here](https://github.com/Future-House/aviary/blob/v0.19.0/pyproject.toml#L39-L94))\n2. Add instructions to `pip install litellm[caching]`\n3. Lazily import `diskcache` with `try`-`except` that p",
          "created_at": "2025-06-07T01:13:56Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "@jamesbraza is this what you wanted ? \n\nhttps://github.com/BerriAI/litellm/pull/11600 ",
          "created_at": "2025-06-10T21:04:20Z"
        },
        {
          "author": "jamesbraza",
          "body": "Yeah correct, looks good",
          "created_at": "2025-06-10T21:50:47Z"
        }
      ]
    },
    {
      "issue_number": 10793,
      "title": "[Bug]: context_window_fallbacks not triggered when using sglang due to unmapped error message",
      "body": "### What happened?\n\nIn sglang, when the context window is exceeded, the error message is:\n\n> The input ({input_token_num} tokens) is longer than the model's context length ({self.context_len} tokens).\n\nHowever, this message is not included in the exception mapping utils:\nhttps://github.com/BerriAI/litellm/blob/e0b49c3b0cbd631f2c57d2e4b5e0ecffd4268cee/litellm/litellm_core_utils/exception_mapping_utils.py#L286-L289\n\nAs a result, litellm does not raise a ContextWindowExceededError, which causes the context_window_fallbacks to not work as expected.\n\nIt might be a good idea to include the string \"is longer than the model's context length\" in the exception mapping.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.69.0\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "snachx",
      "author_type": "User",
      "created_at": "2025-05-13T10:41:04Z",
      "updated_at": "2025-06-10T20:06:58Z",
      "closed_at": "2025-06-10T20:06:58Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10793/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10793",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10793",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:31.305357",
      "comments": []
    },
    {
      "issue_number": 10313,
      "title": "[Bug]: gemini/gemma-3-27b-it function calling is not enabled exception",
      "body": "### What happened?\n\n[model json](https://github.com/BerriAI/litellm/blob/8ed3557ce77667569461ba28f4dea20bd44d35d9/model_prices_and_context_window.json#L5922) shows that `gemini/gemma-3-27b-it` supports function calling, however trying it out via the attached script reports the exception below. Using openai-agents v0.0.13 which I believe is using LiteLLM 1.67.2.\n\nrepro script:\n```\n#!/usr/bin/env -S uv run --script\n\n# /// script\n# requires-python = \">=3.13\"\n# dependencies = [\n#     \"openai-agents[litellm]\",\n# ]\n# ///\n\nimport asyncio\n\nfrom agents import Agent, Runner, function_tool, set_tracing_disabled\nfrom agents.extensions.models.litellm_model import LitellmModel\n\nset_tracing_disabled(disabled=True)\n\n@function_tool\ndef get_weather(city: str):\n    print(f\"[debug] getting weather for {city}\")\n    return f\"The weather in {city} is sunny.\"\n\n\nasync def main(model: str, api_key: str):\n    agent = Agent(\n        name=\"Assistant\",\n        instructions=\"You only respond in haikus.\",\n        model=LitellmModel(model=model, api_key=api_key),\n        tools=[get_weather],\n    )\n\n    result = await Runner.run(agent, \"What's the weather in Tokyo?\")\n    print(result.final_output)\n\n\nif __name__ == \"__main__\":\n    # First try to get model/api key from args\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model\", type=str, required=False)\n    parser.add_argument(\"--api-key\", type=str, required=False)\n    args = parser.parse_args()\n\n    model = args.model\n    if not model:\n        model = input(\"Enter a model name for Litellm: \")\n\n    api_key = args.api_key\n    if not api_key:\n        api_key = input(\"Enter an API key for Litellm: \")\n\n    asyncio.run(main(model, api_key))\n```\n\n### Relevant log output\n\n```shell\n‚ùØ ./litellm-tool-sample.py --api-key=\"...\" --model=gemini/gemma-3-27b-it\nInstalled 56 packages in 375ms\n\nProvider List: https://docs.litellm.ai/docs/providers\n\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nTraceback (most recent call last):\n  File \"/Users/shoda/.cache/uv/environments-v2/litellm-tool-sample-09def778d79087ab/lib/python3.13/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1285, in async_completion\n    response = await client.post(\n               ^^^^^^^^^^^^^^^^^^\n        api_base, headers=headers, json=cast(dict, request_body)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )  # type: ignore\n    ^\n  File \"/Users/shoda/.cache/uv/environments-v2/litellm-tool-sample-09def778d79087ab/lib/python3.13/site-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n    result = await func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/shoda/.cache/uv/environments-v2/litellm-tool-sample-09def778d79087ab/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 256, in post\n    raise e\n  File \"/Users/shoda/.cache/uv/environments-v2/litellm-tool-sample-09def778d79087ab/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 212, in post\n    response.raise_for_status()\n    ~~~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/Users/shoda/.cache/uv/environments-v2/litellm-tool-sample-09def778d79087ab/lib/python3.13/site-packages/httpx/_models.py\", line 829, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Client error '400 Bad Request' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemma-3-27b-it:generateContent?key=AIzaSyAIYQqE4rP2Vizu2GDND1qG8a-WaPwNlkw'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/shoda/.cache/uv/environments-v2/litellm-tool-sample-09def778d79087ab/lib/python3.13/site-packages/litellm/main.py\", line 477, in acompletion\n    response = await init_response\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/shoda/.cache/uv/environments-v2/litellm-tool-sample-09def778d79087ab/lib/python3.13/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1291, in async_completion\n    raise VertexAIError(\n    ...<3 lines>...\n    )\nlitellm.llms.vertex_ai.common_utils.VertexAIError: {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Function calling is not enabled for models/gemma-3-27b-it\",\n    \"status\": \"INVALID_ARGUMENT\"\n  }\n}\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/private/var/folders/ct/x2gct7yn2bxfqs891n8h1dxr0000gn/T/tmp.i86mChDqDW/./litellm-tool-sample.py\", line 52, in <module>\n    asyncio.run(main(model, api_key))\n    ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py\", line 195, in run\n    return runner.run(main)\n           ~~~~~~~~~~^^^^^^\n  File \"/usr/local/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n  File \"/usr/local/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py\", line 719, in run_until_complete\n    return future.result()\n           ~~~~~~~~~~~~~^^\n  File \"/private/var/folders/ct/x2gct7yn2bxfqs891n8h1dxr0000gn/T/tmp.i86mChDqDW/./litellm-tool-sample.py\", line 31, in main\n    result = await Runner.run(agent, \"What's the weather in Tokyo?\")\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/shoda/.cache/uv/environments-v2/litellm-tool-sample-09def778d79087ab/lib/python3.13/site-packages/agents/run.py\", line 218, in run\n    input_guardrail_results, turn_result = await asyncio.gather(\n                                           ^^^^^^^^^^^^^^^^^^^^^\n    ...<19 lines>...\n    )\n    ^\n  File \"/Users/shoda/.cache/uv/environments-v2/litellm-tool-sample-09def778d79087ab/lib/python3.13/site-packages/agents/run.py\", line 757, in _run_single_turn\n    new_response = await cls._get_new_response(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<10 lines>...\n    )\n    ^\n  File \"/Users/shoda/.cache/uv/environments-v2/litellm-tool-sample-09def778d79087ab/lib/python3.13/site-packages/agents/run.py\", line 916, in _get_new_response\n    new_response = await model.get_response(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<10 lines>...\n    )\n    ^\n  File \"/Users/shoda/.cache/uv/environments-v2/litellm-tool-sample-09def778d79087ab/lib/python3.13/site-packages/agents/extensions/models/litellm_model.py\", line 81, in get_response\n    response = await self._fetch_response(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<9 lines>...\n    )\n    ^\n  File \"/Users/shoda/.cache/uv/environments-v2/litellm-tool-sample-09def778d79087ab/lib/python3.13/site-packages/agents/extensions/models/litellm_model.py\", line 273, in _fetch_response\n    ret = await litellm.acompletion(\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<18 lines>...\n    )\n    ^\n  File \"/Users/shoda/.cache/uv/environments-v2/litellm-tool-sample-09def778d79087ab/lib/python3.13/site-packages/litellm/utils.py\", line 1460, in wrapper_async\n    raise e\n  File \"/Users/shoda/.cache/uv/environments-v2/litellm-tool-sample-09def778d79087ab/lib/python3.13/site-packages/litellm/utils.py\", line 1321, in wrapper_async\n    result = await original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/shoda/.cache/uv/environments-v2/litellm-tool-sample-09def778d79087ab/lib/python3.13/site-packages/litellm/main.py\", line 496, in acompletion\n    raise exception_type(\n          ~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<3 lines>...\n        extra_kwargs=kwargs,\n        ^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/shoda/.cache/uv/environments-v2/litellm-tool-sample-09def778d79087ab/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2214, in exception_type\n    raise e\n  File \"/Users/shoda/.cache/uv/environments-v2/litellm-tool-sample-09def778d79087ab/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1217, in exception_type\n    raise BadRequestError(\n    ...<11 lines>...\n    )\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: VertexAIException BadRequestError - {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Function calling is not enabled for models/gemma-3-27b-it\",\n    \"status\": \"INVALID_ARGUMENT\"\n  }\n}\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.67.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "theimpostor",
      "author_type": "User",
      "created_at": "2025-04-25T14:15:49Z",
      "updated_at": "2025-06-10T16:58:05Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10313/reactions",
        "total_count": 3,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10313",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10313",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:31.305379",
      "comments": [
        {
          "author": "nileshtrivedi",
          "body": "It should be possible to mimick tool-calling by adding preprocessing for the prompt and post-processing the response. Perhaps that's not in the scope of Litellm and should be built outside.",
          "created_at": "2025-06-10T07:00:41Z"
        },
        {
          "author": "zbloss",
          "body": "I am seeing the same issue. Gemma3 supports function calling but LiteLLM throws an error saying it does not.",
          "created_at": "2025-06-10T16:58:05Z"
        }
      ]
    },
    {
      "issue_number": 11465,
      "title": "[Bug]: Failed to export traces to otlp.arize.com: StatusCode.UNKNOWN - \"space_key is required\"",
      "body": "### What happened?\n\n**Description:**\nWe are experiencing an issue when exporting traces to `otlp.arize.com` using the OpenTelemetry OTLP gRPC exporter. The exporter fails with `StatusCode.UNKNOWN` and the following error message:\n\n```\nAll spans failed. Found 2 errors: rpc error: code = InvalidArgument desc = space_key is required; rpc error: code = InvalidArgument desc = space_key is required\n```\n\n**Relevant stack trace:**\n\n```\nFile \"/usr/lib/python3.13/site-packages/opentelemetry/exporter/otlp/proto/grpc/exporter.py\", line 262, in _export\n    self._client.Export(\n        request=self._translate_data(data),\n        metadata=self._headers,\n        timeout=self._timeout,\n    )\nFile \"/usr/lib/python3.13/site-packages/grpc/_channel.py\", line 1006, in _end_unary_response_blocking\n    raise _InactiveRpcError(state)\ngrpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\nstatus = StatusCode.UNKNOWN\ndetails = \"All spans failed. Found 2 errors: rpc error: code = InvalidArgument desc = space_key is required; rpc error: code = InvalidArgument desc = space_key is required\"\ndebug_error_string = \"UNKNOWN:Error received from peer {...}\"\n```\n\n**Steps to Reproduce:**\n\n1. Set up OpenTelemetry exporter with default or custom configuration.\n2. Attempt to export traces to `otlp.arize.com`.\n3. Observe the error above in the logs.\n\n**Additional Information:**\n\n* Python version: 3.13\n* OpenTelemetry exporter version: \\[please specify]\n* Any relevant configuration/environment details\n\n**Expected behavior:**\nTraces should be exported successfully to `otlp.arize.com` without the `space_key is required` error.\n\n**Actual behavior:**\nAll spans fail to export with `InvalidArgument` and `space_key is required`.\n\n\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nlatest\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "mkagit",
      "author_type": "User",
      "created_at": "2025-06-06T00:53:59Z",
      "updated_at": "2025-06-10T16:49:19Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11465/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11465",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11465",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:31.478710",
      "comments": [
        {
          "author": "vanities",
          "body": "Same error on my side..\n\nspecifically with lightllm docker:\n\n```\nlitellm-proxy  | Failed to export traces to otlp.arize.com, error code: StatusCode.UNKNOWN\nlitellm-proxy  | Traceback (most recent call last):\nlitellm-proxy  |   File \"/usr/lib/python3.13/site-packages/opentelemetry/exporter/otlp/proto",
          "created_at": "2025-06-06T20:34:10Z"
        },
        {
          "author": "vanities",
          "body": "@mkagit btw, this change fixes it for me: https://github.com/BerriAI/litellm/pull/11595",
          "created_at": "2025-06-10T16:49:19Z"
        }
      ]
    },
    {
      "issue_number": 11002,
      "title": "[Bug]: max_token is ignored when invoking a Fireworks AI model from the LiteLLM Proxy Server",
      "body": "### What happened?\n\nMy OpenAI compatible client is sending requests to a LiteLLM proxy server, and regardless of the specified `max_token`, the output is truncated. Finish reason is \"stop\".\nI think you are not mapping correctly the max_token to the body expected by Fireworks API. Why am I saying this? I tested it directly on FireWorks and the result is different, output is longer.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nghcr.io/berriai/litellm:main-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "AlessandroMondin",
      "author_type": "User",
      "created_at": "2025-05-21T10:53:52Z",
      "updated_at": "2025-06-10T16:15:19Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11002/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11002",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11002",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:31.646869",
      "comments": [
        {
          "author": "ariesta-id",
          "body": "Maybe related. I use aider (https://github.com/Aider-AI/aider) and got error like this. Aider uses litellm\n\n```\nModel fireworks_ai/accounts/fireworks/models/deepseek-r1-0528 has hit a token limit!\nToken counts below are approximate.\n\nInput tokens: ~3,025 of 160,000\nOutput tokens: ~2,041 of 160,000\nT",
          "created_at": "2025-06-10T15:58:05Z"
        },
        {
          "author": "ariesta-id",
          "body": "> Maybe related. I use aider (https://github.com/Aider-AI/aider) and got error like this. Aider uses litellm\n> \n> ```\n> Model fireworks_ai/accounts/fireworks/models/deepseek-r1-0528 has hit a token limit!\n> Token counts below are approximate.\n> \n> Input tokens: ~3,025 of 160,000\n> Output tokens: ~2,",
          "created_at": "2025-06-10T16:15:19Z"
        }
      ]
    },
    {
      "issue_number": 11531,
      "title": "[Bug]: /ui is not accessible",
      "body": "### What happened?\n\nThis happened after I pulled the latest image. I failed to fix it with my existing containers so here's what I did:\n\n1. Verify LiteLLM is still available as an API ‚Üí ‚úÖ \n2. Listed models via `/v1/models` and saved the response as a backup\n3. Removed the running container and deleted the image: `docker stop litellm && docker rm litellm && docker image rm ghcr.io/berriai/litellm:main-latest`\n4. Recreated the container: `docker compose up -d`\n5. Disabled pi-hole on the network just in case\n\nThe DB seems to work fine, but even after this the UI remains inaccessible and shows the log output below and the following in the browser:\n\n<img width=\"502\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/70c6d99f-9d93-4c1f-9a9b-8cfec0450415\" />\n\n---\n\n**My `compose.yaml`:**\n```yaml\nservices:\n  litellm:\n    image: ghcr.io/berriai/litellm:main-latest\n    container_name: litellm\n    env_file: litellm.env\n    volumes:\n      - ./data/config.yaml:/app/config.yaml\n    ports:\n      - 4000:4000\n    command: ['--config=/app/config.yaml']\n    restart: unless-stopped\n\n  litellm_db:\n    image: postgres\n    container_name: litellm_db\n    env_file: litellm.env\n    ports:\n      - 5432:5432\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -d litellm -U llmproxy\"]\n      interval: 1s\n      timeout: 5s\n      retries: 10\n    restart: unless-stopped\n```\n\n**My `litellm.env` file:**\n\n```env\nDATABASE_URL=\"postgresql://llmproxy:*******@litellm_db:5432/litellm\"\nSTORE_MODEL_IN_DB=True\nMASTER_KEY=\"*******\"\n\nPOSTGRES_DB=litellm\nPOSTGRES_USER=llmproxy\nPOSTGRES_PASSWORD=*******\n\n# Ollama\nOLLAMA_API_BASE=*******\nOLLAMA_API_KEY=\"\"\n\nMISTRAL_API_KEY=*******\n\nLANGFUSE_PUBLIC_KEY=pk-*******\nLANGFUSE_SECRET_KEY=sk-*******\nLANGFUSE_HOST=*******\n```\n\n**My LiteLLM config:** \n```yaml\ngeneral_settings:\n  master_key: os.environ/MASTER_KEY\n  pass_through_endpoints:\n    - path: \"/mistral/v1/ocr\"\n      target: \"https://api.mistral.ai/v1/ocr\"\n      headers:\n        Authorization: \"bearer os.environ/MISTRAL_API_KEY\"\n        content-type: application/json\n        accept: application/json\n      forward_headers: True\n\nmodel_list: []\n```\n\n### Relevant log output\n\n```shell\nlitellm  | INFO:     Shutting down\nlitellm  | INFO:     Waiting for application shutdown.\nlitellm  | INFO:     Application shutdown complete.\nlitellm  | INFO:     Finished server process [1]\nlitellm  | prisma:warn Prisma doesn't know which engines to download for the Linux distro \"wolfi\". Falling back to Prisma engines built \"debian\".\nlitellm  | Please report your experience by creating an issue at https://github.com/prisma/prisma/issues so we can add your distro to the list of known supported distros.\nlitellm  | Prisma schema loaded from schema.prisma\nlitellm  | Datasource \"client\": PostgreSQL database \"litellm\", schema \"public\" at \"litellm_db:5432\"\nlitellm  |\nlitellm  | The database is already in sync with the Prisma schema.\nlitellm  |\nRunning generate... - Prisma Client Python (v0.11.0)\nlitellm  |\nlitellm  | Some types are disabled by default due to being incompatible with Mypy, it is highly recommended\nlitellm  | to use Pyright instead and configure Prisma Python to use recursive types. To re-enable certain types:\nlitellm  |\nlitellm  | generator client {\nlitellm  |   provider             = \"prisma-client-py\"\nlitellm  |   recursive_type_depth = -1\nlitellm  | }\nlitellm  |\nlitellm  | If you need to use Mypy, you can also disable this message by explicitly setting the default value:\nlitellm  |\nlitellm  | generator client {\nlitellm  |   provider             = \"prisma-client-py\"\nlitellm  |   recursive_type_depth = 5\nlitellm  | }\nlitellm  |\nlitellm  | For more information see: https://prisma-client-py.readthedocs.io/en/stable/reference/limitations/#default-type-limitations\nlitellm  |\n‚úî Generated Prisma Client Python (v0.11.0) to ./../../prisma in 388ms\nlitellm  |\nlitellm  | INFO:     Started server process [1]\nlitellm  | INFO:     Waiting for application startup.\nlitellm  | INFO:     Application startup complete.\nlitellm  | INFO:     Uvicorn running on http://0.0.0.0:4000 (Press CTRL+C to quit)\nlitellm  |\nlitellm  | #------------------------------------------------------------#\nlitellm  | #                                                            #\nlitellm  | #           'It would help me if you could add...'            #\nlitellm  | #        https://github.com/BerriAI/litellm/issues/new        #\nlitellm  | #                                                            #\nlitellm  | #------------------------------------------------------------#\nlitellm  |\nlitellm  |  Thank you for using LiteLLM! - Krrish & Ishaan\nlitellm  |\nlitellm  |\nlitellm  |\nlitellm  | Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nlitellm  |\nlitellm  |\nlitellm  | INFO:     192.168.1.x:58284 - \"GET /ui/ HTTP/1.1\" 200 OK\nlitellm  | INFO:     192.168.1.x:58284 - \"GET /litellm2/_next/static/chunks/webpack-a426aae3231a8df1.js HTTP/1.1\" 404 Not Found\nlitellm  | INFO:     192.168.1.x:58285 - \"GET /litellm2/_next/static/chunks/fd9d1056-205af899b895cbac.js HTTP/1.1\" 404 Not Found\nlitellm  | INFO:     192.168.1.x:58284 - \"GET /litellm2/_next/static/chunks/117-c4922b1dd81b62ce.js HTTP/1.1\" 404 Not Found\nlitellm  | INFO:     192.168.1.x:58286 - \"GET /litellm2/_next/static/chunks/main-app-4f7318ae681a6d94.js HTTP/1.1\" 404 Not Found\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nalways the latest main-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "thibmaek",
      "author_type": "User",
      "created_at": "2025-06-08T09:08:05Z",
      "updated_at": "2025-06-10T15:16:03Z",
      "closed_at": "2025-06-09T08:37:07Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 19,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11531/reactions",
        "total_count": 10,
        "+1": 8,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 2
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11531",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11531",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:31.815630",
      "comments": [
        {
          "author": "thibmaek",
          "body": "I find it weird that it's trying to look to the `/litellm2/...` directory for assets and after opening a shell in the container I can confirm the assets it's looking for are available in `/app/ui/litellm-dashboard/out/_next/static/chunks`",
          "created_at": "2025-06-08T09:08:52Z"
        },
        {
          "author": "AdithyanI",
          "body": "Ah, someone created this! Thank you. I was going crazy thinking that I made a mistake and doing different things to bring it back up. Looks like it's a bug.  \n\n+1 to @thibmaek . I can't access the UI pane. I did some analysis and debugged and found the exact lines exactly as Thib wrote here: https:/",
          "created_at": "2025-06-08T10:34:35Z"
        },
        {
          "author": "JerryAlbeg",
          "body": "can confirm that the same issue stated for me after pulling the latest docker\nsame 4 error lines appear in logs when accessing /ui\nthe system works, just can't access the admin panels.",
          "created_at": "2025-06-08T12:01:36Z"
        },
        {
          "author": "frenzybiscuit",
          "body": "I cannot access /ui on the latest docker...",
          "created_at": "2025-06-08T16:00:39Z"
        },
        {
          "author": "nmcbride",
          "body": "Same issue here.",
          "created_at": "2025-06-09T01:57:08Z"
        }
      ]
    },
    {
      "issue_number": 11591,
      "title": "[Bug]: SSL verification issues with Azure AI client starting from litellm 1.69",
      "body": "### What happened?\n\nSince updating to 1.72.x , I've encountered SSL errors when connecting to Azure AI models. (Before 1.69 everything worked fine)\nAttempts to bypass verification using:\n\n```\nlitellm.client_session = httpx.Client(verify=False)  \nlitellm.aclient_session = httpx.AsyncClient(verify=False)\n```\nsolves the problem for azure openai models. But doesn't work for Mistral and Deepseek Async client (Sync client works perfectly)\n\n\nError:\n```\nSSLCertificateVerificationError: unable to get local issuer certificate\n```\nAdding `litellm.ssl_verify = False `leads to a new error regarding invalid input format:\n\nError:\n```\nBadRequestError: invalid input error - Input should be a valid dictionary/object, received string.\n```\n\n\nThe problem seems to happen to other models than OpenAI's Azure models\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.72?2\n\n",
      "state": "open",
      "author": "AmineDjeghri",
      "author_type": "User",
      "created_at": "2025-06-10T15:01:54Z",
      "updated_at": "2025-06-10T15:06:28Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11591/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11591",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11591",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:32.028998",
      "comments": []
    },
    {
      "issue_number": 10204,
      "title": "[Bug]: o4-mini models fails to add in dashboard",
      "body": "### What happened?\n\nError occurred while generating model response. Please try again. Error: Error: 400 litellm.BadRequestError: AzureException - Model {modelName} is enabled only for api versions 2024-12-01-preview and later. Received Model Group=azure/o4-mini Available Model Group Fallbacks=None\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.67.0-nightly\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "saitharunsai",
      "author_type": "User",
      "created_at": "2025-04-22T16:25:37Z",
      "updated_at": "2025-06-10T12:52:52Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10204/reactions",
        "total_count": 4,
        "+1": 4,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10204",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10204",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:32.029012",
      "comments": [
        {
          "author": "AdithyanI",
          "body": "Same error for me too. The other models from Azure (like gpt-4.1) work though. ",
          "created_at": "2025-04-30T09:12:04Z"
        },
        {
          "author": "bsormagec",
          "body": "any update ? ",
          "created_at": "2025-06-06T14:58:28Z"
        },
        {
          "author": "benwallacestock",
          "body": "Any progress on this?",
          "created_at": "2025-06-10T12:14:00Z"
        },
        {
          "author": "bsormagec",
          "body": "I fixed the problem by using it with a different prefix, using a prefix like \"**{provider}/o_series/o4-mini**\".\nTry that I hope it helps @benwallacestock ",
          "created_at": "2025-06-10T12:52:36Z"
        }
      ]
    },
    {
      "issue_number": 11407,
      "title": "[Bug]: Missing ID in tool_calls under stream mode",
      "body": "### What happened?\n\nError happened in OpenAI-Compatible Endpoints.\nOnly the first message contains id in tool_calls node when \"stream\": true, expected to also have id in later messages.\nSee the following logs, only the first message contains \"id\":\"tooluse_BawIVFHuQ2uaPLDBdWlblA\".\ntool_calls is an array, later elements missing id will lead to inaccuracy.\n\n### Relevant log output\n\n```shell\ndata: {\"id\":\"chatcmpl-ab2d6b3d-ff93-4e58-a528-ef9c729e73a5\",\"created\":1749032296,\"model\":\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"\",\"role\":\"assistant\",\"tool_calls\":[{\"id\":\"tooluse_BawIVFHuQ2uaPLDBdWlblA\",\"function\":{\"arguments\":\"\",\"name\":\"jira_get_worklog\"},\"type\":\"function\",\"index\":1}]}}],\"provider_specific_fields\":{}}\n\ndata: {\"id\":\"chatcmpl-ab2d6b3d-ff93-4e58-a528-ef9c729e73a5\",\"created\":1749032296,\"model\":\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"\",\"role\":\"assistant\",\"tool_calls\":[{\"function\":{\"arguments\":\"\"},\"type\":\"function\",\"index\":1}]}}],\"provider_specific_fields\":{}}\n\ndata: {\"id\":\"chatcmpl-ab2d6b3d-ff93-4e58-a528-ef9c729e73a5\",\"created\":1749032297,\"model\":\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"\",\"role\":\"assistant\",\"tool_calls\":[{\"function\":{\"arguments\":\"{\\\"is\"},\"type\":\"function\",\"index\":1}]}}],\"provider_specific_fields\":{}}\n\ndata: {\"id\":\"chatcmpl-ab2d6b3d-ff93-4e58-a528-ef9c729e73a5\",\"created\":1749032297,\"model\":\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"\",\"role\":\"assistant\",\"tool_calls\":[{\"function\":{\"arguments\":\"sue_k\"},\"type\":\"function\",\"index\":1}]}}],\"provider_specific_fields\":{}}\n\ndata: {\"id\":\"chatcmpl-ab2d6b3d-ff93-4e58-a528-ef9c729e73a5\",\"created\":1749032297,\"model\":\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"\",\"role\":\"assistant\",\"tool_calls\":[{\"function\":{\"arguments\":\"ey\\\": \\\"\"},\"type\":\"function\",\"index\":1}]}}],\"provider_specific_fields\":{}}\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.71.1-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "langpingxue",
      "author_type": "User",
      "created_at": "2025-06-04T15:59:20Z",
      "updated_at": "2025-06-10T09:26:16Z",
      "closed_at": "2025-06-10T09:26:16Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11407/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11407",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11407",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:32.226041",
      "comments": []
    },
    {
      "issue_number": 11579,
      "title": "[Bug]:  person invited to the team does not belong to the team's organization",
      "body": "### What happened?\n\nI created a department A under organization B in the Admin UI. However, after adding a user to department A, that user does not belong to organization B. The corresponding organization_id in the \"LiteLLM_UserTable\" is also empty.\n\nAdditionally, when I assign a user as an org_admin, that user is unable to manage the organization and its teams after logging into the Admin UI.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.71.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "neuSnail",
      "author_type": "User",
      "created_at": "2025-06-10T08:36:12Z",
      "updated_at": "2025-06-10T08:36:20Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11579/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11579",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11579",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:32.226060",
      "comments": []
    },
    {
      "issue_number": 11019,
      "title": "[Bug]: AWS Sagemaker embedding calls are failing with a Jina endpoint",
      "body": "### What happened?\n\nHi !\n\nI'm trying to call a SageMaker endpoint for embeddings using the following:\n```py\nlitellm.embedding(model=\"sagemaker/<my_jina_endpoint>\", input=[\"hello world !\"])\n```\n\nHowever, I get the following exception:\n```\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: Error code: 400 - {'error': {'message': 'litellm.BadRequestError: SagemakerException - Received server error (500) from primary with message \"Internal Server Error\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/<my_endpoint> in account <account_id> for more information.. Received Model Group=sagemaker/<my_endpoint>\\nAvailable Model Group Fallbacks=None', 'type': None, 'param': None, 'code': '400'}}\n```\nWhen checking in CloudWatch, there's the traceback you can see in the log output below\n\nI thought I could fix it by just wrapping everything in a `data` for my input:\n```py\nlitellm.embedding(model=\"sagemaker/<my_jina_endpoint>\", input={\"data\": {\"text\": \"hello world !\"}})\n```\nBut the issue still occurs, and I believe this is due to how you make the call behind, looking at the debug log:\n```py\n18:32:53 - LiteLLM:DEBUG: litellm_logging.py:901 -\nRequest Sent from LiteLLM:\n\n        response = client.invoke_endpoint(\n            EndpointName=<my_jina_endpoint>,\n            ContentType=\"application/json\",\n            Body=f\"b'{\"text_inputs\": {\"data\": {\"text\": \"hello world !\"}}}'\",  # Use !r for safe representation\n            CustomAttributes=\"accept_eula=true\",\n        )\n```\nI believe the body should be wrapped with `data` here instead of being in `text_inputs` (there's probably more to that).\n\nIt looks like it's quite an edge-case (I haven't tried on any other SageMaker endpoint / models) and pretty much Jina specific, so I'm not sure how or if you should proceed with this issue (maybe a way to handle the body ourselves?) ü§î \n\nFeel free to ask if you need any more information :)\n\n### Relevant log output\n\nCloudWatch output:\n```shell\nERROR:    Exception in ASGI application\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 426, in run_asgi\n    result = await app(  # type: ignore[func-returns-value]\n  File \"/opt/conda/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\n    return await self.app(scope, receive, send)\n  File \"/opt/conda/lib/python3.10/site-packages/fastapi/applications.py\", line 1054, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/opt/conda/lib/python3.10/site-packages/starlette/applications.py\", line 113, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/opt/conda/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 187, in __call__\n    raise exc\n  File \"/opt/conda/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    await self.app(scope, receive, _send)\n  File \"/opt/conda/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/opt/conda/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/opt/conda/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/opt/conda/lib/python3.10/site-packages/starlette/routing.py\", line 715, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/opt/conda/lib/python3.10/site-packages/starlette/routing.py\", line 735, in app\n    await route.handle(scope, receive, send)\n  File \"/opt/conda/lib/python3.10/site-packages/starlette/routing.py\", line 288, in handle\n    await self.app(scope, receive, send)\n  File \"/opt/conda/lib/python3.10/site-packages/starlette/routing.py\", line 76, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"/opt/conda/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/opt/conda/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/opt/conda/lib/python3.10/site-packages/starlette/routing.py\", line 73, in app\n    response = await f(request)\n  File \"/opt/conda/lib/python3.10/site-packages/fastapi/routing.py\", line 301, in app\n    raw_response = await run_endpoint_function(\n  File \"/opt/conda/lib/python3.10/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n    return await dependant.call(**values)\n  File \"/opt/conda/lib/python3.10/site-packages/jina/serve/runtimes/worker/http_csp_app.py\", line 143, in post\n    return await process(input_model(**json_body))\n  File \"pydantic/main.py\", line 341, in pydantic.main.BaseModel.__init__\n\npydantic.error_wrappers.ValidationError: 1 validation error for invocations_input_model\ndata\n  field required (type=value_error.missing)\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.70.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "keyzou",
      "author_type": "User",
      "created_at": "2025-05-21T17:21:13Z",
      "updated_at": "2025-06-10T08:05:20Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11019/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11019",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11019",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:32.226065",
      "comments": [
        {
          "author": "Jacobh2",
          "body": "Having a similar issue: Basically the model that I've deployed to a Sagemaker endpoint requires that the input has the key `inputs`. \n\nIt would be amazing if this key could be modified on a per-model basis.\n\nA suggestion for the CLI is to update it to support something like this:\n\n<img width=\"828\" a",
          "created_at": "2025-06-04T16:43:47Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Hey @Jacobh2 as long as it works on the sdk, it will work on the proxy (proxy uses the `litellm.aembedding(..)` function). \n\nIs there a documented spec for the jina endpoint on sagemaker? ",
          "created_at": "2025-06-04T17:08:38Z"
        },
        {
          "author": "keyzou",
          "body": "Hi @krrishdholakia, I believe there's some specs on the [AWS Marketplace page,](https://aws.amazon.com/marketplace/pp/prodview-bfbctuqmky676?sr=0-3&ref_=beagle&applicationId=AWSMPContessa) or some on their github: https://github.com/jina-ai/jina-sagemaker/blob/main/examples/sample-clip-v2-inference-",
          "created_at": "2025-06-10T08:05:18Z"
        }
      ]
    },
    {
      "issue_number": 11574,
      "title": "[Bug]: Caching completely broken with cache_control parameter when using PromptCachingDeploymentCheck",
      "body": "### What happened?\n\n1. We use LiteLLM's Proxy [PromptCachingDeploymentCheck](https://github.com/BerriAI/litellm/blob/7f44d1a2f81b60f1c3ce0820da4137f10c341798/litellm/router_utils/pre_call_checks/prompt_caching_deployment_check.py#L19) to route requests from the same conversation to the same provider, with the goal of increasing cache hits.\n2. That routing step checks the length of the messages since anything under 1024 tokens won't be cached.\n3. However, LiteLLM cannot count the length of cacheable messages. [They cause a ValueError](https://github.com/BerriAI/litellm/blob/7f44d1a2f81b60f1c3ce0820da4137f10c341798/litellm/litellm_core_utils/token_counter.py#L465), which is [interpreted as being uncacheable](https://github.com/BerriAI/litellm/blob/7f44d1a2f81b60f1c3ce0820da4137f10c341798/litellm/utils.py#L6973-L6975).\n\n\nAn error we get:\n\n```\n  \"message\": \"Error in is_prompt_caching_valid_prompt: Unsupported type <class 'dict'> for key cache_control in message {'content': 'You are presented with the following task:\n\n...our truncated message...\n\ntools.\\\\n', 'role': 'user', 'cache_control': {'type': 'ephemeral'}}\",\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.2.rc\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "hnykda",
      "author_type": "User",
      "created_at": "2025-06-10T06:55:25Z",
      "updated_at": "2025-06-10T06:55:40Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11574/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11574",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11574",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:32.412358",
      "comments": []
    },
    {
      "issue_number": 10761,
      "title": "[Bug]: Custom server root path not working properly",
      "body": "### What happened?\n\nWe've tried to set a custom server root path and is not working. The custom server root path is `/litellm` and UI is set to. `/litellm/ui`, but when accessing the LiteLLM UI, we got redirected to `/sso/key/generate`, when it should be `/litellm/sso/key/generate`\n\nWe have the LiteLLM deployment running behind an NGINX proxy that is forwarding the requests matching `/litellm/*`, so the `/sso/key/generate` doesn't match and a 404 error is returned\n\n**Dockerfile**\n\n```Dockerfile\n# Use the provided base image\nFROM ghcr.io/berriai/litellm:main-v1.68.0-stable\n\n################################################################################\n# Configure custom server root path\n################################################################################\n\n# https://docs.litellm.ai/docs/proxy/deploy#1-custom-server-root-path-proxy-base-url\n# https://github.com/CartoDB/litellm/blob/v1.68.0-stable/docker/Dockerfile.custom_ui\n\n# Set the working directory to /app\nWORKDIR /app\n\n# Install Node.js and npm (adjust version as needed)\n# FIXME: With the latest nodejs version, the build never ends\nRUN apk add --no-cache nodejs=20.18.3-r0 npm\n\n# Copy the UI source into the container\nCOPY ./ui/litellm-dashboard /app/ui/litellm-dashboard\n\n# Set an environment variable for UI_BASE_PATH\n# This can be overridden at build time\n# set UI_BASE_PATH to \"<your server root path>/ui\"\n# üëáüëá Enter your UI_BASE_PATH here\nENV UI_BASE_PATH=\"/litellm/ui\"\n\n# Build the UI with the specified UI_BASE_PATH\nWORKDIR /app/ui/litellm-dashboard\nRUN npm install\nRUN UI_BASE_PATH=${UI_BASE_PATH} npm run build\n\n# Create the destination directory\nRUN mkdir -p /app/litellm/proxy/_experimental/out\n\n# # Move the built files to the appropriate location\n# # Assuming the build output is in ./out directory\n# RUN rm -rf /app/litellm/proxy/_experimental/out/* && \\\n#     mv ./out/* /app/litellm/proxy/_experimental/out/\n\n# Move the built files to the appropriate location\n# Assuming the build output is in ./out directory\nRUN rm -rf /usr/lib/python3.13/site-packages/litellm/proxy/_experimental/out/* && \\\n    mv -f ./out/* /usr/lib/python3.13/site-packages/litellm/proxy/_experimental/out/\n\n################################################################################\n\n################################################################################\n# Allow running with non-root user\n################################################################################\n\n# https://github.com/CartoDB/litellm/blob/v1.68.0-stable/docker/Dockerfile.non_root\n\n# Switch back to the main app directory\nWORKDIR /app\n\n# Create the litellm user (non-root user)\nRUN adduser -D litellm\n\n# Prisma allows you to specify the binary cache directory to use\nENV PRISMA_BINARY_CACHE_DIR=/nonexistent\n\nRUN pip install --no-cache-dir nodejs-bin prisma\n\n# Make a /non-existent folder and assign chown to nobody\nRUN mkdir -p /nonexistent && \\\n    chown -R litellm:litellm /app && \\\n    chown -R litellm:litellm /nonexistent && \\\n    chown -R litellm:litellm /usr/lib/python3.13/site-packages/prisma/\n\nRUN chmod +x docker/entrypoint.sh\nRUN chmod +x docker/prod_entrypoint.sh\n\n# Run Prisma generate as user = litellm\nUSER litellm\n\nRUN prisma generate\n\n################################################################################\n```\n\n**Kubernetes**\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: litellm-config-file\ndata:\n  config.yaml: |\n    litellm_settings:\n      json_logs: true\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: litellm-deployment\n  labels:\n    app: litellm\nspec:\n  selector:\n    matchLabels:\n      app: litellm\n  template:\n    metadata:\n      labels:\n        app: litellm\n    spec:\n      containers:\n      - name: litellm\n        # Custom image with non-root user\n        image: gcr.io/carto-artifacts/litellm:v1.68.0-stable-carto\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 4000\n        volumeMounts:\n        - name: config-volume\n          mountPath: /app/proxy_server_config.yaml\n          subPath: config.yaml\n        - name: litellm-migration-dir\n          mountPath: /tmp/prisma/migrations\n        resources:\n          requests:\n            cpu: 1000m\n            memory: 1024Mi\n          limits:\n            cpu: 2000m\n            memory: 2048Mi\n        env:\n          - name: STORE_MODEL_IN_DB\n            value: \"True\"\n          - name: LITELLM_MASTER_KEY\n            value: sk-xxxxxxxxxxxxxxxxxx\n          - name: LITELLM_SALT_KEY\n            value: sk-xxxxxxxxxxxxxxxxxxxxxx\n          # FIXME: The LiteLLM database and user has been manually created\n          - name: DATABASE_URL\n            value: postgresql://litellm:litellm@10.147.0.3:5432/litellm?sslmode=require\n          # Required for read-only filesystem\n          - name: LITELLM_MIGRATION_DIR\n            value: /tmp/prisma/migrations\n          - name: USE_PRISMA_MIGRATE\n            value: \"True\"\n          - name: SERVER_ROOT_PATH\n            value: /litellm\n          - name: HTTP_PROXY\n            value: http://proxy:3128\n          - name: HTTPS_PROXY\n            value: http://proxy:3128\n          - name: GRPC_PROXY\n            value: http://proxy:3128\n          - name: NO_PROXY\n            value: localhost,127.0.0.1\n        securityContext:\n          runAsUser: 1000\n          runAsGroup: 1000\n          runAsNonRoot: true\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n              - ALL\n              - NET_RAW\n      imagePullSecrets:\n        - name: github-actions-artifacts\n      volumes:\n        - name: config-volume\n          configMap:\n            name: litellm-config-file\n        # Required for read-only filesystem\n        - name: litellm-migration-dir\n          emptyDir: {}\n      securityContext:\n        supplementalGroups: [101]\n        fsGroup: 1000\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: litellm\nspec:\n  selector:\n    app: litellm\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 4000\n  type: ClusterIP\n```\n\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.68.0-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "miguelangelmorenochacon",
      "author_type": "User",
      "created_at": "2025-05-12T13:46:17Z",
      "updated_at": "2025-06-10T06:17:15Z",
      "closed_at": "2025-05-27T05:00:49Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 11,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10761/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "krrishdholakia"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10761",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10761",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:32.412380",
      "comments": [
        {
          "author": "lookitsatravis",
          "body": "Also running into this issue as we are evaluating this app. We are considering an enterprise license @krrishdholakia, but moving forward will require a custom path to work.\n\nThe codebase has many instances of code like this:\n\n```ts\nconst proxyBaseUrl = isLocal ? \"http://localhost:4000\" : null;\n```\n\n",
          "created_at": "2025-05-21T22:39:02Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "Hi @lookitsatravis ack on this. Are you running into this issue on the UI",
          "created_at": "2025-05-22T16:29:25Z"
        },
        {
          "author": "thedevd",
          "body": "I just hit this issue with latest litellm, and blocked \nlitellm[proxy]==1.70.2\n\n```\n((.py12env) ) ‚ûú  export SERVER_ROOT_PATH=\"/api/v1\"\n((.py12env) ) ‚ûú  litellm                          \nserver_root_path is set, forwarding any /ui requests to /api/v1/ui\nINFO:     Started server process [13773]\nINFO: ",
          "created_at": "2025-05-22T19:43:47Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "cc @wagnerjt high priority issue ^ ",
          "created_at": "2025-05-23T01:15:56Z"
        },
        {
          "author": "aleccai8",
          "body": "https://docs.litellm.ai/docs/proxy/deploy#1-custom-server-root-path-proxy-base-url \ndocs shows replace new out dir into /app/litellm/proxy/_experimental/out/ , but in container /usr/lib/python3.13/site-packages/litellm been used, doc mistake or ?",
          "created_at": "2025-05-23T11:01:47Z"
        }
      ]
    },
    {
      "issue_number": 11549,
      "title": "[Bug]: Vertex AI gemini-2.5-pro-preview returns an empty string response in v1.72.2",
      "body": "### What happened?\n\nThis was working correctly in v1.72.1. After upgrading to v1.72.2, all responses from the Vertex AI provider have an empty string as content.\n\nConfig:\n```\n- model_name: google/gemini-2.5-pro-preview\n    litellm_params:\n      vertex_credentials: \"xxx.json\"\n      vertex_location: \"global\"\n      model: vertex_ai/gemini-2.5-pro-preview-06-05\n```\n\n### Relevant log output\n\n```shell\n{\n  \"id\": \"OZZGaMCxN-vJ4_UPmomq2QM\",\n  \"model\": \"gemini-2.5-pro-preview-06-05\",\n  \"usage\": {\n    \"total_tokens\": 42,\n    \"prompt_tokens\": 42,\n    \"completion_tokens\": 0,\n    \"prompt_tokens_details\": {\n      \"audio_tokens\": null,\n      \"cached_tokens\": null\n    },\n    \"completion_tokens_details\": {\n      \"audio_tokens\": null,\n      \"reasoning_tokens\": 0,\n      \"accepted_prediction_tokens\": null,\n      \"rejected_prediction_tokens\": null\n    }\n  },\n  \"object\": \"chat.completion\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"\",\n        \"tool_calls\": null,\n        \"function_call\": null\n      },\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"created\": 1749456442,\n  \"system_fingerprint\": null\n}\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "yorha-l",
      "author_type": "User",
      "created_at": "2025-06-09T09:05:19Z",
      "updated_at": "2025-06-10T05:04:46Z",
      "closed_at": "2025-06-10T05:04:46Z",
      "labels": [
        "bug",
        "high priority"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11549/reactions",
        "total_count": 4,
        "+1": 4,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "krrishdholakia"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11549",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11549",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:32.634377",
      "comments": [
        {
          "author": "jake-ch",
          "body": "I faced the same issue: Updated litellm v1.71.1 -> v1.72.2, and Vertex AI responses have an empty string.",
          "created_at": "2025-06-09T13:49:53Z"
        },
        {
          "author": "emerzon",
          "body": "Issue seems specific to `gemini-2.5-pro` models for streaming mode.\n\n````\n** 2.5 Streaming: **: NOK:\n(base) egomes@jokkmokk:~$ curl https://llm/v1/chat/completions --header \"Authorization: Bearer sk-xxx\" --header 'Content-Type: application/json' -d '{\"model\": \"gemini-2.5-pro-preview-06-05\", \"message",
          "created_at": "2025-06-09T16:44:19Z"
        },
        {
          "author": "Classic298",
          "body": "can reproduce. \n**I ONLY get empty responses** for all gemini models (flash, pro).\n\nHad to downgrade to 1.72.0",
          "created_at": "2025-06-09T18:38:36Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Thanks for the issue. investigating now. ",
          "created_at": "2025-06-10T00:44:39Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Not seeing this on Google AI Studio - despite them both using the same calling logic \n\nInvestigating vertex ai now ",
          "created_at": "2025-06-10T00:55:51Z"
        }
      ]
    },
    {
      "issue_number": 4528,
      "title": "[Feature]: Turn off or tune down noisy library logging",
      "body": "### The Feature\n\nLiteLLM is so noisy! It clogs up our logs, super hard to debug the app as a whole. We don't use the proxy, just the library, and have found no way to get it to quieten down. Its default logging level is way too verbose, and setting logging level to WARN or ERROR does nothing.\n\n### Motivation, pitch\n\nMakes it hard to see what's going on in our app outside LiteLLM\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "AshSourceTable",
      "author_type": "User",
      "created_at": "2024-07-03T20:11:07Z",
      "updated_at": "2025-06-10T04:35:54Z",
      "closed_at": "2024-07-08T17:21:49Z",
      "labels": [
        "enhancement",
        "under review"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/4528/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/4528",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/4528",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:32.867012",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "hi @AshSourceTable sorry for this can you show me\r\n\r\n- what litellm version are you on? do you see noisy logging on latest version of litellm \r\n- are you switching on debugging / `litellm.set_verbose=True` ? \r\n- what noisy logs are you seeing ? Can I see an example ",
          "created_at": "2024-07-04T01:54:57Z"
        },
        {
          "author": "krrishdholakia",
          "body": "bump on this? @AshSourceTable \r\n\r\nWe run the library in prod, and without `LITELLM_DEBUG` our logs are pretty empty \r\n<img width=\"830\" alt=\"Screenshot 2024-07-04 at 4 33 27 PM\" src=\"https://github.com/BerriAI/litellm/assets/17561003/609452d5-82d0-4636-bb56-34311b447f3c\">\r\n",
          "created_at": "2024-07-04T23:33:32Z"
        },
        {
          "author": "jokulamoko",
          "body": "Use:\n```\nimport logging\n\nlitellm_logger = logging.getLogger(\"LiteLLM\")\nlitellm_logger.setLevel(logging.WARNING)\n```\n\nto control the level of logs for LiteLLM.",
          "created_at": "2025-06-10T04:35:53Z"
        }
      ]
    },
    {
      "issue_number": 11557,
      "title": "[Bug]: gemini-2.5-pro doesn't take reasoning parameter error",
      "body": "### What happened?\n\nGemini 2.5 Pro supports reasoning control via thinking_budget ([docs](https://cloud.google.com/vertex-ai/generative-ai/docs/thinking#googlegenaisdk_thinking_budget_with_txt-python_genai_sdk:~:text=24%2C576-,Gemini%C2%A02.5%C2%A0Pro,-128))\n\nBut when I use the reasoning parameters from litellm such as:\n1. reasoning_effort = \"low\"\n2. thinking={\"type\": \"enabled\", \"budget_tokens\": 1000},\n\n\nI do:\n`\nlitellm._turn_on_debug()\ngemini_pro_response =completion(\n  model=\"vertex_ai/gemini-2.5-pro-preview-05-06\",\n  messages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n  thinking={\"type\": \"enabled\", \"budget_tokens\": 1000},\n)\n`\n\nI get this error:\n`    \"message\": \"Unable to submit request because Thinking can't be disabled for this model. Remove 'thinking_config.thinking_budget' from your request and try again.. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini\",\n`\n\n\n### Relevant log output\n\n```shell\n\u001b[92m11:29:52 - LiteLLM:DEBUG\u001b[0m: utils.py:338 - \n\n\u001b[92m11:29:52 - LiteLLM:DEBUG\u001b[0m: utils.py:338 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m11:29:52 - LiteLLM:DEBUG\u001b[0m: utils.py:338 - \u001b[92mlitellm.completion(model='vertex_ai/gemini-2.5-pro-preview-05-06', messages=[{'role': 'user', 'content': 'What is the capital of France?'}], thinking={'type': 'enabled', 'budget_tokens': 1000})\u001b[0m\n\u001b[92m11:29:52 - LiteLLM:DEBUG\u001b[0m: utils.py:338 - \n\n\u001b[92m11:29:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:460 - self.optional_params: {}\n\u001b[92m11:29:52 - LiteLLM:DEBUG\u001b[0m: utils.py:338 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n\u001b[92m11:29:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4299 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-pro-preview-05-06', 'combined_model_name': 'vertex_ai/gemini-2.5-pro-preview-05-06', 'stripped_model_name': 'gemini-2.5-pro-preview-05', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-pro-preview-05', 'custom_llm_provider': 'vertex_ai'}\n\u001b[92m11:29:52 - LiteLLM:INFO\u001b[0m: utils.py:2958 - \nLiteLLM completion() model= gemini-2.5-pro-preview-05-06; provider = vertex_ai\n\u001b[92m11:29:52 - LiteLLM:DEBUG\u001b[0m: utils.py:2961 - \nLiteLLM: Params passed to completion() {'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'What is the capital of France?'}], 'thinking': {'type': 'enabled', 'budget_tokens': 1000}, 'web_search_options': None, 'custom_llm_provider': 'vertex_ai', 'drop_params': None, 'model': 'gemini-2.5-pro-preview-05-06', 'n': None}\n\u001b[92m11:29:52 - LiteLLM:DEBUG\u001b[0m: utils.py:2964 - \nLiteLLM: Non-Default params passed to completion() {'thinking': {'type': 'enabled', 'budget_tokens': 1000}}\n\u001b[92m11:29:52 - LiteLLM:DEBUG\u001b[0m: utils.py:338 - Final returned optional params: {'thinkingConfig': {'includeThoughts': True, 'thinkingBudget': 1000}}\n\u001b[92m11:29:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:460 - self.optional_params: {'thinking': {'type': 'enabled', 'budget_tokens': 1000}}\n\u001b[92m11:29:52 - LiteLLM:DEBUG\u001b[0m: vertex_llm_base.py:305 - Checking cached credentials for project_id: pg-gemini-api-research\n\u001b[92m11:29:52 - LiteLLM:DEBUG\u001b[0m: vertex_llm_base.py:310 - Cached credentials found for project_id: pg-gemini-api-research.\n\u001b[92m11:29:52 - LiteLLM:DEBUG\u001b[0m: vertex_llm_base.py:314 - Using cached credentials\n\u001b[92m11:29:52 - LiteLLM:DEBUG\u001b[0m: vertex_llm_base.py:344 - Validating credentials for project_id: pg-gemini-api-research\n\u001b[92m11:29:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4299 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.5-pro-preview-05-06', 'combined_model_name': 'vertex_ai/gemini-2.5-pro-preview-05-06', 'stripped_model_name': 'gemini-2.5-pro-preview-05', 'combined_stripped_model_name': 'vertex_ai/gemini-2.5-pro-preview-05', 'custom_llm_provider': 'vertex_ai'}\n\u001b[92m11:29:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:907 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://us-central1-aiplatform.googleapis.com/v1/projects/pg-gemini-api-research/locations/us-central1/publishers/google/models/gemini-2.5-pro-preview-05-06:generateContent \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****16' \\\n-d '{'contents': [{'role': 'user', 'parts': [{'text': 'What is the capital of France?'}]}], 'generationConfig': {'thinkingConfig': {'includeThoughts': True, 'thinkingBudget': 1000}}}'\n\u001b[0m\n\n\u001b[92m11:29:52 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2268 - Logging Details: logger_fn - None | callable(logger_fn) - False\n\u001b[92m11:29:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2164 - Logging Details LiteLLM-Failure Call: []\n\n\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n```\n\n\nNot sure if related to https://github.com/BerriAI/litellm/issues/10254\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.71.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "ming-jeng",
      "author_type": "User",
      "created_at": "2025-06-09T18:33:10Z",
      "updated_at": "2025-06-10T03:21:44Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11557/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11557",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11557",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:33.074538",
      "comments": [
        {
          "author": "fengjiajie",
          "body": "The gemini-2.5-pro-preview-_**05-06**_ model you're currently using doesn't natively support the thinking_budget parameter. (If you select this model in the Vertex AI UI, you'll notice the thinking_budget option isn't available).\n\nSupport for this parameter is available in the newer version: gemini-",
          "created_at": "2025-06-10T03:21:34Z"
        }
      ]
    },
    {
      "issue_number": 1437,
      "title": "[Feature]: add litellm embedding to langchain",
      "body": "### The Feature\n\nI noticed langchain [has a litellm backend for chat](https://python.langchain.com/docs/integrations/chat/litellm) but not for embeddings.\n\n### Motivation, pitch\n\nI'm coding [DocToolsLLM](https://github.com/thiswillbeyourgithub/DocToolsLLM) and would like to add support for mistralai instead of OpenAI. But I would preferably use litellm for maximum customizability for the users (changing the LLM model to use with a simple argument change)\r\n\r\nAlso, [mistralai does not yet have embeddings support in langchain](https://github.com/langchain-ai/langchain/pull/15282) and I would prefer to use mistral via litellm than directly\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "thiswillbeyourgithub",
      "author_type": "User",
      "created_at": "2024-01-13T16:36:22Z",
      "updated_at": "2025-06-10T00:02:07Z",
      "closed_at": "2025-06-10T00:02:07Z",
      "labels": [
        "enhancement",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/1437/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/1437",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/1437",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:33.250471",
      "comments": [
        {
          "author": "thiswillbeyourgithub",
          "body": "Up: I would still love that",
          "created_at": "2024-05-26T11:12:17Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "hi @thiswillbeyourgithub - thanks for being an active user / contributor to LiteLLM. I'd love to get on a call and learn how we can improve LiteLLM for you + prioritize your issues. What's the best email to send an invite to ? \r\n\r\nHere's my cal if that's easier: https://calendly.com/d/4mp-gd3-k5k/li",
          "created_at": "2024-06-05T22:23:15Z"
        },
        {
          "author": "thiswillbeyourgithub",
          "body": "> Here's my cal if that's easier: https://calendly.com/d/4mp-gd3-k5k/litellm-1-1-onboarding-chat\r\n\r\nDone",
          "created_at": "2024-06-08T16:42:10Z"
        },
        {
          "author": "pazevedo-hyland",
          "body": "Any updates on this? @thiswillbeyourgithub  / @ishaan-jaff ",
          "created_at": "2024-12-03T09:39:50Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-03-04T00:01:43Z"
        }
      ]
    },
    {
      "issue_number": 6391,
      "title": "[Feature]: Support Computer Control from Anthropic claude-3-5-sonnet-20240620",
      "body": "### The Feature\n\nSupport compute control from Anthropic: https://docs.anthropic.com/en/docs/build-with-claude/computer-use\r\n\r\nMaybe we can consider consolidating that with existing function-calling APIs: https://docs.litellm.ai/docs/completion/function_call#checking-if-a-model-supports-function-calling\n\n### Motivation, pitch\n\nN/A\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "xingyaoww",
      "author_type": "User",
      "created_at": "2024-10-23T04:21:45Z",
      "updated_at": "2025-06-10T00:02:05Z",
      "closed_at": "2025-06-10T00:02:05Z",
      "labels": [
        "enhancement",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/6391/reactions",
        "total_count": 2,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/6391",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/6391",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:33.483007",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-01-28T02:57:01Z"
        },
        {
          "author": "jamesmurdza",
          "body": "Computer use support is documented here: https://docs.litellm.ai/docs/providers/anthropic#computer-tools\n\nI think this can be closed.",
          "created_at": "2025-02-07T14:45:07Z"
        },
        {
          "author": "tanvithakur94",
          "body": "@jamesmurdza  / @krrishdholakia  Is computer use tool supported via bedrock as a provider or is it just for anthropic?",
          "created_at": "2025-03-04T00:58:08Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Currently just anthropic api (`anthropic/`) + bedrock invoke, i think (`bedrock/invoke/*`) - a pr for bedrock converse is welcome! \n\nRelevant code - https://github.com/BerriAI/litellm/blob/19411fbcc7cef17097d8e586fe29c8c4479d0a32/litellm/llms/bedrock/chat/converse_transformation.py#L4",
          "created_at": "2025-03-04T01:35:00Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-03T00:01:54Z"
        }
      ]
    },
    {
      "issue_number": 6635,
      "title": "[Feature]: UI openRouter Models",
      "body": "### The Feature\n\nCurrently there is no openrouter selection from the UI add models provides\r\n\r\ntrying to add them using the compatible openAI api, but seams unhealthy\r\n\r\nmaybe i am missing something\n\n### Motivation, pitch\n\nmaking simple the Add of openRouter models using the API\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "Namec999",
      "author_type": "User",
      "created_at": "2024-11-07T12:47:01Z",
      "updated_at": "2025-06-10T00:02:04Z",
      "closed_at": "2025-06-10T00:02:04Z",
      "labels": [
        "enhancement",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/6635/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/6635",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/6635",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:33.687481",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-02-06T00:01:45Z"
        },
        {
          "author": "ringge",
          "body": "any update on this?",
          "created_at": "2025-02-08T07:16:36Z"
        },
        {
          "author": "OpenSnakeStudio",
          "body": "hi there, \nany update for this issue? currently I can not see the Openrouter provider too.",
          "created_at": "2025-03-04T10:12:41Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-03T00:01:53Z"
        }
      ]
    },
    {
      "issue_number": 8597,
      "title": "[Bug]: VertexAI custom model does not pick up uploaded token",
      "body": "### What happened?\n\nCalls to predict methods on VertexAI's custom deployed models prefer to use the VertexAI tokens configured on the GOOGLE_APPLICATION_CREDENTIALS environment variable instead of the token file uploaded while creating the models.\n\nMeanwhile, VertexAI models configured to use OpenAI like completion endpoints are able to use the tokens uploaded during model creation on proxy UI and produces responses as expected. \n\nWe have a situation in which a specific model from different VertexAI project instead of the default project has to be called through custom predict call. So it will be helpful to get the VertexAI custom deployed models to use the token file uploaded upon model creation during the predict callls. \n\n### Relevant log output\n\n```shell\n{\"message\": \"Trying to fallback b/w models\", \"level\": \"INFO\", \"timestamp\": \"2025-02-17T19:04:24.835020\"}\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/google/api_core/grpc_helpers_async.py\", line 85, in __await__\n    response = yield from self._call.__await__()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/grpc/aio/_call.py\", line 327, in __await__\n    raise _create_rpc_error(\n    ...<2 lines>...\n    )\ngrpc.aio._call.AioRpcError: <AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.PERMISSION_DENIED\n\tdetails = \"Permission 'aiplatform.endpoints.predict' denied on resource '//aiplatform.googleapis.com/projects/*******/locations/us-central1/endpoints/1984786713414729728' (or it may not exist).\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.250.191.138:443 {created_time:\"2025-02-17T19:04:22.270203099+00:00\", grpc_status:7, grpc_message:\"Permission \\'aiplatform.endpoints.predict\\' denied on resource \\'//aiplatform.googleapis.com/projects/******/locations/us-central1/endpoints/1984786713414729728\\' (or it may not exist).\"}\"\n>\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/main.py\", line 466, in acompletion\n    response = await init_response\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_ai_non_gemini.py\", line 738, in async_streaming\n    response_obj = await llm_model.predict(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<2 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/google/cloud/aiplatform_v1/services/prediction_service/async_client.py\", line 404, in predict\n    response = await rpc(\n               ^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/google/api_core/grpc_helpers_async.py\", line 88, in __await__\n    raise exceptions.from_grpc_error(rpc_error) from rpc_error\ngoogle.api_core.exceptions.PermissionDenied: 403 Permission 'aiplatform.endpoints.predict' denied on resource '//aiplatform.googleapis.com/projects/*********/locations/us-central1/endpoints/1984786713414729728' (or it may not exist). [reason: \"IAM_PERMISSION_DENIED\"\ndomain: \"aiplatform.googleapis.com\"\nmetadata {\n  key: \"resource\"\n  value: \"projects/*********/locations/us-central1/endpoints/1984786713414729728\"\n}\nmetadata {\n  key: \"permission\"\n  value: \"aiplatform.endpoints.predict\"\n}\n]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 2889, in async_function_with_fallbacks\n    response = await self.async_function_with_retries(*args, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 3262, in async_function_with_retries\n    raise original_exception\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 3155, in async_function_with_retries\n    response = await self.make_call(original_function, *args, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 3271, in make_call\n    response = await response\n               ^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 1042, in _acompletion\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 1001, in _acompletion\n    response = await _response\n               ^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/utils.py\", line 1394, in wrapper_async\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/utils.py\", line 1253, in wrapper_async\n    result = await original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/main.py\", line 485, in acompletion\n    raise exception_type(\n          ~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<3 lines>...\n        extra_kwargs=kwargs,\n        ^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2202, in exception_type\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2178, in exception_type\n    raise APIConnectionError(\n    ...<8 lines>...\n    )\nlitellm.exceptions.APIConnectionError: litellm.APIConnectionError: 403 Permission 'aiplatform.endpoints.predict' denied on resource '//aiplatform.googleapis.com/projects/*********/locations/us-central1/endpoints/1984786713414729728' (or it may not exist). [reason: \"IAM_PERMISSION_DENIED\"\ndomain: \"aiplatform.googleapis.com\"\nmetadata {\n  key: \"resource\"\n  value: \"projects/*********/locations/us-central1/endpoints/1984786713414729728\"\n}\nmetadata {\n  key: \"permission\"\n  value: \"aiplatform.endpoints.predict\"\n}\n]\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/google/api_core/grpc_helpers_async.py\", line 85, in __await__\n    response = yield from self._call.__await__()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/grpc/aio/_call.py\", line 327, in __await__\n    raise _create_rpc_error(\n    ...<2 lines>...\n    )\ngrpc.aio._call.AioRpcError: <AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.PERMISSION_DENIED\n\tdetails = \"Permission 'aiplatform.endpoints.predict' denied on resource '//aiplatform.googleapis.com/projects/*********/locations/us-central1/endpoints/1984786713414729728' (or it may not exist).\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.250.191.138:443 {created_time:\"2025-02-17T19:04:22.270203099+00:00\", grpc_status:7, grpc_message:\"Permission \\'aiplatform.endpoints.predict\\' denied on resource \\'//aiplatform.googleapis.com/projects/*********/locations/us-central1/endpoints/1984786713414729728\\' (or it may not exist).\"}\"\n>\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/main.py\", line 466, in acompletion\n    response = await init_response\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_ai_non_gemini.py\", line 738, in async_streaming\n    response_obj = await llm_model.predict(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<2 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/google/cloud/aiplatform_v1/services/prediction_service/async_client.py\", line 404, in predict\n    response = await rpc(\n               ^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/google/api_core/grpc_helpers_async.py\", line 88, in __await__\n    raise exceptions.from_grpc_error(rpc_error) from rpc_error\ngoogle.api_core.exceptions.PermissionDenied: 403 Permission 'aiplatform.endpoints.predict' denied on resource '//aiplatform.googleapis.com/projects/*********/locations/us-central1/endpoints/1984786713414729728' (or it may not exist). [reason: \"IAM_PERMISSION_DENIED\"\ndomain: \"aiplatform.googleapis.com\"\nmetadata {\n  key: \"resource\"\n  value: \"projects/*********/locations/us-central1/endpoints/1984786713414729728\"\n}\nmetadata {\n  key: \"permission\"\n  value: \"aiplatform.endpoints.predict\"\n}\n]\n LiteLLM Retried: 1 times, LiteLLM Max Retries: 2\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 3010, in async_function_with_fallbacks\n    get_fallback_model_group(\n    ~~~~~~~~~~~~~~~~~~~~~~~~^\n        fallbacks=fallbacks,  # if fallbacks = [{\"gpt-3.5-turbo\": [\"claude-3-haiku\"]}]\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model_group=cast(str, model_group),\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/router_utils/fallback_event_handlers.py\", line 61, in get_fallback_model_group\n    if list(item.keys())[0] == model_group:  # check exact match\n       ~~~~~~~~~~~~~~~~~^^^\nIndexError: list index out of range\n{\"message\": \"litellm.router.py::async_function_with_fallbacks() - Error occurred while trying to do fallbacks - list index out of range\\nTraceback (most recent call last):\\n  File \\\"/usr/lib/python3.13/site-packages/google/api_core/grpc_helpers_async.py\\\", line 85, in __await__\\n    response = yield from self._call.__await__()\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.13/site-packages/grpc/aio/_call.py\\\", line 327, in __await__\\n    raise _create_rpc_error(\\n    ...<2 lines>...\\n    )\\ngrpc.aio._call.AioRpcError: <AioRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.PERMISSION_DENIED\\n\\tdetails = \\\"Permission 'aiplatform.endpoints.predict' denied on resource '//aiplatform.googleapis.com/projects/*********/locations/us-central1/endpoints/1984786713414729728' (or it may not exist).\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer ipv4:142.250.191.138:443 {created_time:\\\"2025-02-17T19:04:22.270203099+00:00\\\", grpc_status:7, grpc_message:\\\"Permission \\\\'aiplatform.endpoints.predict\\\\' denied on resource \\\\'//aiplatform.googleapis.com/projects/*********/locations/us-central1/endpoints/1984786713414729728\\\\' (or it may not exist).\\\"}\\\"\\n>\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/main.py\\\", line 466, in acompletion\\n    response = await init_response\\n               ^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_ai_non_gemini.py\\\", line 738, in async_streaming\\n    response_obj = await llm_model.predict(\\n                   ^^^^^^^^^^^^^^^^^^^^^^^^\\n    ...<2 lines>...\\n    )\\n    ^\\n  File \\\"/usr/lib/python3.13/site-packages/google/cloud/aiplatform_v1/services/prediction_service/async_client.py\\\", line 404, in predict\\n    response = await rpc(\\n               ^^^^^^^^^^\\n    ...<4 lines>...\\n    )\\n    ^\\n  File \\\"/usr/lib/python3.13/site-packages/google/api_core/grpc_helpers_async.py\\\", line 88, in __await__\\n    raise exceptions.from_grpc_error(rpc_error) from rpc_error\\ngoogle.api_core.exceptions.PermissionDenied: 403 Permission 'aiplatform.endpoints.predict' denied on resource '//aiplatform.googleapis.com/projects/*********/locations/us-central1/endpoints/1984786713414729728' (or it may not exist). [reason: \\\"IAM_PERMISSION_DENIED\\\"\\ndomain: \\\"aiplatform.googleapis.com\\\"\\nmetadata {\\n  key: \\\"resource\\\"\\n  value: \\\"projects/*********/locations/us-central1/endpoints/1984786713414729728\\\"\\n}\\nmetadata {\\n  key: \\\"permission\\\"\\n  value: \\\"aiplatform.endpoints.predict\\\"\\n}\\n]\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/router.py\\\", line 2889, in async_function_with_fallbacks\\n    response = await self.async_function_with_retries(*args, **kwargs)\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/router.py\\\", line 3262, in async_function_with_retries\\n    raise original_exception\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/router.py\\\", line 3155, in async_function_with_retries\\n    response = await self.make_call(original_function, *args, **kwargs)\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/router.py\\\", line 3271, in make_call\\n    response = await response\\n               ^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/router.py\\\", line 1042, in _acompletion\\n    raise e\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/router.py\\\", line 1001, in _acompletion\\n    response = await _response\\n               ^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/utils.py\\\", line 1394, in wrapper_async\\n    raise e\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/utils.py\\\", line 1253, in wrapper_async\\n    result = await original_function(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/main.py\\\", line 485, in acompletion\\n    raise exception_type(\\n          ~~~~~~~~~~~~~~^\\n        model=model,\\n        ^^^^^^^^^^^^\\n    ...<3 lines>...\\n        extra_kwargs=kwargs,\\n        ^^^^^^^^^^^^^^^^^^^^\\n    )\\n    ^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\\\", line 2202, in exception_type\\n    raise e\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\\\", line 2178, in exception_type\\n    raise APIConnectionError(\\n    ...<8 lines>...\\n    )\\nlitellm.exceptions.APIConnectionError: litellm.APIConnectionError: 403 Permission 'aiplatform.endpoints.predict' denied on resource '//aiplatform.googleapis.com/projects/*********/locations/us-central1/endpoints/1984786713414729728' (or it may not exist). [reason: \\\"IAM_PERMISSION_DENIED\\\"\\ndomain: \\\"aiplatform.googleapis.com\\\"\\nmetadata {\\n  key: \\\"resource\\\"\\n  value: \\\"projects/*********/locations/us-central1/endpoints/1984786713414729728\\\"\\n}\\nmetadata {\\n  key: \\\"permission\\\"\\n  value: \\\"aiplatform.endpoints.predict\\\"\\n}\\n]\\nTraceback (most recent call last):\\n  File \\\"/usr/lib/python3.13/site-packages/google/api_core/grpc_helpers_async.py\\\", line 85, in __await__\\n    response = yield from self._call.__await__()\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.13/site-packages/grpc/aio/_call.py\\\", line 327, in __await__\\n    raise _create_rpc_error(\\n    ...<2 lines>...\\n    )\\ngrpc.aio._call.AioRpcError: <AioRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.PERMISSION_DENIED\\n\\tdetails = \\\"Permission 'aiplatform.endpoints.predict' denied on resource '//aiplatform.googleapis.com/projects/*********/locations/us-central1/endpoints/1984786713414729728' (or it may not exist).\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer ipv4:142.250.191.138:443 {created_time:\\\"2025-02-17T19:04:22.270203099+00:00\\\", grpc_status:7, grpc_message:\\\"Permission \\\\'aiplatform.endpoints.predict\\\\' denied on resource \\\\'//aiplatform.googleapis.com/projects/*********/locations/us-central1/endpoints/1984786713414729728\\\\' (or it may not exist).\\\"}\\\"\\n>\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/main.py\\\", line 466, in acompletion\\n    response = await init_response\\n               ^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_ai_non_gemini.py\\\", line 738, in async_streaming\\n    response_obj = await llm_model.predict(\\n                   ^^^^^^^^^^^^^^^^^^^^^^^^\\n    ...<2 lines>...\\n    )\\n    ^\\n  File \\\"/usr/lib/python3.13/site-packages/google/cloud/aiplatform_v1/services/prediction_service/async_client.py\\\", line 404, in predict\\n    response = await rpc(\\n               ^^^^^^^^^^\\n    ...<4 lines>...\\n    )\\n    ^\\n  File \\\"/usr/lib/python3.13/site-packages/google/api_core/grpc_helpers_async.py\\\", line 88, in __await__\\n    raise exceptions.from_grpc_error(rpc_error) from rpc_error\\ngoogle.api_core.exceptions.PermissionDenied: 403 Permission 'aiplatform.endpoints.predict' denied on resource '//aiplatform.googleapis.com/projects/*********/locations/us-central1/endpoints/1984786713414729728' (or it may not exist). [reason: \\\"IAM_PERMISSION_DENIED\\\"\\ndomain: \\\"aiplatform.googleapis.com\\\"\\nmetadata {\\n  key: \\\"resource\\\"\\n  value: \\\"projects/*********/locations/us-central1/endpoints/1984786713414729728\\\"\\n}\\nmetadata {\\n  key: \\\"permission\\\"\\n  value: \\\"aiplatform.endpoints.predict\\\"\\n}\\n]\\n LiteLLM Retried: 1 times, LiteLLM Max Retries: 2\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/router.py\\\", line 3010, in async_function_with_fallbacks\\n    get_fallback_model_group(\\n    ~~~~~~~~~~~~~~~~~~~~~~~~^\\n        fallbacks=fallbacks,  # if fallbacks = [{\\\"gpt-3.5-turbo\\\": [\\\"claude-3-haiku\\\"]}]\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n        model_group=cast(str, model_group),\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n    )\\n    ^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/router_utils/fallback_event_handlers.py\\\", line 61, in get_fallback_model_group\\n    if list(item.keys())[0] == model_group:  # check exact match\\n       ~~~~~~~~~~~~~~~~~^^^\\nIndexError: list index out of range\\n\\n\\nDebug Information:\\nCooldown Deployments=[]\", \"level\": \"ERROR\", \"timestamp\": \"2025-02-17T19:04:24.849290\"}\n{\"message\": \"litellm.proxy.proxy_server.chat_completion(): Exception occured - litellm.APIConnectionError: 403 Permission 'aiplatform.endpoints.predict' denied on resource '//aiplatform.googleapis.com/projects/*********/locations/us-central1/endpoints/1984786713414729728' (or it may not exist). [reason: \\\"IAM_PERMISSION_DENIED\\\"\\ndomain: \\\"aiplatform.googleapis.com\\\"\\nmetadata {\\n  key: \\\"resource\\\"\\n  value: \\\"projects/*********/locations/us-central1/endpoints/1984786713414729728\\\"\\n}\\nmetadata {\\n  key: \\\"permission\\\"\\n  value: \\\"aiplatform.endpoints.predict\\\"\\n}\\n]\\nTraceback (most recent call last):\\n  File \\\"/usr/lib/python3.13/site-packages/google/api_core/grpc_helpers_async.py\\\", line 85, in __await__\\n    response = yield from self._call.__await__()\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.13/site-packages/grpc/aio/_call.py\\\", line 327, in __await__\\n    raise _create_rpc_error(\\n    ...<2 lines>...\\n    )\\ngrpc.aio._call.AioRpcError: <AioRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.PERMISSION_DENIED\\n\\tdetails = \\\"Permission 'aiplatform.endpoints.predict' denied on resource '//aiplatform.googleapis.com/projects/*********/locations/us-central1/endpoints/1984786713414729728' (or it may not exist).\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer ipv4:142.250.191.138:443 {created_time:\\\"2025-02-17T19:04:22.270203099+00:00\\\", grpc_status:7, grpc_message:\\\"Permission \\\\'aiplatform.endpoints.predict\\\\' denied on resource \\\\'//aiplatform.googleapis.com/projects/*********/locations/us-central1/endpoints/1984786713414729728\\\\' (or it may not exist).\\\"}\\\"\\n>\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/main.py\\\", line 466, in acompletion\\n    response = await init_response\\n               ^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_ai_non_gemini.py\\\", line 738, in async_streaming\\n    response_obj = await llm_model.predict(\\n                   ^^^^^^^^^^^^^^^^^^^^^^^^\\n    ...<2 lines>...\\n    )\\n    ^\\n  File \\\"/usr/lib/python3.13/site-packages/google/cloud/aiplatform_v1/services/prediction_service/async_client.py\\\", line 404, in predict\\n    response = await rpc(\\n               ^^^^^^^^^^\\n    ...<4 lines>...\\n    )\\n    ^\\n  File \\\"/usr/lib/python3.13/site-packages/google/api_core/grpc_helpers_async.py\\\", line 88, in __await__\\n    raise exceptions.from_grpc_error(rpc_error) from rpc_error\\ngoogle.api_core.exceptions.PermissionDenied: 403 Permission 'aiplatform.endpoints.predict' denied on resource '//aiplatform.googleapis.com/projects/*********/locations/us-central1/endpoints/1984786713414729728' (or it may not exist). [reason: \\\"IAM_PERMISSION_DENIED\\\"\\ndomain: \\\"aiplatform.googleapis.com\\\"\\nmetadata {\\n  key: \\\"resource\\\"\\n  value: \\\"projects/*********/locations/us-central1/endpoints/1984786713414729728\\\"\\n}\\nmetadata {\\n  key: \\\"permission\\\"\\n  value: \\\"aiplatform.endpoints.predict\\\"\\n}\\n]\\n\\nReceived Model Group=pco-llama3-1-8b-ft-icd-l4-predict\\nAvailable Model Group Fallbacks=None\\nError doing the fallback: list index out of range LiteLLM Retried: 1 times, LiteLLM Max Retries: 2\", \"level\": \"ERROR\", \"timestamp\": \"2025-02-17T19:04:24.855748\", \"stacktrace\": \"Traceback (most recent call last):\\n  File \\\"/usr/lib/python3.13/site-packages/google/api_core/grpc_helpers_async.py\\\", line 85, in __await__\\n    response = yield from self._call.__await__()\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.13/site-packages/grpc/aio/_call.py\\\", line 327, in __await__\\n    raise _create_rpc_error(\\n    ...<2 lines>...\\n    )\\ngrpc.aio._call.AioRpcError: <AioRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.PERMISSION_DENIED\\n\\tdetails = \\\"Permission 'aiplatform.endpoints.predict' denied on resource '//aiplatform.googleapis.com/projects/*********/locations/us-central1/endpoints/1984786713414729728' (or it may not exist).\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer ipv4:142.250.191.138:443 {created_time:\\\"2025-02-17T19:04:22.270203099+00:00\\\", grpc_status:7, grpc_message:\\\"Permission \\\\'aiplatform.endpoints.predict\\\\' denied on resource \\\\'//aiplatform.googleapis.com/projects/*********/locations/us-central1/endpoints/1984786713414729728\\\\' (or it may not exist).\\\"}\\\"\\n>\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/main.py\\\", line 466, in acompletion\\n    response = await init_response\\n               ^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_ai_non_gemini.py\\\", line 738, in async_streaming\\n    response_obj = await llm_model.predict(\\n                   ^^^^^^^^^^^^^^^^^^^^^^^^\\n    ...<2 lines>...\\n    )\\n    ^\\n  File \\\"/usr/lib/python3.13/site-packages/google/cloud/aiplatform_v1/services/prediction_service/async_client.py\\\", line 404, in predict\\n    response = await rpc(\\n               ^^^^^^^^^^\\n    ...<4 lines>...\\n    )\\n    ^\\n  File \\\"/usr/lib/python3.13/site-packages/google/api_core/grpc_helpers_async.py\\\", line 88, in __await__\\n    raise exceptions.from_grpc_error(rpc_error) from rpc_error\\ngoogle.api_core.exceptions.PermissionDenied: 403 Permission 'aiplatform.endpoints.predict' denied on resource '//aiplatform.googleapis.com/projects/*********/locations/us-central1/endpoints/1984786713414729728' (or it may not exist). [reason: \\\"IAM_PERMISSION_DENIED\\\"\\ndomain: \\\"aiplatform.googleapis.com\\\"\\nmetadata {\\n  key: \\\"resource\\\"\\n  value: \\\"projects/*********/locations/us-central1/endpoints/1984786713414729728\\\"\\n}\\nmetadata {\\n  key: \\\"permission\\\"\\n  value: \\\"aiplatform.endpoints.predict\\\"\\n}\\n]\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/proxy/proxy_server.py\\\", line 3587, in chat_completion\\n    responses = await llm_responses\\n                ^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/router.py\\\", line 904, in acompletion\\n    raise e\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/router.py\\\", line 880, in acompletion\\n    response = await self.async_function_with_fallbacks(**kwargs)\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/router.py\\\", line 3071, in async_function_with_fallbacks\\n    raise original_exception\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/router.py\\\", line 2889, in async_function_with_fallbacks\\n    response = await self.async_function_with_retries(*args, **kwargs)\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/router.py\\\", line 3262, in async_function_with_retries\\n    raise original_exception\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/router.py\\\", line 3155, in async_function_with_retries\\n    response = await self.make_call(original_function, *args, **kwargs)\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/router.py\\\", line 3271, in make_call\\n    response = await response\\n               ^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/router.py\\\", line 1042, in _acompletion\\n    raise e\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/router.py\\\", line 1001, in _acompletion\\n    response = await _response\\n               ^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/utils.py\\\", line 1394, in wrapper_async\\n    raise e\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/utils.py\\\", line 1253, in wrapper_async\\n    result = await original_function(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/main.py\\\", line 485, in acompletion\\n    raise exception_type(\\n          ~~~~~~~~~~~~~~^\\n        model=model,\\n        ^^^^^^^^^^^^\\n    ...<3 lines>...\\n        extra_kwargs=kwargs,\\n        ^^^^^^^^^^^^^^^^^^^^\\n    )\\n    ^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\\\", line 2202, in exception_type\\n    raise e\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\\\", line 2178, in exception_type\\n    raise APIConnectionError(\\n    ...<8 lines>...\\n    )\\nlitellm.exceptions.APIConnectionError: litellm.APIConnectionError: 403 Permission 'aiplatform.endpoints.predict' denied on resource '//aiplatform.googleapis.com/projects/*********/locations/us-central1/endpoints/1984786713414729728' (or it may not exist). [reason: \\\"IAM_PERMISSION_DENIED\\\"\\ndomain: \\\"aiplatform.googleapis.com\\\"\\nmetadata {\\n  key: \\\"resource\\\"\\n  value: \\\"projects/*********/locations/us-central1/endpoints/1984786713414729728\\\"\\n}\\nmetadata {\\n  key: \\\"permission\\\"\\n  value: \\\"aiplatform.endpoints.predict\\\"\\n}\\n]\\nTraceback (most recent call last):\\n  File \\\"/usr/lib/python3.13/site-packages/google/api_core/grpc_helpers_async.py\\\", line 85, in __await__\\n    response = yield from self._call.__await__()\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.13/site-packages/grpc/aio/_call.py\\\", line 327, in __await__\\n    raise _create_rpc_error(\\n    ...<2 lines>...\\n    )\\ngrpc.aio._call.AioRpcError: <AioRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.PERMISSION_DENIED\\n\\tdetails = \\\"Permission 'aiplatform.endpoints.predict' denied on resource '//aiplatform.googleapis.com/projects/*********/locations/us-central1/endpoints/1984786713414729728' (or it may not exist).\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer ipv4:142.250.191.138:443 {created_time:\\\"2025-02-17T19:04:22.270203099+00:00\\\", grpc_status:7, grpc_message:\\\"Permission \\\\'aiplatform.endpoints.predict\\\\' denied on resource \\\\'//aiplatform.googleapis.com/projects/*********/locations/us-central1/endpoints/1984786713414729728\\\\' (or it may not exist).\\\"}\\\"\\n>\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/main.py\\\", line 466, in acompletion\\n    response = await init_response\\n               ^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_ai_non_gemini.py\\\", line 738, in async_streaming\\n    response_obj = await llm_model.predict(\\n                   ^^^^^^^^^^^^^^^^^^^^^^^^\\n    ...<2 lines>...\\n    )\\n    ^\\n  File \\\"/usr/lib/python3.13/site-packages/google/cloud/aiplatform_v1/services/prediction_service/async_client.py\\\", line 404, in predict\\n    response = await rpc(\\n               ^^^^^^^^^^\\n    ...<4 lines>...\\n    )\\n    ^\\n  File \\\"/usr/lib/python3.13/site-packages/google/api_core/grpc_helpers_async.py\\\", line 88, in __await__\\n    raise exceptions.from_grpc_error(rpc_error) from rpc_error\\ngoogle.api_core.exceptions.PermissionDenied: 403 Permission 'aiplatform.endpoints.predict' denied on resource '//aiplatform.googleapis.com/projects/*********/locations/us-central1/endpoints/1984786713414729728' (or it may not exist). [reason: \\\"IAM_PERMISSION_DENIED\\\"\\ndomain: \\\"aiplatform.googleapis.com\\\"\\nmetadata {\\n  key: \\\"resource\\\"\\n  value: \\\"projects/*********/locations/us-central1/endpoints/1984786713414729728\\\"\\n}\\nmetadata {\\n  key: \\\"permission\\\"\\n  value: \\\"aiplatform.endpoints.predict\\\"\\n}\\n]\\n\\nReceived Model Group=pco-llama3-1-8b-ft-icd-l4-predict\\nAvailable Model Group Fallbacks=None\\nError doing the fallback: list index out of range LiteLLM Retried: 1 times, LiteLLM Max Retries: 2\"}\n{\"message\": \"{\\\"event\\\": \\\"giveup\\\", \\\"exception\\\": \\\"\\\"}\", \"level\": \"INFO\", \"timestamp\": \"2025-02-17T19:04:24.862449\"}\n{\"message\": \"Giving up chat_completion(...) after 1 tries (litellm.proxy._types.ProxyException)\", \"level\": \"ERROR\", \"timestamp\": \"2025-02-17T19:04:24.867654\"}\n{\"message\": \"litellm.acompletion(model=azure/mlp-genai-npe-eastus2-gpt4o)\\u001b[32m 200 OK\\u001b[0m\", \"level\": \"INFO\", \"timestamp\": \"2025-02-17T19:04:27.345561\"}\n{\"message\": \"disable_spend_logs=True. Skipping writing spend logs to db. Other spend updates - Key/User/Team table will still occur.\", \"level\": \"INFO\", \"timestamp\": \"2025-02-17T19:04:27.346675\"}\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.61.3\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "suresiva",
      "author_type": "User",
      "created_at": "2025-02-17T19:24:55Z",
      "updated_at": "2025-06-10T00:01:59Z",
      "closed_at": "2025-06-10T00:01:59Z",
      "labels": [
        "bug",
        "mlops user request",
        "stale"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8597/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "krrishdholakia"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8597",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8597",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:33.875886",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Hey @suresiva what are steps to repro this issue? could it be related to - #7904 ",
          "created_at": "2025-02-17T19:45:32Z"
        },
        {
          "author": "suresiva",
          "body": "Hey @krrishdholakia - No, its not related to above issue given. We see the OpenAI like completion calls working fine.\n\nWe only have issue with calling the custom model's predict call.\n\nSteps to reproduce this error are,\n- Spin up LiteLLM proxy with GOOGLE_APPLICATION_CREDENTIALS environment var poin",
          "created_at": "2025-02-18T15:40:49Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-03T00:01:48Z"
        }
      ]
    },
    {
      "issue_number": 8620,
      "title": "[Bug]: Memory Leak in `completion()` with `stream=True`",
      "body": "### What happened?\n\nWhen calling `completion()` with `stream=True`, memory usage increases with each request and does not return to the initial level. This issue does not occur with `stream=False`.\n\n```python\nimport os\nimport psutil\nfrom litellm import completion\n\nos.environ[\"OPENAI_API_KEY\"] = \"********\"\n\nprocess = psutil.Process()\ninitial_memory = process.memory_info().rss / (1024 * 1024)\nprint(f\"Initial memory usage: {initial_memory:.2f} MB\")\n\nfor i in range(10):\n    response = completion(\n        model=\"gpt-4o\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": \"Hello!\"}\n        ],\n        stream=True,\n    )\n    for _ in response:\n        pass\n\n    process = psutil.Process()\n    memory_usage = process.memory_info().rss / (1024 * 1024)\n    memory_diff = memory_usage - initial_memory\n    print(f\"Iteration {i+1}: Memory usage {memory_usage:.2f} MB (+{memory_diff:.2f} MB)\")\n```\n\n### Relevant log output\n\n```shell\nInitial memory usage: 148.52 MB\nIteration 1: Memory usage 150.30 MB (+1.79 MB)\nIteration 2: Memory usage 150.58 MB (+2.07 MB)\nIteration 3: Memory usage 150.59 MB (+2.07 MB)\nIteration 4: Memory usage 228.73 MB (+80.21 MB)\nIteration 5: Memory usage 229.10 MB (+80.59 MB)\nIteration 6: Memory usage 229.50 MB (+80.99 MB)\nIteration 7: Memory usage 230.16 MB (+81.65 MB)\nIteration 8: Memory usage 230.42 MB (+81.90 MB)\nIteration 9: Memory usage 230.97 MB (+82.45 MB)\nIteration 10: Memory usage 231.37 MB (+82.85 MB)\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.61.8\n\n### Twitter / LinkedIn details\n\n@iwamot / https://www.linkedin.com/in/iwamot/",
      "state": "closed",
      "author": "iwamot",
      "author_type": "User",
      "created_at": "2025-02-18T15:19:02Z",
      "updated_at": "2025-06-10T00:01:58Z",
      "closed_at": "2025-06-10T00:01:58Z",
      "labels": [
        "bug",
        "stale",
        "feb 2025"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8620/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "ishaan-jaff"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8620",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8620",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:34.109242",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "thanks, I just fixed a memory leak on non-streaming. Will do the same here ",
          "created_at": "2025-02-18T15:27:56Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "able to repro ",
          "created_at": "2025-02-22T20:03:03Z"
        },
        {
          "author": "aguadoenzo",
          "body": "Also seeing this, but we're also seeing memory usage increase even without usage (the only usage would be an HTTP healthcheck on `/`)\n\n![Image](https://github.com/user-attachments/assets/95108e3f-0dd3-489d-8256-2ceb4f7507f1)",
          "created_at": "2025-02-28T14:38:18Z"
        },
        {
          "author": "SunnyWan59",
          "body": "I can work on this if possible\n",
          "created_at": "2025-03-04T04:37:14Z"
        },
        {
          "author": "SunnyWan59",
          "body": "The problem here is probably the OpenAI client. The issue below summarized it pretty well. I'm currently working on a fix\n\nhttps://github.com/openai/openai-python/issues/820",
          "created_at": "2025-03-04T06:20:46Z"
        }
      ]
    },
    {
      "issue_number": 8765,
      "title": "[Feature]: `litellm.supports_reasoning` and `drop_params` working with reasoning",
      "body": "### The Feature\n\nWith `litellm==1.61.2`, you can pass `include_reasoning=True` to `openrouter/deepseek/deepseek-r1` and you get back reasoning in the response.\n\nHowever, when you change the model to something like `gpt-4o` and still pass `include_reasoning=True`, we get blown up:\n\n```none\nBadRequestError: litellm.BadRequestError: OpenAIException - Error code: 400 - {'error': {'message': 'Unrecognized request argument supplied: include_reasoning', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n```\n\nThis happens regardless of the presence of `litellm.drop_params`.\n\nThus the request is two parts:\n- A utility function `litellm.supports_reasoning` that yields if a model supports a separate reasoning field in the response\n- `litellm.drop_params` working with `include_reasoning`\n\n### Motivation, pitch\n\nMaking LiteLLM more fluent with reasoning.\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "jamesbraza",
      "author_type": "User",
      "created_at": "2025-02-24T18:00:29Z",
      "updated_at": "2025-06-10T00:01:55Z",
      "closed_at": null,
      "labels": [
        "enhancement",
        "march 2025"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8765/reactions",
        "total_count": 4,
        "+1": 4,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8765",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8765",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:34.334312",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "Added to roadmap ",
          "created_at": "2025-03-10T15:42:52Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "anything else you want on March 2025 @jamesbraza  ? https://github.com/BerriAI/litellm/discussions/9057 ",
          "created_at": "2025-03-10T15:43:17Z"
        },
        {
          "author": "jamesbraza",
          "body": "Thanks for adding this to the roadmap, and for asking! It would be nice for LiteLLM to resolve some of the `DeprecationWarning`s emitted: https://github.com/Future-House/paper-qa/blob/v5.17.0/pyproject.toml#L271-L278",
          "created_at": "2025-03-10T17:19:41Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-09T00:02:05Z"
        },
        {
          "author": "jamesbraza",
          "body": "Not stale",
          "created_at": "2025-06-09T00:15:43Z"
        }
      ]
    },
    {
      "issue_number": 8968,
      "title": "[Feature]: Add ENFER.AI LLM Provider",
      "body": "### The Feature\n\nENFER.AI is a cost-effective LLM provider with a focus on 1-12b models. The provided API is fully compatible with the OpenAI API spec.\n\nAPI swagger UI - https://api.enfer.ai/swagger-ui/\nGetting started - https://enfer-ai.gitbook.io/enfer.ai-docs\n\n### Motivation, pitch\n\nBy supporting ENFER.AI in LiteLLM, you would provide more cost-effective options for developers and users.\n\nIf this has the green light, I can get started on the implementation.\n\n### Are you a ML Ops Team?\n\nYes\n\n### Twitter / LinkedIn details\n\nhttps://www.linkedin.com/company/tqdm-inc",
      "state": "closed",
      "author": "sssemil",
      "author_type": "User",
      "created_at": "2025-03-04T00:16:29Z",
      "updated_at": "2025-06-10T00:01:53Z",
      "closed_at": "2025-06-10T00:01:53Z",
      "labels": [
        "enhancement",
        "mlops user request",
        "stale"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8968/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8968",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8968",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:34.552569",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-03T00:01:35Z"
        }
      ]
    },
    {
      "issue_number": 8981,
      "title": "[Feature]: Support litellm.base_url",
      "body": "### The Feature\n\nhttps://github.com/openai/openai-python/issues/1051#issuecomment-1884172522\n\n### Motivation, pitch\n\nAlign with OpenAI spec\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "SmartManoj",
      "author_type": "User",
      "created_at": "2025-03-04T15:30:24Z",
      "updated_at": "2025-06-10T00:01:51Z",
      "closed_at": "2025-06-10T00:01:51Z",
      "labels": [
        "enhancement",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8981/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8981",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8981",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:34.779003",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-03T00:01:34Z"
        }
      ]
    },
    {
      "issue_number": 10177,
      "title": "[Feature]: Dark Mode",
      "body": "### The Feature\n\nPlease add a dark theme to the UI panel. I'm going blind.\n\n### Motivation, pitch\n\nOn the way to improving the admin panel.\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "MrMrProgrammer",
      "author_type": "User",
      "created_at": "2025-04-20T07:19:10Z",
      "updated_at": "2025-06-09T23:07:23Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10177/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10177",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10177",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:34.999541",
      "comments": [
        {
          "author": "hongkongkiwi",
          "body": "This is a duplicate of https://github.com/BerriAI/litellm/issues/10110 great minds think alike! Or blind people unite? :)",
          "created_at": "2025-04-20T07:51:22Z"
        },
        {
          "author": "styk-tv",
          "body": "@hongkongkiwi nothing blinds more than a wide view monitor going 100% white at 4am. no coffee required, wide awake.\n\n![Image](https://github.com/user-attachments/assets/77e89615-be53-434f-85c3-f200397d20d1)\nhttps://arxiv.org/pdf/2409.10895\n\n:D ",
          "created_at": "2025-06-09T23:03:22Z"
        }
      ]
    },
    {
      "issue_number": 10517,
      "title": "[Bug]: Async task pending error on adding fallback models to completion",
      "body": "### What happened?\n\nThis has been preventing me from upgrading the litellm lib for a long time now. I was stuck at 1.51.3 because updating would lead to the below error log constantly popping up. I decided to debug a bit today with the latest version and was able to reproduce with a very simple example.\n\n```python\nfrom litellm import completion\nmessages = []\nmessages.append({\"role\": \"user\", \"content\": \"Hello! How are you?\"})\nprint(completion(messages=messages,model=\"anthropic/claude-3-7-sonnet-20250219\",fallbacks=[\"openai/gpt-4o\"]))\n```\nRunning the above should give the below error log. If the `fallbacks` argument is removed then the error goes away. Please help!\n\n### Relevant log output\n\n```shell\n/Users/amanpreets/miniconda3/envs/sqa_lib/bin/python /Users/amanpreets/PycharmProjects/ai2-scholarqa-lib/api/lib_example.py \nSYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\nASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nFinal returned optional params: {}\n_is_function_call: False\nRAW RESPONSE:\n<coroutine object AnthropicChatCompletion.acompletion_function at 0x12344c800>\n\n\n18:58:18 - LiteLLM:WARNING: utils.py:510 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\nRAW RESPONSE:\n{\"id\":\"msg_01Ly2SYodoQLXPfZxyLaiGW4\",\"type\":\"message\",\"role\":\"assistant\",\"model\":\"claude-3-7-sonnet-20250219\",\"content\":[{\"type\":\"text\",\"text\":\"Hello! I'm doing well, thank you for asking. I'm here and ready to help with any questions or topics you'd like to discuss. How are you doing today?\"}],\"stop_reason\":\"end_turn\",\"stop_sequence\":null,\"usage\":{\"input_tokens\":13,\"cache_creation_input_tokens\":0,\"cache_read_input_tokens\":0,\"output_tokens\":39}}\n\n\nAsync Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x114ee9850>>\nModelResponse(id='chatcmpl-58a9fcc4-34cd-4561-b6ad-9c2623022a77', created=1746237499, model='claude-3-7-sonnet-20250219', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Hello! I'm doing well, thank you for asking. I'm here and ready to help with any questions or topics you'd like to discuss. How are you doing today?\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=39, prompt_tokens=13, total_tokens=52, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))\nTask was destroyed but it is pending!\ntask: <Task pending name='Task-4' coro=<Logging.async_success_handler() running at /Users/amanpreets/miniconda3/envs/sqa_lib/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py:1790>>\n/Users/amanpreets/miniconda3/envs/sqa_lib/lib/python3.11/asyncio/base_events.py:679: RuntimeWarning: coroutine 'Logging.async_success_handler' was never awaited\n  self._ready.clear()\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.67.6\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "amanpreet692",
      "author_type": "User",
      "created_at": "2025-05-03T02:07:37Z",
      "updated_at": "2025-06-09T19:35:17Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10517/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10517",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10517",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:35.198386",
      "comments": [
        {
          "author": "Winston-503",
          "body": "Any activity on this one?",
          "created_at": "2025-06-09T19:35:15Z"
        }
      ]
    },
    {
      "issue_number": 11010,
      "title": "[Feature]: MCP Crud DB Operations",
      "body": "### The Feature\n\nCreating the Proxy view to add, remove, and update MCP servers\n\n### Motivation, pitch\n\nAllows multiple users to connect external hosted MCP servers as a proxy for usage\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "wagnerjt",
      "author_type": "User",
      "created_at": "2025-05-21T14:44:08Z",
      "updated_at": "2025-06-09T18:31:05Z",
      "closed_at": "2025-06-09T18:31:05Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11010/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11010",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11010",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:35.371420",
      "comments": []
    },
    {
      "issue_number": 11555,
      "title": "[Bug]: SCIM provisioning not adding members to team",
      "body": "### What happened?\n\nI have enabled SCIM provisioning from Entra ID. Users get created as internal users and the groups created as teams but the members of the groups do not get added as members to the team.\n\nThis can be seen in the following screenshots\n\n![Image](https://github.com/user-attachments/assets/920b2f7e-2acf-4708-b9b5-7756012a394d)\n\n![Image](https://github.com/user-attachments/assets/9d811271-298a-4512-84a5-5239c4aadeee)\n\nEntra ID says adding the users to the groups has failed with the below screen shots.\n\n![Image](https://github.com/user-attachments/assets/78a31aef-3dd3-40bf-9a01-7b4debcdd5e0)\n\n![Image](https://github.com/user-attachments/assets/6a5d77d6-1ea0-4509-a983-cfa3e4ea0caf)\n\nI don't think I have configured anything wrong as I get an api error in the the LiteLLM docker logs which has been included but I am happy to be told otherwise. If anyone could advise or investigate that would be great.\n\n\n\n  \n\n### Relevant log output\n\n```shell\nINFO:     172.22.0.4:51078 - \"PATCH /scim/v2/Groups/a688bdfd-545a-48f7-896f-1942724f949a HTTP/1.1\" 500 Internal Server Error\nERROR:    Exception in ASGI application\n  + Exception Group Traceback (most recent call last):\n  |   File \"/usr/lib/python3.13/site-packages/starlette/_utils.py\", line 76, in collapse_excgroups\n  |     yield\n  |   File \"/usr/lib/python3.13/site-packages/starlette/middleware/base.py\", line 186, in __call__\n  |     async with anyio.create_task_group() as task_group:\n  |                ~~~~~~~~~~~~~~~~~~~~~~~^^\n  |   File \"/usr/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 685, in __aexit__\n  |     raise BaseExceptionGroup(\n  |         \"unhandled errors in a TaskGroup\", self._exceptions\n  |     )\n  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n  +-+---------------- 1 ----------------\n    | Traceback (most recent call last):\n    |   File \"/usr/lib/python3.13/site-packages/uvicorn/protocols/http/h11_impl.py\", line 407, in run_asgi\n    |     result = await app(  # type: ignore[func-returns-value]\n    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |         self.scope, self.receive, self.send\n    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |     )\n    |     ^\n    |   File \"/usr/lib/python3.13/site-packages/uvicorn/middleware/proxy_headers.py\", line 69, in __call__\n    |     return await self.app(scope, receive, send)\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/lib/python3.13/site-packages/fastapi/applications.py\", line 1054, in __call__\n    |     await super().__call__(scope, receive, send)\n    |   File \"/usr/lib/python3.13/site-packages/starlette/applications.py\", line 113, in __call__\n    |     await self.middleware_stack(scope, receive, send)\n    |   File \"/usr/lib/python3.13/site-packages/starlette/middleware/errors.py\", line 187, in __call__\n    |     raise exc\n    |   File \"/usr/lib/python3.13/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    |     await self.app(scope, receive, _send)\n    |   File \"/usr/lib/python3.13/site-packages/starlette/middleware/base.py\", line 185, in __call__\n    |     with collapse_excgroups():\n    |          ~~~~~~~~~~~~~~~~~~^^\n    |   File \"/usr/lib/python3.13/contextlib.py\", line 162, in __exit__\n    |     self.gen.throw(value)\n    |     ~~~~~~~~~~~~~~^^^^^^^\n    |   File \"/usr/lib/python3.13/site-packages/starlette/_utils.py\", line 82, in collapse_excgroups\n    |     raise exc\n    |   File \"/usr/lib/python3.13/site-packages/starlette/middleware/base.py\", line 187, in __call__\n    |     response = await self.dispatch_func(request, call_next)\n    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/lib/python3.13/site-packages/litellm/proxy/middleware/prometheus_auth_middleware.py\", line 47, in dispatch\n    |     response = await call_next(request)\n    |                ^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/lib/python3.13/site-packages/starlette/middleware/base.py\", line 163, in call_next\n    |     raise app_exc\n    |   File \"/usr/lib/python3.13/site-packages/starlette/middleware/base.py\", line 149, in coro\n    |     await self.app(scope, receive_or_disconnect, send_no_error)\n    |   File \"/usr/lib/python3.13/site-packages/starlette/middleware/cors.py\", line 85, in __call__\n    |     await self.app(scope, receive, send)\n    |   File \"/usr/lib/python3.13/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n    |   File \"/usr/lib/python3.13/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    |     raise exc\n    |   File \"/usr/lib/python3.13/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    |     await app(scope, receive, sender)\n    |   File \"/usr/lib/python3.13/site-packages/starlette/routing.py\", line 715, in __call__\n    |     await self.middleware_stack(scope, receive, send)\n    |   File \"/usr/lib/python3.13/site-packages/starlette/routing.py\", line 735, in app\n    |     await route.handle(scope, receive, send)\n    |   File \"/usr/lib/python3.13/site-packages/starlette/routing.py\", line 288, in handle\n    |     await self.app(scope, receive, send)\n    |   File \"/usr/lib/python3.13/site-packages/starlette/routing.py\", line 76, in app\n    |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n    |   File \"/usr/lib/python3.13/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    |     raise exc\n    |   File \"/usr/lib/python3.13/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    |     await app(scope, receive, sender)\n    |   File \"/usr/lib/python3.13/site-packages/starlette/routing.py\", line 73, in app\n    |     response = await f(request)\n    |                ^^^^^^^^^^^^^^^^\n    |   File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 327, in app\n    |     content = await serialize_response(\n    |               ^^^^^^^^^^^^^^^^^^^^^^^^^\n    |     ...<9 lines>...\n    |     )\n    |     ^\n    |   File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 176, in serialize_response\n    |     raise ResponseValidationError(\n    |         errors=_normalize_errors(errors), body=response_content\n    |     )\n    | fastapi.exceptions.ResponseValidationError: 1 validation errors:\n    |   {'type': 'model_attributes_type', 'loc': ('response',), 'msg': 'Input should be a valid dictionary or object to extract fields from', 'input': None}\n    | \n    +------------------------------------\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/uvicorn/protocols/http/h11_impl.py\", line 407, in run_asgi\n    result = await app(  # type: ignore[func-returns-value]\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self.scope, self.receive, self.send\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/uvicorn/middleware/proxy_headers.py\", line 69, in __call__\n    return await self.app(scope, receive, send)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/applications.py\", line 1054, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/usr/lib/python3.13/site-packages/starlette/applications.py\", line 113, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/usr/lib/python3.13/site-packages/starlette/middleware/errors.py\", line 187, in __call__\n    raise exc\n  File \"/usr/lib/python3.13/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    await self.app(scope, receive, _send)\n  File \"/usr/lib/python3.13/site-packages/starlette/middleware/base.py\", line 185, in __call__\n    with collapse_excgroups():\n         ~~~~~~~~~~~~~~~~~~^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 162, in __exit__\n    self.gen.throw(value)\n    ~~~~~~~~~~~~~~^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/starlette/_utils.py\", line 82, in collapse_excgroups\n    raise exc\n  File \"/usr/lib/python3.13/site-packages/starlette/middleware/base.py\", line 187, in __call__\n    response = await self.dispatch_func(request, call_next)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/middleware/prometheus_auth_middleware.py\", line 47, in dispatch\n    response = await call_next(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/starlette/middleware/base.py\", line 163, in call_next\n    raise app_exc\n  File \"/usr/lib/python3.13/site-packages/starlette/middleware/base.py\", line 149, in coro\n    await self.app(scope, receive_or_disconnect, send_no_error)\n  File \"/usr/lib/python3.13/site-packages/starlette/middleware/cors.py\", line 85, in __call__\n    await self.app(scope, receive, send)\n  File \"/usr/lib/python3.13/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/usr/lib/python3.13/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/usr/lib/python3.13/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/usr/lib/python3.13/site-packages/starlette/routing.py\", line 715, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/usr/lib/python3.13/site-packages/starlette/routing.py\", line 735, in app\n    await route.handle(scope, receive, send)\n  File \"/usr/lib/python3.13/site-packages/starlette/routing.py\", line 288, in handle\n    await self.app(scope, receive, send)\n  File \"/usr/lib/python3.13/site-packages/starlette/routing.py\", line 76, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"/usr/lib/python3.13/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/usr/lib/python3.13/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/usr/lib/python3.13/site-packages/starlette/routing.py\", line 73, in app\n    response = await f(request)\n               ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 327, in app\n    content = await serialize_response(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<9 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 176, in serialize_response\n    raise ResponseValidationError(\n        errors=_normalize_errors(errors), body=response_content\n    )\nfastapi.exceptions.ResponseValidationError: 1 validation errors:\n  {'type': 'model_attributes_type', 'loc': ('response',), 'msg': 'Input should be a valid dictionary or object to extract fields from', 'input': None}\n\nINFO:     172.22.0.4:51086 - \"PATCH /scim/v2/Groups/a688bdfd-545a-48f7-896f-1942724f949a HTTP/1.1\" 500 Internal Server Error\nERROR:    Exception in ASGI application\n  + Exception Group Traceback (most recent call last):\n  |   File \"/usr/lib/python3.13/site-packages/starlette/_utils.py\", line 76, in collapse_excgroups\n  |     yield\n  |   File \"/usr/lib/python3.13/site-packages/starlette/middleware/base.py\", line 186, in __call__\n  |     async with anyio.create_task_group() as task_group:\n  |                ~~~~~~~~~~~~~~~~~~~~~~~^^\n  |   File \"/usr/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 685, in __aexit__\n  |     raise BaseExceptionGroup(\n  |         \"unhandled errors in a TaskGroup\", self._exceptions\n  |     )\n  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n  +-+---------------- 1 ----------------\n    | Traceback (most recent call last):\n    |   File \"/usr/lib/python3.13/site-packages/uvicorn/protocols/http/h11_impl.py\", line 407, in run_asgi\n    |     result = await app(  # type: ignore[func-returns-value]\n    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |         self.scope, self.receive, self.send\n    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |     )\n    |     ^\n    |   File \"/usr/lib/python3.13/site-packages/uvicorn/middleware/proxy_headers.py\", line 69, in __call__\n    |     return await self.app(scope, receive, send)\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/lib/python3.13/site-packages/fastapi/applications.py\", line 1054, in __call__\n    |     await super().__call__(scope, receive, send)\n    |   File \"/usr/lib/python3.13/site-packages/starlette/applications.py\", line 113, in __call__\n    |     await self.middleware_stack(scope, receive, send)\n    |   File \"/usr/lib/python3.13/site-packages/starlette/middleware/errors.py\", line 187, in __call__\n    |     raise exc\n    |   File \"/usr/lib/python3.13/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    |     await self.app(scope, receive, _send)\n    |   File \"/usr/lib/python3.13/site-packages/starlette/middleware/base.py\", line 185, in __call__\n    |     with collapse_excgroups():\n    |          ~~~~~~~~~~~~~~~~~~^^\n    |   File \"/usr/lib/python3.13/contextlib.py\", line 162, in __exit__\n    |     self.gen.throw(value)\n    |     ~~~~~~~~~~~~~~^^^^^^^\n    |   File \"/usr/lib/python3.13/site-packages/starlette/_utils.py\", line 82, in collapse_excgroups\n    |     raise exc\n    |   File \"/usr/lib/python3.13/site-packages/starlette/middleware/base.py\", line 187, in __call__\n    |     response = await self.dispatch_func(request, call_next)\n    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/lib/python3.13/site-packages/litellm/proxy/middleware/prometheus_auth_middleware.py\", line 47, in dispatch\n    |     response = await call_next(request)\n    |                ^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/lib/python3.13/site-packages/starlette/middleware/base.py\", line 163, in call_next\n    |     raise app_exc\n    |   File \"/usr/lib/python3.13/site-packages/starlette/middleware/base.py\", line 149, in coro\n    |     await self.app(scope, receive_or_disconnect, send_no_error)\n    |   File \"/usr/lib/python3.13/site-packages/starlette/middleware/cors.py\", line 85, in __call__\n    |     await self.app(scope, receive, send)\n    |   File \"/usr/lib/python3.13/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n    |   File \"/usr/lib/python3.13/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    |     raise exc\n    |   File \"/usr/lib/python3.13/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    |     await app(scope, receive, sender)\n    |   File \"/usr/lib/python3.13/site-packages/starlette/routing.py\", line 715, in __call__\n    |     await self.middleware_stack(scope, receive, send)\n    |   File \"/usr/lib/python3.13/site-packages/starlette/routing.py\", line 735, in app\n    |     await route.handle(scope, receive, send)\n    |   File \"/usr/lib/python3.13/site-packages/starlette/routing.py\", line 288, in handle\n    |     await self.app(scope, receive, send)\n    |   File \"/usr/lib/python3.13/site-packages/starlette/routing.py\", line 76, in app\n    |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n    |   File \"/usr/lib/python3.13/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    |     raise exc\n    |   File \"/usr/lib/python3.13/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    |     await app(scope, receive, sender)\n    |   File \"/usr/lib/python3.13/site-packages/starlette/routing.py\", line 73, in app\n    |     response = await f(request)\n    |                ^^^^^^^^^^^^^^^^\n    |   File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 327, in app\n    |     content = await serialize_response(\n    |               ^^^^^^^^^^^^^^^^^^^^^^^^^\n    |     ...<9 lines>...\n    |     )\n    |     ^\n    |   File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 176, in serialize_response\n    |     raise ResponseValidationError(\n    |         errors=_normalize_errors(errors), body=response_content\n    |     )\n    | fastapi.exceptions.ResponseValidationError: 1 validation errors:\n    |   {'type': 'model_attributes_type', 'loc': ('response',), 'msg': 'Input should be a valid dictionary or object to extract fields from', 'input': None}\n    | \n    +------------------------------------\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/uvicorn/protocols/http/h11_impl.py\", line 407, in run_asgi\n    result = await app(  # type: ignore[func-returns-value]\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self.scope, self.receive, self.send\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/uvicorn/middleware/proxy_headers.py\", line 69, in __call__\n    return await self.app(scope, receive, send)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/applications.py\", line 1054, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/usr/lib/python3.13/site-packages/starlette/applications.py\", line 113, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/usr/lib/python3.13/site-packages/starlette/middleware/errors.py\", line 187, in __call__\n    raise exc\n  File \"/usr/lib/python3.13/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    await self.app(scope, receive, _send)\n  File \"/usr/lib/python3.13/site-packages/starlette/middleware/base.py\", line 185, in __call__\n    with collapse_excgroups():\n         ~~~~~~~~~~~~~~~~~~^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 162, in __exit__\n    self.gen.throw(value)\n    ~~~~~~~~~~~~~~^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/starlette/_utils.py\", line 82, in collapse_excgroups\n    raise exc\n  File \"/usr/lib/python3.13/site-packages/starlette/middleware/base.py\", line 187, in __call__\n    response = await self.dispatch_func(request, call_next)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/middleware/prometheus_auth_middleware.py\", line 47, in dispatch\n    response = await call_next(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/starlette/middleware/base.py\", line 163, in call_next\n    raise app_exc\n  File \"/usr/lib/python3.13/site-packages/starlette/middleware/base.py\", line 149, in coro\n    await self.app(scope, receive_or_disconnect, send_no_error)\n  File \"/usr/lib/python3.13/site-packages/starlette/middleware/cors.py\", line 85, in __call__\n    await self.app(scope, receive, send)\n  File \"/usr/lib/python3.13/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/usr/lib/python3.13/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/usr/lib/python3.13/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/usr/lib/python3.13/site-packages/starlette/routing.py\", line 715, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/usr/lib/python3.13/site-packages/starlette/routing.py\", line 735, in app\n    await route.handle(scope, receive, send)\n  File \"/usr/lib/python3.13/site-packages/starlette/routing.py\", line 288, in handle\n    await self.app(scope, receive, send)\n  File \"/usr/lib/python3.13/site-packages/starlette/routing.py\", line 76, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"/usr/lib/python3.13/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/usr/lib/python3.13/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/usr/lib/python3.13/site-packages/starlette/routing.py\", line 73, in app\n    response = await f(request)\n               ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 327, in app\n    content = await serialize_response(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<9 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 176, in serialize_response\n    raise ResponseValidationError(\n        errors=_normalize_errors(errors), body=response_content\n    )\nfastapi.exceptions.ResponseValidationError: 1 validation errors:\n  {'type': 'model_attributes_type', 'loc': ('response',), 'msg': 'Input should be a valid dictionary or object to extract fields from', 'input': None}\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nmain-stable (Currently v1.72.2.rc)\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "Dudedrakes",
      "author_type": "User",
      "created_at": "2025-06-09T14:57:15Z",
      "updated_at": "2025-06-09T14:57:15Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11555/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11555",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11555",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:35.371442",
      "comments": []
    },
    {
      "issue_number": 11449,
      "title": "[Feature]: Spend Logs API Filter should maintain same response schema when date range filters are given",
      "body": "### The Feature\n\nWhen retrieving /spend/logs without any parameters we get all the spend log data, all invocations as cht-complitions spend for the completion ‚úîÔ∏è\nHowever the start_date and end_date parameters don't simply filter the ranges of results returned, they summerize the data. We are interested in un summarized data.\n\nThis feature is requesting a way to filter without summarization the output from /spend/logs\n\nBreaking method \n\n/spend/logs?start_date=today()&end_date=yesterday() -> returns just the records of completions within that time frame\nand then \n/spend/logs?start_date=today()&end_date=yesterday()&summarize=true -> returns a summary of the spend\n\n\nNon-breaking method\n/spend/logs?start_date=today()&end_date=yesterday()&summarize=false -> returns a summary of the spend\n\nwith a default parameter of summarize=true\n\n\n### Motivation, pitch\n\nThe motivation for this is extract and transform spendlog data into analytics dashboarding tools outside litellm. We capture this data hourly and presently we have to dump all data just to get the recent data.\n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "zachvida",
      "author_type": "User",
      "created_at": "2025-06-05T17:56:55Z",
      "updated_at": "2025-06-09T14:55:28Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11449/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "colesmcintosh"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11449",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11449",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:35.371449",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "@zachvida you should use the `/user/daily/activity` API for a better experience. \n\nIn prod, this is also safer to query as it stores the aggregates vs. each individual call \n\nhttps://litellm-api.up.railway.app/#/Internal%20User%20management/get_user_daily_activity_user_daily_activity_get",
          "created_at": "2025-06-06T06:04:03Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Closing as i think this issue is solved with the `/user/daily/activity`, `/team/daily/activity` and `/tag/daily/activity` api's \n\nThe same api's are also used by our usage dashboard on the UI. ",
          "created_at": "2025-06-06T06:05:06Z"
        },
        {
          "author": "zachvida",
          "body": "The provided endpoints do not provide the required information. I am interested in a complete metadata transaction history.\n\n/user/daily/activity does not provide the complete chat completion log, just a summary with slightly better filtering.\n/team/daily/activity provides a similar summary but at t",
          "created_at": "2025-06-09T13:51:59Z"
        }
      ]
    },
    {
      "issue_number": 11551,
      "title": "[Bug]: Litellm virtual key generation",
      "body": "### What happened?\n\nSeeing a weird behaviour where the /key/generate API puts token_id as token in the response.\n\nThis is the response received from the API gateway - \n\n{'key_alias': 'XX-nova-lite', 'duration': None, 'models': ['nova-lite'], 'spend': 0.0, 'max_budget': None, 'user_id': None, 'team_id': '8061d474-08aa-4319-be33-4d17858e8ece', 'max_parallel_requests': None, 'metadata': {'service': 'XX', 'model': 'nova-lite'}, 'tpm_limit': None, 'rpm_limit': None, 'budget_duration': None, 'allowed_cache_controls': [], 'config': {}, 'permissions': {}, 'model_max_budget': {}, 'model_rpm_limit': None, 'model_tpm_limit': None, 'guardrails': None, 'blocked': None, 'aliases': {}, 'object_permission': None, 'key': 'sk-iq3LAOD-XDxZj-gSvOY1RA', 'budget_id': None, 'tags': None, 'enforced_params': None, 'allowed_routes': [], 'key_name': 'sk-...Y1RA', 'expires': None, 'token_id': 'd5fb789e76b4aa40468d4397d213a791ee0c2aba765c72052570ea6dd4cdefc5', 'litellm_budget_table': None, 'token': 'd5fb789e76b4aa40468d4397d213a791ee0c2aba765c72052570ea6dd4cdefc5', 'created_by': 'default_user_id', 'updated_by': 'default_user_id', 'created_at': '2025-06-09T11:41:26.034000Z', 'updated_at': '2025-06-09T11:41:26.034000Z'}\n\n\nHere is the code which is generating it - \n\n    def create_key(self, deployment: Deployment, key_name: str, team_id: str, model: str) -> Optional[str]:\n        \"\"\"Create a virtual key in the deployment.\"\"\"\n        url = \"/key/generate\"\n        data = {\n            \"key_alias\": key_name,\n            \"team_id\": team_id,\n            \"models\": [model],\n            \"duration\": None,  # No expiration\n            \"metadata\": {\n                \"service\": key_name.split('-')[0],\n                \"model\": model\n            }\n        }\n\n        try:\n            client = self._get_client(deployment.name)\n            import pdb\n            pdb.set_trace()\n            response = client.post(url, json=data)\n            response.raise_for_status()\n            key_info = response.json()\n            token = key_info.get(\"token\", key_info.get(\"token_id\"))\n            self.validate_key(deployment, model_name=model, key=token)\n            print(f\"Created key '{key_name}' in {deployment.name}\")\n            return token\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.72.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "raunakbhansalioai",
      "author_type": "User",
      "created_at": "2025-06-09T11:51:29Z",
      "updated_at": "2025-06-09T11:53:39Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11551/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11551",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11551",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:35.612216",
      "comments": [
        {
          "author": "raunakbhansalioai",
          "body": "Relevant log lines - \n\n11:52:25 - LiteLLM Proxy:DEBUG: key_management_endpoints.py:411 - entered /key/generate                                                                                                                                                                     ‚îÇ\n‚îÇ 11:52:25 - LiteLLM Pr",
          "created_at": "2025-06-09T11:53:14Z"
        }
      ]
    },
    {
      "issue_number": 10941,
      "title": "[Bug]: Watsonx.ai does not allow space ID using the 'deployment/' endpoint",
      "body": "### What happened?\n\nWe are trying to call the Watsonx.ai custom deployment API for an deployed LLM. We are running into the issue that the implementation in LiteLLm when using the /deployment endpoints wants an the \"space id\" to be set as env paramater. If we set this parameter then  the implementation adds the space id also in the request body of the api call. However, the API return an error that \"space id\" cannot be set in the request body. \n\nWe are using the following code: \n\n`from litellm import completion\n\n\nos.environ[\"WATSONX_URL\"] = \"\"\nos.environ[\"WATSONX_DEPLOYMENT_SPACE_ID\"] = \"743ff63d-2f26-4261-9d11-ca963864bf96\"\nos.environ[\"WATSONX_ZENAPIKEY\"]=\"\"\n\nresponse = completion(\n    model=\"watsonx_text/deployment/d634c401-6dc5-4acb-9c41-3395b535ffa4\",\n    messages=[{\"content\": \"what is your favorite colour?\",\"role\": \"user\"}],\n)\nprint(response)`\n\nWe attached the error message when executing the above code. \n\n@ongkhaiwei: Mentioned you here because I have seen that you are active developing that implementation.\n\n### Relevant log output\n\n```shell\n13:48:51 - LiteLLM:DEBUG: utils.py:334 - \n\n13:48:51 - LiteLLM:DEBUG: utils.py:334 - Request to litellm:\n13:48:51 - LiteLLM:DEBUG: utils.py:334 - litellm.completion(model='watsonx\\_text/deployment/d634c401-6dc5-4acb-9c41-3395b535ffa4', messages=\\[{'content': 'what is your favorite colour?', 'role': 'user'}\\], api\\_version='2024-04-18')\n13:48:51 - LiteLLM:DEBUG: utils.py:334 - \n\n13:48:51 - LiteLLM:DEBUG: litellm\\_logging.py:455 - self.optional\\_params: {}\n13:48:51 - LiteLLM:DEBUG: utils.py:334 - SYNC kwargs\\[caching\\]: False; litellm.cache: None; kwargs.get('cache')\\['no-cache'\\]: False\n13:48:51 - LiteLLM:INFO: utils.py:2900 - \nLiteLLM completion() model= deployment/d634c401-6dc5-4acb-9c41-3395b535ffa4; provider = watsonx\\_text\n13:48:51 - LiteLLM:DEBUG: utils.py:2903 - \nLiteLLM: Params passed to completion() {'model': 'deployment/d634c401-6dc5-4acb-9c41-3395b535ffa4', 'functions': None, 'function\\_call': None, 'temperature': None, 'top\\_p': None, 'n': None, 'stream': None, 'stream\\_options': None, 'stop': None, 'max\\_tokens': None, 'max\\_completion\\_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence\\_penalty': None, 'frequency\\_penalty': None, 'logit\\_bias': None, 'user': None, 'custom\\_llm\\_provider': 'watsonx\\_text', 'response\\_format': None, 'seed': None, 'tools': None, 'tool\\_choice': None, 'max\\_retries': None, 'logprobs': None, 'top\\_logprobs': None, 'extra\\_headers': None, 'api\\_version': '2024-04-18', 'parallel\\_tool\\_calls': None, 'drop\\_params': None, 'allowed\\_openai\\_params': None, 'reasoning\\_effort': None, 'additional\\_drop\\_params': None, 'messages': \\[{'content': 'what is your favorite colour?', 'role': 'user'}\\], 'thinking': None}\n13:48:51 - LiteLLM:DEBUG: utils.py:2906 - \nLiteLLM: Non-Default params passed to completion() {}\n13:48:51 - LiteLLM:DEBUG: utils.py:334 - Final returned optional params: {}\n13:48:51 - LiteLLM:DEBUG: litellm\\_logging.py:455 - self.optional\\_params: {}\n13:48:51 - LiteLLM:DEBUG: litellm\\_logging.py:898 - POST Request Sent from LiteLLM:\ncurl -X POST \\\\\n[https://abc.net/deployments/d634c401-6dc5-4acb-9c41-3395b535ffa4/text/generation?version=2024-03-13](https://abc.net/ml/v1/deployments/d634c401-6dc5-4acb-9c41-3395b535ffa4/text/generation?version=2024-03-13) \\\\\n-H 'Content-Type: ap\\*\\*\\*\\*on' -H 'Accept: ap\\*\\*\\*\\*on' -H 'Authorization: Ze\\*\\*\\*\\*o=' \\\\\n-d '{'input': 'what is your favorite colour?', 'moderations': {}, 'parameters': {}, 'space\\_id': '743ff63d-2f26-4261-9d11-ca963864bf96'}'\n\n13:48:51 - LiteLLM:DEBUG: get\\_api\\_base.py:62 - Error occurred in getting api base - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=deployment/d634c401-6dc5-4acb-9c41-3395b535ffa4\n Pass model as E.g. For 'Huggingface' inference endpoints pass in \\`completion(model='huggingface/starcoder',..)\\` Learn more: [https://docs.litellm.ai/docs/providers](https://docs.litellm.ai/docs/providers)\n13:48:51 - LiteLLM:DEBUG: exception\\_mapping\\_utils.py:2261 - Logging Details: logger\\_fn - None | callable(logger\\_fn) - False\n13:48:51 - LiteLLM:DEBUG: litellm\\_logging.py:2156 - Logging Details LiteLLM-Failure Call: \\[\\]\n```\n\n`APIConnectionError: litellm.APIConnectionError: Watsonx\\_textException - {\"errors\":\\[{\"code\":\"json\\_validation\\_error\",\"message\":\"Json document validation error: 'project\\_id' or 'space\\_id' cannot be specified in the request body\",\"more\\_info\":\"[https://cloud.ibm.com/apidocs/watsonx-ai-cp](https://cloud.ibm.com/apidocs/watsonx-ai-cp)\"}\\],\"trace\":\"471392fb-75ca-408a-bd4c-a81a8336e174\",\"status\\_code\":400}`\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.69.3\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "fisser001",
      "author_type": "User",
      "created_at": "2025-05-19T12:09:45Z",
      "updated_at": "2025-06-09T07:50:16Z",
      "closed_at": "2025-06-08T03:31:07Z",
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10941/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10941",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10941",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:35.849822",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Hey @fisser001 what should the expected behaviour here be? ",
          "created_at": "2025-05-20T05:08:27Z"
        },
        {
          "author": "fisser001",
          "body": "Hey @krrishdholakia : I think the ‚Äòspace_id‚Äô in the request body (payload) is not necessary at all. At least when I look at the documentation I don't find a ‚Äòspace_id‚Äô there: https://cloud.ibm.com/apidocs/watsonx-ai#deployments-text-generation\n\nIf I am correct, then the creation of the request would",
          "created_at": "2025-05-20T06:28:08Z"
        },
        {
          "author": "ritujawaghmore",
          "body": "Hi @krrishdholakia \nI‚Äôve attempted to implement potential fixes for the issue occurring when a user tries to use a custom model deployed on watsonx.ai.\nFixes Applied:\n1. Root Cause ‚Äì **Json document validation error** : Identified that model_id, project_id, and space_id were being passed unnecessari",
          "created_at": "2025-05-27T17:23:25Z"
        },
        {
          "author": "alexlang74",
          "body": "Hi all, \n\nis there an outlook when this will be fixed? Running into the same issue...  Thanks in advance!",
          "created_at": "2025-06-06T16:07:44Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Based on the docs, my understanding is that 'space_id' isn't used when calling a watsonx deployment model @alexlang74 @fisser001 is that right? ",
          "created_at": "2025-06-08T01:03:07Z"
        }
      ]
    },
    {
      "issue_number": 11546,
      "title": "[Bug]: Azure OpenAI backend does not raise asyncio.CancelledError on client disconnect (works with Bedrock)",
      "body": "### What happened?\n\nUsing **FastAPI** + **litellm** in streaming mode:\n\n* **Azure OpenAI backend**\n\n  * When the browser forcibly closes the SSE/stream connection *before* completion, the stream stops but **no exception is raised**.\n  * The `finally` block is executed only several minutes later.\n\n* **Amazon Bedrock backend**\n\n  * On the same client disconnect, an `asyncio.CancelledError` is raised **immediately**, the `finally` block runs right away, and the stream ends as expected.\n\nI would expect Azure OpenAI to behave the same way and raise `asyncio.CancelledError` immediately when the client disconnects.\n\n### Reproduction code\n\n```python\nimport asyncio\nfrom typing import AsyncGenerator\n\nfrom fastapi import APIRouter, FastAPI\nfrom fastapi.responses import StreamingResponse\nfrom litellm import CustomStreamWrapper, acompletion\nfrom litellm.types.utils import StreamingChoices\n\nrouter = APIRouter()\n\n@router.post(\"/test\")\nasync def test():\n    return StreamingResponse(generator(), media_type=\"text/event-stream\")\n\nasync def generator():\n    # Azure OpenAI:\n    #   model=\"azure/gpt-4o\"\n    #   api_base=\"https://foo-openai.openai.azure.com\"\n    #   aws_region_name=None\n    #\n    # Amazon Bedrock:\n    #   model=bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0\n    #   api_base=None\n    #   aws_region_name=\"ap-northeast-1\"\n    try:\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\",   \"content\": \"Please provide a long answer.\"},\n        ]\n\n        response = await acompletion(\n            messages=messages,\n            model=model\n            api_base=api_base\n            aws_region_name=aws_region_name,\n            stream=True,\n        )\n\n        if isinstance(response, (CustomStreamWrapper, AsyncGenerator)):\n            async for chunk in response:\n                choice = chunk.choices[0]\n                if not isinstance(choice, StreamingChoices):\n                    raise RuntimeError(f\"Unexpected type: {type(choice)}\")\n                content = choice.delta.content\n                if content:\n                    yield f\"data: {content}\\n\\n\"\n\n    except asyncio.CancelledError as e:\n        # Bedrock raises this immediately; Azure does not\n        print(\"asyncio.CancelledError\", e)\n        raise\n    except Exception as e:\n        print(\"Exception:\", e)\n        raise\n    finally:\n        print(\"finally called\")\n\napp = FastAPI()\napp.include_router(router)\n```\n\nTo reproduce:\n\n1. Start the FastAPI server.\n2. Hit `/test` from a browser or curl, then abort the request (e.g., close tab or press Ctrl-C).\n3. Observe the different behaviors for Bedrock vs Azure OpenAI.\n\n### Environment\n\n| Library         | Version                      |\n| --------------- | ---------------------------- |\n| **litellm**     | 1.72.2                       |\n| **FastAPI**     | 0.115.12                     |\n| Python          | 3.13.2                       |\n| Backends tested | Azure OpenAI, Amazon Bedrock |\n\n### Expected behavior\n\n`asyncio.CancelledError` should be raised immediately on client disconnect for the Azure OpenAI backend, mirroring Bedrock‚Äôs behavior, so that cleanup in `finally` runs promptly.\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "nshmura",
      "author_type": "User",
      "created_at": "2025-06-09T07:15:47Z",
      "updated_at": "2025-06-09T07:15:47Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11546/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11546",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11546",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:36.143045",
      "comments": []
    },
    {
      "issue_number": 11544,
      "title": "[Bug]: Anthropic messages provider config not found for model",
      "body": "### What happened?\n\nWhen I use \n\"model\": \"anthropic.claude-sonnet-4\",\nAn error occuredÔºö\nAnthropic messages provider config not found for model.\nHow to fix this?\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.0\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "biscuit279",
      "author_type": "User",
      "created_at": "2025-06-09T03:24:03Z",
      "updated_at": "2025-06-09T03:29:32Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11544/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11544",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11544",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:36.143065",
      "comments": [
        {
          "author": "biscuit279",
          "body": "litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=anthropic.claude-sonnet-4\n\nBug message above.",
          "created_at": "2025-06-09T03:29:32Z"
        }
      ]
    },
    {
      "issue_number": 8025,
      "title": "[Bug]: latency-based routing fails on openai embedders with \"Object of type timedelta is not JSON serializable\"",
      "body": "### What happened?\n\nLatency-based routing fails for openai embedders.\n- this occurs only with embedding endpoints and not with completion endpoints\n- it is still occurring on 1.57.8 and 1.58.2\n\nSee error in the logs\n\nRelevant litellm config snippet (elided):\n\n```\nrouter_settings:\n  routing_strategy: latency-based-routing\n  routing_strategy_args:\n    lowest_latency_buffer: 0.25\n    ttl: 300\n    max_latency_list_size: 200\n\nmodel_list:\n  - model_name: text-embedding-3-small-8k\n    litellm_params:\n      model: openai/text-embedding-3-small\n```\n\n### Relevant log output\n\n```shell\nLiteLLM Redis Caching: async set() - Got exception from REDIS Object of type timedelta is not JSON serializable, Writing value={\"text-embedding-3-small-8k\":{\"2025-01-21-18-35\":{\"rpm\":0,\"tpm\":0},\"2025-01-21-18-38\":{\"rpm\":1,\"tpm\":0},\"2025-01-21-18-45\":{\"rpm\":1,\"tpm\":0},\"2025-01-21-18-48\":{\"rpm\":1,\"tpm\":0},\"2025-01-21-18-51\":{\"rpm\":2,\"tpm\":0},\"2025-01-21-18-57\":{\"rpm\":2,\"tpm\":0},\"2025-01-21-18-59\":{\"rpm\":1,\"tpm\":0},\"2025-01-21-19-01\":{\"rpm\":1,\"tpm\":0},\"2025-01-21-19-08\":{\"rpm\":1,\"tpm\":0},\"2025-01-21-19-10\":{\"rpm\":1,\"tpm\":0},\"2025-01-21-19-14\":{\"rpm\":1,\"tpm\":0},\"2025-01-21-19-15\":{\"rpm\":1,\"tpm\":0},\"2025-01-21-19-17\":{\"rpm\":2,\"tpm\":0},\"2025-01-21-19-18\":{\"rpm\":1,\"tpm\":0},\"2025-01-21-19-24\":{\"rpm\":1,\"tpm\":0},\"2025-01-21-19-25\":{\"rpm\":1,\"tpm\":0},\"2025-01-21-19-26\":{\"rpm\":1,\"tpm\":0},\"2025-01-21-19-28\":{\"rpm\":1,\"tpm\":0},\"2025-01-21-19-29\":{\"rpm\":1,\"tpm\":0},\"2025-01-21-19-32\":{\"rpm\":2,\"tpm\":0},\"2025-01-21-19-33\":{\"rpm\":1,\"tpm\":0},\"2025-01-21-19-34\":{\"rpm\":1,\"tpm\":0},\"latency\":[0,\"datetime.timedelta(microseconds=183331)\",\"datetime.timedelta(microseconds=625509)\",\"datetime.timedelta(microseconds=207349)\",\"datetime.timedelta(microseconds=857669)\",\"datetime.timedelta(microseconds=201552)\",\"datetime.timedelta(microseconds=206705)\",\"datetime.timedelta(microseconds=201223)\",\"datetime.timedelta(microseconds=420940)\",\"datetime.timedelta(microseconds=167972)\",\"datetime.timedelta(microseconds=191755)\",\"datetime.timedelta(microseconds=174823)\",\"datetime.timedelta(microseconds=200396)\",\"datetime.timedelta(microseconds=228687)\",\"datetime.timedelta(microseconds=190199)\",\"datetime.timedelta(microseconds=220752)\",\"datetime.timedelta(microseconds=243372)\",\"datetime.timedelta(microseconds=169139)\",\"datetime.timedelta(microseconds=176702)\",\"datetime.timedelta(microseconds=168918)\",\"datetime.timedelta(microseconds=257657)\",\"datetime.timedelta(microseconds=262806)\",\"datetime.timedelta(microseconds=218784)\",\"datetime.timedelta(microseconds=337701)\",\"datetime.timedelta(microseconds=428396)\",\"datetime.timedelta(microseconds=206356)\"]}\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.58.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "micahjsmith",
      "author_type": "User",
      "created_at": "2025-01-27T16:19:31Z",
      "updated_at": "2025-06-09T00:02:12Z",
      "closed_at": "2025-06-09T00:02:12Z",
      "labels": [
        "bug",
        "unable to repro",
        "mlops user request",
        "stale"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8025/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "krrishdholakia"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8025",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8025",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:36.325758",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Hi @micahjsmith is this still an issue? \n\nI'm not able to repro the error log ",
          "created_at": "2025-03-02T03:15:04Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Config:\n```yaml\nrouter_settings:\n  routing_strategy: latency-based-routing\n  routing_strategy_args:\n    lowest_latency_buffer: 0.25\n    ttl: 300\n    max_latency_list_size: 200\n\nmodel_list:\n  - model_name: text-embedding-3-small-8k\n    litellm_params:\n      model: openai/text-embedding-3-small\n      ",
          "created_at": "2025-03-02T03:15:40Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-01T00:02:11Z"
        }
      ]
    },
    {
      "issue_number": 8146,
      "title": "[Bug]: custom_llm_provider deepseek - unhealthy health status / invalid base_url",
      "body": "### What happened?\n\nA bug happened!\n\nlitellm/litellm:v1.59.8-stable (and older)\nwebui -> /health Models\n\nIncorrect base_url when checking health status for deepseek models. Access from the code to this model is correct.\n\n### Relevant log output\n\n```shell\n\"unhealthy_endpoints\": [\n    {\n      \"custom_llm_provider\": \"deepseek\",\n      \"model\": \"deepseek/deepseek-reasoner\",\n      \"cache\": {\n        \"no-cache\": true\n      },\n      \"error\": \"litellm.AuthenticationError: AuthenticationError: DeepseekException - Error code: 401 - {'error': {'message': 'Incorrect API key provided: xxxxxxxxxxxxxxxxxxxx. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\\nHave you set 'mode' - https://docs.litellm.ai/docs/proxy/health#embedding-models\\nstack trace: Traceback (most recent call last):\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/llms/openai/openai.py\\\", line 770, in acompletion\\n    headers, response = await self.make_openai_chat_completion_request(\\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n    ...<4 lines>...\\n    )\\n    ^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/logging_utils.py\\\", line 131, in async_wrapper\\n    result = await func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/llms/openai/openai.py\\\", line 418, in make_openai_chat_completion_request\\n    raise e\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/llms/openai/openai.py\\\", line 400, in make_openai_chat_completion_request\\n    await openai_aclient.chat.completions.with_raw_response.create(\\n        **data, timeout=timeout\\n    )\\n  File \\\"/usr/lib/python3.13/site-packages/openai/_legacy_response.py\\\", line 373, in wrapped\\n    return cast(LegacyAPIResponse[R],\"\n    },\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.59.8-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "hipotures",
      "author_type": "User",
      "created_at": "2025-01-31T17:27:22Z",
      "updated_at": "2025-06-09T00:02:11Z",
      "closed_at": "2025-06-09T00:02:11Z",
      "labels": [
        "bug",
        "awaiting: user response",
        "mlops user request",
        "stale"
      ],
      "label_count": 4,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8146/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8146",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8146",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:36.564941",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "> Incorrect API key provided: xxxxxxxxxxxxxxxxxxxx\n\nThis looks like it's an auth error for calling deepseek. \n\nDoes the chat completion call work? @hipotures ",
          "created_at": "2025-02-01T19:35:31Z"
        },
        {
          "author": "hipotures",
          "body": "```\nimport os \nimport litellm\nfrom litellm import completion\n\nmessages = [{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n\nresponse = completion(\n    messages=messages,\n    model=\"litellm_proxy/deepseek-chat\",\n    api_base = \"https://litellm-proxy-xxx.app\",\n    api_key = \"xxxx\",\n)\n\nprint(respons",
          "created_at": "2025-02-02T09:37:53Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Can you share the config yaml?\n\nTrying to repro the setup \n",
          "created_at": "2025-02-02T15:30:36Z"
        },
        {
          "author": "hipotures",
          "body": "Created from WebUI as Model DB.\n\n![Image](https://github.com/user-attachments/assets/66aa96f5-f47b-4b4b-a5f2-5f122a06dc39)\n![Image](https://github.com/user-attachments/assets/80c1fc01-d1e1-4f68-bcc4-77a449e795ec)\n\n```\n\n{\n  \"model_name\": \"deepseek-reasoner\",\n  \"litellm_params\": {\n    \"custom_llm_prov",
          "created_at": "2025-02-02T15:41:38Z"
        },
        {
          "author": "Krzemq",
          "body": "Got the same error, it looks like it is trying to reach openai api instead of deepseek.\n\n`\n      \"error\": \"litellm.AuthenticationError: AuthenticationError: DeepseekException - Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-f2913***********************cafa. You can find your",
          "created_at": "2025-03-03T16:07:26Z"
        }
      ]
    },
    {
      "issue_number": 8564,
      "title": "[Bug]: Can't stream Deepseek on Vertex AI  Model Garden",
      "body": "### What happened?\n\nI can't get Deepseek deployed on Vertex AI Model Garden to work with streaming.\n\nThis:\n\n```\n\nresponse = completion(\n  model=\"vertex_ai/<MY-MODEL-ID>\",\n  messages=[{\"role\": \"user\", \"content\": \"Tell me a joke.\"}],\n  vertex_credentials=vertex_credentials_json,\n  vertex_project=\"<MY-PROJECT-ID>\", \n  vertex_location=\"<MY-LOCATION>\",\n  stream=True\n)\n\nfor chunk in response:\n    print(chunk)\n\nprint(\"Response:\", response)\n```\n\n\n### Relevant log output\n\n```shell\nlitellm.exceptions.APIConnectionError: litellm.APIConnectionError: 400 The output data is not valid json. Original output: {\"predictions\": [\"?\\n\\n\"]}{\"predictions\": [\"</think>\"]}{\"predictions\": [\"<think>\"]}{\"predictions\": [\"\\n\\n\"]}{\"predictions\": [\"</think>\"]}{\"predictions\": [\"\\n\\n\"]}{\"predictions\": [\"Sure\"]}{\"predictions\": [\",\"]}{\"predictions\": [\" here\"]}{\"predictions\": [\"'s\"]}{\"predictions\": [\" a\"]}{\"predictions\": [\" light\"]}{\"predictions\": [\"-hearted\"]}{\"predictions\": [\" joke\"]}{\"predictions\": [\" for\"]}{\"predictions\": [\" you\"]}.\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1739618750.912123 1044321 init.cc:232] grpc_wait_for_shutdown_with_timeout() timed out.\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.61.3\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "emorling",
      "author_type": "User",
      "created_at": "2025-02-15T21:33:45Z",
      "updated_at": "2025-06-09T00:02:10Z",
      "closed_at": "2025-06-09T00:02:10Z",
      "labels": [
        "bug",
        "stale",
        "feb 2025"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8564/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8564",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8564",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:36.775823",
      "comments": [
        {
          "author": "emorling",
          "body": "Is there anything I can try to get this to work?",
          "created_at": "2025-02-18T11:25:34Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Similar to bedrock, I think we'll need to translate this. \n",
          "created_at": "2025-03-02T20:42:45Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-01T00:02:07Z"
        }
      ]
    },
    {
      "issue_number": 8809,
      "title": "[Bug]: Vertex AI - files stored in GCS without file extension are not processed",
      "body": "### What happened?\n\nVertex AI LLMs support Google Cloud Storage Urls:\n\n```\nclient = genai.Client(http_options=HttpOptions(api_version=\"v1\"))\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash-001\",\n    contents=[\n        \"What is shown in this image?\",\n        Part.from_uri(\n            file_uri=\"gs://cloud-samples-data/generative-ai/image/scones.jpg\",\n            mime_type=\"image/jpeg\",\n        ),\n    ],\n) \n```\nNative Vertex AI API supports explicit specification the mime_type (as it could be seen in the previous example)\nThis means that the file URI doesn't have to have a valid file extension that maps to a mime-type so the following example works as well:\n\n```\nclient = genai.Client(http_options=HttpOptions(api_version=\"v1\"))\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash-001\",\n    contents=[\n        \"What is shown in this image?\",\n        Part.from_uri(\n            file_uri=\"gs://cloud-samples-data/generative-ai/image/scones\",\n            mime_type=\"image/jpeg\",\n        ),\n    ],\n) \n```\n\nSine liteLLM tries to guess the mime_type based on url extension providing a Google Cloud Storage url without extension results in failure.\n\nThis issue can be reproduced by\n\n```\nimport litellm\nimport os\n\nos.environ[\"GEMINI_API_KEY\"] = \"\" \n\nlitellm.set_verbose = True # üëà See Raw call \n\nmodel = \"gemini/gemini-1.5-flash\"\nresponse = litellm.completion(\n    model=model,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"Please summarize the file.\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": \"gs://...\" # üëà SET THE cloud storage bucket url with the url that doesn't have a valid file extension (like .jpeg)\n                },\n            ],\n        }\n    ],\n)\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.61.16\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "alexanderar",
      "author_type": "User",
      "created_at": "2025-02-25T16:54:05Z",
      "updated_at": "2025-06-09T00:02:03Z",
      "closed_at": "2025-06-09T00:02:03Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8809/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8809",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8809",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:36.975007",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Hey @alexanderar acknowledging this. How would you want us to deal with this? ",
          "created_at": "2025-03-02T20:40:27Z"
        },
        {
          "author": "alexanderar",
          "body": "I already opened a PR that deals with it. You can see it in the references (https://github.com/BerriAI/litellm/pull/8802)",
          "created_at": "2025-03-03T05:26:07Z"
        },
        {
          "author": "krrishdholakia",
          "body": "responded there üëç ",
          "created_at": "2025-03-03T06:04:17Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-02T00:01:40Z"
        }
      ]
    },
    {
      "issue_number": 8952,
      "title": "[Feature]: Can base_url be set instead of using the official base_url when calling the model",
      "body": "### The Feature\n\nCan base_url be set instead of using the official base_url when calling the model\n\n### Motivation, pitch\n\nCan base_url be set instead of using the official base_url when calling the model\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "baiyi-os",
      "author_type": "User",
      "created_at": "2025-03-03T09:21:59Z",
      "updated_at": "2025-06-09T00:02:01Z",
      "closed_at": "2025-06-09T00:02:01Z",
      "labels": [
        "enhancement",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8952/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8952",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8952",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:37.193125",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-02T00:01:34Z"
        }
      ]
    },
    {
      "issue_number": 8953,
      "title": "[Bug]: finish_reason is content_filter but litellm.ContentPolicyViolationError not raised and content_filter_results are passing",
      "body": "### What happened?\n\nI have the following:\n\n```\nresp = client.chat.completions.create(\n    model=\"azure/gpt-4o-2024-08-06\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke!\"}],\n    extra_body=dict(\n        temperature=0,\n    ),\n)\nresp.model_dump()\n# result: \n{'id': 'chatcmpl-B6yR7yHGU7YYj2CYEt5GurtH7q7oE',\n 'choices': [{'finish_reason': 'content_filter',\n   'index': 0,\n   'logprobs': None,\n   'message': {'content': None,\n    'refusal': None,\n    'role': 'assistant',\n    'audio': None,\n    'function_call': None,\n    'tool_calls': None}}],\n 'created': 1741001985,\n 'model': 'azure/gpt-4o-2024-08-06',\n 'object': 'chat.completion',\n 'service_tier': None,\n 'system_fingerprint': 'fp_b705f0c291',\n 'usage': {'completion_tokens': 14,\n  'prompt_tokens': 12,\n  'total_tokens': 26,\n  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n   'audio_tokens': 0,\n   'reasoning_tokens': 0,\n   'rejected_prediction_tokens': 0},\n  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n 'prompt_filter_results': [{'prompt_index': 0,\n   'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'},\n    'self_harm': {'filtered': False, 'severity': 'safe'},\n    'sexual': {'filtered': False, 'severity': 'safe'},\n    'violence': {'filtered': False, 'severity': 'safe'}}}]}\n```\nyou can see there is no content in the message and `finish_reason` is `content_filter`. The problem is that this is returned as a valid message, instead of raising `litellm.ContentPolicyViolationError` (which is what I assume should happen, as this doesn't trigger fallbacks and stuff). \n\nI am really confused about when this happens, because it seems that it does work in some cases as expected, and I haven't figured out the pattern yet. It's called via proxy, so something like \"exhausting all possible endpoints\" could play a role as well? Not sure.\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.61.7\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "hnykda",
      "author_type": "User",
      "created_at": "2025-03-03T11:48:40Z",
      "updated_at": "2025-06-09T00:02:00Z",
      "closed_at": "2025-06-09T00:02:00Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8953/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8953",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8953",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:37.410859",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-02T00:01:33Z"
        }
      ]
    },
    {
      "issue_number": 8955,
      "title": "[Feature]: Use timstamp with timezone in LLM Proxy DB schema instead",
      "body": "### The Feature\n\nEffectively replacing  all occurrences of dates such as `\"startTime\" timestamp without time zone not null` with `\"startTime\" timestamp with time zone not null`\n\n### Motivation, pitch\n\n...as it's confusing what timezone it's stored in (I think UTC) when orienting in the DB\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "hnykda",
      "author_type": "User",
      "created_at": "2025-03-03T12:11:31Z",
      "updated_at": "2025-06-09T00:01:59Z",
      "closed_at": "2025-06-09T00:01:59Z",
      "labels": [
        "enhancement",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8955/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8955",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8955",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:37.611990",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-02T00:01:32Z"
        }
      ]
    },
    {
      "issue_number": 8960,
      "title": "Erro in Vertex AI call with Streamming and async mode",
      "body": "I am using the code \n\nfile_path = 'cred.json'\n\n# Load the JSON file\nwith open(file_path, 'r') as file:\n    vertex_credentials = json.load(file)\n\n# Convert to JSON string\nvertex_credentials_json = json.dumps(vertex_credentials)\n\n## COMPLETION CALL \nresponse = await acompletion(\n      vertex_credentials=vertex_credentials_json,\n      model=\"vertex_ai/gemini-2.0-flash-exp\",\n      messages=messages,\n    #   tools=tools,\n      stream = True\n)\n\n    async for line in response:\n        print(line)\n\n\nwith the follow erro \"litellm.APIConnectionError: Vertex_ai_betaException\"\n\n----------------------------------------------------------------------\n\nBut when I use the google lib, google-genai, it works\n\nimport os\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"key_dev.json\"\n\naiplatform.init(project=\"project_id\" ) #Replace with your project ID\n\nclient = genai.Client(\n      vertexai=True,\n      project=\"project_id\",\n      location=\"us-central1\"  )\n\nresponse = await client.aio.models.generate_content_stream(\n    model='gemini-1.5-flash', contents='oi'\n)\n\nlines = []\n\nasync for line in response:\n    lines.append(line)\n    print(line)\n\nanybody with the same error?",
      "state": "closed",
      "author": "caanpaip",
      "author_type": "User",
      "created_at": "2025-03-03T21:08:12Z",
      "updated_at": "2025-06-09T00:01:58Z",
      "closed_at": "2025-06-09T00:01:58Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8960/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8960",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8960",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:40.348845",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-02T00:01:31Z"
        }
      ]
    },
    {
      "issue_number": 11542,
      "title": "[Bug]: Upgrade boto3",
      "body": "### What happened?\n\nSome core libraries (such as aiobotocore) recent versions pin boto3/botocore to >1.35. Litellm uses 1.34.34, which is from Feb 2024. Would it be possible to upgrade boto3 to a newer version?\n\n### Relevant log output\n\n```shell\nCollecting botocore<1.35.0,>=1.34.34 (from boto3==1.34.34->litellm[proxy]==1.72.2)\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.72.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "hakan-77",
      "author_type": "User",
      "created_at": "2025-06-08T23:03:35Z",
      "updated_at": "2025-06-08T23:03:44Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11542/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11542",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11542",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:40.522993",
      "comments": []
    },
    {
      "issue_number": 11529,
      "title": "[Bug]: Gemini - Invalid content part type: file",
      "body": "### What happened?\n\nEvent:\n```json\n{\n  \"event\": \"on_chain_start\",\n  \"data\": {\n    \"input\": {\n      \"messages\": [\n        {\n          \"lc\": 1,\n          \"type\": \"constructor\",\n          \"id\": [\n            \"langchain_core\",\n            \"messages\",\n            \"HumanMessage\"\n          ],\n          \"kwargs\": {\n            \"content\": [\n              {\n                \"type\": \"text\",\n                \"text\": \"summarize\"\n              },\n              {\n                \"type\": \"file\",\n                \"file\": {\n                  \"file_id\": \"https://neighborly-cuttlefish-201.convex.cloud/api/storage/3ac9f7f1-c8e9-4faa-8ca3-5748190c2c1c\",\n                  \"format\": \"application/pdf\"\n                }\n              }\n            ],\n            \"additional_kwargs\": {},\n            \"response_metadata\": {}\n          }\n        }\n      ]\n    }\n  },\n  \"name\": \"LangGraph\",\n  \"tags\": [],\n  \"run_id\": \"d88f912e-fd5e-4020-b9c0-12b402929166\",\n  \"metadata\": {\n    \"thread_id\": \"k172eq6t9d423grc2k3m687pb97hfeh0\",\n    \"langgraph_step\": 2,\n    \"langgraph_node\": \"agent\",\n    \"langgraph_triggers\": [\n      \"branch:passToShouldPlan:condition:agent\"\n    ],\n    \"langgraph_path\": [\n      \"__pregel_pull\",\n      \"agent\"\n    ],\n    \"langgraph_checkpoint_ns\": \"agent:3f11b1a7-0999-5c42-9f1f-a5d7f6cd452a\",\n    \"__pregel_task_id\": \"3f11b1a7-0999-5c42-9f1f-a5d7f6cd452a\",\n    \"checkpoint_ns\": \"agent:3f11b1a7-0999-5c42-9f1f-a5d7f6cd452a\"\n  }\n}\n```\n\nError:\n```log\nUncaught Error: Uncaught Error: 400 litellm.BadRequestError: OpenAIException - Error code: 400 - [{'error': {'code': 400, 'message': 'Invalid content part type: file', 'status': 'INVALID_ARGUMENT'}}]. Received Model Group=gemini-2.5-flash\nAvailable Model Group Fallbacks=None\n```\n\nConfig:\n```yaml\nmodel_list:\n  - model_name: gemini-2.5-flash\n    litellm_params:\n      model: openai/gemini-2.5-flash-preview-05-20\n      base_url: https://generativelanguage.googleapis.com/v1beta/openai/\n      api_key: os.environ/GOOGLE_API_KEY\n      tags: [\"text\", \"image\", \"audio\", \"video\", \"pdf\"]\n  - model_name: embeddings\n    litellm_params:\n      model: gemini/text-embedding-004\n      api_key: os.environ/GOOGLE_API_KEY\n      tags: [\"text\", \"embeddings\"]\n\nlitellm_settings:\n  drop_params: true\n\ngeneral_settings:\n  master_key: os.environ/OPENAI_API_KEY\n```\n\n### Relevant log output\n\n```shell\nUncaught Error: Uncaught Error: 400 litellm.BadRequestError: OpenAIException - Error code: 400 - [{'error': {'code': 400, 'message': 'Invalid content part type: file', 'status': 'INVALID_ARGUMENT'}}]. Received Model Group=gemini-2.5-flash\nAvailable Model Group Fallbacks=None\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nghcr.io/berriai/litellm:main-latest\n\n### Twitter / LinkedIn details\n\nhttps://x.com/barre_of_lube",
      "state": "open",
      "author": "mantrakp04",
      "author_type": "User",
      "created_at": "2025-06-08T03:12:24Z",
      "updated_at": "2025-06-08T16:22:39Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11529/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11529",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11529",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:40.523018",
      "comments": [
        {
          "author": "mantrakp04",
          "body": "using gemini as the provider\n```yaml\nmodel_list:\n  - model_name: gemini-2.5-flash\n    litellm_params:\n      model: gemini/gemini-2.5-flash-preview-05-20\n      api_key: os.environ/GOOGLE_API_KEY\n      tags: [\"text\", \"image\", \"audio\", \"video\", \"pdf\"]\n  - model_name: embeddings\n    litellm_params:\n    ",
          "created_at": "2025-06-08T03:27:29Z"
        },
        {
          "author": "mantrakp04",
          "body": "Gemini also doesn't allow passing file urls\n\n```log\nUncaught Error: Uncaught Error: 400\ndata: {\n  \"error\": {\n    \"message\": \"litellm.BadRequestError: VertexAIException BadRequestError - b'{\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Unsupported file uri: https://neighborly-cuttlefish-201.convex.c",
          "created_at": "2025-06-08T16:22:39Z"
        }
      ]
    },
    {
      "issue_number": 7129,
      "title": "[Bug]:  end-user budget_duration stills not seem to reset ",
      "body": "### What happened?\n\nThis issue is the same as #5651 but i can't reopen it.\r\n\r\nThe fixed version (1.52.12) still have the issue and the customer budget are never reset\n\n### Relevant log output\n\n_No response_\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.52.15-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "gregoryboue",
      "author_type": "User",
      "created_at": "2024-12-10T07:44:52Z",
      "updated_at": "2025-06-08T15:39:15Z",
      "closed_at": "2025-06-08T15:39:15Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/7129/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "krrishdholakia"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/7129",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/7129",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:40.692573",
      "comments": [
        {
          "author": "gregoryboue",
          "body": "Is someone has same issue or any workaround ?",
          "created_at": "2025-01-02T15:48:22Z"
        },
        {
          "author": "ehsanul",
          "body": "I'm experiencing this as well. The workaround I've used is deleting the user and re-creating them with the same user id, which is not ideal obviously.\n\n@krrishdholakia There seems to be a fix in https://github.com/BerriAI/litellm/pull/8460, any blockers on that PR?",
          "created_at": "2025-03-20T19:55:55Z"
        },
        {
          "author": "mccahill",
          "body": "@gregoryboue and @ehsanul \nI'm also seeing this problem in release 1.67.4 - but the release notes for 1.68.0 say that it is fixed so it looks like it is time to update. see [https://docs.litellm.ai/release_notes/v1.68.0-stable#spend-tracking--budget-improvements](https://docs.litellm.ai/release_note",
          "created_at": "2025-05-17T15:31:19Z"
        }
      ]
    },
    {
      "issue_number": 11535,
      "title": "[Bug]:",
      "body": "### What happened?\n\nWhen I add model, before adding model, I test the connection. But it shows an error.\n\nConnection Test Results\nConnection to claude-4-sonnet-20250514 failed\nError:\nConnection test failed: 500\n\nTroubleshooting Details\nConnection test failed: 500 \n\nAPI Request\nNo request data available\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n 1.72.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "SNU-SE",
      "author_type": "User",
      "created_at": "2025-06-08T11:37:11Z",
      "updated_at": "2025-06-08T11:37:11Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11535/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11535",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11535",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:40.875161",
      "comments": []
    },
    {
      "issue_number": 11364,
      "title": "[Bug]: Wrong cost for Anthropic models, cached tokens cost not being correctly considered.",
      "body": "### What happened?\n\nI have noticed that spend amount for Anthropic models seem to be wrong where the cost of prompt tokens are been counted as a regular input instead of a cached token, Not sure if I am missing a configuration or something or if it is an actual bug.\n\n\n### Relevant log output\n\n```shell\nI can see that also on the logs.\n\n  \"usage\": {\n    \"total_tokens\": 31048,\n    \"prompt_tokens\": 30946,\n    \"completion_tokens\": 102,\n    \"prompt_tokens_details\": {\n      \"audio_tokens\": null,\n      \"cached_tokens\": 0\n    },\n    \"cache_read_input_tokens\": 30940,\n    \"completion_tokens_details\": {\n      \"audio_tokens\": null,\n      \"reasoning_tokens\": 0,\n      \"accepted_prediction_tokens\": null,\n      \"rejected_prediction_tokens\": null\n    },\n    \"cache_creation_input_tokens\": 460\n  },\n\n\nGave me:\n\nTokens:31048 (30946+102)\nCost:$0.096093\nCache Hit:False\nStatus:Success\n\n\nAnd I can see on the metadata the correct information (sensitive data replaced with [REDACTED]):\n\n{\n  \"batch_models\": null,\n  \"usage_object\": {\n    \"total_tokens\": 31048,\n    \"prompt_tokens\": 30946,\n    \"completion_tokens\": 102,\n    \"prompt_tokens_details\": {\n      \"audio_tokens\": null,\n      \"cached_tokens\": 0\n    },\n    \"cache_read_input_tokens\": 30940,\n    \"completion_tokens_details\": {\n      \"audio_tokens\": null,\n      \"reasoning_tokens\": 0,\n      \"accepted_prediction_tokens\": null,\n      \"rejected_prediction_tokens\": null\n    },\n    \"cache_creation_input_tokens\": 460\n  },\n  \"user_api_key\": \"[REDACTED]\",\n  \"applied_guardrails\": [],\n  \"user_api_key_alias\": \"[REDACTED]\",\n  \"user_api_key_org_id\": null,\n  \"requester_ip_address\": \"\",\n  \"user_api_key_team_id\": \"[REDACTED]\",\n  \"user_api_key_user_id\": \"[REDACTED]\",\n  \"guardrail_information\": null,\n  \"model_map_information\": {\n    \"model_map_key\": \"claude-3-7-sonnet-20250219\",\n    \"model_map_value\": {\n      \"key\": \"claude-3-7-sonnet-20250219\",\n      \"rpm\": null,\n      \"tpm\": null,\n      \"mode\": \"chat\",\n      \"max_tokens\": 128000,\n      \"supports_vision\": true,\n      \"litellm_provider\": \"anthropic\",\n      \"max_input_tokens\": 200000,\n      \"max_output_tokens\": 128000,\n      \"output_vector_size\": null,\n      \"supports_pdf_input\": true,\n      \"supports_reasoning\": true,\n      \"supports_web_search\": true,\n      \"input_cost_per_query\": null,\n      \"input_cost_per_token\": 0.000003,\n      \"supports_audio_input\": false,\n      \"supports_tool_choice\": true,\n      \"input_cost_per_second\": null,\n      \"output_cost_per_image\": null,\n      \"output_cost_per_token\": 0.000015,\n      \"supports_audio_output\": false,\n      \"supports_computer_use\": true,\n      \"output_cost_per_second\": null,\n      \"supported_openai_params\": [\n        \"stream\",\n        \"stop\",\n        \"temperature\",\n        \"top_p\",\n        \"max_tokens\",\n        \"max_completion_tokens\",\n        \"tools\",\n        \"tool_choice\",\n        \"extra_headers\",\n        \"parallel_tool_calls\",\n        \"response_format\",\n        \"user\",\n        \"reasoning_effort\",\n        \"web_search_options\",\n        \"thinking\"\n      ],\n      \"supports_prompt_caching\": true,\n      \"input_cost_per_character\": null,\n      \"supports_response_schema\": true,\n      \"supports_system_messages\": null,\n      \"output_cost_per_character\": null,\n      \"supports_function_calling\": true,\n      \"supports_native_streaming\": null,\n      \"input_cost_per_audio_token\": null,\n      \"supports_assistant_prefill\": true,\n      \"cache_read_input_token_cost\": 3e-7,\n      \"output_cost_per_audio_token\": null,\n      \"input_cost_per_token_batches\": null,\n      \"output_cost_per_token_batches\": null,\n      \"search_context_cost_per_query\": {\n        \"search_context_size_low\": 0.01,\n        \"search_context_size_high\": 0.01,\n        \"search_context_size_medium\": 0.01\n      },\n      \"supports_embedding_image_input\": false,\n      \"cache_creation_input_token_cost\": 0.00000375,\n      \"output_cost_per_reasoning_token\": null,\n      \"input_cost_per_token_above_128k_tokens\": null,\n      \"input_cost_per_token_above_200k_tokens\": null,\n      \"output_cost_per_token_above_128k_tokens\": null,\n      \"output_cost_per_token_above_200k_tokens\": null,\n      \"output_cost_per_character_above_128k_tokens\": null\n    }\n  },\n  \"mcp_tool_call_metadata\": null,\n  \"additional_usage_values\": {\n    \"prompt_tokens_details\": {\n      \"text_tokens\": null,\n      \"audio_tokens\": null,\n      \"image_tokens\": null,\n      \"cached_tokens\": 0\n    },\n    \"cache_read_input_tokens\": 30940,\n    \"completion_tokens_details\": {\n      \"text_tokens\": null,\n      \"audio_tokens\": null,\n      \"reasoning_tokens\": 0,\n      \"accepted_prediction_tokens\": null,\n      \"rejected_prediction_tokens\": null\n    },\n    \"cache_creation_input_tokens\": 460\n  },\n  \"user_api_key_team_alias\": \"[REDACTED]\",\n  \"vector_store_request_metadata\": null\n}\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.71.1\n\n### Twitter / LinkedIn details\n\n@das_rdsm",
      "state": "open",
      "author": "regismesquita",
      "author_type": "User",
      "created_at": "2025-06-03T13:24:32Z",
      "updated_at": "2025-06-08T10:10:30Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11364/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 1,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11364",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11364",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:40.875179",
      "comments": [
        {
          "author": "regismesquita",
          "body": "I did some more digging. when manually crafting requests it worked as expected calculating the right price with passthrough, openai and the /v1/messages endpoint. Still when using claude code I get the wrong value.\n\nI noticed on my logs that the only real difference that I can observe is that the ma",
          "created_at": "2025-06-06T18:01:16Z"
        },
        {
          "author": "regismesquita",
          "body": "The below seems to fix it... \n```diff\ndiff --git a/litellm/litellm_core_utils/llm_cost_calc/utils.py b/litellm/litellm_core_utils/llm_cost_calc/utils.py\nindex 3b3e15cae..cb060086d 100644\n--- a/litellm/litellm_core_utils/llm_cost_calc/utils.py\n+++ b/litellm/litellm_core_utils/llm_cost_calc/utils.py\n@",
          "created_at": "2025-06-06T19:30:24Z"
        }
      ]
    },
    {
      "issue_number": 11437,
      "title": "[Bug]:  stream response from /v1/chat/completions  should return same created param",
      "body": "### What happened?\n\n![Image](https://github.com/user-attachments/assets/76c71ec5-7f88-4473-89dd-0b1f65f5960b)\n\nlangchain's merge_dicts function will raise excepiton on different value.\n\n![Image](https://github.com/user-attachments/assets/5ba90017-6329-4120-b6a7-739281cbfd5c)\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.71.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "781574155",
      "author_type": "User",
      "created_at": "2025-06-05T10:22:18Z",
      "updated_at": "2025-06-08T03:50:08Z",
      "closed_at": "2025-06-08T03:50:08Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11437/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "krrishdholakia"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11437",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11437",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:41.055219",
      "comments": [
        {
          "author": "781574155",
          "body": "vllm return same created param. litellm modify it.",
          "created_at": "2025-06-05T10:23:26Z"
        }
      ]
    },
    {
      "issue_number": 11479,
      "title": "[Feature]: Support for Tool Calling with Amazon SageMaker AI",
      "body": "### The Feature\n\nAs of v0.32, DJL LMI container supports automatic tool calling powered by vLLM. This means that models can now perform tool calling. We should introduce native tool calling support for LiteLLM with Amazon SageMaker AI as model provider, given the user has deployed the model using the right container with the right tool parser.\n\n### Motivation, pitch\n\nCustomers want to build agentic AI solutions powered by Amazon SageMaker AI. Without support for native tool calling, this is impossible. By delivering this capability, customers will be able to use 1000s of models on Amazon SageMaker AI for agentic AI workloads, powered by LiteLLM. This will instantly make it compatible with all open-source frameworks that are already powered by LiteLLM under the hood.\n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\n@DGallitelli95 / https://www.linkedin.com/in/dgallitelli/",
      "state": "open",
      "author": "dgallitelli",
      "author_type": "User",
      "created_at": "2025-06-06T10:07:50Z",
      "updated_at": "2025-06-08T03:42:23Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11479/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11479",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11479",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:41.210720",
      "comments": [
        {
          "author": "dgallitelli",
          "body": "For reference: https://dgallitelli95.medium.com/tool-calling-with-amazon-sagemaker-ai-and-djl-serving-inference-6a97dc854881 ",
          "created_at": "2025-06-06T10:08:41Z"
        },
        {
          "author": "sumansuhag",
          "body": "Hi!!!!\n\nThanks, dgallitelli, for laying out this feature request so clearly! Adding native tool support for Amazon SageMaker AI models in LiteLLM is a key improvement.\n\nYou're absolutely right: tool calling is essential for creating effective AI solutions. Connecting thousands of SageMaker models wi",
          "created_at": "2025-06-08T03:42:22Z"
        }
      ]
    },
    {
      "issue_number": 11245,
      "title": "[Enhancement]: Support vertex ai passthrough, when vertex credentials set in env var not via `vertex_credentials` param",
      "body": "### What happened?\n\nI couldn't get Vertex AI pass-through to work with GCP Service Account authentication. My setup is like so:\n```yaml\nmodel_list:\n  - model_name: vertex_ai/*\n    litellm_params:\n      model: vertex_ai/*\n      vertex_project: XXX\n      vertex_location: us-central1\n      use_in_pass_through: true\n```\nDeployed on Google Cloud Run. Normal requests through ```vertex_ai``` models work fine, but trying to use pass-through, I get the below error.\n\n### Relevant log output\n\n```shell\nNo credentials found on proxy for project_name=XXX + location=us-central1, check /model/info for allowed project + region combinations with use_in_pass_through: true. Headers were passed through directly but request failed with error: { \"error\": { \"code\": 401, \"message\": \"Request is missing required authentication credential. Expected OAuth 2 access token, login cookie or other valid authentication credential. See https://developers.google.com/identity/sign-in/web/devconsole-project.\", \"status\": \"UNAUTHENTICATED\", \"details\": [ { \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\", \"reason\": \"CREDENTIALS_MISSING\", \"domain\": \"googleapis.com\", \"metadata\": { \"service\": \"aiplatform.googleapis.com\", \"method\": \"google.cloud.aiplatform.v1.PredictionService.PredictLongRunning\" } } ] } }\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.71.2-nightly\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "Ziyann",
      "author_type": "User",
      "created_at": "2025-05-29T20:52:59Z",
      "updated_at": "2025-06-08T03:31:07Z",
      "closed_at": "2025-06-08T03:31:07Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11245/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "krrishdholakia",
        "colesmcintosh"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11245",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11245",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:41.427198",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "What was your request to litellm on the passthrough? @Ziyann ",
          "created_at": "2025-06-04T22:21:04Z"
        },
        {
          "author": "Ziyann",
          "body": "> What was your request to litellm on the passthrough? [@Ziyann](https://github.com/Ziyann)\n\nFailing case (passthrough), ending with the above error message:\n```\ncurl http://localhost:4000/vertex_ai/v1/projects/XXX/locations/us-central1/publishers/google/models/gemini-2.5-flash-preview-05-20:generat",
          "created_at": "2025-06-05T20:42:01Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Unable to repro - investigating further\n\n<img width=\"1026\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/0b944590-96d3-4e1c-8155-6088e277a15a\" />",
          "created_at": "2025-06-07T23:02:27Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Able to repro this when vertex is setup on config.yaml vs. the UI ",
          "created_at": "2025-06-07T23:22:33Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Oh, i see the issue - `vertex_credentials` is not set ",
          "created_at": "2025-06-07T23:26:43Z"
        }
      ]
    },
    {
      "issue_number": 11530,
      "title": "[Feature]: POE.com API proxy",
      "body": "### The Feature\n\n[Support ](https://creator.poe.com/docs/external-application-guide)\n\nPOE.com have API we could use, could we include that into liteLLM? it's running fastapi_poe\n\n### Motivation, pitch\n\nüöÄ Pitch: Integrate POE.com API (fastapi_poe) into liteLLM! üöÄ\nWhy This Matters\n\nliteLLM is quickly becoming the go-to solution for connecting to multiple LLM providers via a single, unified interface. But imagine supercharging it further by adding support for the [POE.com API](https://poe.com/), one of the fastest-growing platforms for accessing a variety of chatbots and LLMs!\n\nHere‚Äôs why now is the perfect time to do this:\nüö¶ Unlock More Models Instantly\n\nPOE.com gives users access to a wide range of LLMs (including OpenAI, Claude, Llama, and more) under one API. By integrating fastapi_poe, liteLLM users could instantly tap into even more models, without new accounts or API keys for each.\n‚ö° Speed and Scalability\n\nThe fastapi_poe framework is designed for speed and ease of use, with async support and robust handling. Integrating it would let liteLLM users leverage POE's infrastructure for blazing-fast, reliable LLM queries.\nüåç Community Demand\n\nThe LLM community is always hunting for more endpoints, more choice, and more flexibility. Multiple requests have already come up for POE.com support in LLM wrappers‚Äîbe the first to deliver!\nüõ†Ô∏è Easy Integration, Maximum Impact\n\n    fastapi_poe is well-documented and actively maintained.\n    The integration would be straightforward: just add a new provider class in liteLLM, following the existing pluggable pattern.\n    Minimal maintenance overhead, as POE handles most backend scaling and model management.\n\nü§ù Why It‚Äôs a Win-Win\n\n    For developers: Unlocks a new audience and more use-cases for liteLLM.\n    For users: One API, even more models and endpoints‚Äîno more juggling tokens!\n    For the community: Open source, extensible, and future-proof.\n\nLet‚Äôs Make It Happen!\n\nIntegrating POE.com (via fastapi_poe) into liteLLM would:\n\n    Expand the provider list\n    Make liteLLM the ultimate LLM API aggregator\n    Drive more adoption and contributions\n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "lkthomas",
      "author_type": "User",
      "created_at": "2025-06-08T03:29:52Z",
      "updated_at": "2025-06-08T03:29:52Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11530/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11530",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11530",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:41.663017",
      "comments": []
    },
    {
      "issue_number": 11473,
      "title": "[Bug]: UI Page Logo Missing",
      "body": "### What happened?\n\nWhen logging in using `/ui`, a UI logo exception occurs\n\n<img width=\"211\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/dcd223a6-0f5e-4bb0-aa81-6dd918d06e43\" />\n\n<img width=\"440\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/438d94a5-3fe6-4063-ade9-7bf58ff0deb7\" />\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "sxueck",
      "author_type": "User",
      "created_at": "2025-06-06T05:52:19Z",
      "updated_at": "2025-06-08T01:23:59Z",
      "closed_at": "2025-06-08T01:23:59Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11473/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11473",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11473",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:41.663036",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Thanks for the issue @sxueck, This was caused by my recent work on custom server root path. I'm planning on having it fixed by the stable release. ",
          "created_at": "2025-06-06T06:05:52Z"
        },
        {
          "author": "krrishdholakia",
          "body": "This is fixed on main! ",
          "created_at": "2025-06-08T01:23:57Z"
        }
      ]
    },
    {
      "issue_number": 11507,
      "title": "[Bug]: Config document uses the outdated model gpt-3.5",
      "body": "### What happened?\n\nWhen user follows the config guide (https://docs.litellm.ai/docs/proxy/configs), they will get an error from Azure because the platform removed all gpt-3.5 models.\n\n### Relevant log output\n\n```shell\n{\"error\":{\"message\":\"litellm.BadRequestError: AzureException - The chatCompletion operation does not work with the specified model, gpt-35-turbo. Please choose different model and try again. You can learn more about which models can be used with each operation here: https://go.microsoft.com/fwlink/?linkid=2197993.. Received Model Group=azure-gpt-35-turbo\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"400\"}}%\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "vuanhtu52",
      "author_type": "User",
      "created_at": "2025-06-07T02:48:34Z",
      "updated_at": "2025-06-08T01:11:25Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11507/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11507",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11507",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:41.938781",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "Can we close this, you made a fix PR for this right @vuanhtu52 ? ",
          "created_at": "2025-06-07T16:13:30Z"
        },
        {
          "author": "vuanhtu52",
          "body": "@ishaan-jaff Yes I have made this PR to fix the document: https://github.com/BerriAI/litellm/pull/11508.\n\nMy apologies for the missing issue number in the PR. You can close the issue now.",
          "created_at": "2025-06-08T01:11:25Z"
        }
      ]
    },
    {
      "issue_number": 8938,
      "title": "[Bug]: Error encountered with LiteLLM ‚Äì Debug mode not resolving issue",
      "body": "### What happened?\n\nA bug happened!\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.60.2\n\n### Twitter / LinkedIn details\n\nhttps://www.linkedin.com/in/shirazali8",
      "state": "closed",
      "author": "shirazkk",
      "author_type": "User",
      "created_at": "2025-03-02T05:16:26Z",
      "updated_at": "2025-06-08T00:02:00Z",
      "closed_at": "2025-06-08T00:02:00Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8938/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8938",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8938",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:42.205378",
      "comments": [
        {
          "author": "shirazkk",
          "body": "Error encountered with LiteLLM ‚Äì Debug mode not resolving issue",
          "created_at": "2025-03-02T05:16:58Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-01T00:01:56Z"
        }
      ]
    },
    {
      "issue_number": 8941,
      "title": "Add \"R7\" in \"model_prices_and_context_window.json\"",
      "body": "Source: <SOURCE_URL>\n\nWe need to update both [model_prices_and_context_window.json](https://github.com/BerriAI/litellm/blob/88b1e315c8ed3160e0d519e97b198591ca2bac45/model_prices_and_context_window.json) and [model_prices_and_context_window_backup.json](https://github.com/BerriAI/litellm/blob/88b1e315c8ed3160e0d519e97b198591ca2bac45/litellm/model_prices_and_context_window_backup.json) to reflect the new model.\n",
      "state": "closed",
      "author": "Hazem-azim",
      "author_type": "User",
      "created_at": "2025-03-02T07:52:28Z",
      "updated_at": "2025-06-08T00:01:59Z",
      "closed_at": "2025-06-08T00:01:59Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8941/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8941",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8941",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:42.396486",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-01T00:01:54Z"
        }
      ]
    },
    {
      "issue_number": 8945,
      "title": "[Bug]: 500 Server Error thrown instead of 4xx Response when posting to `/budget/new` with an existing budget id",
      "body": "### What happened?\n\nWithin the first few minutes of setting up LiteLLM proxy server, encountered two errors and another point of confusion. \n\nThis error appears to be unreported. When posting to `/budget/new` with an existing budget id, a 500 Internal Server Error thrown, raised at the prisma level. Not a huge issue, but could be handled more gracefully with a relevant 4xx response such as a 409 Conflict, or else by making the endpoint idempotent.\n\n```python\nimport requests\n\nstandard_tier = {\n    \"budget_id\": \"standard\",\n    \"max_budget\": 1,\n}\n\nadd_budget_url = f\"{LLM_GATEWAY_BASE_URL}/budget/new\"\nbody = standard_tier\nheaders = {\"Authorization\": f\"Bearer {LLM_GATEWAY_API_KEY}\"}\n\nresp = requests.post(add_budget_url, json=body, headers=headers)\ntry:\n    resp.raise_for_status()\n    print(resp.json())\nexcept requests.exceptions.HTTPError as exc:\n    print(exc)\n```\n\nAs an aside, the other points of confusion I had were in relation to the Python SDK vs the proxy server. I assumed that the Python SDK would work with a proxy server as an API wrapper, but it seems we need to choose one or the other. For example, I thought `ChatLiteLLM` would work in Langchain with a proxy server, but it seems the proxy server is intended to be accessed by an OpenAI client instead. Similarly, the `BudgetManager` is pointing to an endpoint that no longer exists in the proxy server, which was reported a year ago in https://github.com/BerriAI/litellm/issues/2094. Tbh, I'm concerned by what other issues could surface if using this for longer than a few hours or in production. Also, minor nitpicks include warning logs on initial startup of the proxy in relation to Pydantic `model` name-spaced fields, which may be an issue that Pydantic need to handle better, owing to the widespread prevalence of the `model_` prefix for AI-related fields these days. LiteLLM looks awesome, and amazing that it is open source, but the number of unresolved issues (currently >1,000) suggests to me that maintenance could be better and it doesn't exactly inspire me with confidence to avail of LiteLLM enterprise solutions going forward. Just my 2 cents, overall it looks extremely useful, thank youüôè\n\n### Relevant log output\n\n```shell\n2025-03-02 16:17:06   File \"/usr/local/lib/python3.11/site-packages/litellm/proxy/proxy_server.py\", line 5979, in new_budget\n2025-03-02 16:17:06     response = await prisma_client.db.litellm_budgettable.create(\n2025-03-02 16:17:06                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-03-02 16:17:06   File \"/usr/local/lib/python3.11/site-packages/prisma/actions.py\", line 183, in create\n2025-03-02 16:17:06     resp = await self._client._execute(\n2025-03-02 16:17:06            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-03-02 16:17:06   File \"/usr/local/lib/python3.11/site-packages/prisma/client.py\", line 528, in _execute\n2025-03-02 16:17:06     return await self._engine.query(builder.build(), tx_id=self._tx_id)\n2025-03-02 16:17:06            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-03-02 16:17:06   File \"/usr/local/lib/python3.11/site-packages/prisma/engine/query.py\", line 244, in query\n2025-03-02 16:17:06     return await self.request(\n2025-03-02 16:17:06            ^^^^^^^^^^^^^^^^^^^\n2025-03-02 16:17:06   File \"/usr/local/lib/python3.11/site-packages/prisma/engine/http.py\", line 141, in request\n2025-03-02 16:17:06     return utils.handle_response_errors(resp, errors_data)\n2025-03-02 16:17:06            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-03-02 16:17:06   File \"/usr/local/lib/python3.11/site-packages/prisma/engine/utils.py\", line 192, in handle_response_errors\n2025-03-02 16:17:06     raise exc(error)\n2025-03-02 16:17:06 prisma.errors.UniqueViolationError: Unique constraint failed on the fields: (`budget_id`)\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.61.20\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "Saran33",
      "author_type": "User",
      "created_at": "2025-03-02T16:42:16Z",
      "updated_at": "2025-06-08T00:01:58Z",
      "closed_at": "2025-06-08T00:01:58Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8945/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8945",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8945",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:42.626118",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-06-01T00:01:53Z"
        }
      ]
    },
    {
      "issue_number": 11526,
      "title": "[Bug]: Amazon Bedrock Agents Usage always zero (litellm SDK)",
      "body": "### What happened?\n\nI am exploring LiteLLM with Amazon Bedrock Agents.  When using Bedrock Agents, I am not seeing any token usage being returned from my requests (all usage is set to zero).  I don't know if this is a limitation of LiteLLM or of Amazon Bedrock Agents.\n\n```python\nimport litellm\nfrom litellm import completion\n\nlitellm.success_callback = [\"langfuse\"] \nlitellm.failure_callback = [\"langfuse\"]\n\ndef handler(event,context):\n    messages = [{\"content\": user_input, \"role\": \"user\"}]\n    response = completion(\n        model=\"bedrock/agent/NGOOGRNDY0/1HOXPOGKDJ\",  \n        messages=messages,\n    )\n\n    print(response)\n```\n\nOutputs the following. \n\n```bash\nModelResponse(id='chatcmpl-5c8f1dfc-ca19-4726-b47f-97afd08fc6d3', created=1749336384, model='anthropic.claude-3-5-sonnet-20241022-v2:0', object='\nchat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Paris is the capital city of France.', role=\n'assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion\n_tokens_details=None, prompt_tokens_details=None))\n```\n\nIf I make a call to Bedrock directly (not a Bedrock Agent request), the usage is reported as expected\n\n```python\nimport litellm\nfrom litellm import completion\n\nlitellm.success_callback = [\"langfuse\"] \nlitellm.failure_callback = [\"langfuse\"]\n\ndef handler(event,context):\n    messages = [{\"content\": user_input, \"role\": \"user\"}]\n    response = completion(\n        model=\"bedrock/anthropic.claude-3-sonnet-20240229-v1:0\",  \n        messages=messages,\n    )\n\n    print(response)\n```\n\nprints non-zero usage information\n```bash\nModelResponse(id='chatcmpl-d2395b7c-98f0-40e4-b674-c89b4c34e31b', created=1749336655, model='anthropic.claude-3-sonnet-20240229-v1:0', object='ch\nat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='The capital of France is Paris.', role='assist\nant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=10, prompt_tokens=14, total_tokens=24, completion_tok\nens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_inpu\nt_tokens=0, cache_read_input_tokens=0))\n```\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.0\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "SethThomas",
      "author_type": "User",
      "created_at": "2025-06-07T22:52:53Z",
      "updated_at": "2025-06-07T22:52:53Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11526/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11526",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11526",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:42.820203",
      "comments": []
    },
    {
      "issue_number": 9784,
      "title": "[Bug]: Unable to link LiteLLM with Cline",
      "body": "### What happened?\n\n# What happened?\n\ni did setup LiteLLM \nhttp://0.0.0.0:4000/ui/ \nSo i can use https://replicate.com/collections/language-models like: deepseek-r1 and claude-3.7-sonnet in [Cline](https://github.com/cline/cline) \n\ni did created a team and add the models to the team and test the key. \n\n![Image](https://github.com/user-attachments/assets/0c29148f-aa10-45dc-8f3a-2e338a35893c)\n![Image](https://github.com/user-attachments/assets/91c80044-296d-462e-957e-ef3e3f6dc6a8)\n\nso far so good.\n\nthe problem happen when try to link LiteLLM with [Cline](https://github.com/cline/cline) \ni get this error:\n\n`500 litellm.APIError: ReplicateException - sequence item 1: expected str instance, list found. Received Model Group=deepseek-r1\nAvailable Model Group Fallbacks=None`\n\n\n`500 litellm.APIError: ReplicateException - sequence item 1: expected str instance, list found. Received Model Group=claude-3.7-sonnet\nAvailable Model Group Fallbacks=None`\n\n![Image](https://github.com/user-attachments/assets/d99dd9ad-7598-4900-b5f4-24750a70bc36)\n![Image](https://github.com/user-attachments/assets/77015176-c793-4399-a5eb-3a243a345afb)\n\n\n# Steps to Reproduce \n## config.yaml\n```# Model-specific parameters\nmodel_list:\n  - model_name: claude-3.7-sonnet # model alias\n    litellm_params: # actual params for litellm.completion()\n      model: \"replicate/anthropic/claude-3.7-sonnet\" \n      api_key: os.environ/REPLICATE_API_KEY\n      initial_prompt_value: \"\\n\"\n      # roles: {\"system\":{\"pre_message\":\"<|im_start|>system\\n\", \"post_message\":\"<|im_end|>\"}, \"assistant\":{\"pre_message\":\"<|im_start|>assistant\\n\",\"post_message\":\"<|im_end|>\"}, \"user\":{\"pre_message\":\"<|im_start|>user\\n\",\"post_message\":\"<|im_end|>\"}}\n      final_prompt_value: \"\\n\"\n      bos_token: \"<s>\"\n      eos_token: \"</s>\"\n      max_tokens: 64000\n\n\n  - model_name: deepseek-r1\n    litellm_params: # all params accepted by litellm.completion() - https://docs.litellm.ai/docs/completion/input\n      model: \"replicate/deepseek-ai/deepseek-r1\"       \n      api_key: \"os.environ/REPLICATE_API_KEY\"\n      initial_prompt_value: \"\\n\"\n      final_prompt_value: \"\\n\"\n      bos_token: \"<s>\"\n      eos_token: \"</s>\"\n      roles: {\"system\":{\"pre_message\":\"<|im_start|>system\\n\", \"post_message\":\"<|im_end|>\"}, \"assistant\":{\"pre_message\":\"<|im_start|>assistant\\n\",\"post_message\":\"<|im_end|>\"}, \"user\":{\"pre_message\":\"<|im_start|>user\\n\",\"post_message\":\"<|im_end|>\"}}\n```\n\n\n\ni don't know what i am messing here?\nwhy can't link with cline?\n\n### Relevant log output\n\n```shell\n$ litellm --config config.yaml --debug\n\nPrisma schema loaded from schema.prisma\nDatasource \"client\": PostgreSQL database \"LiteLLM_DB\", schema \"public\" at \"localhost:5432\"\n\nThe database is already in sync with the Prisma schema.\n\nRunning generate... - Prisma Client Python (v0.15.0)\n\nSome types are disabled by default due to being incompatible with Mypy, it is highly recommended\nto use Pyright instead and configure Prisma Python to use recursive types. To re-enable certain types:\n\ngenerator client {\n  provider             = \"prisma-client-py\"\n  recursive_type_depth = -1\n}\n\nIf you need to use Mypy, you can also disable this message by explicitly setting the default value:\n\ngenerator client {\n  provider             = \"prisma-client-py\"\n  recursive_type_depth = 5\n}\n\nFor more information see: https://prisma-client-py.readthedocs.io/en/stable/reference/limitations/#default-type-limitations\n\n‚úî Generated Prisma Client Python (v0.15.0) to ./../../prisma in 358ms\n\nINFO:     Started server process [154249]\nINFO:     Waiting for application startup.\n\n#------------------------------------------------------------#\n#                                                            #\n#       'This feature doesn't meet my needs because...'       #\n#        https://github.com/BerriAI/litellm/issues/new        #\n#                                                            #\n#------------------------------------------------------------#\n\n Thank you for using LiteLLM! - Krrish & Ishaan\n\n\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n\n\nLiteLLM: Proxy initialized with Config, Set models:\n    claude-3.7-sonnet\n    deepseek-r1\n02:57:33 - LiteLLM Router:INFO: router.py:643 - Routing strategy: simple-shuffle\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:4000 (Press CTRL+C to quit)\n02:57:35 - LiteLLM Proxy:INFO: utils.py:1275 - All necessary views exist!\n02:57:46 - LiteLLM Proxy:INFO: parallel_request_limiter.py:68 - Current Usage of key in this minute: None\n02:57:46 - LiteLLM Proxy:INFO: parallel_request_limiter.py:68 - Current Usage of user in this minute: None\n02:57:46 - LiteLLM Proxy:INFO: parallel_request_limiter.py:68 - Current Usage of team in this minute: None\n02:57:46 - LiteLLM:INFO: utils.py:3052 - \nLiteLLM completion() model= deepseek-ai/deepseek-r1; provider = replicate\n02:57:47 - LiteLLM Router:INFO: router.py:1072 - litellm.acompletion(model=replicate/deepseek-ai/deepseek-r1) Exception litellm.APIError: ReplicateException - sequence item 1: expected str instance, list found\n02:57:47 - LiteLLM Router:INFO: router.py:3483 - Retrying request with num_retries: 2\n02:57:47 - LiteLLM:INFO: utils.py:3052 - \nLiteLLM completion() model= deepseek-ai/deepseek-r1; provider = replicate\n02:57:47 - LiteLLM Router:INFO: router.py:1072 - litellm.acompletion(model=replicate/deepseek-ai/deepseek-r1) Exception litellm.APIError: ReplicateException - sequence item 1: expected str instance, list found\n02:57:48 - LiteLLM:INFO: utils.py:3052 - \nLiteLLM completion() model= deepseek-ai/deepseek-r1; provider = replicate\n02:57:48 - LiteLLM Router:INFO: router.py:1072 - litellm.acompletion(model=replicate/deepseek-ai/deepseek-r1) Exception litellm.APIError: ReplicateException - sequence item 1: expected str instance, list found\n02:57:49 - LiteLLM Router:INFO: router.py:3191 - Trying to fallback b/w models\n02:57:49 - LiteLLM Proxy:ERROR: common_request_processing.py:298 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.APIError: ReplicateException - sequence item 1: expected str instance, list found. Received Model Group=deepseek-r1\nAvailable Model Group Fallbacks=None LiteLLM Retried: 1 times, LiteLLM Max Retries: 2\nTraceback (most recent call last):\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py\", line 3621, in prompt_factory\n    return hf_chat_template(original_model_name, messages)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py\", line 403, in hf_chat_template\n    raise Exception(\"No chat template found\")\nException: No chat template found\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/main.py\", line 1778, in completion\n    model_response = replicate_chat_completion(  # type: ignore\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/llms/replicate/chat/handler.py\", line 142, in completion\n    input_data = replicate_config.transform_request(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/llms/replicate/chat/transformation.py\", line 201, in transform_request\n    prompt = prompt_factory(model=model, messages=messages)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py\", line 3623, in prompt_factory\n    return default_pt(\n           ^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py\", line 43, in default_pt\n    return \" \".join(message[\"content\"] for message in messages)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: sequence item 1: expected str instance, list found\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/proxy/proxy_server.py\", line 3354, in chat_completion\n    return await base_llm_response_processor.base_process_llm_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/proxy/common_request_processing.py\", line 210, in base_process_llm_request\n    responses = await llm_responses\n                ^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/router.py\", line 938, in acompletion\n    raise e\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/router.py\", line 914, in acompletion\n    response = await self.async_function_with_fallbacks(**kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/router.py\", line 3349, in async_function_with_fallbacks\n    raise original_exception\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/router.py\", line 3162, in async_function_with_fallbacks\n    response = await self.async_function_with_retries(*args, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/router.py\", line 3541, in async_function_with_retries\n    raise original_exception\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/router.py\", line 3432, in async_function_with_retries\n    response = await self.make_call(original_function, *args, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/router.py\", line 3550, in make_call\n    response = await response\n               ^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/router.py\", line 1077, in _acompletion\n    raise e\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/router.py\", line 1036, in _acompletion\n    response = await _response\n               ^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/utils.py\", line 1438, in wrapper_async\n    raise e\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/utils.py\", line 1299, in wrapper_async\n    result = await original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/main.py\", line 491, in acompletion\n    raise exception_type(\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/main.py\", line 464, in acompletion\n    init_response = await loop.run_in_executor(None, func_with_context)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/utils.py\", line 969, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/main.py\", line 3149, in completion\n    raise exception_type(\n          ^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2214, in exception_type\n    raise e\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 666, in exception_type\n    raise APIError(\nlitellm.exceptions.APIError: litellm.APIError: ReplicateException - sequence item 1: expected str instance, list found. Received Model Group=deepseek-r1\nAvailable Model Group Fallbacks=None LiteLLM Retried: 1 times, LiteLLM Max Retries: 2\n02:57:49 - LiteLLM Proxy:INFO: proxy_server.py:2972 - {\"event\": \"giveup\", \"exception\": \"\"}\n02:57:49 - LiteLLM Proxy:ERROR: _common.py:120 - Giving up chat_completion(...) after 1 tries (litellm.proxy._types.ProxyException)\nINFO:     127.0.0.1:42556 - \"POST /chat/completions HTTP/1.1\" 500 Internal Server Error\n02:57:49 - LiteLLM Proxy:INFO: db_spend_update_writer.py:366 - Writing spend log to db - request_id: c3ef0512-c630-4901-af2d-6af628bb76f1, spend: 0.0\n02:57:49 - LiteLLM Proxy:INFO: parallel_request_limiter.py:68 - Current Usage of key in this minute: {'current_requests': 0, 'current_tpm': 0, 'current_rpm': 1}\n02:57:49 - LiteLLM Proxy:INFO: parallel_request_limiter.py:68 - Current Usage of user in this minute: {'current_requests': 1, 'current_tpm': 0, 'current_rpm': 1}\n02:57:49 - LiteLLM Proxy:INFO: parallel_request_limiter.py:68 - Current Usage of team in this minute: {'current_requests': 1, 'current_tpm': 0, 'current_rpm': 1}\n02:57:49 - LiteLLM:INFO: utils.py:3052 - \nLiteLLM completion() model= deepseek-ai/deepseek-r1; provider = replicate\n02:57:49 - LiteLLM Router:INFO: router.py:1072 - litellm.acompletion(model=replicate/deepseek-ai/deepseek-r1) Exception litellm.APIError: ReplicateException - sequence item 1: expected str instance, list found\n02:57:49 - LiteLLM Router:INFO: router.py:3483 - Retrying request with num_retries: 2\n02:57:50 - LiteLLM:INFO: utils.py:3052 - \nLiteLLM completion() model= deepseek-ai/deepseek-r1; provider = replicate\n02:57:50 - LiteLLM Router:INFO: router.py:1072 - litellm.acompletion(model=replicate/deepseek-ai/deepseek-r1) Exception litellm.APIError: ReplicateException - sequence item 1: expected str instance, list found\n02:57:50 - LiteLLM:INFO: utils.py:3052 - \nLiteLLM completion() model= deepseek-ai/deepseek-r1; provider = replicate\n02:57:50 - LiteLLM Router:INFO: router.py:1072 - litellm.acompletion(model=replicate/deepseek-ai/deepseek-r1) Exception litellm.APIError: ReplicateException - sequence item 1: expected str instance, list found\n02:57:51 - LiteLLM Router:INFO: router.py:3191 - Trying to fallback b/w models\n02:57:51 - LiteLLM Proxy:ERROR: common_request_processing.py:298 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.APIError: ReplicateException - sequence item 1: expected str instance, list found. Received Model Group=deepseek-r1\nAvailable Model Group Fallbacks=None LiteLLM Retried: 1 times, LiteLLM Max Retries: 2\nTraceback (most recent call last):\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py\", line 3621, in prompt_factory\n    return hf_chat_template(original_model_name, messages)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py\", line 403, in hf_chat_template\n    raise Exception(\"No chat template found\")\nException: No chat template found\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/main.py\", line 1778, in completion\n    model_response = replicate_chat_completion(  # type: ignore\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/llms/replicate/chat/handler.py\", line 142, in completion\n    input_data = replicate_config.transform_request(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/llms/replicate/chat/transformation.py\", line 201, in transform_request\n    prompt = prompt_factory(model=model, messages=messages)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py\", line 3623, in prompt_factory\n    return default_pt(\n           ^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py\", line 43, in default_pt\n    return \" \".join(message[\"content\"] for message in messages)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: sequence item 1: expected str instance, list found\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/proxy/proxy_server.py\", line 3354, in chat_completion\n    return await base_llm_response_processor.base_process_llm_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/proxy/common_request_processing.py\", line 210, in base_process_llm_request\n    responses = await llm_responses\n                ^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/router.py\", line 938, in acompletion\n    raise e\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/router.py\", line 914, in acompletion\n    response = await self.async_function_with_fallbacks(**kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/router.py\", line 3349, in async_function_with_fallbacks\n    raise original_exception\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/router.py\", line 3162, in async_function_with_fallbacks\n    response = await self.async_function_with_retries(*args, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/router.py\", line 3541, in async_function_with_retries\n    raise original_exception\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/router.py\", line 3432, in async_function_with_retries\n    response = await self.make_call(original_function, *args, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/router.py\", line 3550, in make_call\n    response = await response\n               ^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/router.py\", line 1077, in _acompletion\n    raise e\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/router.py\", line 1036, in _acompletion\n    response = await _response\n               ^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/utils.py\", line 1438, in wrapper_async\n    raise e\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/utils.py\", line 1299, in wrapper_async\n    result = await original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/main.py\", line 491, in acompletion\n    raise exception_type(\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/main.py\", line 464, in acompletion\n    init_response = await loop.run_in_executor(None, func_with_context)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/utils.py\", line 969, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/main.py\", line 3149, in completion\n    raise exception_type(\n          ^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2214, in exception_type\n    raise e\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 666, in exception_type\n    raise APIError(\nlitellm.exceptions.APIError: litellm.APIError: ReplicateException - sequence item 1: expected str instance, list found. Received Model Group=deepseek-r1\nAvailable Model Group Fallbacks=None LiteLLM Retried: 1 times, LiteLLM Max Retries: 2\n02:57:52 - LiteLLM Proxy:INFO: proxy_server.py:2972 - {\"event\": \"giveup\", \"exception\": \"\"}\n02:57:52 - LiteLLM Proxy:ERROR: _common.py:120 - Giving up chat_completion(...) after 1 tries (litellm.proxy._types.ProxyException)\nINFO:     127.0.0.1:42564 - \"POST /chat/completions HTTP/1.1\" 500 Internal Server Error\n02:57:52 - LiteLLM Proxy:INFO: db_spend_update_writer.py:366 - Writing spend log to db - request_id: 40394964-3f5d-4fd9-a967-ea49cf600097, spend: 0.0\n02:57:52 - LiteLLM Proxy:INFO: parallel_request_limiter.py:68 - Current Usage of key in this minute: {'current_requests': 0, 'current_tpm': 0, 'current_rpm': 2}\n02:57:52 - LiteLLM Proxy:INFO: parallel_request_limiter.py:68 - Current Usage of user in this minute: {'current_requests': 2, 'current_tpm': 0, 'current_rpm': 2}\n02:57:52 - LiteLLM Proxy:INFO: parallel_request_limiter.py:68 - Current Usage of team in this minute: {'current_requests': 2, 'current_tpm': 0, 'current_rpm': 2}\n02:57:52 - LiteLLM:INFO: utils.py:3052 - \nLiteLLM completion() model= deepseek-ai/deepseek-r1; provider = replicate\n02:57:52 - LiteLLM Router:INFO: router.py:1072 - litellm.acompletion(model=replicate/deepseek-ai/deepseek-r1) Exception litellm.APIError: ReplicateException - sequence item 1: expected str instance, list found\n02:57:52 - LiteLLM Router:INFO: router.py:3483 - Retrying request with num_retries: 2\n02:57:53 - LiteLLM:INFO: utils.py:3052 - \nLiteLLM completion() model= deepseek-ai/deepseek-r1; provider = replicate\n02:57:53 - LiteLLM Router:INFO: router.py:1072 - litellm.acompletion(model=replicate/deepseek-ai/deepseek-r1) Exception litellm.APIError: ReplicateException - sequence item 1: expected str instance, list found\n02:57:53 - LiteLLM Proxy:INFO: utils.py:2624 - Processed 1 daily spend transactions in 0.01s\n02:57:53 - LiteLLM:INFO: utils.py:3052 - \nLiteLLM completion() model= deepseek-ai/deepseek-r1; provider = replicate\n02:57:53 - LiteLLM Router:INFO: router.py:1072 - litellm.acompletion(model=replicate/deepseek-ai/deepseek-r1) Exception litellm.APIError: ReplicateException - sequence item 1: expected str instance, list found\n02:57:54 - LiteLLM Router:INFO: router.py:3191 - Trying to fallback b/w models\n02:57:54 - LiteLLM Proxy:ERROR: common_request_processing.py:298 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.APIError: ReplicateException - sequence item 1: expected str instance, list found. Received Model Group=deepseek-r1\nAvailable Model Group Fallbacks=None LiteLLM Retried: 1 times, LiteLLM Max Retries: 2\nTraceback (most recent call last):\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py\", line 3621, in prompt_factory\n    return hf_chat_template(original_model_name, messages)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py\", line 403, in hf_chat_template\n    raise Exception(\"No chat template found\")\nException: No chat template found\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/main.py\", line 1778, in completion\n    model_response = replicate_chat_completion(  # type: ignore\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/llms/replicate/chat/handler.py\", line 142, in completion\n    input_data = replicate_config.transform_request(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/llms/replicate/chat/transformation.py\", line 201, in transform_request\n    prompt = prompt_factory(model=model, messages=messages)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py\", line 3623, in prompt_factory\n    return default_pt(\n           ^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py\", line 43, in default_pt\n    return \" \".join(message[\"content\"] for message in messages)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: sequence item 1: expected str instance, list found\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/proxy/proxy_server.py\", line 3354, in chat_completion\n    return await base_llm_response_processor.base_process_llm_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/proxy/common_request_processing.py\", line 210, in base_process_llm_request\n    responses = await llm_responses\n                ^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/router.py\", line 938, in acompletion\n    raise e\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/router.py\", line 914, in acompletion\n    response = await self.async_function_with_fallbacks(**kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/router.py\", line 3349, in async_function_with_fallbacks\n    raise original_exception\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/router.py\", line 3162, in async_function_with_fallbacks\n    response = await self.async_function_with_retries(*args, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/router.py\", line 3541, in async_function_with_retries\n    raise original_exception\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/router.py\", line 3432, in async_function_with_retries\n    response = await self.make_call(original_function, *args, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/router.py\", line 3550, in make_call\n    response = await response\n               ^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/router.py\", line 1077, in _acompletion\n    raise e\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/router.py\", line 1036, in _acompletion\n    response = await _response\n               ^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/utils.py\", line 1438, in wrapper_async\n    raise e\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/utils.py\", line 1299, in wrapper_async\n    result = await original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/main.py\", line 491, in acompletion\n    raise exception_type(\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/main.py\", line 464, in acompletion\n    init_response = await loop.run_in_executor(None, func_with_context)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/utils.py\", line 969, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/main.py\", line 3149, in completion\n    raise exception_type(\n          ^^^^^^^^^^^^^^^\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2214, in exception_type\n    raise e\n  File \"/ubuntu/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 666, in exception_type\n    raise APIError(\nlitellm.exceptions.APIError: litellm.APIError: ReplicateException - sequence item 1: expected str instance, list found. Received Model Group=deepseek-r1\nAvailable Model Group Fallbacks=None LiteLLM Retried: 1 times, LiteLLM Max Retries: 2\n02:57:54 - LiteLLM Proxy:INFO: proxy_server.py:2972 - {\"event\": \"giveup\", \"exception\": \"\"}\n02:57:54 - LiteLLM Proxy:ERROR: _common.py:120 - Giving up chat_completion(...) after 1 tries (litellm.proxy._types.ProxyException)\nINFO:     127.0.0.1:42578 - \"POST /chat/completions HTTP/1.1\" 500 Internal Server Error\n02:57:54 - LiteLLM Proxy:INFO: db_spend_update_writer.py:366 - Writing spend log to db - request_id: a58715a9-2bfa-4ee1-9c58-dc097499f17c, spend: 0.0\n02:58:02 - LiteLLM Proxy:INFO: utils.py:2624 - Processed 1 daily spend transactions in 0.01s\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nLatest version just installed with uv install 'litellm[proxy]'\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "xMrAli",
      "author_type": "User",
      "created_at": "2025-04-06T03:10:21Z",
      "updated_at": "2025-06-07T22:43:38Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9784/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9784",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9784",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:42.820223",
      "comments": [
        {
          "author": "FJiangArthur",
          "body": "Having same issue; ",
          "created_at": "2025-06-07T22:43:37Z"
        }
      ]
    },
    {
      "issue_number": 11520,
      "title": "[Bug]: Getting litellm.supports_reasoning == False, even for supported models.",
      "body": "### What happened?\n\nWhen calling this code:\nModelToUse = \"openrouter/deepseek/deepseek-r1:free\"\nIfSupportReasoning = litellm.supports_reasoning(model=ModelToUse)\n        #if IfSupportReasoning == True:\n        expert_reasoning = response.choices[0].message.reasoning_content.strip()\n\n\nGetting False for this model, even it returns reasoning text very well (when commenting the 'if' part).\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.72.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "OfferLa",
      "author_type": "User",
      "created_at": "2025-06-07T17:28:16Z",
      "updated_at": "2025-06-07T17:28:16Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11520/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11520",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11520",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:43.011843",
      "comments": []
    },
    {
      "issue_number": 11030,
      "title": "[Feature]: Support Grok `search_parameters` param",
      "body": "### The Feature\n\nExpose Grok‚Äôs **Live Search** capability through LiteLLM by letting users pass a `search_parameters` object in any `chat.completions` call.\n\n* Accept a `search_parameters` argument\n* Translate that into the JSON body expected by Grok:\n\n```json\n\"search_parameters\": { \"mode\": \"<auto|on|off>\" }\n```\n\n* Validate the string value and raise a clear error on anything else.\n* Support additional live search args (sources, excluding sites, etc...)\n\nThis keeps the existing LiteLLM philosophy of thin, transparent pass-through while hiding provider quirks behind a consistent interface.\n\n[xAI Live Search Docs](https://docs.x.ai/docs/guides/live-search)\n\n### Motivation, pitch\n\nGrok‚Äôs Live Search connects straight to the public web **and** to X, so each request can pull the latest news articles, blog posts, and social media threads right into the model context. This real-time feed of external content is hugely valuable for anyone who needs up-to-the-minute information from reputable news outlets or trending conversations on X.\n\nhttps://docs.x.ai/docs/guides/live-search\nhttps://docs.x.ai/docs/models\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n@colesmcintosh / https://www.linkedin.com/in/colemcintosh/",
      "state": "closed",
      "author": "colesmcintosh",
      "author_type": "User",
      "created_at": "2025-05-21T22:37:29Z",
      "updated_at": "2025-06-07T16:08:53Z",
      "closed_at": "2025-06-02T20:55:39Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11030/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11030",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11030",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:43.011867",
      "comments": [
        {
          "author": "spammenotinoz",
          "body": "This would be great",
          "created_at": "2025-05-22T07:00:39Z"
        },
        {
          "author": "shivamvverma",
          "body": "Would appreciate this üôè",
          "created_at": "2025-05-24T23:00:33Z"
        },
        {
          "author": "vic-shen",
          "body": "hi @krrishdholakia @ishaan-jaff would love this soon please üôèüôè",
          "created_at": "2025-06-02T20:12:30Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "@vic-shen this is live on 1.72.0 https://github.com/BerriAI/litellm/commit/06484f6e5a7a2f4e45c490266782ed28b51b7db6 ",
          "created_at": "2025-06-02T20:42:03Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Closing as this is now live",
          "created_at": "2025-06-02T20:55:39Z"
        }
      ]
    },
    {
      "issue_number": 11510,
      "title": "[Bug]: The contributing code document shows outdated test directory",
      "body": "### What happened?\n\nThe current document for Contributing Code (https://docs.litellm.ai/docs/extras/contributing_code) refers to the test directory at tests/litellm, but this directory no longer exists. It will be confusing for new contributors who want to write tests for their code.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "vuanhtu52",
      "author_type": "User",
      "created_at": "2025-06-07T05:32:28Z",
      "updated_at": "2025-06-07T14:35:02Z",
      "closed_at": "2025-06-07T14:35:02Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11510/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11510",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11510",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:43.267583",
      "comments": [
        {
          "author": "sumansuhag",
          "body": "Hi!\n\nThis is a big issue for any open-source project. Having clear and accurate contribution guidelines is key to drawing in and keeping new developers. If the path to the test directories is outdated, it creates a quick roadblock.\n\n**Immediate Steps**\n1. **Find the New Test Directory**: First, you ",
          "created_at": "2025-06-07T05:35:03Z"
        },
        {
          "author": "vuanhtu52",
          "body": "@sumansuhag Yes I completely agree with these points. I've already made a PR to fix the issue here: https://github.com/BerriAI/litellm/pull/11511.\n\nCurrently the doc is quite poorly maintained so I am going through them and try to update as much as I can.",
          "created_at": "2025-06-07T12:34:46Z"
        }
      ]
    },
    {
      "issue_number": 11345,
      "title": "[Bug]: All requests getting timed out and not reaching Anthropic",
      "body": "### What happened?\n\nThe symptom:\n\n- We have a streaming use-case and are currently using LiteLLM's \"acompletion\"\n- Initially the calls are going through fine, but after certain time (not fixed and now known right now), we start getting timeout for all the LLM calls.\n- When checked the Anthropic console, no logs found, essentially meaning requests were not sent to Anthropic.\nCode snippet being used:\n`from litellm import acompletion\nstream = await acompletion(\n        model=model,\n        messages=messages,\n        tools=tools,\n        temperature=temperature,\n        stream=True,\n    )\nasync for chunk in stream:\n    pass`\n\nDid our initial research, could this be due to pool of connections being maintained and them not being closed properly (assuming the existing connections are not closed on their own for a very long time in case of streaming), leading to all new requests waiting for a new connection and eventually timing out after 600s?\n\nWhat would be an effective solution for this?\n\n### Relevant log output\n\n```shell\nFile \"/usr/local/lib/python3.11/site-packages/litellm/utils.py\", line 1358, in wrapper_async\n    raise e\n  File \"/usr/local/lib/python3.11/site-packages/litellm/utils.py\", line 1217, in wrapper_async\n    result = await original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/litellm/main.py\", line 485, in acompletion\n    raise exception_type(\n  File \"/usr/local/lib/python3.11/site-packages/litellm/main.py\", line 466, in acompletion\n    response = await init_response\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/litellm/llms/anthropic/chat/handler.py\", line 180, in acompletion_stream_function\n    completion_stream, headers = await make_call(\n                                 ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/litellm/llms/anthropic/chat/handler.py\", line 75, in make_call\n    raise e\n  File \"/usr/local/lib/python3.11/site-packages/litellm/llms/anthropic/chat/handler.py\", line 59, in make_call\n    response = await client.post(\n               ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_utils.py\", line 131, in async_wrapper\n    result = await func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 216, in post\n    raise litellm.Timeout(\nlitellm.exceptions.Timeout: litellm.Timeout: Connection timed out after 600.0 seconds.\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.60.6\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "kashish-prodigal",
      "author_type": "User",
      "created_at": "2025-06-03T04:57:48Z",
      "updated_at": "2025-06-07T13:52:01Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11345/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11345",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11345",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:43.479178",
      "comments": [
        {
          "author": "zaheerabbas-prodigal",
          "body": "@krrishdholakia - can you please help check this issue. We are facing this in production. The issue resolves when we rebuild the docker image and deploy again. \n\nWhat info can we provide you to help debug this? Please let us know.",
          "created_at": "2025-06-07T13:52:00Z"
        }
      ]
    },
    {
      "issue_number": 8659,
      "title": "[Feature]: Add Support for Requesty.ai in LiteLLM",
      "body": "### The Feature\n\nEnable support for Requesty.ai as a router in LiteLLM, similar to OpenRouter. This would allow developers to integrate Requesty.ai seamlessly into applications that rely on LiteLLM, such as OpenHands.\n\n### Motivation, pitch\n\nRequesty.ai is a powerful AI router with diverse model support, similar to OpenRouter. Without LiteLLM integration, applications like OpenHands cannot leverage its benefits. Adding support would expand options for developers, enhance flexibility, and improve model routing efficiency.\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "ViezeVingertjes",
      "author_type": "User",
      "created_at": "2025-02-19T17:23:53Z",
      "updated_at": "2025-06-07T11:03:40Z",
      "closed_at": "2025-04-24T15:31:12Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8659/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8659",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8659",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:43.684054",
      "comments": [
        {
          "author": "Thibault00",
          "body": "Happy to help here (from the Requesty Team) ! We're an Openai compatible router. Do you have any documentation on how to add new providers? If not, I can just add a PR as well.\n\nthe only changes to be made to add Requesty:\nbase_url=\"https://router.requesty.ai/v1\"\n\nyou can find our docs here:\nhttps:/",
          "created_at": "2025-02-19T17:33:38Z"
        },
        {
          "author": "Nasreddine",
          "body": "I tried to use requesty with dspy that uses litellm in the backend; but without success.\n\n```\nROUTER_API_KEY = os.getenv(\"REQUESTY_API_KEY\")\ngoogle_gemini_2_5_flash_preview_04_17 = dspy.LM(\n    api_key=ROUTER_API_KEY, \n    base_url=\"https://router.requesty.ai/v1\",\n    model=\"google/gemini-2.5-flash-",
          "created_at": "2025-04-24T11:09:02Z"
        },
        {
          "author": "gesman",
          "body": "To add requesty.ai support, if using LiteLLM proxy, add this to litellm_config.yaml :\n\n```\n  # See: https://docs.litellm.ai/docs/providers/openai_compatible\n  - model_name: requesty/*\n    litellm_params:\n      model: openai/*                             # add openai/ prefix to route as OpenAI provid",
          "created_at": "2025-06-07T11:03:38Z"
        }
      ]
    },
    {
      "issue_number": 11442,
      "title": "[Bug]:  litellm.APIConnectionError: 'str' object has no attribute 'get'",
      "body": "### What happened?\n\n**What happened?**\nI'm using litellm as proxy for [frigate](https://docs.frigate.video/configuration/genai/) to descripe the events.\nBy sending the events to litellm the error Type: APIConnectionError, Message: litellm.APIConnectionError: 'str' object has no attribute 'get' raised\n\n\nGenAI Config frigate:\ngenai:\n  enabled: true\n  provider: openai\n  api_key: sk-xxxxxxxxxxxxxxxxxxx\n  base_url: xxxxxxxxxxxxxxxxxx\n  model: moondream\n  prompt: \"Describe the {label} in the sequence of images with as much detail as possible. Do not describe the background\nobject_prompts:\n   cat: \"My special cat prompt.\"\n    bird: \"My special bird prompt.\"\n    person: \"My special person prompt.\"\n\nRequest from frigate:\n{\n \"model\": \"moondream\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\n          \"type\": \"image_url\",\n          \"image_url\": {\n            \"url\": \"data:image/jpeg;base64,/9j/4AAQSkZJRgABA  ......  (truncated 10359 chars)\",\n            \"detail\": \"low\"\n          }\n        },\n        \"My special person prompt.\"\n      ]\n    }\n\n\nIs this a bug in litellm or for figrate?\n\n### Relevant log output\n\n```shell\nType:\nAPIConnectionError\nMessage:\nlitellm.APIConnectionError: 'str' object has no attribute 'get'\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/main.py\", line 2929, in completion\n    response = base_llm_http_handler.completion(\n        model=model,\n    ...<13 lines>...\n        client=client,\n    )\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 307, in completion\n    data = provider_config.transform_request(\n        model=model,\n    ...<3 lines>...\n        headers=headers,\n    )\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/ollama/completion/transformation.py\", line 340, in transform_request\n    modified_prompt = ollama_pt(model=model, messages=messages)\n  File \"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/prompt_templates/factory.py\", line 206, in ollama_pt\n    if m.get(\"type\", \"\") == \"image_url\":\n       ^^^^^\nAttributeError: 'str' object has no attribute 'get'\n. Received Model Group=moondream\nAvailable Model Group Fallbacks=None LiteLLM Retried: 1 times, LiteLLM Max Retries: 2\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.71.1 \n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "tjfroese",
      "author_type": "User",
      "created_at": "2025-06-05T14:07:53Z",
      "updated_at": "2025-06-07T09:03:25Z",
      "closed_at": "2025-06-07T09:03:25Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11442/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11442",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11442",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:43.857876",
      "comments": [
        {
          "author": "VigneshwarRajasekaran",
          "body": "Hi @tjfroese \nLitellm requires the request for vision models to be a dictionary following the[ OpenAIs vision API](https://platform.openai.com/docs/guides/vision) , but your request contains String.\n`}\n},\n\"My special person prompt.\"\n]\n}`\n\nHence the error.\n\nIf you think this is resolved, please close",
          "created_at": "2025-06-07T02:54:30Z"
        },
        {
          "author": "tjfroese",
          "body": "Hi Vigneshwar,\nthanks a lot! Your open my eyes to the right spot.\n\nI've analyse the problem more in detail on frigate site. Therefore Version with the latest tag (0.15.0) has this problem.\n\nProblem solved by using the newst version >0.16",
          "created_at": "2025-06-07T09:01:53Z"
        }
      ]
    },
    {
      "issue_number": 5757,
      "title": "[Feature]: Add unit test for Gemini / Google AI Studio + Cloudflare AI Gateway",
      "body": "### The Feature\n\nPretty sure this isn't working due to a bad API key header, so I should probably add a unit test for this. (We had a private fix in our fork, but it was reverted due to a merge conflict IIRC.)\r\n\r\nRelated to #5428.\n\n### Motivation, pitch\n\nWould be nice if it worked.\n\n### Twitter / LinkedIn details\n\nhttps://www.linkedin.com/in/davidmanouchehri/",
      "state": "closed",
      "author": "Manouchehri",
      "author_type": "User",
      "created_at": "2024-09-18T03:34:21Z",
      "updated_at": "2025-06-07T06:36:21Z",
      "closed_at": "2025-05-07T00:02:51Z",
      "labels": [
        "enhancement",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/5757/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Manouchehri"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/5757",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/5757",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:44.043836",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-01-28T02:58:30Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-04-29T00:04:08Z"
        },
        {
          "author": "gauravvgat",
          "body": "Let's open this.",
          "created_at": "2025-06-07T06:36:19Z"
        }
      ]
    },
    {
      "issue_number": 11116,
      "title": "[Bug]: Azure GPT-3.5 Turbo Fails Due to Retired Model Version (0125) ‚Äì Docs Should Reflect Updated Deployment Requirement",
      "body": "### What happened?\n\nThe current Azure OpenAI configuration examples for gpt-3.5-turbo (e.g., in litellm_config.yaml) result in 404 errors if the user‚Äôs deployment references the retired model version 0125.\n\nAs of April 30, 2025, Microsoft Azure officially retired model version gpt-35-turbo-0125. Deployments using this version return teh attached output.\nAdditionally, in the Azure portal, users see this warning:\n\n‚ÄúRetired model version: This deployment is using retired model gpt-35-turbo version 0125 which was retired on 4/30/2025. Inferencing on this endpoint will return error responses.‚Äù\n\n### Relevant log output\n\n```shell\n{\n  \"error\": {\n    \"code\": \"404\",\n    \"message\": \"Resource not found\"\n  }\n}\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv  1.70.4\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "VigneshwarRajasekaran",
      "author_type": "User",
      "created_at": "2025-05-24T06:56:44Z",
      "updated_at": "2025-06-07T06:12:16Z",
      "closed_at": "2025-05-29T15:27:57Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11116/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11116",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11116",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:44.289808",
      "comments": [
        {
          "author": "vuanhtu52",
          "body": "Hi @VigneshwarRajasekaran , I was facing the same issue while following the Deployment guide for docker. Also managed to make a fix on that document. I think the same issues are lying across the document pages so if time permits, it might be a good idea to go through the entire document and make the",
          "created_at": "2025-06-07T06:12:15Z"
        }
      ]
    },
    {
      "issue_number": 11505,
      "title": "[Bug]: Deployment documentation uses the outdated model GPT-3.5",
      "body": "### What happened?\n\nWhen user follows the deployment guide (https://docs.litellm.ai/docs/proxy/deploy), they will receive an error from Azure because the GPT-3.5-turbo model was retired. The doc should update to a new model version like the GPT-4o\n\n### Relevant log output\n\n```shell\n{\"error\":{\"message\":\"litellm.BadRequestError: AzureException - The chatCompletion operation does not work with the specified model, gpt-35-turbo. Please choose different model and try again. You can learn more about which models can be used with each operation here: https://go.microsoft.com/fwlink/?linkid=2197993.. Received Model Group=azure-gpt-35-turbo\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"400\"}}%\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "vuanhtu52",
      "author_type": "User",
      "created_at": "2025-06-07T01:24:44Z",
      "updated_at": "2025-06-07T03:35:16Z",
      "closed_at": "2025-06-07T03:35:16Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11505/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11505",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11505",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:44.468533",
      "comments": []
    },
    {
      "issue_number": 9500,
      "title": "[Bug]: IndexError: list index out of range in Azure GPT Calls",
      "body": "### What happened?\n\n## Description  \nAfter upgrading `litellm` from version `1.63.12` to `1.63.14`, I encountered an error when making Azure GPT calls. The error occurs during the processing of `optional_params` and seems to relate to API version handling.\n\n### Steps to Reproduce\n- Upgrade from 1.63.12 to 1.63.14.\n- Use Azure GPT endpoints with litellm.\n- Observe the error during runtime.\n\n\n### Relevant log output\n\n```python\nlitellm.APIConnectionError: AzureException APIConnectionError - list index out of range\nTraceback (most recent call last):\n  File \"/var/task/litellm/main.py\", line 1082, in completion\n    optional_params = get_optional_params(\n                      ^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/litellm/utils.py\", line 3638, in get_optional_params\n    optional_params = litellm.AzureOpenAIConfig().map_openai_params(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/litellm/llms/azure/chat/gpt_transformation.py\", line 140, in map_openai_params\n    api_version_month = api_version_times[1]\n                        ~~~~~~~~~~~~~~~~~^^^\nIndexError: list index out of range\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.63.14\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "aybruhm",
      "author_type": "User",
      "created_at": "2025-03-24T14:12:43Z",
      "updated_at": "2025-06-07T03:17:21Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9500/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9500",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9500",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:44.468552",
      "comments": [
        {
          "author": "vuanhtu52",
          "body": "Hey @aybruhm. Not sure if you managed to fix the issue but I think the issue is because you did not provide the correct form for api_version in your config file. It should be something like this: 2025-01-01-preview. Without the hyphens the code will break, as you can see on line 140 in gpt_transform",
          "created_at": "2025-06-07T03:17:20Z"
        }
      ]
    },
    {
      "issue_number": 7287,
      "title": "[Bug]: valid tokens do not longer work after 1.52.14",
      "body": "### What happened?\n\nA bug happened!\r\nrel 1.52.15 and later do not accept valid tokens and render complete setups obsolete.\r\n\r\nusing litellm as a central api-proxy-hub for dozens of apps does not work any longer after updating beyond 1.52.14.\r\n\r\n### working version\r\nversion <= 1.42.14\r\n```\r\ncurl   'https://api.sub.tld.org/models'   -H 'accept: application/json'   -H 'Authorization: Bearer sk-my-token'|jq|grep '\"id\":'\r\n      \"id\": \"groq/llama-3.1-8b-instant\",\r\n      \"id\": \"groq/whisper-large-v3\",\r\n      \"id\": \"groq/llama-3.3-70b-versatile\",\r\n      \"id\": \"groq/llama-3.2-11b-vision-preview\",\r\n      \"id\": \"jina_ai/colbert-v2\",\r\n‚Ä¶\r\n```\r\n\r\n### not working\r\nversion >= 1.42.15\r\n\r\n```\r\ncurl   'https://api.sub.tld.org/models'   -H 'accept: application/json'   -H 'Authorization: Bearer sk-my-token'\r\n{\"error\":{\"message\":\"Authentication Error, Invalid proxy server token passed. valid_token=None.\",\"type\":\"auth_error\",\"param\":\"None\",\"code\":\"401\"}}\r\n```\r\nnothing was changed in the setup but updated litellm-database (docker)\n\n### Relevant log output\n\n```shell\n13:18:55 - LiteLLM Proxy:ERROR: user_api_key_auth.py:1192 - litellm.proxy.proxy_server.user_api_key_auth(): Exception occured - Invalid proxy server token passed. valid_token=None.\r\nRequester IP Address:172.21.0.6\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.11/site-packages/litellm/proxy/auth/user_api_key_auth.py\", line 1161, in user_api_key_auth\r\n    _is_route_allowed = _is_allowed_route(\r\n                        ^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/litellm/proxy/auth/user_api_key_auth.py\", line 188, in _is_allowed_route\r\n    return _is_api_route_allowed(\r\n           ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/litellm/proxy/auth/user_api_key_auth.py\", line 158, in _is_api_route_allowed\r\n    raise Exception(\"Invalid proxy server token passed. valid_token=None.\")\r\nException: Invalid proxy server token passed. valid_token=None.\r\nINFO:     172.21.0.6:40244 - \"GET /models HTTP/1.1\" 401 Unauthorized\n```\n\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.53.15 and later\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "chymian",
      "author_type": "User",
      "created_at": "2024-12-18T13:43:41Z",
      "updated_at": "2025-06-07T00:02:09Z",
      "closed_at": "2025-06-07T00:02:09Z",
      "labels": [
        "bug",
        "awaiting: user response",
        "stale"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/7287/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/7287",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/7287",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:44.640377",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "hi @chymian - we've not seen other reports of this issue. **Are you sure it's not related to your environment** ",
          "created_at": "2024-12-18T15:23:28Z"
        },
        {
          "author": "chymian",
          "body": "@ishaan-jaff \r\nas I have done nothing but updated litellm and apps like anythingllm, hoarder, etc stopped working I investigated and checked with different versions of litellm and found the last known good to be 1.42.14.\r\neven the simple curl cmd. which I was using to check does not work anymore.\r\n\r",
          "created_at": "2024-12-18T15:45:26Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "- on 1.52.14 does it show the DB connected on /health/readiness \r\n- is your proxy using the latest schema ? Do you see `custom_llm_provider` in SpendLogs ? \r\n- Share the full strack trace on the proxy server when you see this error \r\n",
          "created_at": "2024-12-18T16:10:16Z"
        },
        {
          "author": "krrishdholakia",
          "body": "@chymian can you confirm you see this on v1.55.4 https://github.com/BerriAI/litellm/releases/tag/v1.55.4\r\n\r\ni think i remember seeing this a few weeks ago, but it was resolved shortly after - iirc it had to do with an issue in the get_data from the prisma client ",
          "created_at": "2024-12-19T21:30:34Z"
        },
        {
          "author": "krrishdholakia",
          "body": "bump on this? @chymian ",
          "created_at": "2024-12-24T22:26:14Z"
        }
      ]
    },
    {
      "issue_number": 8351,
      "title": "[Bug]: Default values in json responses issue",
      "body": "### What happened?\n\nSeems to be temperamental as sometime it does return default in the response and other times not.\n\nwith google gemini/gemini-2.0-flash-001 model.\n\n \n\n### Relevant log output\n\n```shell\n{\n    \"tools\": [\n        {\n            \"function_declarations\": [\n                {\n                    \"name\": \"AgentList2\",\n                    \"description\": \"Correctly extracted `AgentList2` with all the required parameters with correct types\",\n                    \"parameters\": {\n                        \"properties\": {\n                            \"myagents\": {\n                                \"description\": \"List of Agents defined within this crew\",\n                                \"items\": {\n                                    \"properties\": {\n                                        \"agent_name\": {\n                                            \"description\": \"Name of the agent\",\n                                            \"type\": \"string\"\n                                        },\n                                        \"agent_role\": {\n                                            \"description\": \"Role or primary function of the agent\",\n                                            \"type\": \"string\"\n                                        },\n                                        \"agent_goal\": {\n                                            \"description\": \"Primary goal of the agent\",\n                                            \"type\": \"string\"\n                                        },\n                                        \"agent_backstory\": {\n                                            \"description\": \"Background story or context for the agent\",\n                                            \"type\": \"string\"\n                                        },\n                                        \"agent_tools\": {\n                                            \"default\": null,\n                                            \"description\": \"Tools available to the agent  crewai tool name rather than description\",\n                                            \"items\": {\n                                                \"type\": \"string\"\n                                            },\n                                            \"type\": \"array\",\n                                            \"nullable\": true\n                                        },\n                                        \"agent_llm\": {\n                                            \"default\": null,\n                                            \"description\": \"Language model to be used\",\n                                            \"type\": \"string\",\n                                            \"nullable\": true\n                                        },\n                                        \"agent_llm_temperature\": {\n                                            \"default\": 0.7,\n                                            \"description\": \"Temperature setting for the LLM\",\n                                            \"type\": \"number\"\n                                        },\n                                        \"myagent_tasks\": {\n                                            \"description\": \"Tasks assigned to the agent\",\n                                            \"items\": {\n                                                \"$ref\": \"#/$defs/AgentTaskSpecs\"\n                                            },\n                                            \"type\": \"array\",\n                                            \"nullable\": true\n                                        }\n                                    },\n                                    \"required\": [\n                                        \"agent_name\",\n                                        \"agent_role\",\n                                        \"agent_goal\",\n                                        \"agent_backstory\"\n                                    ],\n                                    \"type\": \"object\"\n                                },\n                                \"type\": \"array\"\n                            }\n                        },\n                        \"type\": \"object\",\n                        \"required\": [\n                            \"myagents\"\n                        ]\n                    }\n                }\n            ]\n        }\n    ]\n}\n\n===========================================\n\nlitellm.BadRequestError: VertexAIException BadRequestError - {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[0].value.items.properties[4].value': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[0].value.items.properties[5].value': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[0].value.items.properties[6].value': Cannot find field.\\nInvalid JSON payload received. Unknown name \\\"$ref\\\" at 'tools[0].function_declarations[0].parameters.properties[0].value.items.properties[7].value.items': Cannot find field.\",\n    \"status\": \"INVALID_ARGUMENT\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.BadRequest\",\n        \"fieldViolations\": [\n          {\n            \"field\": \"tools[0].function_declarations[0].parameters.properties[0].value.items.properties[4].value\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[0].value.items.properties[4].value': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools[0].function_declarations[0].parameters.properties[0].value.items.properties[5].value\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[0].value.items.properties[5].value': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools[0].function_declarations[0].parameters.properties[0].value.items.properties[6].value\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"default\\\" at 'tools[0].function_declarations[0].parameters.properties[0].value.items.properties[6].value': Cannot find field.\"\n          },\n          {\n            \"field\": \"tools[0].function_declarations[0].parameters.properties[0].value.items.properties[7].value.items\",\n            \"description\": \"Invalid JSON payload received. Unknown name \\\"$ref\\\" at 'tools[0].function_declarations[0].parameters.properties[0].value.items.properties[7].value.items': Cannot find field.\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.60.6\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "andrewn3",
      "author_type": "User",
      "created_at": "2025-02-07T09:57:32Z",
      "updated_at": "2025-06-07T00:02:05Z",
      "closed_at": "2025-06-07T00:02:04Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8351/reactions",
        "total_count": 3,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8351",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8351",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:44.867690",
      "comments": [
        {
          "author": "andrewn3",
          "body": "still an intermittent issue in 160.8 when google sometimes returns default in the response.",
          "created_at": "2025-02-09T06:41:49Z"
        },
        {
          "author": "vkhobor",
          "body": "I have encountered this as well.",
          "created_at": "2025-02-22T21:14:04Z"
        },
        {
          "author": "JohanBekker",
          "body": "Same",
          "created_at": "2025-03-01T17:45:10Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-05-31T00:01:50Z"
        }
      ]
    },
    {
      "issue_number": 8594,
      "title": "[Bug]: KeyError: 'name' error with local ollama models",
      "body": "### What happened?\n\nHey all!\n\nI am experiencing this error with locally hosted ollama models (I tried the 3 smallest deepseek + mistral models). API is working when I am calling it from terminal but with liteLLM I am getting the following errors:\n\n### Relevant log output\n\n```shell\nTraceback (most recent call last):\n  File \"/Users/laszlohajdu/Documents/GitHub/cognee/cognee/ontology_testing_SANDBOX/RDF_testing/rdf_pipeline.py\", line 113, in <module>\n    loop.run_until_complete(main())\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/Users/laszlohajdu/Documents/GitHub/cognee/cognee/ontology_testing_SANDBOX/RDF_testing/rdf_pipeline.py\", line 42, in main\n    await cognee.add(file_path)\n  File \"/Users/laszlohajdu/Documents/GitHub/cognee/cognee/api/v1/add/add_v2.py\", line 32, in add\n    await test_llm_connection()\n  File \"/Users/laszlohajdu/Documents/GitHub/cognee/cognee/infrastructure/llm/utils.py\", line 52, in test_llm_connection\n    raise e\n  File \"/Users/laszlohajdu/Documents/GitHub/cognee/cognee/infrastructure/llm/utils.py\", line 44, in test_llm_connection\n    await llm_adapter.acreate_structured_output(\n  File \"/Users/laszlohajdu/Documents/GitHub/cognee/cognee/infrastructure/llm/generic_llm_api/adapter.py\", line 35, in acreate_structured_output\n    return await self.aclient.chat.completions.create(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/laszlohajdu/Documents/GitHub/cognee_development_env/cognee_dev_new/lib/python3.12/site-packages/instructor/client.py\", line 387, in create\n    return await self.create_fn(\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/laszlohajdu/Documents/GitHub/cognee_development_env/cognee_dev_new/lib/python3.12/site-packages/instructor/patch.py\", line 161, in new_create_async\n    response = await retry_async(\n               ^^^^^^^^^^^^^^^^^^\n  File \"/Users/laszlohajdu/Documents/GitHub/cognee_development_env/cognee_dev_new/lib/python3.12/site-packages/instructor/retry.py\", line 262, in retry_async\n    raise InstructorRetryException(\ninstructor.exceptions.InstructorRetryException: litellm.APIConnectionError: 'name'\nTraceback (most recent call last):\n  File \"/Users/laszlohajdu/Documents/GitHub/cognee_development_env/cognee_dev_new/lib/python3.12/site-packages/litellm/main.py\", line 467, in acompletion\n    response = await init_response\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/laszlohajdu/Documents/GitHub/cognee_development_env/cognee_dev_new/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 182, in async_completion\n    return provider_config.transform_response(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/laszlohajdu/Documents/GitHub/cognee_development_env/cognee_dev_new/lib/python3.12/site-packages/litellm/llms/ollama/completion/transformation.py\", line 263, in transform_response\n    \"name\": function_call[\"name\"],\n            ~~~~~~~~~~~~~^^^^^^^^\nKeyError: 'name'\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nTried 1.61.1 and 1.57.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "hajdul88",
      "author_type": "User",
      "created_at": "2025-02-17T17:57:17Z",
      "updated_at": "2025-06-07T00:02:03Z",
      "closed_at": "2025-06-07T00:02:03Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8594/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8594",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8594",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:45.060504",
      "comments": [
        {
          "author": "enzofjh",
          "body": "**Same error with Version: 1.61.8 on Windows Machine**\n\nException: Failed to generate schema: litellm.APIConnectionError: 'name'\nTraceback (most recent call last):\n  File \".\\Lib\\site-packages\\litellm\\main.py\", line 2808, in completion\n    response = base_llm_http_handler.completion(\n               ^",
          "created_at": "2025-02-28T01:53:36Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-05-30T00:02:16Z"
        }
      ]
    },
    {
      "issue_number": 8671,
      "title": "Increased around 40ms ASR Latency at P50 After Integrating with LiteLLM",
      "body": "**Description:**\nWe are observing an increase in ASR (Automatic Speech Recognition) latency after integrating LiteLLM into our workflow.\n\n**Issue:**\nBefore integration with LiteLLM: Using Groq's ASR functionality, we were achieving an average P50 latency of around 180ms.\nAfter integrating LiteLLM: The latency has increased to approximately 220ms.\n\n**Network Considerations:**\nThe LiteLLM instance is operating in an internal network, so any network-related latency increase is negligible and should not contribute significantly to the observed delay.\n\n**Impact:**\nThe increase in latency (around 40ms) is notable and affects real-time ASR performance.\nWe would appreciate any insights or suggestions to address this increased latency.\n\nThank you for your support!",
      "state": "closed",
      "author": "yin250",
      "author_type": "User",
      "created_at": "2025-02-20T06:13:36Z",
      "updated_at": "2025-06-07T00:02:00Z",
      "closed_at": "2025-06-07T00:02:00Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8671/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8671",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8671",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:45.288887",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "hi @yin250, do you see the same overhead on /chat/completions ? or is this just ASR ? \n\ncan you also share your config.yaml ? ",
          "created_at": "2025-02-20T20:49:39Z"
        },
        {
          "author": "krrishdholakia",
          "body": "bump on this? @yin250 ",
          "created_at": "2025-02-21T07:34:07Z"
        },
        {
          "author": "yin250",
          "body": "> hi [@yin250](https://github.com/yin250), do you see the same overhead on /chat/completions ? or is this just ASR ?\n> \n> can you also share your config.yaml ?\n\nYes, just ASR. And this is out config.yaml below\nmodel_list:\n  - model_name: openai/whisper-1\n    litellm_params:\n      model: openai/whisp",
          "created_at": "2025-02-28T06:15:54Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-05-30T00:02:13Z"
        }
      ]
    },
    {
      "issue_number": 8824,
      "title": "Facing this Error \"litellm.NotFoundError: VertexAIException\" while using \"gemini-pro\" model",
      "body": "Document content is invalid. Reason: Error during validation: litellm.NotFoundError: VertexAIException - { \"error\": { \"code\": 404, \"message\": \"models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\", \"status\": \"NOT_FOUND\" } }",
      "state": "closed",
      "author": "Sharan-KnarahS",
      "author_type": "User",
      "created_at": "2025-02-26T05:55:17Z",
      "updated_at": "2025-06-07T00:01:58Z",
      "closed_at": "2025-06-07T00:01:58Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8824/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8824",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8824",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:45.528393",
      "comments": [
        {
          "author": "rob-lega",
          "body": "Getting same error.",
          "created_at": "2025-02-28T00:54:40Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-05-30T00:01:58Z"
        }
      ]
    },
    {
      "issue_number": 8831,
      "title": "[Bug]: Task was destroyed but it is pending! with new litellme version",
      "body": "I was looking to integrate the Sonnet 3.7 thinking mode into my app and hence update litellm to the latest version and started seeing this error repeatedly...\n\n```\n2025-02-26 00:07:30,285 - asyncio - ERROR - [1a9020eb-e21c-420e-9097-36cb36a1f8f0] - Task was destroyed but it is pending!\ntask: <Task pending name='Task-18' coro=<Logging.async_success_handler() running at /Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/litellm/litellm_core_utils/litellm_logging.py:1518>>\n/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/asyncio/base_events.py:678: RuntimeWarning: coroutine 'Logging.async_success_handler' was never awaited\n  self._ready.clear()\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 3/6 [00:27<00:27,  9.32s/it]\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\n00:07:30 - LiteLLM:ERROR: fallback_utils.py:54 - Fallback attempt failed for model anthropic/claude-3-7-sonnet-20250219: litellm.InternalServerError: AnthropicException - Event loop is closed. Handle with `litellm.InternalServerError`.\nTraceback (most recent call last):\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/litellm/llms/anthropic/chat/handler.py\", line 229, in acompletion_function\n    response = await async_handler.post(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_utils.py\", line 131, in async_wrapper\n    result = await func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 238, in post\n    raise e\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 191, in post\n    response = await self.client.send(req, stream=stream)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/httpx/_client.py\", line 1629, in send\n    response = await self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/httpx/_client.py\", line 1657, in _send_handling_auth\n    response = await self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/httpx/_client.py\", line 1694, in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/httpx/_client.py\", line 1730, in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/httpx/_transports/default.py\", line 394, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\n    raise exc from None\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\n    response = await connection.handle_async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/httpcore/_async/connection.py\", line 103, in handle_async_request\n    return await self._connection.handle_async_request(request)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/httpcore/_async/http11.py\", line 135, in handle_async_request\n    await self._response_closed()\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/httpcore/_async/http11.py\", line 250, in _response_closed\n    await self.aclose()\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/httpcore/_async/http11.py\", line 258, in aclose\n    await self._network_stream.aclose()\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n    await self._stream.aclose()\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/anyio/streams/tls.py\", line 193, in aclose\n    await self.transport_stream.aclose()\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 1261, in aclose\n    self._transport.close()\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/asyncio/selector_events.py\", line 839, in close\n    self._loop.call_soon(self._call_connection_lost, None)\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/asyncio/base_events.py\", line 761, in call_soon\n    self._check_closed()\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/asyncio/base_events.py\", line 519, in _check_closed\n    raise RuntimeError('Event loop is closed')\nRuntimeError: Event loop is closed\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/litellm/main.py\", line 467, in acompletion\n    response = await init_response\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/litellm/llms/anthropic/chat/handler.py\", line 248, in acompletion_function\n    raise AnthropicError(\nlitellm.llms.anthropic.common_utils.AnthropicError: Event loop is closed\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/litellm/litellm_core_utils/fallback_utils.py\", line 48, in async_completion_with_fallbacks\n    response = await litellm.acompletion(**completion_kwargs, model=model)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/litellm/utils.py\", line 1394, in wrapper_async\n    raise e\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/litellm/utils.py\", line 1253, in wrapper_async\n    result = await original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/litellm/main.py\", line 486, in acompletion\n    raise exception_type(\n          ^^^^^^^^^^^^^^^\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2202, in exception_type\n    raise e\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 558, in exception_type\n    raise litellm.InternalServerError(\nlitellm.exceptions.InternalServerError: litellm.InternalServerError: AnthropicException - Event loop is closed. Handle with `litellm.InternalServerError`.\n2025-02-26 00:07:30,316 - LiteLLM - ERROR\nTraceback (most recent call last):\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/litellm/llms/anthropic/chat/handler.py\", line 229, in acompletion_function\n    response = await async_handler.post(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_utils.py\", line 131, in async_wrapper\n    result = await func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 238, in post\n    raise e\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 191, in post\n    response = await self.client.send(req, stream=stream)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/httpx/_client.py\", line 1629, in send\n    response = await self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/httpx/_client.py\", line 1657, in _send_handling_auth\n    response = await self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/httpx/_client.py\", line 1694, in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/httpx/_client.py\", line 1730, in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/httpx/_transports/default.py\", line 394, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\n    raise exc from None\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\n    response = await connection.handle_async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/httpcore/_async/connection.py\", line 103, in handle_async_request\n    return await self._connection.handle_async_request(request)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/httpcore/_async/http11.py\", line 135, in handle_async_request\n    await self._response_closed()\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/httpcore/_async/http11.py\", line 250, in _response_closed\n    await self.aclose()\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/httpcore/_async/http11.py\", line 258, in aclose\n    await self._network_stream.aclose()\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n    await self._stream.aclose()\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/anyio/streams/tls.py\", line 193, in aclose\n    await self.transport_stream.aclose()\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 1261, in aclose\n    self._transport.close()\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/asyncio/selector_events.py\", line 839, in close\n    self._loop.call_soon(self._call_connection_lost, None)\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/asyncio/base_events.py\", line 761, in call_soon\n    self._check_closed()\n  File \"/Users/aps/miniconda3/envs/scholarqa/lib/python3.11/asyncio/base_events.py\", line 519, in _check_closed\n    raise RuntimeError('Event loop is closed')\nRuntimeError: Event loop is closed\n```",
      "state": "closed",
      "author": "amanpreet692",
      "author_type": "User",
      "created_at": "2025-02-26T08:10:12Z",
      "updated_at": "2025-06-07T00:01:57Z",
      "closed_at": "2025-06-07T00:01:57Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8831/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8831",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8831",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:45.793908",
      "comments": [
        {
          "author": "mpierangeli",
          "body": "Similar issue for me, it ends up returning a new response from sonnet. but async logging is indicating an error.\n\n`from litellm import completion\nresult = completion(model=\"openai/gpt-4o-mini\", \n                    messages=[{\"role\":\"user\",\"content\":\"Capitales de America Latina\"}],\n                 ",
          "created_at": "2025-02-26T22:39:31Z"
        },
        {
          "author": "amanpreet692",
          "body": "That maybe the case for me too... but it just becomes too difficult to parse anything that's happening because of the logs and then the process eventually dies out. I thought this was limited to Claude but it's happening with open ai models in the new library version as well",
          "created_at": "2025-02-26T22:59:02Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-05-30T00:01:56Z"
        }
      ]
    },
    {
      "issue_number": 8906,
      "title": "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.",
      "body": "can u please set this up",
      "state": "closed",
      "author": "Aneesha20",
      "author_type": "User",
      "created_at": "2025-03-01T01:13:04Z",
      "updated_at": "2025-06-07T00:01:56Z",
      "closed_at": "2025-06-07T00:01:56Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8906/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8906",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8906",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:46.034807",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-05-31T00:01:42Z"
        }
      ]
    },
    {
      "issue_number": 8909,
      "title": "[Bug]: Deepseek-r1's reasoning content is missing in latest version v1.61.20.rc",
      "body": "### What happened?\n\n\"reasoning_content\" is no longer returned from deepseek/deepseek-reasoner model.\n\nIt seems to work on `v1.61.15-nightly`.\n\nStarting from `v1.61.16-nightly`, `reasoning_content` is returned from deepseek if `stream` in the completion request is set to `false`. However, in the latest version, `reasoning_content` content is missing even if `stream` is set to false.\n\nI suspect something got broken when claude's thinking was added.\n\nI am using official deepseek API... following is my configuration\n``` yaml\nmodel_list:\n  - model_name: deepseek-reasoner\n    litellm_params:\n      model: deepseek/deepseek-reasoner\n      api_key: sk-123\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.61.20.rc\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "v3i1y",
      "author_type": "User",
      "created_at": "2025-03-01T02:00:56Z",
      "updated_at": "2025-06-07T00:01:55Z",
      "closed_at": "2025-06-07T00:01:55Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8909/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8909",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8909",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:46.266563",
      "comments": [
        {
          "author": "fatwang2",
          "body": "it works on v1.61.13, both stream and no stream, I have reported the bug for a long time",
          "created_at": "2025-03-01T12:36:14Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-05-31T00:01:40Z"
        }
      ]
    },
    {
      "issue_number": 11309,
      "title": "[Bug]: _transform_responses_api_content_to_chat_completion_content` doesn't support file content type",
      "body": "### What happened?\n\n### Description\nWhen using litellm's Responses API with non-OpenAI models (e.g., Anthropic), the function `_transform_responses_api_content_to_chat_completion_content` fails to properly handle file content types. It always adds a \"text\" field and drops the file information.\n\n### Current Behavior\nWhen passing file content in the `input` parameter, the transformation function only preserves the \"type\" field (converted from \"file\" to \"file\") but adds a \"text\" field instead of preserving the \"file\" object with its \"file_id\".\n\nCurrent code in `litellm/responses/litellm_completion_transformation/transformation.py` (lines 408-415):\n```python\ncontent_list.append(\n    {\n        \"type\": LiteLLMCompletionResponsesConfig._get_chat_completion_request_content_type(\n            item.get(\"type\") or \"text\"\n        ),\n        \"text\": item.get(\"text\"),\n    }\n)\n```\n\nThe function always looks for a \"text\" field and doesn't handle other content structures like file objects:\n```python\n{\n    'type': 'file',\n    'file': {\n        'file_id': file_id,\n    }\n}\n```\n\n### Expected Behavior\nThe transformation function should handle different content types properly, including preserving file objects when transforming from Responses API to Chat Completion API.\n\nWhen encountering a 'file' type content item, it should preserve the 'file' object with its 'file_id' rather than trying to convert it to a text-based format.\n\n### Steps to Reproduce\n1. Use litellm's Responses API with a non-OpenAI model (e.g., Anthropic)\n2. Include a file in the input parameter:\n```python\nresponse = litellm.responses(\n  model='claude...',\n  input=[\n      {\"role\": \"user\", \"content\": [\n          {\"type\": \"text\", \"text\": \"Here's a document:\"},\n          {\n              \"type\": \"file\",\n              \"file\": {\n                  \"file_id\": \"file_123456\"\n              }\n          }\n      ]}\n  ])\n```\n3. Observe that the file information is lost in the transformation process\n\n\n### Suggested Fix\nModify the `_transform_responses_api_content_to_chat_completion_content` function to handle different content types properly, particularly for 'file' types:\n\n```python\nif item.get(\"type\") == \"file\":\n    content_list.append({\n        \"type\": \"file\",\n        \"file\": item.get(\"file\")\n    })\nelse:\n    content_list.append({\n        \"type\": LiteLLMCompletionResponsesConfig._get_chat_completion_request_content_type(\n            item.get(\"type\") or \"text\"\n        ),\n        \"text\": item.get(\"text\"),\n    })\n```\n\nNot sure if this is the most elegant way to solve it, or if there are other content types to handle but tested this and it lets me pass in a `file` content object with anthropic models and behaves as expected. (eg I can ask questions about the file).\n\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.0\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "jaymegordo",
      "author_type": "User",
      "created_at": "2025-06-01T21:54:49Z",
      "updated_at": "2025-06-06T20:20:47Z",
      "closed_at": "2025-06-06T20:20:47Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11309/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11309",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11309",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:46.467283",
      "comments": []
    },
    {
      "issue_number": 11498,
      "title": "[Bug]: not able to use async_redis.SSLConnection with Redis >=6.0.0",
      "body": "### What happened?\n\nWhen using Redis 6.0.0, arg_spec returns the empty list. so, could not use sslconnection while REDIS_SSL set with env.\n\nIssue lies here: https://github.com/BerriAI/litellm/blob/main/litellm/_redis.py#L27\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nV1.68.0\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "bipulsimkhada-cr",
      "author_type": "User",
      "created_at": "2025-06-06T20:16:35Z",
      "updated_at": "2025-06-06T20:17:39Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11498/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11498",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11498",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:46.467314",
      "comments": []
    },
    {
      "issue_number": 11495,
      "title": "[Bug]: Incorrect cost calculation for Gemini models with tiered token pricing",
      "body": "### What happened?\n\nThe cost_per_token method does not correctly calculate costs for Gemini models that have tiered pricing for tokens (<= 200k vs > 200k). The function only applies the input token pricing logic to output tokens as well, instead of correctly using the output token price tiers. As a result, the cost is wrong for scenarios where input and output tokens fall into different tiers.\n\n### Relevant log output\n\n```shell\nModel: gemini/gemini-2.5-pro-preview-05-06\n======================================================================\n\nScenario 1:\n  Input tokens:  299,950 -> Cost: $0.749875 ($2.50 per 1M)\n  Output tokens: 29,950 -> Cost: $0.449250 ($15.00 per 1M)\n  Total cost: $1.199125\n\nScenario 2:\n  Input tokens:  15,000 -> Cost: $0.018750 ($1.25 per 1M)\n  Output tokens: 201,000 -> Cost: $2.010000 ($10.00 per 1M)\n  Total cost: $2.028750\n\nScenario 3:\n  Input tokens:  200,050 -> Cost: $0.500125 ($2.50 per 1M)\n  Output tokens: 200,050 -> Cost: $3.000750 ($15.00 per 1M)\n  Total cost: $3.500875\n\n======================================================================\nSummary:\nScenario     Tokens (in/out)      Total Cost   Input $/1M   Output $/1M \n----------------------------------------------------------------------\nScenario 1   299,950/29,950 $1.199125    $2.50        $15.00      \nScenario 2   15,000/201,000 $2.028750    $1.25        $10.00      \nScenario 3   200,050/200,050 $3.500875    $2.50        $15.00\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.72.1\n\n### Twitter / LinkedIn details\n\nhttps://www.linkedin.com/in/suyash-jagdale-18229816b/",
      "state": "open",
      "author": "suyash7243",
      "author_type": "User",
      "created_at": "2025-06-06T18:57:06Z",
      "updated_at": "2025-06-06T18:57:06Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11495/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11495",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11495",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:46.467323",
      "comments": []
    },
    {
      "issue_number": 10436,
      "title": "[Feature]: Support Vertex AI dedicated endpoints",
      "body": "### The Feature\n\nVertex AI Model Garden deployments allow deploying on the following types of endpoints:\n\n1. shared endpoint public\n2. dedicated public\n3. dedicated private\n\nThe 1. works great with just `model: vertex_ai/openai/<endpoint-id>`. However, I didn't manage to make the dedicated public endpoint work. \n\nThe way how it works when calling via OpenAI api client is e.g.:\n\n```python\nfrom google.auth.transport import requests\nimport google.auth.transport.requests\n\n\nimport google.auth\nimport openai\n\ncreds, project = google.auth.default()\nauth_req = google.auth.transport.requests.Request()\ncreds.refresh(auth_req)\n\nendpoint = \"https://222409431968854252.europe-west4-841529064496.prediction.vertexai.goog/v1/projects/<project-id>/locations/europe-west4/endpoints/2224094319688548252\"\n\nclient = openai.OpenAI(base_url=endpoint, api_key=creds.token)\n\nmodel_response = client.chat.completions.create(\n    model=\"irrelevant\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"Tell a joke\"},\n            ],\n        }\n    ],\n    temperature=0,\n    max_tokens=50,\n)\n```\n\nand that does work. But I wasn't able to torture litellm proxy to accept this. I tried setting the `api_url` or `base_url` to the endpoint URL, but I only kept getting `404` from vertex ai.\n\n### Motivation, pitch\n\nDedicated endpoints are recommended because they support higher throughput, lower latency, etc. And some deployment deployments do not even allow shared public endpoint and you MUST deploy via dedicated one, meaning this feature become from an optimization a necessity.\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "hnykda",
      "author_type": "User",
      "created_at": "2025-04-30T11:59:08Z",
      "updated_at": "2025-06-06T18:47:55Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10436/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10436",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10436",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:46.467333",
      "comments": [
        {
          "author": "antonkulaga",
          "body": "I have the same issues. It makes litellm totally useless for working with most of the google cloud offerings (like model garden). Has anybody found a workaround or I should drop litellm entirely?",
          "created_at": "2025-06-05T12:31:15Z"
        },
        {
          "author": "hnykda",
          "body": "I haven't found a workaround ü§∑ ... ",
          "created_at": "2025-06-05T19:50:43Z"
        },
        {
          "author": "mamalovesyou",
          "body": "Same here, could not find a workaround..",
          "created_at": "2025-06-05T22:55:39Z"
        },
        {
          "author": "mamalovesyou",
          "body": "Actually after a deeper look I was able to make it work with an openai compatible model (Deepseek-R1-Qwen3-8B-0528):\n\n```python\nfrom google.auth import default\nimport google.auth.transport.requests\nfrom litellm import completion\n\nproject_id = \"project_id\"\nlocation = \"us-central1\"\nmodel_id = \"model_i",
          "created_at": "2025-06-06T18:47:27Z"
        }
      ]
    },
    {
      "issue_number": 11487,
      "title": "[Bug]: FileNotFoundError for custom_callbacks.py when deploying via Helm (works with Docker)",
      "body": "### What happened?\n\nI'm encountering a `FileNotFoundError: [Errno 2] No such file or directory: '/etc/litellm/custom_callbacks.py'` when deploying LiteLLM via Helm with a custom_callbacks.py file. The same setup works fine when running LiteLLM via Docker directly.\n\nWith Docker-compose:\n```\nservices:\n  litellm:\n    build:\n      context: .\n      args:\n        target: runtime\n    image: ghcr.io/berriai/litellm:main-stable\n    #########################################\n    ## Uncomment these lines to start proxy with a config.yaml file ##\n    # volumes:\n    #  - ./config.yaml:/app/config.yaml <<- this is missing in the docker-compose file currently\n    # command:\n    #  - \"--config=/app/config.yaml\"\n    ##############################################\n    ports:\n      - \"4000:4000\" # Map the container port to the host, change the host port if necessary\n    volumes:\n      - ./litellm_config.yaml:/app/config.yaml\n      - ../litellm_custom_callback_class/custom_callbacks.py:/app/custom_callbacks.py\n    environment:\n      DATABASE_URL: \"postgresql://llmproxy:dbpassword9090@db:5432/litellm\"\n      STORE_MODEL_IN_DB: \"True\" # allows adding models to proxy via UI\n      AZURE_API_KEY: \"XXXXXXXXXXXXXXXXXXX\"\n    env_file:\n      - .env # Load local .env file\n    depends_on:\n      - db # Indicates that this service depends on the 'db' service, ensuring 'db' starts first\n    healthcheck:\n      # Defines the health check configuration for the container\n      test: [ \"CMD-SHELL\", \"wget --no-verbose --tries=1 http://localhost:4000/health/liveliness || exit 1\" ] # Command to execute for health check\n      interval: 30s # Perform health check every 30 seconds\n      timeout: 10s # Health check command times out after 10 seconds\n      retries: 3 # Retry up to 3 times if health check fails\n      start_period: 40s # Wait 40 seconds after container start before beginning health checks\n    command: [ \"--config\", \"/app/config.yaml\", \"--port\", \"4000\" ]\n\n  db:\n    image: postgres:16\n    restart: always\n    container_name: litellm_db\n    environment:\n      POSTGRES_DB: litellm\n      POSTGRES_USER: llmproxy\n      POSTGRES_PASSWORD: dbpassword9090\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres_data:/var/lib/postgresql/data # Persists Postgres data across container restarts\n    healthcheck:\n      test: [ \"CMD-SHELL\", \"pg_isready -d litellm -U llmproxy\" ]\n      interval: 1s\n      timeout: 5s\n      retries: 10\n\nvolumes:\n  postgres_data:\n    name: litellm_postgres_data\n```\n\nWith Helm Templates:\n\n1. Values.yaml\n```\nproxy_config:\n  litellm_settings:\n    callbacks: custom_callbacks.proxy_handler_instance\n\nvolumes:\n  - name: litellm-custom-callbacks\n    configMap:\n      name: litellm-custom-callbacks\n\n# Additional volumeMounts on the output Deployment definition.\nvolumeMounts: \n  - name: litellm-custom-callbacks\n    mountPath: /app/custom_callbacks.py\n    subPath: custom_callbacks.py\n```\n\n2. litellm-custom-callbacks (configMap)\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: {{ include \"litellm.fullname\" . }}-custom-callbacks\ndata:\n  custom_callbacks.py: |-\n{{ .Files.Get \"custom_callbacks.py\" | indent 4 }}\n```\n\n3. File appears to exist in the container:\n```\nkubectl exec -it <pod_name> -- ls /app\nDockerfile                            litellm-proxy-extras\nLICENSE                               mcp_servers.json\nMakefile                              model_prices_and_context_window.json\nREADME.md                             package-lock.json\nci_cd                                 package.json\ncodecov.yaml                          poetry.lock\ncustom_callbacks.py                   prometheus.yml\ndb_scripts                            proxy_server_config.yaml\ndeploy                                pyproject.toml\ndist                                  pyrightconfig.json\ndocker                                render.yaml\ndocker-compose.yml                    requirements.txt\nenterprise                            ruff.toml\nindex.yaml                            schema.prisma\nlitellm                               security.md\nlitellm-js                            ui\n```\n\n### Relevant log output\n\n```shell\nkubectl logs:\nprisma:warn Prisma doesn't know which engines to download for the Linux distro \"wolfi\". Falling back to Prisma engines built \"debian\".\nPlease report your experience by creating an issue at https://github.com/prisma/prisma/issues so we can add your distro to the list of known supported distros.\nprisma:warn Prisma doesn't know which engines to download for the Linux distro \"wolfi\". Falling back to Prisma engines built \"debian\".\nPlease report your experience by creating an issue at https://github.com/prisma/prisma/issues so we can add your distro to the list of known supported distros.\nPrisma schema loaded from schema.prisma\nDatasource \"client\": PostgreSQL database \"litellm\", schema \"public\" at \"postgres-dev\"\n\nThe database is already in sync with the Prisma schema.\n\nRunning generate... - Prisma Client Python (v0.11.0)\n\nSome types are disabled by default due to being incompatible with Mypy, it is highly recommended\nto use Pyright instead and configure Prisma Python to use recursive types. To re-enable certain types:\n\ngenerator client {\n  provider             = \"prisma-client-py\"\n  recursive_type_depth = -1\n}\n\nIf you need to use Mypy, you can also disable this message by explicitly setting the default value:\n\ngenerator client {\n  provider             = \"prisma-client-py\"\n  recursive_type_depth = 5\n}\n\nFor more information see: https://prisma-client-py.readthedocs.io/en/stable/reference/limitations/#default-type-limitations\n\n‚úî Generated Prisma Client Python (v0.11.0) to ./../../prisma in 391ms\n\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nERROR:    Traceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/starlette/routing.py\", line 693, in lifespan\n    async with self.lifespan_context(app) as maybe_state:\n               ~~~~~~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n\n#------------------------------------------------------------#\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n#                                                            #\n    return await anext(self.gen)\n#           'I get frustrated when the product...'            #\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/fastapi/routing.py\", line 133, in merged_lifespan\n#        https://github.com/BerriAI/litellm/issues/new        #\n#                                                            #\n    async with original_context(app) as maybe_original_state:\n               ~~~~~~~~~~~~~~~~^^^^^\n#------------------------------------------------------------#\n  File \"/usr/lib/python3.13/contextlib.py\", line 214, in __aenter__\n\n    return await anext(self.gen)\n Thank you for using LiteLLM! - Krrish & Ishaan\n\n\n\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/proxy_server.py\", line 560, in proxy_startup_event\n    await initialize(**worker_config)\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/proxy_server.py\", line 2928, in initialize\n    ) = await proxy_config.load_config(router=llm_router, config_file_path=config)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/proxy_server.py\", line 1680, in load_config\n    initialize_callbacks_on_proxy(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        value=value,\n        ^^^^^^^^^^^^\n    ...<2 lines>...\n        litellm_settings=litellm_settings,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/common_utils/callback_utils.py\", line 276, in initialize_callbacks_on_proxy\n    get_instance_fn(\n    ~~~~~~~~~~~~~~~^\n        value=value,\n        ^^^^^^^^^^^^\n        config_file_path=config_file_path,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/types_utils/utils.py\", line 50, in get_instance_fn\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/types_utils/utils.py\", line 32, in get_instance_fn\n    spec.loader.exec_module(module)  # type: ignore\n    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^\n  File \"<frozen importlib._bootstrap_external>\", line 1022, in exec_module\n  File \"<frozen importlib._bootstrap_external>\", line 1159, in get_code\n  File \"<frozen importlib._bootstrap_external>\", line 1217, in get_data\nFileNotFoundError: [Errno 2] No such file or directory: '/etc/litellm/custom_callbacks.py'\n\nERROR:    Application startup failed. Exiting.\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nlitellm-database:main-v1.71.1-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "luisgfelipeg",
      "author_type": "User",
      "created_at": "2025-06-06T16:50:44Z",
      "updated_at": "2025-06-06T16:57:03Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11487/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11487",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11487",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:46.688110",
      "comments": []
    },
    {
      "issue_number": 9653,
      "title": "Add Azure Responses API support",
      "body": "> @ishaan-jaff it looks like Responses is available in Azure now with the `2025-03-01-preview` API. We're hoping to see this added to LiteLLM soon :)\n> \n> https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/responses?tabs=python-secure \n\n _Originally posted by @marty-sullivan in [#9146](https://github.com/BerriAI/litellm/issues/9146#issuecomment-2754828442)_",
      "state": "open",
      "author": "ishaan-jaff",
      "author_type": "User",
      "created_at": "2025-03-30T19:49:22Z",
      "updated_at": "2025-06-06T16:32:28Z",
      "closed_at": null,
      "labels": [
        "april 2025"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 19,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9653/reactions",
        "total_count": 4,
        "+1": 4,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9653",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9653",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:46.688124",
      "comments": [
        {
          "author": "marty-sullivan",
          "body": "Is there anything special needed to support `computer-use-preview` models in OpenAI & Azure via proxy once Responses API is implemented? or just needs to be added to pricing?",
          "created_at": "2025-03-30T22:25:57Z"
        },
        {
          "author": "taralika",
          "body": "@ishaan-jaff do you have an ETA for this support? Thanks!",
          "created_at": "2025-04-11T21:23:41Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "Aiming for April 18th @taralika, does that work ? cc @marty-sullivan ?",
          "created_at": "2025-04-11T21:41:30Z"
        },
        {
          "author": "marty-sullivan",
          "body": "That sounds good to me. @ishaan-jaff \n\nWhat are your thoughts on the conmputer-use-preview? Will that be usable and cost-tracked through your implementation for Responses API?",
          "created_at": "2025-04-12T00:33:14Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "yes we should be able to support it with cost tracking @marty-sullivan ",
          "created_at": "2025-04-15T04:41:10Z"
        }
      ]
    },
    {
      "issue_number": 11429,
      "title": "[Bug]: /images/edits reports an error on azure gpt-image-1",
      "body": "### What happened?\n\ndeployment: docker\nversion: ghcr.io/berriai/litellm:main-v1.72.1-nightly\n\n```\nmodel_list:\n  - model_name: azure/gpt-image-1\n    litellm_params:\n      model: azure/gpt-image-1\n      api_base: os.environ/AZURE_API_BASE_WUS3_03\n      api_key: os.environ/AZURE_API_KEY_WUS3_03\n```\n\n```request\ncurl --location --request POST 'http://127.0.0.1:24000/images/edits' \\\n--header 'Authorization: Bearer sk-1234' \\\n--header 'Accept: */*' \\\n--header 'Host: 127.0.0.1:24000' \\\n--header 'Connection: keep-alive' \\\n--header 'Content-Type: multipart/form-data; boundary=--------------------------311314164617804501995051' \\\n--form 'prompt=\"ÁªÑÂêàËøô‰∏§Âè™Â∞èÁå´Âí™ Âπ∂‰∏îÁîüÊàê‰∏Ä‰∏™Â•∂Áå´ÁâàÊú¨\"' \\\n--form 'n=\"1\"' \\\n--form 'model=\"azure/gpt-image-1\"' \\\n--form 'size=\"1024x1536\"' \\\n--form 'image=@\"/Users/zai/Desktop/WechatIMG51.jpg\"' \\\n--form 'image=@\"/Users/zai/Desktop/WechatIMG2583.jpg\"' \\\n--form 'timeout=\"600\"' \\\n--form 'quality=\"high\"'\n```\n\n### Relevant log output\n\n```shell\n{\n    \"error\": {\n        \"message\": \"litellm.APIError: AzureException APIError - '<=' not supported between instances of 'str' and 'int'\",\n        \"type\": null,\n        \"param\": null,\n        \"code\": \"500\"\n    }\n}\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nmain-v1.72.1-nightly\n\n### Other\n\n- as mentioned in [this](https://github.com/BerriAI/litellm/issues/10314#issuecomment-2910577032) azure gpt-image-1 should work\n- and should support image[]= like [openai curl example](https://platform.openai.com/docs/api-reference/images/createEdit) ",
      "state": "closed",
      "author": "ZyairYH",
      "author_type": "User",
      "created_at": "2025-06-05T06:42:35Z",
      "updated_at": "2025-06-06T15:48:20Z",
      "closed_at": "2025-06-06T15:48:19Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11429/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11429",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11429",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:46.927942",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "please follow the openai curl example ",
          "created_at": "2025-06-05T20:54:21Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "this works, we just onboarded a customer with this ",
          "created_at": "2025-06-06T15:48:19Z"
        }
      ]
    },
    {
      "issue_number": 11484,
      "title": "[Bug]: uvicorn dependency version too low",
      "body": "### What happened?\n\nSimilar to this https://github.com/BerriAI/litellm/issues/7768 issue, the new releases of litellm specifies the `uvicorn` poetry dependency as: `uvicorn = {version = \"^0.29.0\", optional = true}`, which is `>=0.29.0 <0.30.0` (https://github.com/conda-forge/litellm-feedstock/issues/305)\n\nThis causes dependency conflicts in projects that use `google-adk`,\nbecause `google-adk` requires `uvicorn>=0.34.0`, while litellm enforces `uvicorn<0.30.0`.\n\nCould you please review and publish a new release with higher `uvicorn` dependency?\nThis would greatly help avoid installation issues for downstream projects.\n\nThank you in advance! Let me know if there's anything else I can do to assist.\n\n\n### Relevant log output\n\n```shell\nThe following packages are incompatible\n‚îú‚îÄ google-adk =* * is installable and it requires\n‚îÇ  ‚îî‚îÄ uvicorn >=0.34.0 * with the potential options\n‚îÇ     ‚îú‚îÄ uvicorn [0.34.0|0.34.1|0.34.2|0.34.3], which can be installed;\n‚îÇ     ‚îî‚îÄ uvicorn [0.34.0|0.34.1|0.34.2|0.34.3] would require\n‚îÇ        ‚îî‚îÄ __win =* *, which is missing on the system;\n‚îî‚îÄ litellm =* * is not installable because there are no viable options\n   ‚îú‚îÄ litellm [1.34.0|1.34.1|...|1.63.0] would require\n   ‚îÇ  ‚îî‚îÄ uvicorn >=0.22.0,<0.23.0 *, which conflicts with any installable versions previously reported;\n   ‚îî‚îÄ litellm [1.63.0|1.63.11|...|1.72.0] would require\n      ‚îî‚îÄ uvicorn >=0.29.0,<0.30.0 *, which conflicts with any installable versions previously reported.\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.0\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "zhiruiwang",
      "author_type": "User",
      "created_at": "2025-06-06T15:14:16Z",
      "updated_at": "2025-06-06T15:14:16Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11484/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11484",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11484",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:47.157506",
      "comments": []
    },
    {
      "issue_number": 11328,
      "title": "[Bug]: Mismatch in uvicorn dependency on conda-forge",
      "body": "### What happened?\n\nSimilar to this https://github.com/BerriAI/litellm/issues/7768 issue, I've noticed that new releases of litellm on conda-forge still pin `uvicorn` to `>=0.22.0,<0.30.0` (based on the package [METADATA](https://github.com/conda-forge/litellm-feedstock/blob/main/recipe/meta.yaml)), while the GitHub repository shows an updated requirement for `uvicorn>=0.29.0`.\n\nThis discrepancy causes dependency conflicts in projects that use `google-adk`,\nbecause `google-adk` requires `uvicorn>=0.34.0`, while litellm on conda-forge enforces `uvicorn<0.30.0`.\n\nCould you please review and publish a new release on conda-forge that reflects the correct `uvicorn` dependency?\nThis would greatly help avoid installation issues for downstream projects.\n\nThank you in advance! Let me know if there's anything else I can do to assist.\n\n\n### Relevant log output\n\n```shell\nThe following packages are incompatible\n‚îú‚îÄ google-adk =* * is installable and it requires\n‚îÇ  ‚îî‚îÄ uvicorn >=0.34.0 * with the potential options\n‚îÇ     ‚îú‚îÄ uvicorn [0.34.0|0.34.1|0.34.2|0.34.3], which can be installed;\n‚îÇ     ‚îî‚îÄ uvicorn [0.34.0|0.34.1|0.34.2|0.34.3] would require\n‚îÇ        ‚îî‚îÄ __win =* *, which is missing on the system;\n‚îî‚îÄ litellm =* * is not installable because there are no viable options\n   ‚îú‚îÄ litellm [1.34.0|1.34.1|...|1.63.0] would require\n   ‚îÇ  ‚îî‚îÄ uvicorn >=0.22.0,<0.23.0 *, which conflicts with any installable versions previously reported;\n   ‚îî‚îÄ litellm [1.63.0|1.63.11|...|1.72.0] would require\n      ‚îî‚îÄ uvicorn >=0.29.0,<0.30.0 *, which conflicts with any installable versions previously reported.\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.0\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "zhiruiwang",
      "author_type": "User",
      "created_at": "2025-06-02T17:36:26Z",
      "updated_at": "2025-06-06T15:02:34Z",
      "closed_at": "2025-06-06T15:02:34Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11328/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11328",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11328",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:47.157526",
      "comments": []
    },
    {
      "issue_number": 11083,
      "title": "[Bug]: litellm fails to block requests over end-user budget when user header used ",
      "body": "### What happened?\n\n\nRight now when you set a budget for a user and they exceed that budget LiteLLM doesn't do anything, they can continue as normal without being rejected.\n\nThis can result in incurring massive, unexpected costs.\n\n<img width=\"1185\" alt=\"image\" src=\"https://github.com/user-attachments/assets/5daa4c8c-b4dc-4348-9223-f806d8490e3c\" />\n\nPlease see the various discussions and attempts to raise this as a serious concern here: https://github.com/BerriAI/litellm/pull/9658\n\nThis has also been confirmed by @ross-w @amyb-asu @enthusiastio\n\n---\n\nFixes:\n\n- I raised a PR that fixes this on March 31: https://github.com/BerriAI/litellm/pull/9658 (closed without the bug being fixed)\n- ross-w's Also has a confirmed working fix here: https://github.com/BerriAI/litellm/compare/main...ross-w:litellm:main\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nmain-latest (main-v1.70.4-nightly)\n",
      "state": "open",
      "author": "sammcj",
      "author_type": "User",
      "created_at": "2025-05-23T06:20:12Z",
      "updated_at": "2025-06-06T13:39:23Z",
      "closed_at": null,
      "labels": [
        "bug",
        "awaiting: user response"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 26,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11083/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11083",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11083",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:47.157533",
      "comments": []
    },
    {
      "issue_number": 11480,
      "title": "[Bug]: In-memory Prompt Injection Detection not working despite being activated in config",
      "body": "### What happened?\n\nHi!\n\nI've configured LiteLLM to use In-memory Prompt Injection Detection by adding the relevant block to the config as below, however, the detect_prompt_injection hook does not seem to be applied to any requests. I have tested this using the curl commands in the documentation and from other external systems but all requests simply pass. There is also no indication in the logs of the config being loaded or triggering despite requests passing through. Could I perhaps be missing a part of the configuration?\n\n```litellm_settings: \n  callbacks: [\"detect_prompt_injection\"]\n  prompt_injection_params:\n    heuristics_check: true\n    similarity_check: true\n    llm_api_check: true\n    llm_api_name: Model1\n    llm_api_system_prompt: \"Detect if prompt is safe to run. Return 'UNSAFE' if not.\" \n    llm_api_fail_call_string: \"UNSAFE\" \n  drop_params: True\n  success_callback: [\"langfuse\"]\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "lyle-sec",
      "author_type": "User",
      "created_at": "2025-06-06T11:21:24Z",
      "updated_at": "2025-06-06T11:21:24Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11480/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11480",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11480",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:47.157539",
      "comments": []
    },
    {
      "issue_number": 11476,
      "title": "[Bug]: Much longer running time Vision Anthropic model compared to anthropic SDK",
      "body": "### What happened?\n\n![Image](https://github.com/user-attachments/assets/479a87de-ad69-4b3d-bdf3-43c0f975dd66)\n\n\nSame problem in other anthropic models like `claude-3-7-sonnet-latest`, etc.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "anhnh2002",
      "author_type": "User",
      "created_at": "2025-06-06T08:19:58Z",
      "updated_at": "2025-06-06T08:22:08Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11476/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11476",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11476",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:47.157545",
      "comments": []
    },
    {
      "issue_number": 11322,
      "title": "[Bug] [litellm proxy]: Gemini second requests never works until proxy is restarted",
      "body": "### What happened?\n\nFirst chat passes, second ones dies on\n```\n    raise self._make_status_error_from_response(err.response) from None\nopenai.InternalServerError: Error code: 500 - {'error': {'message': 'litellm.APIConnectionError: Could not resolve project_id\n\n\\nTraceback (most recent call last):\\n  File \"/usr/lib/python3.13/site-packages/litellm/main.py\", line 512, in acompletion\\n    response = await init_response\\n               ^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1308, in async_completion\\n    _auth_header, vertex_project = await self._ensure_access_token_async(\\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n    ...<3 lines>...\\n    )\\n    ^\\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 373, in _ensure_access_token_async\\n    raise e\\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 368, in _ensure_access_token_async\\n    return await asyncify(self.get_access_token)(\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n    ...<2 lines>...\\n    )\\n    ^\\n  File \"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/asyncify.py\", line 57, in wrapper\\n    return await anyio.to_thread.run_sync(\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n    ...<3 lines>...\\n    )\\n    ^\\n  File \"/usr/lib/python3.13/site-packages/anyio/to_thread.py\", line 56, in run_sync\\n    return await get_async_backend().run_sync_in_worker_thread(\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n    )\\n    ^\\n  File \"/usr/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 2357, in run_sync_in_worker_thread\\n    return await future\\n           ^^^^^^^^^^^^\\n  File \"/usr/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 864, in run\\n    result = context.run(func, *args)\\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 349, in get_access_token\\n    raise ValueError(\"Could not resolve project_id\")\\nValueError: Could not resolve project_id\\n. Received Model Group=gemini-2.0-flash\\nAvailable Model Group Fallbacks=None', 'type': None, 'param': None, 'code': '500'}}\n```\n\nIf model is \"gpt-4\" it works fine. No gemini models work correctly.\n```python\nfrom openai import OpenAI\n\napi_key = \"sk-***\"\nbase_url = \"http://localhost:4000\"\nmodel = \"gemini-2.0-flash\"\n\nif __name__ == \"__main__\":\n    client = OpenAI(api_key=api_key, base_url=base_url)\n    \n    print(\"1. First chat completion:\")\n    response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": \"Hello, who are you?\"}]\n    )\n    print(response.choices[0].message.content)\n    print()\n    \n    print(\"2. Second chat completion:\")\n    response2 = client.chat.completions.create(\n        model=model,\n       messages=[{\"role\": \"user\", \"content\": \"Can you count to 5?\"}]\n    )\n    print(response2.choices[0].message.content)\n```\n\n### Relevant log output\n\n```shell\nlitellm\nAvailable Model Group Fallbacks=None LiteLLM Retried: 1 times, LiteLLM Max Retries: 2\n\nlitellm\nReceived Model Group=gemini-2.0-flash\n\nlitellm\nValueError: Could not resolve project_id\n\nlitellm\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 349, in get_access_token\n    raise ValueError(\"Could not resolve project_id\")\n\nlitellm\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 368, in _ensure_access_token_async\n    return await asyncify(self.get_access_token)()\n\nlitellm\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1308, in async_completion\n    _auth_header, vertex_project = await self._ensure_access_token_async()\n\nlitellm\n  File \"/usr/lib/python3.13/site-packages/litellm/main.py\", line 512, in acompletion\n    response = await init_response\n\nlitellm\n  File \"/usr/lib/python3.13/site-packages/litellm/utils.py\", line 1353, in wrapper_async\n    result = await original_function(*args, **kwargs)\n\nlitellm\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 3737, in make_call\n    response = await self.make_call(original_function, *args, **kwargs)\n\nlitellm\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 3619, in async_function_with_retries\n    raise original_exception\n\nlitellm\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 3350, in async_function_with_fallbacks\n    response = await self.async_function_with_fallbacks(**kwargs)\n\nlitellm\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 966, in acompletion\n    responses = await llm_responses\n\nlitellm\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/common_request_processing.py\", line 391, in base_process_llm_request\n    raise APIConnectionError(\"Could not resolve project_id\")\n\nlitellm\nlitellm.exceptions.APIConnectionError: litellm.APIConnectionError: Could not resolve project_id\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nmain-v1.71.1-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "KadekM",
      "author_type": "User",
      "created_at": "2025-06-02T09:09:33Z",
      "updated_at": "2025-06-06T08:13:22Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11322/reactions",
        "total_count": 3,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11322",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11322",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:47.157550",
      "comments": [
        {
          "author": "gjmveloso",
          "body": "Are you running the proxy on GCP/GKE/Cloud Run?\n\nI have similar issues omitting `vertex_ai_project` in `config.yaml` as it works for `/chat/completions` but not for `/images`.\n\nSeems like the logic for building endpoints URLs (and caching credentials) are partially broken when solely relying on GCP'",
          "created_at": "2025-06-05T22:46:13Z"
        },
        {
          "author": "KadekM",
          "body": "in GKE, and it seems like caching issue to me from what I can tell (not much though)",
          "created_at": "2025-06-05T22:47:50Z"
        },
        {
          "author": "KadekM",
          "body": "@gjmveloso your suggestion helped, for completness sake, adding:\n```\nlitellm_params:\n            model: vertex_ai/gemini-2.0-flash\n            vertex_project: my_project\n```\nhelped\n\nbut the bug with caching is real so Im keeping this open (as its non sensible that it works for 1st request and not fo",
          "created_at": "2025-06-06T08:13:21Z"
        }
      ]
    },
    {
      "issue_number": 9051,
      "title": "[Feature]: Support Mistral OCR",
      "body": "### The Feature\n\nSupport for sending stuff to Mistral OCR\n\n### Motivation, pitch\n\nI am working on a solution where OCR would be very helpful, and given that Mistral has the SOTA, it would be awesome to be able to use them. \n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "MalteHB",
      "author_type": "User",
      "created_at": "2025-03-07T12:11:55Z",
      "updated_at": "2025-06-06T08:10:42Z",
      "closed_at": "2025-04-15T05:06:34Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9051/reactions",
        "total_count": 7,
        "+1": 7,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9051",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9051",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:47.367265",
      "comments": [
        {
          "author": "wagnerjt",
          "body": "Maybe you can use the [custom provider](https://docs.litellm.ai/docs/providers/custom_llm_server) and write against the rest client. The REST spec is [here](https://docs.mistral.ai/api/#tag/ocr/)",
          "created_at": "2025-03-07T15:13:01Z"
        },
        {
          "author": "antonkulaga",
          "body": "I will be super-useful if it will be implemened",
          "created_at": "2025-03-08T18:59:27Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "hi @antonkulaga, @MalteHB what API spec should mistral OCR follow on litellm ? \n\n\nDo you want to call it in the native mistral api spec ? If it's on the proxy, we can add a mistral api pass through like we do for cohere: https://docs.litellm.ai/docs/pass_through/cohere ",
          "created_at": "2025-03-10T14:07:00Z"
        },
        {
          "author": "qdrddr",
          "body": "This potentially can be used with `/files` endpoint or a pass through I think",
          "created_at": "2025-03-28T23:06:55Z"
        },
        {
          "author": "krrishdholakia",
          "body": "So it looks like a new endpoint we need to onboard - /ocr\n\nthe response doesn't look openai like - how would you expect to call this?\n\n\n<img width=\"567\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/d4ca265a-b905-4826-a321-709f3c4f1e10\" />\n",
          "created_at": "2025-04-04T18:51:05Z"
        }
      ]
    },
    {
      "issue_number": 9863,
      "title": "[Bug]: GCP Service Account Credentials not refreshing appropriately",
      "body": "### What happened?\n\nI believe that after the token expiration, LiteLLM is not refreshing the access token as needed. My application is running properly for some amount of time, but at some point I receive the errors below. I'm assuming the token has a lifetime of 12hrs so this is hard to pin down since it occurs only overnight. The error is manifesting as cannot resolve `project_id`, it only happens during the refresh cycle. A brief glance at the code, the project_id is optional and I guess the initial access token is fine without it but refresh is not.\n\nThis specifically related to VertexAI on GCP via built in Service Account. Not a service account file, ADC, etc although those may have similar issues. In this scenario, providing a specific project is not necessary since I'm retrieving and refreshing a token within the same project. Perhaps in this scenario, project_id should be set once the initial token is received based on the metadata.\n\nOther notes:\n\n1. versions\n* `1.65.4.post1` - issue occurred\n* `1.65.3` - in testing\n1. I'm 8+ hours into a fresh deployment and the issue has not yet occurred but I assume will around the 12hr mark. UPDATE: I am 24 hours in and haven't seen the issue but I did downgrade the version as well. I will keep an eye out and may try upgrading again.\n1. I presume this will show up on ADC based credentials as well but haven't tested with this length of time. \n1. When using the official GCP libraries and running on GCP, these token issues should automatically handled. \n1. My app runs in Cloud Run with a service account and has no issues initially but eventually hits this without a redeployment.\n1. Hard to tell if #8086, #8424, #8771 are all related\n\n\n[1] https://cloud.google.com/docs/authentication/token-types#at-lifetime\n\nPotential Mitigation:\n\nI'm going to try to see if this fixes the issue:\n\n```\n    response = completion(\n        model=\"gemini-2.0-flash-001\",\n        vertex_project=\"my_project_id\",\n...\n```\n\n\n### Relevant log output\n\n```shell\nERROR 2025-04-09T17:50:26.228856Z Traceback (most recent call last): File \"/usr/local/lib/python3.11/site-packages/litellm/main.py\", line 2449, in completion model_response = vertex_chat_completion.completion( # type: ignore\nDEFAULT 2025-04-09T17:50:26.228858Z ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nDEFAULT 2025-04-09T17:50:26.228861Z File \"/usr/local/lib/python3.11/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1292, in completion\nDEFAULT 2025-04-09T17:50:26.228864Z _auth_header, vertex_project = self._ensure_access_token(\nDEFAULT 2025-04-09T17:50:26.228867Z ^^^^^^^^^^^^^^^^^^^^^^^^^^\nDEFAULT 2025-04-09T17:50:26.228870Z File \"/usr/local/lib/python3.11/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 136, in _ensure_access_token\nDEFAULT 2025-04-09T17:50:26.228873Z return self.get_access_token(\nDEFAULT 2025-04-09T17:50:26.228875Z ^^^^^^^^^^^^^^^^^^^^^^\nDEFAULT 2025-04-09T17:50:26.228878Z File \"/usr/local/lib/python3.11/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 336, in get_access_token\nDEFAULT 2025-04-09T17:50:26.228882Z raise ValueError(\"Could not resolve project_id\")\nDEFAULT 2025-04-09T17:50:26.228887Z ValueError: Could not resolve project_id\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.65.4.post1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "ryanc4ai",
      "author_type": "User",
      "created_at": "2025-04-09T21:13:02Z",
      "updated_at": "2025-06-06T07:33:50Z",
      "closed_at": "2025-05-19T03:43:02Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 34,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9863/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9863",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9863",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:49.644571",
      "comments": []
    },
    {
      "issue_number": 8040,
      "title": "[Bug]: 'Mode batch not supported' when trying to set up an Azure GlobalBatch deployment.",
      "body": "### What happened?\n\n- add a new [batch deployment model](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/batch?tabs=standard-input&pivots=programming-language-python#global-batch-deployment) in Azure OpenAI\n- Make a new entry in proxy-config.yaml of litellm proxy for the newly added batch model:\n```\nmodel_name: gpt-4o-2024-11-20-batch-no-filter\n    litellm_params:\n      model: azure/gpt-4o-2024-11-20-batch-no-filter\n      api_base: <API_BASE>\n      api_key: <API_KEY>\n    model_info:\n      base_model: azure/global-standard/gpt-4o-2024-11-20\n      mode: batch\n\n```\n- Run health checks\n\n### Relevant log output\n\n```shell\n\"unhealthy_endpoints\": [\n    {\n      \"api_base\": <API_BASE>,\n      \"model\": \"azure/gpt-4o-2024-11-20-batch-no-filter\",\n      \"cache\": {\n        \"no-cache\": true\n      },\n      \"error\": \"Mode batch not supported. See modes here: https://docs.litellm.ai/docs/proxy/health\\nHave you set 'mode' - https://docs.litellm.ai/docs/proxy/health#embedding-models\\nstack trace: Traceback (most recent call last):\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/main.py\\\", line 5363, in ahealth_check\\n    raise Exception(\\n        f\\\"Mode {mode} not supported. See modes here: https://docs.litellm.ai/docs/proxy/health\\\"\\n    )\\nException: Mode batch not supported. See modes here: https://docs.litellm.ai/docs/proxy/health\\n\"\n    }\n  ]\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.58.4\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "antisilent",
      "author_type": "User",
      "created_at": "2025-01-28T00:48:14Z",
      "updated_at": "2025-06-06T07:30:34Z",
      "closed_at": "2025-05-06T00:02:00Z",
      "labels": [
        "bug",
        "mlops user request",
        "stale"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8040/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8040",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8040",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:49.644596",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-04-29T00:01:54Z"
        },
        {
          "author": "realmorrisliu",
          "body": "same issue",
          "created_at": "2025-06-06T07:03:01Z"
        },
        {
          "author": "realmorrisliu",
          "body": "@ishaan-jaff @antisilent I'm trying to fix this issue in PR #11475 ",
          "created_at": "2025-06-06T07:30:33Z"
        }
      ]
    },
    {
      "issue_number": 11430,
      "title": "[Bug]: bedrock does not support parameters: ['tools'], for model=us.meta.llama4-scout-17b-instruct-v1:0",
      "body": "### What happened?\n\n```\nLiteLLM completion() model= us.meta.llama4-scout-17b-instruct-v1:0; provider = bedrock\n2025-06-05 14:19:44,120 - ERROR - fast_api.py:788 - Error in event_generator: litellm.UnsupportedParamsError: bedrock does not support parameters: ['tools'], for model=us.meta.llama4-scout-17b-instruct-v1:0.\n```\n\n![Image](https://github.com/user-attachments/assets/36bf22d4-fd09-4fc8-878a-58ca763276c1)\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "dttran-glo",
      "author_type": "User",
      "created_at": "2025-06-05T07:25:02Z",
      "updated_at": "2025-06-06T06:37:04Z",
      "closed_at": "2025-06-06T06:37:04Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11430/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11430",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11430",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:49.870817",
      "comments": []
    },
    {
      "issue_number": 11441,
      "title": "[Bug]:",
      "body": "### What happened?\n\nBedrock Nova family of models: Amazon Nova Premier, Amazon Nova Pro, Amazon Nova Lite, Amazon Nova \n\nHave incorrect model input(context window), and output tokens according to https://docs.aws.amazon.com/nova/latest/userguide/what-is-nova.html\n\ni.e https://github.com/BerriAI/litellm/blob/69c9d75f2024aa6d8062e5c9cbd89fb56065bcc9/model_prices_and_context_window.json#L9985 says 4096 when it should be 10K\n\n\n\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.72.0\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "zachvida",
      "author_type": "User",
      "created_at": "2025-06-05T13:09:27Z",
      "updated_at": "2025-06-06T06:25:19Z",
      "closed_at": "2025-06-06T06:25:19Z",
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11441/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11441",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11441",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:49.870841",
      "comments": []
    },
    {
      "issue_number": 8372,
      "title": "[Feature]: Add Huggingface Text Embeddings Inference (TEI) provider for `/rerank` ",
      "body": "### The Feature\n\nLocally running [TEI](https://huggingface.co/docs/text-embeddings-inference) provider for LiteLLM Proxy. It supports various endpoints such as:\n- /decode\n- /embed\n- /embed_all\n- /embed_sparse\n- /health\n- /info\n- /metrics\n- /predict\n- /rerank\n- /similarity\n- /tokenize\n- /v1/embeddings\n- and has a Swagger at `/docs`\n\n**Embeddings** & **Rerankings** are the most valuable. Also, TEI supports ApiKey Authorization to be empty or set with `--api-key`.\n\nExample of how to run TEI on macOS:\n1. Install\n```shell\nbrew install text-embeddings-inference\n```\n2. run\n```shell\nHF_TEI_LOCAL_API_KEY=\"mykey\"\nmodel=BAAI/bge-reranker-v2-m3\nbatchsize=32\nrevision=refs/heads/main\n\n/opt/homebrew/bin/text-embeddings-router --model-id $model \\\n  --api-key \"${HF_TEI_LOCAL_API_KEY}\" \\\n  --revision $revision \\\n  --port 8073 \\\n  --max-client-batch-size $batchsize\n```\n\n3. Try\n```shell\nHF_TEI_LOCAL_API_KEY=\"mykey\"                                                                 \ncurl -X 'POST' \\\n  'http://localhost:8073/rerank' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -H \"Authorization: Bearer $HF_TEI_LOCAL_API_KEY\" \\\n  -d '{\n  \"query\": \"What RAID is supported by VyOS?\",\n  \"raw_scores\": false,\n  \"return_text\": false,\n  \"texts\": [\n    \"RAID-1 {#raid} A Redundant Array of Independent Disks (RAID) uses two or more hard disk drives to improve disk speed, store more data, and/or provide fault tolerance. There are several storage schemes possible in a RAID array, each offering a different combination of storage, reliability, and/or performance. The VyOS system supports a \\\"RAID 1\\\" deployment. RAID 1 allows two or more disks to mirror one another to provide system fault tolerance. In a RAID 1 solution, every sector of one disk is duplicated onto every sector of all disks in the array. Provided even one disk in the RAID 1 set is operational, the system continues to run, even through disk replacement (provided that the hardware supports in-service replacement of drives). RAID 1 can be implemented using special hardware or it can be implemented in software. The VyOS system supports software RAID 1 on two disks. The VyOS implementation of RAID 1 allows the following:\",\n    \"Installation Implications {#raid_instalation} The VyOS systems installation utility provides several options for installing to a RAID 1 set. You can: Use the install system to create the RAID 1 set Use the underlying Linux commands to create a RAID 1 set before running the install system command. Use a previously-created RAID 1 set. :::: note ::: title Note ::: Before a permanent installation, VyOS runs a live installation :::: Configuration Single disk, install as normal When the VyOS system is installed, it automatically detects the presence of two disks not currently part of a RAID array. In these cases, the VyOS installation utility automatically offers you the option of configuring RAID 1 mirroring for the drives, with the following prompt. none Would you like to configure RAID 1 mirroring on them? If you do not want to configure RAID 1 mirroring, enter \\\"No\\\" at the prompt and continue with installation in the normal way.\"\n  ],\n  \"truncate\": false,\n  \"truncation_direction\": \"Right\"\n}'\n```\n4. result\n```json\n[{\"index\":0,\"score\":0.9957188},{\"index\":1,\"score\":0.6728693}]\n```\n\n### Motivation, pitch\n\nHuggingface TEI can run locally on your servers, macOS, Linux & Windows and allows users almost unlimited access to world-popular embeddings and reranking models.\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\nhttps://www.linkedin.com/in/damien-e",
      "state": "closed",
      "author": "qdrddr",
      "author_type": "User",
      "created_at": "2025-02-07T21:07:41Z",
      "updated_at": "2025-06-06T06:23:02Z",
      "closed_at": "2025-06-06T06:23:02Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8372/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8372",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8372",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:49.870849",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Hi @qdrddr TEI is already supported - https://docs.litellm.ai/docs/providers/huggingface#embedding\n",
          "created_at": "2025-02-08T05:21:32Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Closing for now. Let me know if there's a specific route you needed. ",
          "created_at": "2025-02-08T05:22:13Z"
        },
        {
          "author": "qdrddr",
          "body": "[/huggingface](https://docs.litellm.ai/docs/providers/huggingface#embedding) Provider does not work with locally running TEI with apikey set @krrishdholakia ",
          "created_at": "2025-02-08T20:08:01Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Hi @qdrddr can you share the error you see?",
          "created_at": "2025-02-08T20:22:24Z"
        },
        {
          "author": "qdrddr",
          "body": "I have tried both ways with `extra_headers` and `api_key` in the LiteLLM Proxy config @krrishdholakia \n```shell\nHF_TEI_LOCAL_API_KEY=\"myapiky1\"\nHF_TEI_LOCAL_API_KEY_BEARER=\"Bearer myapiky1\"\n```\n```yaml\n    - model_name: \"hf-tei/bge-reranker-v2-m3\"\n      litellm_params:\n        #TEI does not work yet",
          "created_at": "2025-02-10T16:51:10Z"
        }
      ]
    },
    {
      "issue_number": 7970,
      "title": "[Feature]: Support for Anthropic Citations API",
      "body": "### The Feature\n\nThere is a new citations API: https://www.anthropic.com/news/introducing-citations-api. \n\nUnfortunately, it requires a richer message structure, such as this (content, text, type, ...):\n\n```\n messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"document\",\n                    \"source\": {\n                        \"type\": \"text\",\n                        \"media_type\": \"text/plain\",\n                        \"data\": \"The grass is green. The sky is blue.\",\n                    },\n                    \"title\": \"My Document\",\n                    \"context\": \"This is a trustworthy document.\",\n                    \"citations\": {\"enabled\": True},\n                },\n                {\"type\": \"text\", \"text\": \"What color is the grass and sky?\"},\n            ],\n        }\n    ]\n```\n\nIf you try that with latest litellm, e.g.:\n```\nfrom litellm import completion\n\ncompletion(\n    model=\"vertex_ai/claude-3-5-sonnet-v2@20241022\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"document\",\n                    \"source\": {\n                        \"type\": \"text\",\n                        \"media_type\": \"text/plain\",\n                        \"data\": \"The grass is green. The sky is blue.\",\n                    },\n                    \"title\": \"My Document\",\n                    \"context\": \"This is a trustworthy document.\",\n                    \"citations\": {\"enabled\": True},\n                },\n                {\"type\": \"text\", \"text\": \"What color is the grass and sky?\"},\n            ],\n        }\n    ],\n)\n```\n\nyou will get:\n```\nException: Invalid user message={'role': 'user', 'content': [{'type': 'document', 'source': {'type': 'text', 'media_type': 'text/plain', 'data': 'The grass is green. The sky is blue.'}, 'title': 'My Document', 'context': 'This is a trustworthy document.', 'citations': {'enabled': True}}, {'type': 'text', 'text': 'What color is the grass and sky?'}]} at index 0. Please ensure all user messages are valid OpenAI chat completion messages.\n```\n\n### Motivation, pitch\n\nEh, it's in the post! \n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "hnykda",
      "author_type": "User",
      "created_at": "2025-01-24T11:12:37Z",
      "updated_at": "2025-06-06T06:17:49Z",
      "closed_at": "2025-02-08T06:27:03Z",
      "labels": [
        "enhancement",
        "feb 2025",
        "anthropic"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 28,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/7970/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 1,
        "eyes": 0
      },
      "assignees": [
        "krrishdholakia"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/7970",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/7970",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:50.069112",
      "comments": []
    },
    {
      "issue_number": 3222,
      "title": "[Bug]: Slack alerting - requests are hanging triggered on failed requests ",
      "body": "### What happened?\n\nWhen a request fails we see `requests are hanging` alerts triggered too. This should be fixed. If a request fails != request hanging \n\n### Relevant log output\n\n_No response_\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "ishaan-jaff",
      "author_type": "User",
      "created_at": "2024-04-22T21:42:47Z",
      "updated_at": "2025-06-06T06:11:14Z",
      "closed_at": "2024-04-27T15:34:06Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/3222/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/3222",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/3222",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:50.069137",
      "comments": [
        {
          "author": "YiShiYiYuan",
          "body": "Has it fixed? I still get lots of hanging alert when using embedding models, whether success or failed.\n\nhttps://github.com/BerriAI/litellm/pull/11343",
          "created_at": "2025-06-06T06:11:13Z"
        }
      ]
    },
    {
      "issue_number": 11466,
      "title": "[Bug]: [Integration] Only logs appear in Lunary Cloud dashboard, but no tracing or conversations (with LiteLLM proxy)",
      "body": "### What happened?\n\nI have successfully integrated LiteLLM proxy with Lunary Cloud (SaaS) following the documentation. My events are reaching Lunary Cloud (I can see logs in the dashboard under ‚ÄúLogs‚Äù tab), but I do not see any data under ‚ÄúTracing‚Äù or ‚ÄúConversations‚Äù.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nlatest\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "mkagit",
      "author_type": "User",
      "created_at": "2025-06-06T00:58:27Z",
      "updated_at": "2025-06-06T00:58:27Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11466/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11466",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11466",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:50.263853",
      "comments": []
    },
    {
      "issue_number": 7571,
      "title": "[Bug]: cohere multimodal embedding not considering images",
      "body": "### What happened?\n\nI am trying to test the [cohere multimodal embeddings](https://docs.cohere.com/docs/multimodal-embeddings). Based on the [doc](https://docs.litellm.ai/docs/embedding/supported_embedding#image-embeddings) this is how to send images\r\n\r\n```json\r\n{\r\n  \"model\": \"cohere/embed-english-v3.0\",\r\n  \"input\": [\"<base64 encoded image>\"]\r\n}\r\n```\r\n\r\nwith debug I see it is handled as text and not image\r\n\r\n```bash\r\ncurl -X POST \\\r\nhttps://api.cohere.ai/v1/embed \\\r\n....\r\n-d '{'model': 'embed-english-v3.0', 'texts': ['/9j/4AAQSkZJRgABAQAAAQABAAD//....'], 'input_type': 'search_document'}'\r\n```\r\n\r\nBased on [cohere doc](https://docs.cohere.com/docs/multimodal-embeddings) I used this post sample to https://api.cohere.com/v1/embed\r\n\r\n```json\r\n{\r\n    \"model\": \"embed-english-v3.0\",\r\n    \"input_type\": \"image\", \r\n    \"embedding_types\": [\"float\"],\r\n    \"images\": [\"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD//....\"]\r\n}\r\n```\r\n\r\nand got a correct response like this\r\n\r\n```json\r\n{\r\n    \"id\": \"5e1c96a0-9755-4434-9bb0-bae910351928\",\r\n    \"texts\": [],\r\n    \"images\": [\r\n        {\r\n            \"width\": 400,\r\n            \"height\": 400,\r\n            \"format\": \"jpeg\",\r\n            \"bit_depth\": 24\r\n        }\r\n    ],\r\n    \"embeddings\": {\r\n        \"float\": [\r\n            [\r\n                -0.007247925,\r\n                -0.041229248,\r\n                -0.023223877,\r\n                -0.08392334,\r\n                ...\r\n            ]\r\n        ]\r\n    },\r\n    \"meta\": {\r\n        \"api_version\": {\r\n            \"version\": \"1\"\r\n        },\r\n        \"billed_units\": {\r\n            \"images\": 1\r\n        }\r\n    },\r\n    \"response_type\": \"embeddings_by_type\"\r\n}\r\n```\r\n\r\nCan liteLLM API be update to translate to this usage?\n\n### Relevant log output\n\n_No response_\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nmain-v1.55.12\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "ladrians",
      "author_type": "User",
      "created_at": "2025-01-05T12:22:37Z",
      "updated_at": "2025-06-06T00:02:02Z",
      "closed_at": "2025-06-06T00:02:02Z",
      "labels": [
        "bug",
        "stale",
        "feb 2025"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/7571/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/7571",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/7571",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:50.263874",
      "comments": [
        {
          "author": "ladrians",
          "body": "the issue seems to be more general? now testing with `vertex_ai/multimodalembedding@001` something like this\r\n\r\n```json\r\n{\r\n    \"model\":\"vertex_ai/multimodalembedding@001\",\r\n    \"input\":[\"iVBORw0KGgoAAAANSUhEUgAAAGQAAABkBAMAAACCzIhnAAAAG1BMVEURAAD///...\"\r\n    ]\r\n}\r\n```\r\n\r\nnoticing is handled as text",
          "created_at": "2025-01-05T12:43:24Z"
        },
        {
          "author": "ladrians",
          "body": "I got access to [amazon.titan-embed-image-v1](https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/model-catalog/serverless/amazon.titan-embed-image-v1) and worked fine, the mistake was that I was sending the data as follows:\r\n\r\n`\"input\":[\"base_64\"]\r\n`\r\n\r\nbut need to be\r\n\r\n`\"input",
          "created_at": "2025-01-15T18:22:36Z"
        },
        {
          "author": "ladrians",
          "body": "any comment on this issue? I am interested in continuing the evaluation on `vertex_ai/multimodalembedding`.\nthanks in advance,",
          "created_at": "2025-01-28T23:00:43Z"
        },
        {
          "author": "Jonarod",
          "body": "Can confirm this bug, but I think I figured it out: the issue comes from the `is_base64_encoded(s=input_element):` check [here](https://github.com/BerriAI/litellm/blob/ea985dda0b46cefe75205ad285a22fe6310b51cd/litellm/llms/vertex_ai/multimodal_embeddings/embedding_handler.py#L228)\n\nThis check is enfo",
          "created_at": "2025-02-18T19:45:36Z"
        },
        {
          "author": "ladrians",
          "body": "great! I could validate that removing from the image \"data:image/png;base64,\" worked. Is this fix comming for the upcoming versions?",
          "created_at": "2025-02-27T18:27:09Z"
        }
      ]
    },
    {
      "issue_number": 8897,
      "title": "[Bug]: monthly spend chart not showing all spends",
      "body": "### What happened?\n\nThe \"monthly spend\" chart in LiteLLM UI for internal_user role may show incomplete spend:\n\n![Image](https://github.com/user-attachments/assets/da72a678-b030-4f4e-aed9-d491853ef463)\n\n(also note that dates in this chart are off-by-one in my timezone, which is reported in #8393 but remains un-fixed for months!)\n\n-----\n\nDevTools confirms that the `/global/spend/logs` API returns multiple entries in a same day, and the UI only renders the last entry:\n\n![Image](https://github.com/user-attachments/assets/d18a3b00-40c9-4516-a05a-34359e7bf5ef)\n\nThe [source code](https://github.com/BerriAI/litellm/blame/5670a9f8b7f0c2753ebed81908d76a78f090f747/litellm/proxy/spend_tracking/spend_management_endpoints.py#L2178) shows that LiteLLM runs `SELECT * FROM \"MonthlyGlobalSpendPerUserPerKey\"  WHERE \"user\" = $1 ORDER BY \"date\";` for this API.\n\nThis SQL does not sum the result if the user has multiple keys:\n\n![Image](https://github.com/user-attachments/assets/51e24225-c051-4dde-943f-3fda0f74f59f)\n\nTherefore, to fix this bug, we should sum the spend for all `api_key`s either in SQL or in [the frontend](https://github.com/BerriAI/litellm/blame/5670a9f8b7f0c2753ebed81908d76a78f090f747/ui/litellm-dashboard/src/components/usage.tsx#L384).\n\n### Relevant log output\n\n```shell\nN/A\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.61.13-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "xmcp",
      "author_type": "User",
      "created_at": "2025-02-28T17:26:10Z",
      "updated_at": "2025-06-06T00:01:55Z",
      "closed_at": "2025-06-06T00:01:54Z",
      "labels": [
        "bug",
        "mlops user request",
        "stale"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8897/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8897",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8897",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:50.501510",
      "comments": [
        {
          "author": "xmcp",
          "body": "I can contribute PRs for this issue if needed, but note that it is a design decision between accumulating the result in the API and in the frontend, so it's your choice.",
          "created_at": "2025-02-28T17:30:56Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-05-30T00:01:52Z"
        }
      ]
    },
    {
      "issue_number": 8893,
      "title": "[Bug]: Task was destroyed but it's pending with new package versions",
      "body": "### What happened?\n\nA bug happened! \n#8831 #\n\n### Relevant log output\n\n```shell\nIn the original issue ticket, couldn't tag it as bug so opening a new one with reference\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.61.19\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "amanpreet692",
      "author_type": "User",
      "created_at": "2025-02-28T08:34:56Z",
      "updated_at": "2025-06-06T00:01:55Z",
      "closed_at": "2025-06-06T00:01:55Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8893/reactions",
        "total_count": 3,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8893",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8893",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:50.681293",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-05-30T00:01:53Z"
        }
      ]
    },
    {
      "issue_number": 11462,
      "title": "[Bug]: Caching logic breaks cost calculation when `merge_reasoning_content_in_choices` is set.",
      "body": "### What happened?\n\nWhen testing Open WebUI with LiteLLM I'm facing an issue where triggering \"Regenerate\" action in Open WebUI (then retrieving the responses from LiteLLM's cache) fails consistently.\n\nConfig.yaml\n\n```\n    model_list:\n      - model_name: claude-3.7-sonnet\n        litellm_params:\n          model: vertex_ai/claude-3-7-sonnet@20250219\n          vertex_ai_location: \"us-east5\"\n          thinking: {\"type\": \"enabled\", \"budget_tokens\": 1024}\n          merge_reasoning_content_in_choices: true\n\n    router_settings:\n      routing_strategy: simple-shuffle\n      redis_host: os.environ/REDIS_HOST\n      redis_port: os.environ/REDIS_PORT\n\n    litellm_settings:\n      cache: True\n      cache_params:\n        type: redis\n        host: os.environ/REDIS_HOST\n        port: os.environ/REDIS_PORT\n```\n\nScreenshot:\n\n<img width=\"1123\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/ca661ef1-4bb7-41cd-b725-103cb395661c\" />\n\n\nSetting `cache: False` prevents this issue from happening.\n\n\n### Relevant log output\n\n```shell\n{\"message\": \"Logging Details LiteLLM-Async Success Call, cache_hit=True\", \"level\": \"DEBUG\", \"timestamp\": \"2025-06-05T23:27:59.361748\"}\n{\"message\": \"Async success callbacks: Got a complete streaming response\", \"level\": \"DEBUG\", \"timestamp\": \"2025-06-05T23:27:59.361869\"}\n{\"message\": \"Model=claude-3-7-sonnet@20250219; cost=0.0\", \"level\": \"DEBUG\", \"timestamp\": \"2025-06-05T23:27:59.361974\"}\n{\"message\": \"litellm.cost_calculator.py::_get_provider_for_cost_calc() - Error inferring custom_llm_provider - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=claude-3-7-sonnet@20250219\\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\", \"level\": \"DEBUG\", \"timestamp\": \"2025-06-05T23:27:59.362769\"}\n{\"message\": \"selected model name for cost calculation: claude-3-7-sonnet@20250219\", \"level\": \"INFO\", \"timestamp\": \"2025-06-05T23:27:59.362895\"}\n{\"message\": \"litellm.cost_calculator.py::completion_cost() - Error inferring custom_llm_provider - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=claude-3-7-sonnet@20250219\\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\", \"level\": \"DEBUG\", \"timestamp\": \"2025-06-05T23:27:59.363355\"}\n{\"message\": \"litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=claude-3-7-sonnet@20250219 - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=claude-3-7-sonnet@20250219\\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\", \"level\": \"DEBUG\", \"timestamp\": \"2025-06-05T23:27:59.363728\"}\n{\"message\": \"selected model name for cost calculation: claude-3-7-sonnet@20250219\", \"level\": \"INFO\", \"timestamp\": \"2025-06-05T23:27:59.363808\"}\n{\"message\": \"litellm.cost_calculator.py::completion_cost() - Error inferring custom_llm_provider - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=claude-3-7-sonnet@20250219\\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\", \"level\": \"DEBUG\", \"timestamp\": \"2025-06-05T23:27:59.364202\"}\n{\"message\": \"litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=claude-3-7-sonnet@20250219 - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=claude-3-7-sonnet@20250219\\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\", \"level\": \"DEBUG\", \"timestamp\": \"2025-06-05T23:27:59.364542\"}\n{\"message\": \"Logging Details LiteLLM-Success Call: Cache_hit=True\", \"level\": \"DEBUG\", \"timestamp\": \"2025-06-05T23:27:59.366775\"}\n{\"message\": \"Logging Details LiteLLM-Success Call streaming complete\", \"level\": \"DEBUG\", \"timestamp\": \"2025-06-05T23:27:59.366962\"}\n{\"message\": \"response_cost: 0.0\", \"level\": \"DEBUG\", \"timestamp\": \"2025-06-05T23:27:59.367066\"}\n{\"message\": \"litellm.cost_calculator.py::_get_provider_for_cost_calc() - Error inferring custom_llm_provider - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=claude-3-7-sonnet@20250219\\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\", \"level\": \"DEBUG\", \"timestamp\": \"2025-06-05T23:27:59.367932\"}\n{\"message\": \"selected model name for cost calculation: claude-3-7-sonnet@20250219\", \"level\": \"INFO\", \"timestamp\": \"2025-06-05T23:27:59.368091\"}\n{\"message\": \"litellm.cost_calculator.py::completion_cost() - Error inferring custom_llm_provider - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=claude-3-7-sonnet@20250219\\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\", \"level\": \"DEBUG\", \"timestamp\": \"2025-06-05T23:27:59.368588\"}\n{\"message\": \"litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=claude-3-7-sonnet@20250219 - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=claude-3-7-sonnet@20250219\\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\", \"level\": \"DEBUG\", \"timestamp\": \"2025-06-05T23:27:59.369056\"}\n{\"message\": \"selected model name for cost calculation: claude-3-7-sonnet@20250219\", \"level\": \"INFO\", \"timestamp\": \"2025-06-05T23:27:59.369179\"}\n{\"message\": \"litellm.cost_calculator.py::completion_cost() - Error inferring custom_llm_provider - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=claude-3-7-sonnet@20250219\\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\", \"level\": \"DEBUG\", \"timestamp\": \"2025-06-05T23:27:59.369649\"}\n{\"message\": \"litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=claude-3-7-sonnet@20250219 - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=claude-3-7-sonnet@20250219\\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\", \"level\": \"DEBUG\", \"timestamp\": \"2025-06-05T23:27:59.370048\"}\n{\"message\": \"response_cost_failure_debug_information: {'error_str': \\\"litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=claude-3-7-sonnet@20250219\\\\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\\\", 'traceback_str': 'Traceback (most recent call last):\\\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/litellm_logging.py\\\", line 1127, in _response_cost_calculator\\\\n    response_cost = litellm.response_cost_calculator(\\\\n        **response_cost_calculator_kwargs\\\\n    )\\\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/cost_calculator.py\\\", line 1031, in response_cost_calculator\\\\n    raise e\\\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/cost_calculator.py\\\", line 1016, in response_cost_calculator\\\\n    response_cost = completion_cost(\\\\n        completion_response=response_object,\\\\n    ...<9 lines>...\\\\n        router_model_id=router_model_id,\\\\n    )\\\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/cost_calculator.py\\\", line 932, in completion_cost\\\\n    raise e\\\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/cost_calculator.py\\\", line 925, in completion_cost\\\\n    raise e\\\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/cost_calculator.py\\\", line 887, in completion_cost\\\\n    ) = cost_per_token(\\\\n        ~~~~~~~~~~~~~~^\\\\n        model=model,\\\\n        ^^^^^^^^^^^^\\\\n    ...<14 lines>...\\\\n        rerank_billed_units=rerank_billed_units,\\\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n    )\\\\n    ^\\\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/cost_calculator.py\\\", line 204, in cost_per_token\\\\n    _, custom_llm_provider, _, _ = litellm.get_llm_provider(model=model)\\\\n                                   ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^\\\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\\\", line 373, in get_llm_provider\\\\n    raise e\\\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\\\", line 350, in get_llm_provider\\\\n    raise litellm.exceptions.BadRequestError(  # type: ignore\\\\n    ...<8 lines>...\\\\n    )\\\\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=claude-3-7-sonnet@20250219\\\\n Pass model as E.g. For \\\\'Huggingface\\\\' inference endpoints pass in `completion(model=\\\\'huggingface/starcoder\\\\',..)` Learn more: https://docs.litellm.ai/docs/providers\\\\n', 'model': 'claude-3-7-sonnet@20250219', 'cache_hit': False, 'custom_llm_provider': None, 'base_model': None, 'call_type': 'acompletion', 'custom_pricing': False}\", \"level\": \"DEBUG\", \"timestamp\": \"2025-06-05T23:27:59.377416\"}\n{\"message\": \"checking potential_model_names in litellm.model_cost: {'split_model': 'claude-3-7-sonnet@20250219', 'combined_model_name': 'claude-3-7-sonnet@20250219', 'stripped_model_name': 'claude-3-7-sonnet@20250219', 'combined_stripped_model_name': 'claude-3-7-sonnet@20250219', 'custom_llm_provider': None}\", \"level\": \"DEBUG\", \"timestamp\": \"2025-06-05T23:27:59.378308\"}\n{\"message\": \"Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\", \"level\": \"DEBUG\", \"timestamp\": \"2025-06-05T23:27:59.378421\"}\n{\"message\": \"Model=claude-3-7-sonnet@20250219 is not mapped in model cost map. Defaulting to None model_cost_information for standard_logging_payload\", \"level\": \"DEBUG\", \"timestamp\": \"2025-06-05T23:27:59.378491\"}\n{\"message\": \"response_cost_failure_debug_information: {'error_str': \\\"litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=claude-3-7-sonnet@20250219\\\\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\\\", 'traceback_str': 'Traceback (most recent call last):\\\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/litellm_logging.py\\\", line 1127, in _response_cost_calculator\\\\n    response_cost = litellm.response_cost_calculator(\\\\n        **response_cost_calculator_kwargs\\\\n    )\\\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/cost_calculator.py\\\", line 1031, in response_cost_calculator\\\\n    raise e\\\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/cost_calculator.py\\\", line 1016, in response_cost_calculator\\\\n    response_cost = completion_cost(\\\\n        completion_response=response_object,\\\\n    ...<9 lines>...\\\\n        router_model_id=router_model_id,\\\\n    )\\\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/cost_calculator.py\\\", line 932, in completion_cost\\\\n    raise e\\\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/cost_calculator.py\\\", line 925, in completion_cost\\\\n    raise e\\\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/cost_calculator.py\\\", line 887, in completion_cost\\\\n    ) = cost_per_token(\\\\n        ~~~~~~~~~~~~~~^\\\\n        model=model,\\\\n        ^^^^^^^^^^^^\\\\n    ...<14 lines>...\\\\n        rerank_billed_units=rerank_billed_units,\\\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n    )\\\\n    ^\\\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/cost_calculator.py\\\", line 204, in cost_per_token\\\\n    _, custom_llm_provider, _, _ = litellm.get_llm_provider(model=model)\\\\n                                   ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^\\\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\\\", line 373, in get_llm_provider\\\\n    raise e\\\\n  File \\\"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\\\", line 350, in get_llm_provider\\\\n    raise litellm.exceptions.BadRequestError(  # type: ignore\\\\n    ...<8 lines>...\\\\n    )\\\\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=claude-3-7-sonnet@20250219\\\\n Pass model as E.g. For \\\\'Huggingface\\\\' inference endpoints pass in `completion(model=\\\\'huggingface/starcoder\\\\',..)` Learn more: https://docs.litellm.ai/docs/providers\\\\n', 'model': 'claude-3-7-sonnet@20250219', 'cache_hit': False, 'custom_llm_provider': None, 'base_model': None, 'call_type': 'acompletion', 'custom_pricing': False}\", \"level\": \"DEBUG\", \"timestamp\": \"2025-06-05T23:27:59.380648\"}\n{\"message\": \"checking potential_model_names in litellm.model_cost: {'split_model': 'claude-3-7-sonnet@20250219', 'combined_model_name': 'claude-3-7-sonnet@20250219', 'stripped_model_name': 'claude-3-7-sonnet@20250219', 'combined_stripped_model_name': 'claude-3-7-sonnet@20250219', 'custom_llm_provider': None}\", \"level\": \"DEBUG\", \"timestamp\": \"2025-06-05T23:27:59.381464\"}\n{\"message\": \"Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\", \"level\": \"DEBUG\", \"timestamp\": \"2025-06-05T23:27:59.381573\"}\n{\"message\": \"Model=claude-3-7-sonnet@20250219 is not mapped in model cost map. Defaulting to None model_cost_information for standard_logging_payload\", \"level\": \"DEBUG\", \"timestamp\": \"2025-06-05T23:27:59.381625\"}\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.71.1-stable\n\n### Twitter / LinkedIn details\n",
      "state": "open",
      "author": "gjmveloso",
      "author_type": "User",
      "created_at": "2025-06-05T23:43:48Z",
      "updated_at": "2025-06-05T23:44:54Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11462/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11462",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11462",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:50.897501",
      "comments": []
    },
    {
      "issue_number": 11427,
      "title": "[Bug]: Gemini calls fail with `format` == `email` in schema",
      "body": "### What happened?\n\nDoes not occur with vertex_ai but does with gemini\n\n```\n                             GenerateContentRequest.tools[0].function_declarations[6].parameters.p                    \n                             roperties[repositoryUrl].format: only 'enum' and 'date-time' are                         \n                             supported for STRING type\\n*                                                             \n                             GenerateContentRequest.tools[0].function_declarations[7].parameters.p                    \n                             roperties[author].properties[email].format: only 'enum' and                              \n                             'date-time' are supported for STRING type\\n\",                                            \n                                 \"status\": \"INVALID_ARGUMENT\"                                                         \n                               }                                                                                      \n                             }                              \n```\n\nschema\n```\n{\n    \"type\": \"object\",\n    \"properties\": {\n        \"path\": {\n            \"type\": \"string\",\n            \"minLength\": 1,\n            \"default\": \".\",\n            \"description\": \"Path to the Git repository. Defaults to the directory set via `git_set_working_dir` for the session; set \\'git_set_working_dir\\' if not set.\"\n        },\n        \"message\": {\n            \"type\": \"string\",\n            \"minLength\": 1,\n            \"description\": \"Commit message. Follow Conventional Commits format: `type(scope): subject`. Example: `feat(api): add user signup endpoint`\"\n        },\n        \"author\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"name\": {\n                    \"type\": \"string\",\n                    \"description\": \"Author name for the commit\"\n                },\n                \"email\": {\n                    \"type\": \"string\",\n                    \"format\": \"email\",\n                    \"description\": \"Author email for the commit\"\n                }\n            },\n            \"required\": [\n                \"name\",\n                \"email\"\n            ],\n            \"additionalProperties\": false,\n            \"description\": \"Overrides the commit author information (name and email). Use only when necessary (e.g., applying external patches).\"\n        },\n        \"allowEmpty\": {\n            \"type\": \"boolean\",\n            \"default\": false,\n            \"description\": \"Allow creating empty commits\"\n        },\n        \"amend\": {\n            \"type\": \"boolean\",\n            \"default\": false,\n            \"description\": \"Amend the previous commit instead of creating a new one\"\n        },\n        \"forceUnsignedOnFailure\": {\n            \"type\": \"boolean\",\n            \"default\": false,\n            \"description\": \"If true and signing is enabled but fails, attempt the commit without signing instead of failing.\"\n        },\n        \"filesToStage\": {\n            \"type\": \"array\",\n            \"items\": {\n                \"type\": \"string\",\n                \"minLength\": 1\n            },\n            \"description\": \"Optional array of specific file paths (relative to the repository root) to stage automatically before committing. If provided, only these files will be staged.\"\n        }\n    },\n    \"required\": [\n        \"message\"\n    ],\n    \"additionalProperties\": false,\n    \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}                \n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.72.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "strawgate",
      "author_type": "User",
      "created_at": "2025-06-05T04:20:01Z",
      "updated_at": "2025-06-05T22:51:53Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11427/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11427",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11427",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:50.897521",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Hi @strawgate appreciate the schema, but can you share a minimal call to repro the issue? ",
          "created_at": "2025-06-05T05:53:11Z"
        },
        {
          "author": "strawgate",
          "body": "```\nimport litellm\n\n\ndef tool_call(text: str | None) -> str:\n    return text or \"No text provided\"\n\n\ntool = {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"git_commit\",\n        \"description\": \"Commits changes to the Git repository\",\n        \"parameters\": {\n            \"type\": \"object\",\n",
          "created_at": "2025-06-05T22:51:52Z"
        }
      ]
    },
    {
      "issue_number": 10567,
      "title": "[Bug]: Reasoning tokens not counted in costs",
      "body": "### What happened?\n\nI have setup a model with custom costs:\n\n![Image](https://github.com/user-attachments/assets/4dec4984-03a1-48ef-9c76-fc676e1551c3)\n\nI then send a request through the \"test key\" LiteLLM menu. \n\n![Image](https://github.com/user-attachments/assets/f7633ee5-c20a-4f5c-8c2a-ab43f1c7d75f)\n\nIssue: Reasoning tokens are not counted in the total, nor are they counted in costs\n\n![Image](https://github.com/user-attachments/assets/10e218ce-24c6-424f-a69a-0885e8354cd6)\n\nExpected behavior: Cost should account for reasoning tokens and amount to about 0.116$ in the example\n\nNote the issue is not specific to perplexity as for example o3 seems to also ignore reasoning tokens in costs.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.67.4\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "alex-a-r-d",
      "author_type": "User",
      "created_at": "2025-05-05T15:40:27Z",
      "updated_at": "2025-06-05T22:25:24Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10567/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10567",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10567",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:51.079071",
      "comments": [
        {
          "author": "ming-jeng",
          "body": "Wonder if it's similar issue as mine https://github.com/BerriAI/litellm/issues/10691",
          "created_at": "2025-06-05T22:25:24Z"
        }
      ]
    },
    {
      "issue_number": 11358,
      "title": "[Bug]: Latest Claude Code doesn't work with LiteLLM",
      "body": "### What happened?\n\nVersion 1.0.8 and version 1.0.9 of Claude Code is no longer able to use the LiteLLM Anthropic compatible endpoints, resulting in an `API Error: request ended without sending any chunks` error from Claude Code.\n\nLast known version to work is 1.0.7.\n\nThis was tested against `a366f9247aa092cbf36afcae091bdbbd355b5413` (head of main doesn't run for some reason) from main with a very default config.\n\n### Relevant log output\n\n```shell\nINFO:     Started server process [52549]\nINFO:     Waiting for application startup.\n\n#------------------------------------------------------------#\n#                                                            #\n#       'This feature doesn't meet my needs because...'       #\n#        https://github.com/BerriAI/litellm/issues/new        #\n#                                                            #\n#------------------------------------------------------------#\n\n Thank you for using LiteLLM! - Krrish & Ishaan\n\n\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n\n\nLiteLLM: Proxy initialized with Config, Set models:\n    claude-sonnet-4-20250514\n    claude-3.7-sonnet\n    claude-3-5-haiku-20241022\n    deepseek-r1\n13:39:13 - LiteLLM Router:INFO: router.py:660 - Routing strategy: simple-shuffle\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:4000 (Press CTRL+C to quit)\n13:39:24 - LiteLLM Proxy:INFO: parallel_request_limiter.py:68 - Current Usage of key in this minute: None\n13:39:24 - LiteLLM Proxy:INFO: parallel_request_limiter.py:68 - Current Usage of customer in this minute: None\n13:39:24 - LiteLLM Proxy:INFO: parallel_request_limiter.py:68 - Current Usage of key in this minute: {'current_requests': 1, 'current_tpm': 0, 'current_rpm': 1}\n13:39:24 - LiteLLM Proxy:INFO: parallel_request_limiter.py:68 - Current Usage of customer in this minute: {'current_requests': 1, 'current_tpm': 0, 'current_rpm': 1}\n13:39:25 - LiteLLM Router:INFO: router.py:2514 - anthropic_messages(model=claude-3-5-haiku-20241022) Exception Client error '400 Bad Request' for url 'https://bedrock-runtime.us-east-1.amazonaws.com/model/arn%3Aaws%3Abedrock%3Aus-east-1%3A566603766008%3Aapplication-inference-profile%2Fuu8m39vufutr/invoke'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400\n13:39:25 - LiteLLM Proxy:ERROR: endpoints.py:265 - litellm.proxy.proxy_server.anthropic_response(): Exception occured - Client error '400 Bad Request' for url 'https://bedrock-runtime.us-east-1.amazonaws.com/model/arn%3Aaws%3Abedrock%3Aus-east-1%3A566603766008%3Aapplication-inference-profile%2Fuu8m39vufutr/invoke'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400\nTraceback (most recent call last):\n  File \"/Users/mads/git/rando/litellm/litellm/proxy/anthropic_endpoints/endpoints.py\", line 213, in anthropic_response\n    response = await llm_response\n               ^^^^^^^^^^^^^^^^^^\n  File \"/Users/mads/git/rando/litellm/litellm/router.py\", line 3261, in async_wrapper\n    return await self._ageneric_api_call_with_fallbacks(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mads/git/rando/litellm/litellm/router.py\", line 2519, in _ageneric_api_call_with_fallbacks\n    raise e\n  File \"/Users/mads/git/rando/litellm/litellm/router.py\", line 2506, in _ageneric_api_call_with_fallbacks\n    response = await response  # type: ignore\n               ^^^^^^^^^^^^^^\n  File \"/Users/mads/git/rando/litellm/litellm/utils.py\", line 1492, in wrapper_async\n    raise e\n  File \"/Users/mads/git/rando/litellm/litellm/utils.py\", line 1353, in wrapper_async\n    result = await original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mads/git/rando/litellm/litellm/llms/anthropic/experimental_pass_through/messages/handler.py\", line 88, in anthropic_messages\n    response = await init_response\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mads/git/rando/litellm/litellm/llms/custom_httpx/llm_http_handler.py\", line 1169, in async_anthropic_messages_handler\n    response = await async_httpx_client.post(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mads/git/rando/litellm/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n    result = await func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mads/git/rando/litellm/litellm/llms/custom_httpx/http_handler.py\", line 276, in post\n    raise e\n  File \"/Users/mads/git/rando/litellm/litellm/llms/custom_httpx/http_handler.py\", line 232, in post\n    response.raise_for_status()\n  File \"/Users/mads/Library/Caches/pypoetry/virtualenvs/litellm-HTpeX-vO-py3.12/lib/python3.12/site-packages/httpx/_models.py\", line 829, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Client error '400 Bad Request' for url 'https://bedrock-runtime.us-east-1.amazonaws.com/model/arn%3Aaws%3Abedrock%3Aus-east-1%3A566603766008%3Aapplication-inference-profile%2Fuu8m39vufutr/invoke'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400\nINFO:     127.0.0.1:64550 - \"POST /v1/messages?beta=true HTTP/1.1\" 400 Bad Request\n13:39:25 - LiteLLM:INFO: cost_calculator.py:655 - selected model name for cost calculation: bedrock/dc5866dde93a011454d96869b2742dcee2d62b72a1df76cdbad8d3c10521fcaf\n13:39:25 - LiteLLM Router:INFO: router.py:2509 - anthropic_messages(model=bedrock/converse/us.deepseek.r1-v1:0) 200 OK\n13:39:25 - LiteLLM Proxy:ERROR: endpoints.py:68 - litellm.proxy.proxy_server.async_data_generator(): Exception occured - validationException {\"message\":\"Validation Error\"}\nTraceback (most recent call last):\n  File \"/Users/mads/git/rando/litellm/litellm/proxy/anthropic_endpoints/endpoints.py\", line 56, in async_data_generator_anthropic\n    async for chunk in response:\n  File \"/Users/mads/git/rando/litellm/litellm/llms/bedrock/chat/invoke_handler.py\", line 1503, in aiter_bytes\n    message = self._parse_message_from_event(event)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mads/git/rando/litellm/litellm/llms/bedrock/chat/invoke_handler.py\", line 1522, in _parse_message_from_event\n    raise BedrockError(\nlitellm.llms.bedrock.common_utils.BedrockError: validationException {\"message\":\"Validation Error\"}\nINFO:     127.0.0.1:64548 - \"POST /v1/messages?beta=true HTTP/1.1\" 400 Bad Request\n13:39:25 - LiteLLM Proxy:INFO: parallel_request_limiter.py:68 - Current Usage of key in this minute: {'current_requests': 2, 'current_tpm': 0, 'current_rpm': 2}\n13:39:25 - LiteLLM Proxy:INFO: parallel_request_limiter.py:68 - Current Usage of customer in this minute: {'current_requests': 2, 'current_tpm': 0, 'current_rpm': 2}\n13:39:28 - LiteLLM:INFO: cost_calculator.py:655 - selected model name for cost calculation: bedrock/72951156fcac47c630247aefa4dda07b6605192559f7681c7bfab64dae3685b5\n13:39:28 - LiteLLM Router:INFO: router.py:2509 - anthropic_messages(model=bedrock/us.anthropic.claude-3-7-sonnet-20250219-v1:0) 200 OK\nINFO:     127.0.0.1:64548 - \"POST /v1/messages?beta=true HTTP/1.1\" 200 OK\n13:39:28 - LiteLLM Proxy:INFO: parallel_request_limiter.py:68 - Current Usage of key in this minute: {'current_requests': 3, 'current_tpm': 0, 'current_rpm': 3}\n13:39:28 - LiteLLM Proxy:INFO: parallel_request_limiter.py:68 - Current Usage of customer in this minute: {'current_requests': 3, 'current_tpm': 0, 'current_rpm': 3}\n13:39:30 - LiteLLM Router:INFO: router.py:2509 - anthropic_messages(model=bedrock/us.anthropic.claude-3-7-sonnet-20250219-v1:0) 200 OK\n13:39:30 - LiteLLM Proxy:INFO: endpoints.py:259 - \nResponse from Litellm:\n{'id': 'msg_bdrk_01JzhM2ED8myioRLbzmRXdF1', 'type': 'message', 'role': 'assistant', 'model': 'claude-3-7-sonnet-20250219', 'content': [{'type': 'text', 'text': 'Hi! How can I help you today?'}], 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 4, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 13637, 'output_tokens': 12}}\n13:39:30 - LiteLLM:INFO: cost_calculator.py:655 - selected model name for cost calculation: bedrock/us.anthropic.claude-3-7-sonnet-20250219-v1:0\n13:39:30 - LiteLLM:INFO: cost_calculator.py:655 - selected model name for cost calculation: bedrock/us.anthropic.claude-3-7-sonnet-20250219-v1:0\nINFO:     127.0.0.1:64550 - \"POST /v1/messages?beta=true HTTP/1.1\" 200 OK\n^CINFO:     Shutting down\nINFO:     Waiting for application shutdown.\n13:39:40 - LiteLLM Proxy:INFO: proxy_server.py:478 - Shutting down LiteLLM Proxy Server\nINFO:     Application shutdown complete.\nINFO:     Finished server process [52549]\n\nAborted!\nUnclosed client session\nclient_session: <aiohttp.client.ClientSession object at 0x10cc4bdd0>\nUnclosed connector\nconnections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x10cd79eb0>, 48597.428163041)]']\nconnector: <aiohttp.connector.TCPConnector object at 0x10cd92cf0>\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\na366f9247aa092cbf36afcae091bdbbd355b5413\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "mrh-chain",
      "author_type": "User",
      "created_at": "2025-06-03T11:40:16Z",
      "updated_at": "2025-06-05T19:03:41Z",
      "closed_at": "2025-06-05T19:03:40Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11358/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11358",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11358",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:51.249744",
      "comments": [
        {
          "author": "naudo",
          "body": "I am able to reproduce the above. As a workaround we had to downgrade to v1.0.7 of claude code. Hope this helps for anyone else who stumbles on this.\n\n1. npm install @anthropic-ai/claude-code@1.0.7\n2. Disable Auto updates with claude config set -g autoUpdaterStatus disabled\n3. run claude\n",
          "created_at": "2025-06-03T16:47:10Z"
        },
        {
          "author": "regismesquita",
          "body": "I was not able to replicate that using Anthropic through LiteLLM ( 1.71.1 ), I am using it without any issues with the latest Claude Code version.\n\nI noticed that the report mentions bedrock , I am not using bedrock. @naudo are you also using bedrock?",
          "created_at": "2025-06-05T13:45:08Z"
        },
        {
          "author": "mrh-chain",
          "body": "This appears to have been fixed. in the latest claude version (1.0.11)",
          "created_at": "2025-06-05T19:03:40Z"
        }
      ]
    },
    {
      "issue_number": 10379,
      "title": "[Bug]: PostgreSQL Integer Overflow Error in Spend Tracking System",
      "body": "### What happened?\n\nIssue Description\nWhen running LiteLLM with high usage, I encountered a PostgreSQL error indicating integer overflow in the spend tracking system. This occurs during the execution of the db_update_spend_transaction_handler function.\n\n\n\n### Relevant log output\n\n```shell\nConnectorError(ConnectorError { user_facing_error: None, kind: QueryError(PostgresError { code: \"22003\", message: \"integer out of range\", severity: \"ERROR\", detail: None, column: None, hint: None }), transient: false })\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nmain-v1.67.4-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "vovolie",
      "author_type": "User",
      "created_at": "2025-04-28T10:36:47Z",
      "updated_at": "2025-06-05T16:50:23Z",
      "closed_at": "2025-05-09T22:11:15Z",
      "labels": [
        "bug",
        "may 2025"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 28,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10379/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "krrishdholakia"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10379",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10379",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:51.465476",
      "comments": []
    },
    {
      "issue_number": 10812,
      "title": "[Bug]: I/O operation on closed pipe while using litellm with mcp",
      "body": "### What happened?\n\nHere was the code I used\n\n```python3\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\nfrom litellm import experimental_mcp_client\nimport litellm\n\nimport asyncio\nimport json\n\n\nserver_params = StdioServerParameters(\n    command=\"npx\",\n    args=[\"-y\", \"tavily-mcp@0.2.0\"],\n    env={\"TAVILY_API_KEY\": \"<key>\"}\n)\n\nasync def main():\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(read, write) as session:\n            # Initialize the connection\n            await session.initialize()\n\n            # Get tools\n            tools = await experimental_mcp_client.load_mcp_tools(session, 'openai')\n            print(\"MCP TOOLS: \", tools)\n\n            messages = [{\"role\": \"user\", \"content\": \"What is the newest Salesforce model?\"}]\n            llm_response = await litellm.acompletion(\n                model=\"ollama/hf.co/DevQuasar/Salesforce.Llama-xLAM-2-8b-fc-r-GGUF:Q8_0\",\n                api_key='sk-1234',\n                messages=messages,\n                tools=tools,\n            )\n            print(\"LLM RESPONSE: \", json.dumps(llm_response, indent=4, default=str))\n\nasyncio.run(main)\n\n### Relevant log output\n\n```shell\nMCP TOOLS:  [{'type': 'function', 'function': {'name': 'tavily-search', 'description': \"A powerful web search tool that provides comprehensive, real-time results using Tavily's AI search engine. Returns relevant web content with customizable parameters for result count, content type, and domain filtering. Ideal for gathering current information, news, and detailed web content analysis.\", 'parameters': {'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'Search query'}, 'search_depth': {'type': 'string', 'enum': ['basic', 'advanced'], 'description': \"The depth of the search. It can be 'basic' or 'advanced'\", 'default': 'basic'}, 'topic': {'type': 'string', 'enum': ['general', 'news'], 'description': 'The category of the search. This will determine which of our agents will be used for the search', 'default': 'general'}, 'days': {'type': 'number', 'description': \"The number of days back from the current date to include in the search results. This specifies the time frame of data to be retrieved. Please note that this feature is only available when using the 'news' search topic\", 'default': 3}, 'time_range': {'type': 'string', 'description': \"The time range back from the current date to include in the search results. This feature is available for both 'general' and 'news' search topics\", 'enum': ['day', 'week', 'month', 'year', 'd', 'w', 'm', 'y']}, 'max_results': {'type': 'number', 'description': 'The maximum number of search results to return', 'default': 10, 'minimum': 5, 'maximum': 20}, 'include_images': {'type': 'boolean', 'description': 'Include a list of query-related images in the response', 'default': False}, 'include_image_descriptions': {'type': 'boolean', 'description': 'Include a list of query-related images and their descriptions in the response', 'default': False}, 'include_raw_content': {'type': 'boolean', 'description': 'Include the cleaned and parsed HTML content of each search result', 'default': False}, 'include_domains': {'type': 'array', 'items': {'type': 'string'}, 'description': 'A list of domains to specifically include in the search results, if the user asks to search on specific sites set this to the domain of the site', 'default': []}, 'exclude_domains': {'type': 'array', 'items': {'type': 'string'}, 'description': 'List of domains to specifically exclude, if the user asks to exclude a domain set this to the domain of the site', 'default': []}}, 'required': ['query']}, 'strict': False}}, {'type': 'function', 'function': {'name': 'tavily-extract', 'description': 'A powerful web content extraction tool that retrieves and processes raw content from specified URLs, ideal for data collection, content analysis, and research tasks.', 'parameters': {'type': 'object', 'properties': {'urls': {'type': 'array', 'items': {'type': 'string'}, 'description': 'List of URLs to extract content from'}, 'extract_depth': {'type': 'string', 'enum': ['basic', 'advanced'], 'description': \"Depth of extraction - 'basic' or 'advanced', if usrls are linkedin use 'advanced' or if explicitly told to use advanced\", 'default': 'basic'}, 'include_images': {'type': 'boolean', 'description': 'Include a list of images extracted from the urls in the response', 'default': False}}, 'required': ['urls']}, 'strict': False}}, {'type': 'function', 'function': {'name': 'tavily-crawl', 'description': 'A powerful web crawler that initiates a structured web crawl starting from a specified base URL. The crawler expands from that point like a tree, following internal links across pages. You can control how deep and wide it goes, and guide it to focus on specific sections of the site.', 'parameters': {'type': 'object', 'properties': {'url': {'type': 'string', 'description': 'The root URL to begin the crawl'}, 'max_depth': {'type': 'integer', 'description': 'Max depth of the crawl. Defines how far from the base URL the crawler can explore.', 'default': 1, 'minimum': 1}, 'max_breadth': {'type': 'integer', 'description': 'Max number of links to follow per level of the tree (i.e., per page)', 'default': 20, 'minimum': 1}, 'limit': {'type': 'integer', 'description': 'Total number of links the crawler will process before stopping', 'default': 50, 'minimum': 1}, 'query': {'type': 'string', 'description': 'Natural language instructions for the crawler'}, 'select_paths': {'type': 'array', 'items': {'type': 'string'}, 'description': 'Regex patterns to select only URLs with specific path patterns (e.g., /docs/.*, /api/v1.*)', 'default': []}, 'select_domains': {'type': 'array', 'items': {'type': 'string'}, 'description': 'Regex patterns to select crawling to specific domains or subdomains (e.g., ^docs\\\\.example\\\\.com$)', 'default': []}, 'allow_external': {'type': 'boolean', 'description': 'Whether to allow following links that go to external domains', 'default': False}, 'categories': {'type': 'array', 'items': {'type': 'string', 'enum': ['Careers', 'Blog', 'Documentation', 'About', 'Pricing', 'Community', 'Developers', 'Contact', 'Media']}, 'description': 'Filter URLs using predefined categories like documentation, blog, api, etc', 'default': []}, 'extract_depth': {'type': 'string', 'enum': ['basic', 'advanced'], 'description': 'Advanced extraction retrieves more data, including tables and embedded content, with higher success but may increase latency', 'default': 'basic'}}, 'required': ['url']}, 'strict': False}}, {'type': 'function', 'function': {'name': 'tavily-map', 'description': 'A powerful web mapping tool that creates a structured map of website URLs, allowing you to discover and analyze site structure, content organization, and navigation paths. Perfect for site audits, content discovery, and understanding website architecture.', 'parameters': {'type': 'object', 'properties': {'url': {'type': 'string', 'description': 'The root URL to begin the mapping'}, 'max_depth': {'type': 'integer', 'description': 'Max depth of the mapping. Defines how far from the base URL the crawler can explore', 'default': 1, 'minimum': 1}, 'max_breadth': {'type': 'integer', 'description': 'Max number of links to follow per level of the tree (i.e., per page)', 'default': 20, 'minimum': 1}, 'limit': {'type': 'integer', 'description': 'Total number of links the crawler will process before stopping', 'default': 50, 'minimum': 1}, 'query': {'type': 'string', 'description': 'Natural language instructions for the crawler'}, 'select_paths': {'type': 'array', 'items': {'type': 'string'}, 'description': 'Regex patterns to select only URLs with specific path patterns (e.g., /docs/.*, /api/v1.*)', 'default': []}, 'select_domains': {'type': 'array', 'items': {'type': 'string'}, 'description': 'Regex patterns to select crawling to specific domains or subdomains (e.g., ^docs\\\\.example\\\\.com$)', 'default': []}, 'allow_external': {'type': 'boolean', 'description': 'Whether to allow following links that go to external domains', 'default': False}, 'categories': {'type': 'array', 'items': {'type': 'string', 'enum': ['Careers', 'Blog', 'Documentation', 'About', 'Pricing', 'Community', 'Developers', 'Contact', 'Media']}, 'description': 'Filter URLs using predefined categories like documentation, blog, api, etc', 'default': []}}, 'required': ['url']}, 'strict': False}}]\nLLM RESPONSE:  \"ModelResponse(id='chatcmpl-10c94bda-c04c-4447-9db7-5c7e45a2e413', created=1747182311, model='ollama/hf.co/DevQuasar/Salesforce.Llama-xLAM-2-8b-fc-r-GGUF:Q8_0', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\\\"query\\\": \\\"newest Salesforce model\\\"}', name='tavily-search'), id='call_a6a7ed64-1eab-486c-ad08-48b3143c11ff', type='function')], function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=22, prompt_tokens=1840, total_tokens=1862, completion_tokens_details=None, prompt_tokens_details=None))\"\nException ignored in: <function BaseSubprocessTransport.__del__ at 0x0000012D73FB04A0>\nTraceback (most recent call last):\n  File \"C:\\Users\\Fractal\\AppData\\Roaming\\uv\\python\\cpython-3.11.11-windows-x86_64-none\\Lib\\asyncio\\base_subprocess.py\", line 125, in __del__\n    _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Fractal\\AppData\\Roaming\\uv\\python\\cpython-3.11.11-windows-x86_64-none\\Lib\\asyncio\\base_subprocess.py\", line 78, in __repr__\n    info.append(f'stdout={stdout.pipe}')\n                ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Fractal\\AppData\\Roaming\\uv\\python\\cpython-3.11.11-windows-x86_64-none\\Lib\\asyncio\\proactor_events.py\", line 80, in __repr__\n    info.append(f'fd={self._sock.fileno()}')\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Fractal\\AppData\\Roaming\\uv\\python\\cpython-3.11.11-windows-x86_64-none\\Lib\\asyncio\\windows_utils.py\", line 102, in fileno\n    raise ValueError(\"I/O operation on closed pipe\")\nValueError: I/O operation on closed pipe\nException ignored in: <function _ProactorBasePipeTransport.__del__ at 0x0000012D73FB1D00>\nTraceback (most recent call last):\n  File \"C:\\Users\\Fractal\\AppData\\Roaming\\uv\\python\\cpython-3.11.11-windows-x86_64-none\\Lib\\asyncio\\proactor_events.py\", line 116, in __del__\n    _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Fractal\\AppData\\Roaming\\uv\\python\\cpython-3.11.11-windows-x86_64-none\\Lib\\asyncio\\proactor_events.py\", line 80, in __repr__\n    info.append(f'fd={self._sock.fileno()}')\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Fractal\\AppData\\Roaming\\uv\\python\\cpython-3.11.11-windows-x86_64-none\\Lib\\asyncio\\windows_utils.py\", line 102, in fileno\n    raise ValueError(\"I/O operation on closed pipe\")\nValueError: I/O operation on closed pipe\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.69.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "Fractal-0",
      "author_type": "User",
      "created_at": "2025-05-14T00:30:27Z",
      "updated_at": "2025-06-05T15:40:24Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10812/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10812",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10812",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:51.465496",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "cc @wagnerjt this is a good issue ",
          "created_at": "2025-05-14T03:55:27Z"
        },
        {
          "author": "Fractal-0",
          "body": "This looks like an issue with the MCP SDK, and I have made a [ticket](https://github.com/modelcontextprotocol/python-sdk/issues/716) with them there. This is still a litellm issue with the experimental mcp client because if i leave the function empty nothing happens.",
          "created_at": "2025-05-14T18:09:03Z"
        },
        {
          "author": "Fractal-0",
          "body": "bump",
          "created_at": "2025-05-18T20:24:08Z"
        },
        {
          "author": "bachya",
          "body": "Anecdotally, I've seen this without anything MCP-related (i.e., on a straight call to an LLM). Unfortunately, attempts to consistently reproduce it have been challenging...",
          "created_at": "2025-06-05T15:39:07Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "cc @wagnerjt do you see this ? ",
          "created_at": "2025-06-05T15:40:23Z"
        }
      ]
    },
    {
      "issue_number": 10402,
      "title": "[Bug]: Unexpected argument error when using gpt-images-1 model with OpenAI",
      "body": "### What happened?\n\nWhen attempting to use the `gpt-images-1` model via the OpenAI provider in LiteLLM, the following error is thrown:\n\nlitellm.exceptions.APIConnectionError: litellm.APIConnectionError: APIConnectionError: OpenAIException - AsyncImages.generate() got an unexpected keyword argument 'output_format'\n\n### Reproduction Steps\n\n1. Configure LiteLLM to use the `gpt-images-1` model from OpenAI.\n2. Send a request through the LiteLLM interface.\n3. Observe the crash with the unexpected keyword argument error.\n\n### Expected Behavior\n\nThe `gpt-images-1` model should be handled gracefully, or LiteLLM should explicitly indicate that it is not supported if that's the case.\n\n### Environment\n\n- Provider: OpenAI\n- Model: `gpt-images-1`\n\n### Additional Notes\n\nThis model might not conform to the same API structure as chat/completion models. It could require a special-case handling or should be excluded with a clear error.\n\nLet me know if you need help testing a fix or workaround.\n\n\n\n### Relevant log output\n\n```shell\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/main.py\", line 4550, in aimage_generation\n    response = await init_response  # type: ignore\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/openai/openai.py\", line 1271, in aimage_generation\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/openai/openai.py\", line 1254, in aimage_generation\n    response = await openai_aclient.images.generate(**data, timeout=timeout)  # type: ignore\n                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: AsyncImages.generate() got an unexpected keyword argument 'output_format'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 3381, in async_function_with_fallbacks\n    raise original_exception\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 3237, in async_function_with_fallbacks\n    response = await self.async_function_with_retries(*args, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 3615, in async_function_with_retries\n    raise original_exception\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 3506, in async_function_with_retries\n    response = await self.make_call(original_function, *args, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 3624, in make_call\n    response = await response\n               ^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 1884, in _aimage_generation\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 1871, in _aimage_generation\n    response = await response\n               ^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/utils.py\", line 1460, in wrapper_async\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/utils.py\", line 1321, in wrapper_async\n    result = await original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/main.py\", line 4557, in aimage_generation\n    raise exception_type(\n          ~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<3 lines>...\n        extra_kwargs=kwargs,\n        ^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2222, in exception_type\n    raise e  # it's already mapped\n    ^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 469, in exception_type\n    raise APIConnectionError(\n    ...<7 lines>...\n    )\nlitellm.exceptions.APIConnectionError: litellm.APIConnectionError: APIConnectionError: OpenAIException - AsyncImages.generate() got an unexpected keyword argument 'output_format'\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nmain-latest\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "rodrigo-lurnova",
      "author_type": "User",
      "created_at": "2025-04-29T09:16:25Z",
      "updated_at": "2025-06-05T15:18:34Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10402/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "S1LV3RJ1NX"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10402",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10402",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:51.616944",
      "comments": [
        {
          "author": "Ziyann",
          "body": "Same problem if you try to specify `moderation` (supported only by `gpt-image-1`).\nAnd to be clear, with no extra params, just model and prompt, it works.",
          "created_at": "2025-04-29T09:40:17Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "Hi @rodrigo-lurnova are you calling the /chat/completions endpoint here ? can you give me a clear way to repro this ",
          "created_at": "2025-04-29T22:02:26Z"
        },
        {
          "author": "danieldjupvik",
          "body": "@ishaan-jaff is other params like `background` and `output_compression` supported? Because when I pass `background` I get similar error mentioned above except LiteLLM doesn‚Äôt crash. Is there a way to drop these params if they are not supported by LiteLLM?",
          "created_at": "2025-04-29T23:35:49Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "what error do you see @danieldjupvik ?",
          "created_at": "2025-04-29T23:56:40Z"
        },
        {
          "author": "danieldjupvik",
          "body": "@ishaan-jaff \n500 litellm.APIConnectionError: APIConnectionError: OpenAIException - AsyncImages.generate() got an unexpected keyword argument 'background'. Received Model Group=gpt-image-1\nAvailable Model Group Fallbacks=None",
          "created_at": "2025-04-30T00:07:32Z"
        }
      ]
    },
    {
      "issue_number": 11444,
      "title": "[Bug]: Unknown service in passthrough mode for vertex ai Claude model",
      "body": "### What happened?\n\nWhen using Anthropic models via vertex AI in pass-through mode, LiteLLM is unable to map the model correctly, it's just \"unknown\". Ideally it's correctly mapped to the respective model so we have proper budget control.\n\nCan this be easily solved in any way?\nCurrently using v1.71.1-stable\n<img width=\"1439\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/cf91dadd-7328-4ec1-a783-d534adca3825\" />\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.71.1-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "gboston",
      "author_type": "User",
      "created_at": "2025-06-05T15:17:36Z",
      "updated_at": "2025-06-05T15:17:36Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11444/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11444",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11444",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:51.802884",
      "comments": []
    },
    {
      "issue_number": 11238,
      "title": "[Bug]: Pass-through creation and deletion not working",
      "body": "### What happened?\n\nWhen creating a pass-through endpoint via the UI, the APIs do not get registered. \n\nWhen testing dummy API, Creation seems to work fine and gets reflected on the UI, however I think no endpoint_id is getting created, and that parameter is required to delete the endpoints.\n\nOnce I try to delete the dummy passthrough endpoints, deleting fails because of the endpoint_id not being present.\n\n![Image](https://github.com/user-attachments/assets/b978cf7c-60f3-4b38-a3d8-7420ce58be26)\n\n### Relevant log output\n\n```shell\nINFO:     10.3.17.183:40284 - \"DELETE /config/pass_through_endpoint/v1 HTTP/1.1\" 405 Method Not Allowed\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.71.1-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "ma-armenta",
      "author_type": "User",
      "created_at": "2025-05-29T14:36:53Z",
      "updated_at": "2025-06-05T12:59:32Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11238/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11238",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11238",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:51.802906",
      "comments": [
        {
          "author": "TSJasonH",
          "body": "Confirming the same here. Manually sending a curl that specified endpoint_id works fine (I only tested with DELETE).\nJust not working through the UI. ",
          "created_at": "2025-06-05T12:59:31Z"
        }
      ]
    },
    {
      "issue_number": 11435,
      "title": "[Feature]: Add support for custom certificates in Redis and Postgres",
      "body": "### The Feature\n\nHi, we have the need to connect to a database and Redis with custom certificate, could you please add support for this? Thanks!\n\n### Motivation, pitch\n\nFor security concerns we have to use Redis and a PostgreSQL with custom certificates\n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "miguelangelmorenochacon",
      "author_type": "User",
      "created_at": "2025-06-05T09:41:52Z",
      "updated_at": "2025-06-05T09:41:52Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11435/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11435",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11435",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:51.997885",
      "comments": []
    },
    {
      "issue_number": 7173,
      "title": "[Bug]: [Helm chart] Migration job pod cannot run on OpenShift and behind https proxy",
      "body": "### What happened?\n\nHello,\r\n\r\nwe currently facing issues getting the migration job pod running. We have several issues here:\r\n\r\n1. It is missing the proxy environment variables `HTTP_PROXY` and `HTTPS_PROXY` but also `NO_PROXY` (which can be fixed by changing the helm chart accordingly) \r\n\r\n2. On OpenShift the file system is read only by default, therefore when `prisma generate` is called, it tries to install the Prisma CLI and fails, because the standard cache directory is read only (this can be fixed by setting `PRISMA_BINARY_CACHE_DIR` to a temporary directory, but also here a helm chart change is necessary)\r\n\r\n3. But the worst problem is, that prisma tries to install nodejs over `python -m nodeenv <cache directory>`. We have an https company proxy, which is not supported by the `urllib.requests` library used by `nodeenv`. So it cannot install nodejs and fails. There are two possible solutions to that:\r\n  * either nodejs is installed inside the Dockerfile for the migration job pod and prisma is setup accordingly to use the preinstalled nodejs version (see https://prisma-client-py.readthedocs.io/en/stable/reference/config/ \"Use nodejs-bin\")\r\n  * or inside the Dockerfile `RUN prisma version` is executed, which installs the prisma CLI during image build and therefore no further installation is necessary when running the pod on Kubernetes. This would also solve issue 2 and probably also issue 1, depending where the database is running\r\n\r\nIs there the chance to add the prisma and nodejs binaries already during the image building?\r\n\r\nP.S.: The helm chart version is `0.1.547`.\n\n### Relevant log output\n\n_No response_\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.54.0\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "anetbnd",
      "author_type": "User",
      "created_at": "2024-12-11T08:44:32Z",
      "updated_at": "2025-06-05T08:24:43Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 17,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/7173/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/7173",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/7173",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:51.997901",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "This should already be fixed with this - https://github.com/BerriAI/litellm/pkgs/container/litellm-non_root",
          "created_at": "2025-02-06T17:42:47Z"
        },
        {
          "author": "freinold",
          "body": "We are having the same problem, even when using the non_root image.\n\nFull log of litellm-migrations Job with the latest Helm Chart Version 0.1.625 and latest stable image version \"litellm_stable_release_branch-v1.61.13-stable\"\n\n```log\n\u001b[92m14:17:24 - LiteLLM Proxy:ERROR\u001b[0m: prisma_migration.py:71 -",
          "created_at": "2025-02-28T14:22:10Z"
        },
        {
          "author": "krrishdholakia",
          "body": "@anetbnd can you confirm the non_root image worked for you on v1.61.13?\n\nI haven't seen anyone else report the error @freinold ",
          "created_at": "2025-02-28T14:39:14Z"
        },
        {
          "author": "freinold",
          "body": "I tried to investigate a littler further: \nIt seems like there is already a schema inside the directory, dated to the container build time.\n\nSo we probably don't even need to generate the schema again and just have to push it to the DB if we are inside the non_root container.\n\n@krrishdholakia What i",
          "created_at": "2025-02-28T14:43:01Z"
        },
        {
          "author": "anetbnd",
          "body": "@krrishdholakia We did not test fully, but at least the migration job now works without errors when using the non_root image. @mohittalele can give more details.",
          "created_at": "2025-03-03T07:39:38Z"
        }
      ]
    },
    {
      "issue_number": 11122,
      "title": "[Feature]: Support Gemini URL context tool",
      "body": "### The Feature\n\nThis is a new Gemini feature: https://ai.google.dev/gemini-api/docs/url-context\n\n### Motivation, pitch\n\nLiteLLM already supports the Google search & code execution tools. It should also support the new URL context tool.\n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "Ziyann",
      "author_type": "User",
      "created_at": "2025-05-24T15:33:46Z",
      "updated_at": "2025-06-05T06:56:22Z",
      "closed_at": "2025-06-05T06:56:22Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11122/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11122",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11122",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:52.265091",
      "comments": []
    },
    {
      "issue_number": 11350,
      "title": "[Bug]: requester_ip_address is empty in custom callback",
      "body": "### What happened?\n\nWhen implementing a custom proxy_handler_instance callback for LiteLLM, I noticed that the proxy_metadata[\"requester_ip_address\"] field is always an empty string (\"\"). How do I get this value?\n\n### Relevant log output\n\n```\nProxy Metadata: {\n  'requester_ip_address': '', \n  'requester_metadata': {}, \n  'user_api_key_hash': 'acb42a4fa3d3621a7eeccc0cc79a3727e1c37572145ac3d1e7f773dde84de728', \n  'user_api_key_alias': None, \n  'user_api_key_team_id': None, \n  'user_api_key_user_id': 'default_user_id', \n  'user_api_key_org_id': None,\n  'user_api_key_team_alias': None, \n  'user_api_key_end_user_id': None, \n  'user_api_key_user_email': None,\n   'user_api_key': 'acb42a4fa3d3621a7eeccc0cc79a3727e1c37572145ac3d1e7f773dde84de728', \n  'user_api_end_user_max_budget': None, \n  'litellm_api_version': '1.71.1', \n  'global_max_parallel_requests': None,\n  'user_api_key_team_max_budget': None,\n  'user_api_key_team_spend': None,\n  'user_api_key_spend': 0.0, \n  'user_api_key_max_budget': None, \n  'user_api_key_model_max_budget': {},\n  'user_api_key_metadata': {},\n  'headers': {'host': '0.0.0.0:4000', 'user-agent': 'curl/7.81.0', 'accept': '*/*', 'content-type': 'application/json', 'content-length': '220'}, 'endpoint': 'http://0.0.0.0:4000/chat/completions',\n  'litellm_parent_otel_span': None, \n  'model_group': 'mistral-small3.1',\n  'model_group_size': 1, \n  'deployment': 'ollama/mistral-small3.1:24b-instruct-2503-q8_0',\n  'model_info': {'id': '4952ba1b169f9d4b840908f334979ca9d104adba58945af4954b74b80a52ac3d', 'db_model': False},\n  'api_base': 'http://172.17.0.1:11434', \n  'caching_groups': None, \n  'hidden_params': {'custom_llm_provider': 'ollama', 'region_name': None, 'optional_params': {'stream': False, 'max_retries': 0}, 'litellm_call_id': '4b72b557-c469-47cb-bf1d-70e7f8e0066c', 'api_base': 'http://172.17.0.1:11434', 'model_id': '4952ba1b169f9d4b840908f334979ca9d104adba58945af4954b74b80a52ac3d', \n  'response_cost': 0.0, \n  'additional_headers': {'x-litellm-model-group': 'mistral-small3.1', 'x-litellm-attempted-retries': 0, 'x-litellm-attempted-fallbacks': 0}, 'litellm_model_name': 'ollama/mistral-small3.1:24b-instruct-2503-q8_0', 'litellm_overhead_time_ms': 6.815, '_response_ms': 569.9739999999999}\n}\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.71.1-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "FreshLucas-git",
      "author_type": "User",
      "created_at": "2025-06-03T09:04:06Z",
      "updated_at": "2025-06-05T06:43:10Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11350/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11350",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11350",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:52.265112",
      "comments": []
    },
    {
      "issue_number": 11428,
      "title": "[Bug]: prometheus non blocking error",
      "body": "### What happened?\n\nsee logs\n\n### Relevant log output\n\n```shell\n05:14:23 - LiteLLM:ERROR: litellm_logging.py:2079 - LiteLLM.LoggingError: [Non-Blocking] Exception occurred while success logging Traceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/litellm_logging.py\", line 2011, in async_success_handler\n    await callback.async_log_success_event(\n    ...<4 lines>...\n    )\n  File \"/usr/lib/python3.13/site-packages/litellm/integrations/prometheus.py\", line 464, in async_log_success_event\n    self._increment_top_level_request_and_spend_metrics(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        end_user_id=end_user_id,\n        ^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<7 lines>...\n        enum_values=enum_values,\n        ^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/integrations/prometheus.py\", line 645, in _increment_top_level_request_and_spend_metrics\n    self.litellm_requests_metric.labels(**_labels).inc()\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'PrometheusLogger' object has no attribute 'litellm_requests_metric'\n\nINFO:     ('172.18.0.1', 55068) - \"WebSocket /v1/realtime?model=openai/gpt-4o-realtime-preview\" [accepted]\nINFO:     connection open\nINFO:     172.18.0.8:53504 - \"GET /metrics HTTP/1.1\" 307 Temporary Redirect\nINFO:     172.18.0.8:53504 - \"GET /metrics/ HTTP/1.1\" 200 OK\nINFO:     connection closed\n05:14:47 - LiteLLM:ERROR: litellm_logging.py:2079 - LiteLLM.LoggingError: [Non-Blocking] Exception occurred while success logging Traceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/litellm_core_utils/litellm_logging.py\", line 2011, in async_success_handler\n    await callback.async_log_success_event(\n    ...<4 lines>...\n    )\n  File \"/usr/lib/python3.13/site-packages/litellm/integrations/prometheus.py\", line 464, in async_log_success_event\n    self._increment_top_level_request_and_spend_metrics(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        end_user_id=end_user_id,\n        ^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<7 lines>...\n        enum_values=enum_values,\n        ^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/integrations/prometheus.py\", line 645, in _increment_top_level_request_and_spend_metrics\n    self.litellm_requests_metric.labels(**_labels).inc()\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'PrometheusLogger' object has no attribute 'litellm_requests_metric'\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.72.1 nightly\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "superpoussin22",
      "author_type": "User",
      "created_at": "2025-06-05T05:24:19Z",
      "updated_at": "2025-06-05T06:32:32Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11428/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11428",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11428",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:52.265119",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "Do you have prometheus set as a callback @superpoussin22 ? ",
          "created_at": "2025-06-05T05:25:33Z"
        },
        {
          "author": "superpoussin22",
          "body": "Yes",
          "created_at": "2025-06-05T06:32:31Z"
        }
      ]
    },
    {
      "issue_number": 2608,
      "title": "[Bug] Sagemaker Embedding endpoint exception with text_inputs key",
      "body": "This causes Exception https://github.com/BerriAI/litellm/blob/cace0bd6fbd77e3abf4723db7c1d459c90e5abe2/litellm/llms/sagemaker.py#L593\r\n\r\n`{\r\n    \"error\": {\r\n        \"message\": \"Received client error (400) from primary with message \\\"{\\n  \\\"code\\\": 400,\\n  \\\"type\\\": \\\"InternalServerException\\\",\\n  \\\"message\\\": \\\"text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\\\"\\n}\\n\\\". See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/hf-bge-large-en-v15 in account <> for more information.\",\r\n        \"type\": null,\r\n        \"param\": null,\r\n        \"code\": 500\r\n    }\r\n}`\r\n\r\nShould be changed to \r\n`    data = json.dumps({\"inputs\": input}).encode(\"utf-8\")\r\n`",
      "state": "closed",
      "author": "gauravnbcu",
      "author_type": "User",
      "created_at": "2024-03-20T20:17:25Z",
      "updated_at": "2025-06-05T05:47:44Z",
      "closed_at": "2025-05-08T00:03:10Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 11,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/2608/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "krrishdholakia"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/2608",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/2608",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:52.460446",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "@gauravnbcu can I see how you're making the call to litellm ? ",
          "created_at": "2024-03-20T20:26:00Z"
        },
        {
          "author": "gauravnbcu",
          "body": "Through Postman POST call for /`v1/embeddings` with following body\r\n`{\r\n    \"model\":\"bge-embedding-large\",\r\n    \"input\":\"this is a test sentence\"\r\n}`",
          "created_at": "2024-03-20T20:46:30Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "@gauravnbcu can we live debug this together ? Want to make sure I understand the issue:\r\nI'm on here if you're free: https://meet.google.com/psj-bgtf-uba\r\n\r\nSharing a link to my cal for your convenience: \r\nhttps://calendly.com/d/4mp-gd3-k5k/berriai-1-1-onboarding-litellm-hosted-version?month=2023-10",
          "created_at": "2024-03-20T21:56:03Z"
        },
        {
          "author": "gauravnbcu",
          "body": "Sure, We can meet at 3pm EST or earlier, let me know your availability ",
          "created_at": "2024-03-21T14:12:35Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "@gauravnbcu what's the best email to send an invite to? You can DM on Linkedin if you prefer: https://www.linkedin.com/in/reffajnaahsi/ ",
          "created_at": "2024-03-21T14:32:27Z"
        }
      ]
    },
    {
      "issue_number": 11423,
      "title": "[Bug]: /model/delete API breaks when using multiple workers",
      "body": "### What happened?\n\nWhen I set `NUM_WORKERS` to 8 (presumably any number greater than 1), model information remains in `/v1/models` and `/v1/model/info` even though I deleted the model via the `/model/delete` API.\n\nHere is some sample code that reproduces this issue, along with the output:\n\n(The environment was created according to https://docs.litellm.ai/docs/proxy/deploy, except that `NUM_WORKERS: 8` was added to the environment variable at the following location.)\n\nhttps://github.com/BerriAI/litellm/blob/049e65a84e519f5efadf7d7bb8a0513dced9d87f/docker-compose.yml#L18\n\n```py\nimport requests\n\nLITELLM_API_URL = \"http://localhost:4000\"\nLITELLM_KEY = \"sk-1234\"\n\ndef get_litellm_models():\n    response = requests.get(\n        f\"{LITELLM_API_URL}/v1/models\",\n        headers={\n            \"accept\": \"application/json\",\n            \"Authorization\": f\"Bearer {LITELLM_KEY}\",\n        },\n    )\n    return response.json()\n\n\ndef get_litellm_model_info():\n    response = requests.get(\n        f\"{LITELLM_API_URL}/v1/model/info\",\n        headers={\n            \"accept\": \"application/json\",\n            \"Authorization\": f\"Bearer {LITELLM_KEY}\",\n        },\n    )\n    models = []\n    for model in response.json()[\"data\"]:\n        models.append(\n            {\n                \"model_name\": model[\"model_name\"],\n                \"model_litellm_id\": model[\"model_info\"][\"id\"],\n            }\n        )\n    return models\n\n\ndef add_litellm_model(model_name: str):\n    response = requests.post(\n        f\"{LITELLM_API_URL}/model/new\",\n        headers={\n            \"accept\": \"application/json\",\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {LITELLM_KEY}\",\n        },\n        json={\n            \"model_name\": model_name,\n            \"litellm_params\": {\n                \"model\": f\"openai/{model_name}\",\n            },\n            \"model_info\": {\"version\": 2},\n        },\n    )\n    return response.json()\n\n\ndef delete_litellm_model(model_name: str):\n    litellm_models = get_litellm_model_info()\n    ret = []\n    for model in litellm_models:\n        if model[\"model_name\"] == model_name:\n            # todo: error handling\n            response = requests.post(\n                f\"{LITELLM_API_URL}/model/delete\",\n                headers={\n                    \"accept\": \"application/json\",\n                    \"Content-Type\": \"application/json\",\n                    \"Authorization\": f\"Bearer {LITELLM_KEY}\",\n                },\n                json={\"id\": model[\"model_litellm_id\"]},\n            )\n            if response.status_code != 200:\n                raise RuntimeError(f\"Failed to delete model {model_name}: {response.text}\")\n            ret.append(response.json())\n    return ret\n\n\ndef main():\n    for i in range(10):\n        print(i, get_litellm_models())\n    print(add_litellm_model(\"model1\"))\n    for i in range(10):\n        print(i, get_litellm_models())\n    print(delete_litellm_model(\"model1\"))\n    for i in range(10):\n        print(i, get_litellm_models())\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### Relevant log output\n\n```shell\n0 {'data': [], 'object': 'list'}\n1 {'data': [], 'object': 'list'}\n2 {'data': [], 'object': 'list'}\n3 {'data': [], 'object': 'list'}\n4 {'data': [], 'object': 'list'}\n5 {'data': [], 'object': 'list'}\n6 {'data': [], 'object': 'list'}\n7 {'data': [], 'object': 'list'}\n8 {'data': [], 'object': 'list'}\n9 {'data': [], 'object': 'list'}\n{'model_id': '32ba4821-fb74-41e1-b965-cadb53888736', 'model_name': 'model1', 'litellm_params': {'model': 'QvI3p+IdHJmyvcXw4FznKzwjJXyhPzdwcmyeKNxjH7H54vW22+lDffNmHJgeE4qu3R93hJQ=', 'use_litellm_proxy': False, 'use_in_pass_through': False, 'merge_reasoning_content_in_choices': False}, 'model_info': {'id': '32ba4821-fb74-41e1-b965-cadb53888736', 'version': 2, 'db_model': False}, 'created_at': '2025-06-05T01:56:24.965000Z', 'created_by': 'default_user_id', 'updated_at': '2025-06-05T01:56:24.965000Z', 'updated_by': 'default_user_id'}\n0 {'data': [{'id': 'model1', 'object': 'model', 'created': 1677610602, 'owned_by': 'openai'}], 'object': 'list'}\n1 {'data': [{'id': 'model1', 'object': 'model', 'created': 1677610602, 'owned_by': 'openai'}], 'object': 'list'}\n2 {'data': [{'id': 'model1', 'object': 'model', 'created': 1677610602, 'owned_by': 'openai'}], 'object': 'list'}\n3 {'data': [{'id': 'model1', 'object': 'model', 'created': 1677610602, 'owned_by': 'openai'}], 'object': 'list'}\n4 {'data': [{'id': 'model1', 'object': 'model', 'created': 1677610602, 'owned_by': 'openai'}], 'object': 'list'}\n5 {'data': [{'id': 'model1', 'object': 'model', 'created': 1677610602, 'owned_by': 'openai'}], 'object': 'list'}\n6 {'data': [{'id': 'model1', 'object': 'model', 'created': 1677610602, 'owned_by': 'openai'}], 'object': 'list'}\n7 {'data': [{'id': 'model1', 'object': 'model', 'created': 1677610602, 'owned_by': 'openai'}], 'object': 'list'}\n8 {'data': [{'id': 'model1', 'object': 'model', 'created': 1677610602, 'owned_by': 'openai'}], 'object': 'list'}\n9 {'data': [{'id': 'model1', 'object': 'model', 'created': 1677610602, 'owned_by': 'openai'}], 'object': 'list'}\n[{'message': 'Model: 5fc30770-ae54-4517-9af9-008466b0395f deleted successfully'}]\n0 {'data': [{'id': 'model1', 'object': 'model', 'created': 1677610602, 'owned_by': 'openai'}], 'object': 'list'}\n1 {'data': [], 'object': 'list'}\n2 {'data': [{'id': 'model1', 'object': 'model', 'created': 1677610602, 'owned_by': 'openai'}], 'object': 'list'}\n3 {'data': [{'id': 'model1', 'object': 'model', 'created': 1677610602, 'owned_by': 'openai'}], 'object': 'list'}\n4 {'data': [], 'object': 'list'}\n5 {'data': [{'id': 'model1', 'object': 'model', 'created': 1677610602, 'owned_by': 'openai'}], 'object': 'list'}\n6 {'data': [{'id': 'model1', 'object': 'model', 'created': 1677610602, 'owned_by': 'openai'}], 'object': 'list'}\n7 {'data': [{'id': 'model1', 'object': 'model', 'created': 1677610602, 'owned_by': 'openai'}], 'object': 'list'}\n8 {'data': [], 'object': 'list'}\n9 {'data': [{'id': 'model1', 'object': 'model', 'created': 1677610602, 'owned_by': 'openai'}], 'object': 'list'}\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.71.1-stable.patch1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "kota-iizuka",
      "author_type": "User",
      "created_at": "2025-06-05T03:31:40Z",
      "updated_at": "2025-06-05T03:54:11Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11423/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11423",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11423",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:52.679935",
      "comments": [
        {
          "author": "kota-iizuka",
          "body": "- Data inconsistency seems to occur only when the model list becomes empty after executing `/model/delete`. The results of `/model/new` model1, `/model/new` model2, `/model/delete` model1 were accurate (except for the behavior that asynchronous database updates take time).\n- As a workaround, you can",
          "created_at": "2025-06-05T03:54:10Z"
        }
      ]
    },
    {
      "issue_number": 11422,
      "title": "[Bug]: Anthropic web search cost",
      "body": "### What happened?\n\nWhen using `litellm.anthropic.messages.acreate` with model `claude-3-5-sonnet-latest`.\n\nTotal costs returned by litellm are not taking web_search server tool cost into account. \n\nIn one of the call to `acreate` it returned Total Cost as `0.051909000000000004` \n\nwhich is same as computing \n\n`cost_per_token(model=response_data[\"model\"], prompt_tokens=response_data[\"usage\"][\"input_tokens\"], completion_tokens=response_data[\"usage\"][\"output_tokens\"])`\n\non the following response data from `anthropic.messages.acreate` (content is removed from the response)\n\n`\nresponse_data = {\n  \"id\": \"msg_01MzCuLUQV1aPSyXJprRcxDt\",\n  \"type\": \"message\",\n  \"role\": \"assistant\",\n  \"model\": \"claude-3-5-sonnet-20241022\",\n  \"content\": [\n  ],\n  \"stop_reason\": \"max_tokens\",\n  \"stop_sequence\": null,\n  \"usage\": {\n    \"input_tokens\": 16438,\n    \"cache_creation_input_tokens\": 0,\n    \"cache_read_input_tokens\": 0,\n    \"output_tokens\": 173,\n    \"service_tier\": \"standard\",\n    \"server_tool_use\": {\n      \"web_search_requests\": 1\n    }\n  }\n}\n`\n\nWe can see that call to anthropic uses web search requests, So is this the intended behaviour, should it also not add the costs from `StandardBuiltInToolCostTracking.get_cost_for_anthropic_web_search` as well?\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "newmanifold",
      "author_type": "User",
      "created_at": "2025-06-05T02:53:30Z",
      "updated_at": "2025-06-05T02:53:30Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11422/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11422",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11422",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:52.904876",
      "comments": []
    },
    {
      "issue_number": 9308,
      "title": "[Bug]: Adding models via UI only saves the first selected model for a provider",
      "body": "### What happened?\n\nWhen adding a model via the UI, only the first selected model for a provider is saved. E.g. when selecting gpt-4o and gpt-3.5-turbo only gpt-4o is saved.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.63.11\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "captain-nemo",
      "author_type": "User",
      "created_at": "2025-03-17T11:12:26Z",
      "updated_at": "2025-06-05T01:49:21Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9308/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9308",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9308",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:52.904900",
      "comments": [
        {
          "author": "nati0n",
          "body": "Just noting, I'm also experiencing this bug on 1.65.0-stable, specifically when running inside a docker container.",
          "created_at": "2025-03-27T12:36:39Z"
        },
        {
          "author": "jtong99",
          "body": "Hi, seems like this issue is fixed in the latest version of Litellm.",
          "created_at": "2025-06-05T01:49:20Z"
        }
      ]
    },
    {
      "issue_number": 9248,
      "title": "[Bug]: ollama calls resulting in All connections failed error",
      "body": "### What happened?\n\nApp config:\n========\n  - model_name: llama3\n    litellm_params:\n      model: ollama_chat/llama3.2\n      #keep_alive: \"8m\" # Optional: Overrides default keep_alive, use -1 for Forever\n      api_base: \"http://localhost:11434\"\n    model_info:\n      supports_function_calling: true\n\nPost Call:\n========\ncurl -i -X POST 'http://localhost:4000/chat/completions' \\\n-H 'Content-Type: application/json' \\\n-d '{\n    \"model\": \"llama3\",\n    \"messages\": [\n\n      {\n        \"role\": \"user\",\n        \"content\": \"is litellm a ai gateway\"\n      }\n    ],\n   \"stream\": true\n}'\n\nPost call from litellm:\n===============\n\nlitellm_layer-1  | POST Request Sent from LiteLLM:\nlitellm_layer-1  | curl -X POST \\\nlitellm_layer-1  | http://localhost:11434/api/generate \\\nlitellm_layer-1  | -d '{'model': 'llama3.2', 'prompt': '### User:\\nlitellm is an ai gateway\\n\\n', 'options': {}, 'stream': True}'\n\nNOTE\n=====\nIf I run the ollama post call directly in the host, by replacing single quotes in -d with double quotes, it is working fine.\n\ncurl -X POST \\\nhttp://localhost:11434/api/generate \\\n-d '{\"model\": \"llama3.2\", \"prompt\": \"### User:\\n is litellm an ai gateway\\n\\n\\n\\n\", \"options\": {}, \"stream\": true}'\n\n\n\n### Relevant log output\n\n```shell\nlitellm Error:\n============\n\nlitellm_layer-1  | 13:44:20 - LiteLLM Router:INFO: router.py:948 - litellm.acompletion(model=ollama/llama3.2) 200 OK\nlitellm_layer-1  | 13:44:20 - LiteLLM Router:DEBUG: router.py:2647 - Async Response: <async_generator object ollama_async_streaming at 0x7f2fc42ffa10>\nlitellm_layer-1  | INFO:     localhost:57988 - \"POST /chat/completions HTTP/1.1\" 200 OK\nlitellm_layer-1  | 13:44:20 - LiteLLM Proxy:DEBUG: proxy_server.py:2667 - inside generator\nlitellm_layer-1  | 13:44:20 - LiteLLM Proxy:ERROR: proxy_server.py:2691 - litellm.proxy.proxy_server.async_data_generator(): Exception occured - All connection attempts failed\nlitellm_layer-1  | Traceback (most recent call last):\nlitellm_layer-1  |   File \"/usr/local/lib/python3.11/site-packages/httpx/_transports/default.py\", line 69, in map_httpcore_exceptions\nlitellm_layer-1  |     yield\nlitellm_layer-1  |   File \"/usr/local/lib/python3.11/site-packages/httpx/_transports/default.py\", line 373, in handle_async_request\nlitellm_layer-1  |     resp = await self._pool.handle_async_request(req)\nlitellm_layer-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlitellm_layer-1  |   File \"/usr/local/lib/python3.11/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\nlitellm_layer-1  |     raise exc from None\nlitellm_layer-1  |   File \"/usr/local/lib/python3.11/site-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\nlitellm_layer-1  |     response = await connection.handle_async_request(\nlitellm_layer-1  |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlitellm_layer-1  |   File \"/usr/local/lib/python3.11/site-packages/httpcore/_async/connection.py\", line 101, in handle_async_request\nlitellm_layer-1  |     raise exc\nlitellm_layer-1  |   File \"/usr/local/lib/python3.11/site-packages/httpcore/_async/connection.py\", line 78, in handle_async_request\nlitellm_layer-1  |     stream = await self._connect(request)\nlitellm_layer-1  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlitellm_layer-1  |   File \"/usr/local/lib/python3.11/site-packages/httpcore/_async/connection.py\", line 124, in _connect\nlitellm_layer-1  |     stream = await self._network_backend.connect_tcp(**kwargs)\nlitellm_layer-1  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlitellm_layer-1  |   File \"/usr/local/lib/python3.11/site-packages/httpcore/_backends/auto.py\", line 31, in connect_tcp\nlitellm_layer-1  |     return await self._backend.connect_tcp(\nlitellm_layer-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlitellm_layer-1  |   File \"/usr/local/lib/python3.11/site-packages/httpcore/_backends/anyio.py\", line 113, in connect_tcp\nlitellm_layer-1  |     with map_exceptions(exc_map):\nlitellm_layer-1  |   File \"/usr/local/lib/python3.11/contextlib.py\", line 158, in __exit__\nlitellm_layer-1  |     self.gen.throw(typ, value, traceback)\nlitellm_layer-1  |   File \"/usr/local/lib/python3.11/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\nlitellm_layer-1  |     raise to_exc(exc) from exc\nlitellm_layer-1  | httpcore.ConnectError: All connection attempts failed\nlitellm_layer-1  |\nlitellm_layer-1  | The above exception was the direct cause of the following exception:\nlitellm_layer-1  |\nlitellm_layer-1  | Traceback (most recent call last):\nlitellm_layer-1  |   File \"/usr/local/lib/python3.11/site-packages/litellm/proxy/proxy_server.py\", line 2670, in async_data_generator\nlitellm_layer-1  |     async for chunk in response:\nlitellm_layer-1  |   File \"/usr/local/lib/python3.11/site-packages/litellm/llms/ollama.py\", line 525, in ollama_async_streaming\nlitellm_layer-1  |     raise e  # don't use verbose_logger.exception, if exception is raised\nlitellm_layer-1  |     ^^^^^^^\nlitellm_layer-1  |   File \"/usr/local/lib/python3.11/site-packages/litellm/llms/ollama.py\", line 464, in ollama_async_streaming\nlitellm_layer-1  |     async with client.stream(\nlitellm_layer-1  |   File \"/usr/local/lib/python3.11/contextlib.py\", line 210, in __aenter__\nlitellm_layer-1  |     return await anext(self.gen)\nlitellm_layer-1  |            ^^^^^^^^^^^^^^^^^^^^^\nlitellm_layer-1  |   File \"/usr/local/lib/python3.11/site-packages/httpx/_client.py\", line 1617, in stream\nlitellm_layer-1  |     response = await self.send(\nlitellm_layer-1  |                ^^^^^^^^^^^^^^^^\nlitellm_layer-1  |   File \"/usr/local/lib/python3.11/site-packages/httpx/_client.py\", line 1661, in send\nlitellm_layer-1  |     response = await self._send_handling_auth(\nlitellm_layer-1  |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlitellm_layer-1  |   File \"/usr/local/lib/python3.11/site-packages/httpx/_client.py\", line 1689, in _send_handling_auth\nlitellm_layer-1  |     response = await self._send_handling_redirects(\nlitellm_layer-1  |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlitellm_layer-1  |   File \"/usr/local/lib/python3.11/site-packages/httpx/_client.py\", line 1726, in _send_handling_redirects\nlitellm_layer-1  |     response = await self._send_single_request(request)\nlitellm_layer-1  |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlitellm_layer-1  |   File \"/usr/local/lib/python3.11/site-packages/httpx/_client.py\", line 1763, in _send_single_request\nlitellm_layer-1  |     response = await transport.handle_async_request(request)\nlitellm_layer-1  |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlitellm_layer-1  |   File \"/usr/local/lib/python3.11/site-packages/httpx/_transports/default.py\", line 372, in handle_async_request\nlitellm_layer-1  |     with map_httpcore_exceptions():\nlitellm_layer-1  |   File \"/usr/local/lib/python3.11/contextlib.py\", line 158, in __exit__\nlitellm_layer-1  |     self.gen.throw(typ, value, traceback)\nlitellm_layer-1  |   File \"/usr/local/lib/python3.11/site-packages/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\nlitellm_layer-1  |     raise mapped_exc(message) from exc\nlitellm_layer-1  | httpx.ConnectError: All connection attempts failed\nlitellm_layer-1  | 13:44:20 - LiteLLM Proxy:DEBUG: proxy_server.py:2701 - An error occurred: All connection attempts failed\nlitellm_layer-1  |\nlitellm_layer-1  |  Debug this by setting `--debug`, e.g. `litellm --model gpt-3.5-turbo --debug`\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.53.7\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "pss4659",
      "author_type": "User",
      "created_at": "2025-03-14T13:56:00Z",
      "updated_at": "2025-06-05T01:47:41Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9248/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9248",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9248",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:53.065093",
      "comments": [
        {
          "author": "jtong99",
          "body": "Hi, this issue is unreproducible in the latest version of Litellm.",
          "created_at": "2025-06-05T01:47:39Z"
        }
      ]
    },
    {
      "issue_number": 7518,
      "title": "[Bug]: error 504 returned after 60s even if timeout settings are set to 900.",
      "body": "### What happened?\n\nI have the following settings in config.yaml:\r\n\r\nlitellm_settings: \r\n  request_timeout: 900\r\n  \r\nrouter_settings:\r\n  timeout: 900\r\n\r\nBut getting a HTTP 504 error after 60s. There is no log on the Litellm side. \n\n### Relevant log output\n\n```shell\nNo logs\n```\n\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.56.6\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "stephaneminisini",
      "author_type": "User",
      "created_at": "2025-01-03T14:39:21Z",
      "updated_at": "2025-06-05T00:02:07Z",
      "closed_at": "2025-06-05T00:02:07Z",
      "labels": [
        "bug",
        "mlops user request",
        "stale"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/7518/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/7518",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/7518",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:53.230936",
      "comments": [
        {
          "author": "Leonardo-Costa",
          "body": "i had the same issue, turns out it was the load balancer timeout, nothing to do with litellm",
          "created_at": "2025-02-27T23:22:08Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-05-29T00:01:53Z"
        }
      ]
    },
    {
      "issue_number": 8309,
      "title": "[Bug]: Deepseek is unresponsive after configuration.",
      "body": "### What happened?\n\nA bug happened!\nDeepseek is unresponsive after configuration.\n\n![Image](https://github.com/user-attachments/assets/fbb87e27-db72-4178-a1e5-c3d1feafb4c1)\n\n![Image](https://github.com/user-attachments/assets/078f5192-df02-450d-b080-535f3102abb3)\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.5\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "cai417",
      "author_type": "User",
      "created_at": "2025-02-06T08:43:51Z",
      "updated_at": "2025-06-05T00:02:04Z",
      "closed_at": "2025-06-05T00:02:04Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8309/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "krrishdholakia"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8309",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8309",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:53.485081",
      "comments": [
        {
          "author": "ohmyboroda",
          "body": "I'm also seeing problems since yesterday, before that everything was working.\n\n```\nError occurred while generating model response. Please try again. Error: Error: 500 litellm.APIConnectionError: APIConnectionError: DeepseekException - 'NoneType' object has no attribute 'rstrip' Received Model Group=",
          "created_at": "2025-02-06T09:48:50Z"
        },
        {
          "author": "hipur",
          "body": "Same here... When doing a health check, I get this:\n```\n   {\n      \"custom_llm_provider\": \"deepseek\",\n      \"use_in_pass_through\": false,\n      \"model\": \"deepseek/deepseek-reasoner\",\n      \"cache\": {\n        \"no-cache\": true\n      },\n      \"error\": \"litellm.APIConnectionError: APIConnectionError: De",
          "created_at": "2025-02-08T21:07:11Z"
        },
        {
          "author": "admon",
          "body": "same here.",
          "created_at": "2025-02-14T01:46:02Z"
        },
        {
          "author": "neubig",
          "body": "same here",
          "created_at": "2025-02-22T03:13:44Z"
        },
        {
          "author": "krrishdholakia",
          "body": "> pi_base = api_base.rstrip(\\\"/\\\")\\n               ^^^^^^^^^^^^^^^\\nAttributeError: 'NoneType'\n\nlooks like this happens when the api_base is None ",
          "created_at": "2025-02-22T04:11:01Z"
        }
      ]
    },
    {
      "issue_number": 8453,
      "title": "[Feature]: Mistral Moderation API",
      "body": "### The Feature\n\nLink : https://docs.mistral.ai/capabilities/guardrailing/\nAPI: https://docs.mistral.ai/api/#tag/classifiers/operation/moderations_v1_moderations_post\n\nMistral provider is available but it doesn't support the `mistral-moderation-latest` model, i tried using API base as: https://api.mistral.ai/v1/moderations , still no luck\n\nI am unable to see this model here as well: https://models.litellm.ai/\n\n### Motivation, pitch\n\nAdding this will help us capture the mistral content moderations response and all, as well fallbacks etc.\n\n### Are you a ML Ops Team?\n\nYes\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "KvelKrishna",
      "author_type": "User",
      "created_at": "2025-02-11T06:27:53Z",
      "updated_at": "2025-06-05T00:02:03Z",
      "closed_at": "2025-06-05T00:02:03Z",
      "labels": [
        "enhancement",
        "mlops user request",
        "stale"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8453/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8453",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8453",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:53.824410",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Oh - do you want this to follow the openai `/moderations` route? @KvelKrishna \n\nhttps://github.com/BerriAI/litellm/blob/caa848649cb9249fa94338113bcfcaa676af9e48/litellm/main.py#L4326",
          "created_at": "2025-02-12T04:57:54Z"
        },
        {
          "author": "KvelKrishna",
          "body": "> Oh - do you want this to follow the openai `/moderations` route? [@KvelKrishna](https://github.com/KvelKrishna)\n> \n> [litellm/litellm/main.py](https://github.com/BerriAI/litellm/blob/caa848649cb9249fa94338113bcfcaa676af9e48/litellm/main.py#L4326)\n> \n> Line 4326 in [caa8486](/BerriAI/litellm/commit",
          "created_at": "2025-02-12T08:26:01Z"
        },
        {
          "author": "mkagit",
          "body": "I also want to use Mistral moderation",
          "created_at": "2025-02-27T23:57:22Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-05-29T00:01:46Z"
        }
      ]
    },
    {
      "issue_number": 8838,
      "title": "[Bug]: Langfuse integration not working as expected",
      "body": "### What happened?\n\nWe followed instructions here for integrating litellm + langfuse: https://oss-llmops-stack.com/docs. But we couldn't get integration working reliably. When it does, we're not able to see the output in the trace. Please see the below image, when it did work. When the integration doesn't work, the whole litellm crashes with no logs to know what's happening. It just stops responding.\n\n![Image](https://github.com/user-attachments/assets/3edf1d12-5d65-4f4d-ae54-efeb645da113)\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\n1.60.8\n\n### Twitter / LinkedIn details\n\nhttps://www.linkedin.com/in/sasank-chilamkurthy/",
      "state": "closed",
      "author": "chsasank",
      "author_type": "User",
      "created_at": "2025-02-26T13:10:45Z",
      "updated_at": "2025-06-05T00:01:57Z",
      "closed_at": "2025-06-05T00:01:57Z",
      "labels": [
        "bug",
        "mlops user request",
        "stale"
      ],
      "label_count": 3,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8838/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8838",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8838",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:54.076211",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "Can you run litellm with detailed debug ?\n\n https://docs.litellm.ai/docs/proxy/debugging#detailed-debug ",
          "created_at": "2025-02-26T16:17:08Z"
        },
        {
          "author": "superpoussin22",
          "body": "3.28.2. And 3.28.3 are buggy, please upgrade ",
          "created_at": "2025-02-26T20:50:32Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "@superpoussin22 its not related to litellm correct? ",
          "created_at": "2025-02-27T00:49:37Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-05-29T00:01:36Z"
        }
      ]
    },
    {
      "issue_number": 8877,
      "title": "[Bug]: Cant Add Ollama Model in the UI (DB Model)",
      "body": "### What happened?\n\nAs soon as i select \"Ollama\" and start typing a model name.  The UI errors out.\nTried various versions with the same issue....currently on today's build 1.61.20\n\n![Image](https://github.com/user-attachments/assets/863c710c-8422-4fff-919c-20006cdf9f41)\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.61.20\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "metalshanked",
      "author_type": "User",
      "created_at": "2025-02-27T22:10:43Z",
      "updated_at": "2025-06-05T00:01:56Z",
      "closed_at": "2025-06-05T00:01:56Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8877/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8877",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8877",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:54.278076",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-05-29T00:01:34Z"
        }
      ]
    },
    {
      "issue_number": 11379,
      "title": "[Bug]: Reasoning parameters are conflicting with each other.",
      "body": "### What happened?\nThis is the [documentation](https://docs.litellm.ai/docs/providers/gemini#pass-thinking-to-gemini-models) for passing reasoning tokens to Gemini models:\n\n![Image](https://github.com/user-attachments/assets/b1efc17d-85a9-4509-800f-7a6fbf6a0309)\n\nAn example of reasoning configuration done normally:\n![Image](https://github.com/user-attachments/assets/a6600c03-07c2-4cb4-8b3b-24642a1d2369)\nAn example of not adhering to thinking.type when there is budget_tokens:\n![Image](https://github.com/user-attachments/assets/58375c66-0f2b-4d2f-a7b1-79ab535e7e9e)\nReasoning effort and thinking is not mutually exclusive:\n![Image](https://github.com/user-attachments/assets/460a72c8-7331-43ca-ac18-f1eb347dec47)\nSeems to respect thinking.budget_tokens as highest priority:\n![Image](https://github.com/user-attachments/assets/f0bf3cff-b65e-4203-a8a0-a25c69fd5775)\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.71.2 \n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "ming-jeng",
      "author_type": "User",
      "created_at": "2025-06-03T21:39:15Z",
      "updated_at": "2025-06-04T22:27:14Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11379/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11379",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11379",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:54.471527",
      "comments": [
        {
          "author": "colesmcintosh",
          "body": "Looking at the [official Gemini API documentation for thinking](https://ai.google.dev/gemini-api/docs/thinking), it appears that Gemini only supports the `budget_tokens` parameter for controlling reasoning behavior, not `reasoning_effort`. The budget essentially acts as a sliding scale that gives th",
          "created_at": "2025-06-04T22:26:55Z"
        }
      ]
    },
    {
      "issue_number": 11395,
      "title": "[Bug]: Duplicate server_id generation in MCP server configuration causes key mismatch",
      "body": "### What happened?\n\nfile:\nlitellm/proxy/_experimental/mcp_server/mcp_server_manager.py\n\n\nIt appears the server_id is being generated twice here. MCPServer likely generates its own server_id upon instantiation (line 76). Then, a new server_id is generated on line 85 and used as the key for self.config_mcp_servers. This could lead to a mismatch where the server_id attribute of the MCPServer object doesn't match the key it's stored under in the dictionary. Could this cause issues when trying to retrieve or manage these servers later using their internal server_id?\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv.1.72.0.rc2\n\n### Twitter / LinkedIn details\n\n@josephbenraz   /   https://www.linkedin.com/in/josephbenraz/",
      "state": "closed",
      "author": "Joseph1977",
      "author_type": "User",
      "created_at": "2025-06-04T08:08:35Z",
      "updated_at": "2025-06-04T21:55:38Z",
      "closed_at": "2025-06-04T21:55:38Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11395/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11395",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11395",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:54.633013",
      "comments": [
        {
          "author": "superpoussin22",
          "body": "sound fixed in : https://github.com/BerriAI/litellm/releases/tag/v1.72.0.dev1",
          "created_at": "2025-06-04T12:03:20Z"
        }
      ]
    },
    {
      "issue_number": 9883,
      "title": "[Bug]: [UI] Make it clear regenerate key is an enterprise feature",
      "body": "### What happened?\n\nWhen trying to regenerate Virtual key in the UI, it fails with a \"Failed to regenerate API Key\" message. According to the logfile (see below), this is an enterprise feature, but noch message is displayed to the user. \n\n### Relevant log output\n\n```shell\n807 File \"/usr/local/lib/python3.13/site-packages/litellm/proxy/management_endpoints/key_management_endpoints.py\", line 1689, in regenerate_key_fn\n808 raise ValueError(\n809 f\"Regenerating Virtual Keys is an Enterprise feature, {CommonProxyErrors.not_premium_user.value}\"\n810 )\n811 ValueError: Regenerating Virtual Keys is an Enterprise feature, You must be a LiteLLM Enterprise user to use this feature. If you have a license please set `LITELLM_LICENSE` in your env. Get a 7 day trial key here: https://www.litellm.ai/#trial.\n812 Pricing: https://www.litellm.ai/#pricing\n813 INFO: 10.130.4.2:49348 - \"POST /key/cec6ebd7b6874e0eb2090b2f8f669d06448020c23559f6a6b40f9d7d5f319371/regenerate HTTP/1.1\" 500 Internal Server Error\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.65.4\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "arnebe",
      "author_type": "User",
      "created_at": "2025-04-10T14:28:44Z",
      "updated_at": "2025-06-04T19:05:57Z",
      "closed_at": "2025-06-04T19:05:57Z",
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9883/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9883",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9883",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:54.822131",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Noted - we can hide this feature or gray it out to make it clear ",
          "created_at": "2025-04-10T21:45:12Z"
        }
      ]
    },
    {
      "issue_number": 11383,
      "title": "[Bug]: Vertex AI any_of issues for Description and Default",
      "body": "### What happened?\n\nThe fix in https://github.com/BerriAI/litellm/pull/11195 does not do what it says it does \n\nThe fix that was merged only works if `title` is specified, if default or description are specified it doesnt do anything and so any_of is still broken\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.72.0\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "strawgate",
      "author_type": "User",
      "created_at": "2025-06-03T22:33:37Z",
      "updated_at": "2025-06-04T18:56:08Z",
      "closed_at": "2025-06-04T05:35:52Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11383/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11383",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11383",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:55.017109",
      "comments": [
        {
          "author": "strawgate",
          "body": "this should fix it, will try to make a pr\n\n```\ndef _filter_anyof_fields(schema_dict: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    When anyof is present, only keep the anyof field and its contents - otherwise VertexAI will throw an error - https://github.com/BerriAI/litellm/issues/11164\n    Filter ",
          "created_at": "2025-06-03T22:34:12Z"
        },
        {
          "author": "krrishdholakia",
          "body": ">             (title or description or default)\n\n\nisn't 'default' just ignored here? ",
          "created_at": "2025-06-04T00:00:38Z"
        },
        {
          "author": "strawgate",
          "body": "@krrishdholakia i thought default also causes problems so I stripped it out -- did you test and found that default does not cause a problem when used with anyof? I can test today",
          "created_at": "2025-06-04T14:03:51Z"
        },
        {
          "author": "krrishdholakia",
          "body": "It's not clear what default would mean inside the any of objects. \n\nMy understanding is that default at the root meant the value was optional",
          "created_at": "2025-06-04T14:48:57Z"
        },
        {
          "author": "strawgate",
          "body": "That makes sense, I'll follow up if I still have issues!",
          "created_at": "2025-06-04T18:56:07Z"
        }
      ]
    },
    {
      "issue_number": 6600,
      "title": "[Feature]: Add support for SAP AI Core",
      "body": "### The Feature\n\nAdd support for SAP AI Core which has the standard OpenAI models like GPT-4o\n\n### Motivation, pitch\n\nIs there a guide for how to add new LLMs anywhere?\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "aantn",
      "author_type": "User",
      "created_at": "2024-11-05T12:56:52Z",
      "updated_at": "2025-06-04T18:26:44Z",
      "closed_at": "2025-02-11T00:01:44Z",
      "labels": [
        "enhancement",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/6600/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/6600",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/6600",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:55.251383",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-02-04T00:01:34Z"
        },
        {
          "author": "JohannKaspar",
          "body": "would be interested as well\n",
          "created_at": "2025-06-04T18:06:10Z"
        }
      ]
    },
    {
      "issue_number": 11412,
      "title": "[Feature]:",
      "body": "### The Feature\n\nLiteLLM supporting text to video models:\namazon.nova-reel-v1:0\namazon.nova-reel-v1:1\n\n### Motivation, pitch\n\nNova Reel v1:1 was released in April 2025 with multi-shot capabilities\nProcessing time: ~90 seconds for 6-second videos, 14-17 minutes for 2-minute videos\nAWS documentation: https://docs.aws.amazon.com/nova/latest/userguide/video-generation.html\n\n\nCan LiteLLM handle Nova Reel video generation with the same ease as other Bedrock models, abstracting away the complexity while providing access to advanced features like multi-shot generation.\n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\nlinkedin.com/in/hmaya",
      "state": "open",
      "author": "HaithamMaya",
      "author_type": "User",
      "created_at": "2025-06-04T17:36:09Z",
      "updated_at": "2025-06-04T17:36:09Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11412/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11412",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11412",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:55.462110",
      "comments": []
    },
    {
      "issue_number": 9638,
      "title": "[Bug]: Circular reference detected",
      "body": "### What happened?\n\nI used Docker to deploy litellm and encountered a 500 Circular reference detected error while using an internal user to create a key call\n\n### Relevant log output\n\n```shell\ncommon_request_processing.py:298 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - Circular reference detected\nlitellm-1  | Traceback (most recent call last):\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/litellm/proxy/proxy_server.py\", line 3552, in chat_completion\nlitellm-1  |     return await base_llm_response_processor.base_process_llm_request(\nlitellm-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlitellm-1  |     ...<16 lines>...\nlitellm-1  |     )\nlitellm-1  |     ^\nlitellm-1  |   File \"/usr/lib/python3.13/site-packages/litellm/proxy/common_request_processing.py\", line 127, in base_process_llm_request\nlitellm-1  |     \"Request received by LiteLLM:\\n{}\".format(json.dumps(self.data, indent=4)),\nlitellm-1  |                                               ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^\nlitellm-1  |   File \"/usr/lib/python3.13/json/__init__.py\", line 238, in dumps\nlitellm-1  |     **kw).encode(obj)\nlitellm-1  |           ~~~~~~^^^^^\nlitellm-1  |   File \"/usr/lib/python3.13/json/encoder.py\", line 200, in encode\nlitellm-1  |     chunks = self.iterencode(o, _one_shot=True)\nlitellm-1  |   File \"/usr/lib/python3.13/json/encoder.py\", line 261, in iterencode\nlitellm-1  |     return _iterencode(o, 0)\nlitellm-1  | ValueError: Circular reference detected\nlitellm-1  | 21:53:42 - LiteLLM Proxy:DEBUG: common_request_processing.py:307 - An error occurred: Circular reference detected \nlitellm-1  | \nlitellm-1  |  Debug this by setting `--debug`, e.g. `litellm --model gpt-3.5-turbo --debug`\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nghcr.io/berriai/litellm:main-latest\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "wenwen12345",
      "author_type": "User",
      "created_at": "2025-03-29T22:11:03Z",
      "updated_at": "2025-06-04T17:11:26Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9638/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9638",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9638",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:55.462132",
      "comments": [
        {
          "author": "ishaan-jaff",
          "body": "how do i repro this error @wenwen12345 ? ",
          "created_at": "2025-03-30T19:37:15Z"
        },
        {
          "author": "AinnovateSolutions",
          "body": "I incurred the same error on my local hosted. I restarted all the containers and still the same error. I tried on another device and it was fine. \nClear browser cache and cookies and try again. Mine worked after doing those. ",
          "created_at": "2025-03-31T03:50:48Z"
        },
        {
          "author": "thelooter",
          "body": "Here's a bit of a more complete log that I have. I have all my Models added through OpenRouter and am using LiteLLM with OpenWebUI for Interacting.\n\nVersion: 1.65.0-stable\n\n```log\n18:17:56 - LiteLLM Proxy:ERROR: common_request_processing.py:298 - litellm.proxy.proxy_server._handle_llm_api_exception(",
          "created_at": "2025-03-31T18:19:16Z"
        },
        {
          "author": "josh-at-straker",
          "body": "To solve this on my side (LiteLLM and Opern WebUI, external DB) I needed to create a whole new key, re-assign models, then attach to OpenWebUI again. \n",
          "created_at": "2025-04-03T19:29:04Z"
        },
        {
          "author": "egyroloine",
          "body": "I had the same problem and found the same workaround. It broke again though next time i went to add another model to the key.",
          "created_at": "2025-04-26T20:43:15Z"
        }
      ]
    },
    {
      "issue_number": 11302,
      "title": "[Bug]: reasoning content is not returned from anthropic when using aresponses",
      "body": "### What happened?\n\nWhen using the `aresponses` api with anthropic, it calls `acompletions` under the hood and when you are using thinking mode, reasoning content will be streamed back but those thinking content will not be output as reasoning chunks in the openai responses api format, you will instead get empty string output_text deltas.\n\n[logs.txt](https://github.com/user-attachments/files/20537521/logs.txt)\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.71.3-rc\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "ryan-castner",
      "author_type": "User",
      "created_at": "2025-06-01T02:11:46Z",
      "updated_at": "2025-06-04T16:33:15Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11302/reactions",
        "total_count": 4,
        "+1": 4,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11302",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11302",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:56.180862",
      "comments": [
        {
          "author": "knowsuchagency",
          "body": "## Additional Context: Tool Calling with Anthropic Thinking Models\n\nI've been investigating a related issue with tool calling and Anthropic thinking models. Here's what I found that might help with implementing a fix:\n\n### The Problem\n\nWhen using Anthropic thinking models (e.g., claude-sonnet-4-2025",
          "created_at": "2025-06-04T16:33:14Z"
        }
      ]
    },
    {
      "issue_number": 2293,
      "title": "[Help]: ModuleNotFoundError: No module named 'openai._models'",
      "body": "### What happened?\n\nWhen i ran the litellm acompletion for my fastapi endpoint, threw this error pls help.\n\n### Relevant log output\n\n```shell\nFile \"D:\\Infiheal\\lite_llm\\issues.py\", line 16, in <module>\r\n    from litellm import acompletion\r\n  File \"C:\\Users\\Rahul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\litellm\\__init__.py\", line 4, in <module>\r\n    from litellm.caching import Cache\r\n  File \"C:\\Users\\Rahul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\litellm\\caching.py\", line 14, in <module>\r\n    from openai._models import BaseModel as OpenAIObject\r\nModuleNotFoundError: No module named 'openai._models'\n```\n\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "vancityrahul",
      "author_type": "User",
      "created_at": "2024-03-02T05:48:18Z",
      "updated_at": "2025-06-04T16:30:09Z",
      "closed_at": "2024-03-19T02:00:38Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 11,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/2293/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/2293",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/2293",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:56.368926",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "what version of openai are you using? @vancityrahul ",
          "created_at": "2024-03-02T15:42:53Z"
        },
        {
          "author": "vancityrahul",
          "body": "The latest one provided by the litellm library. @krrishdholakia \r\n",
          "created_at": "2024-03-05T14:39:11Z"
        },
        {
          "author": "krrishdholakia",
          "body": "can you run `pip show openai` and share the value? ",
          "created_at": "2024-03-06T17:02:37Z"
        },
        {
          "author": "vancityrahul",
          "body": "I resolved it, turns out it was an environment issue as i was trying it on my virtual environment.",
          "created_at": "2024-03-08T05:41:02Z"
        },
        {
          "author": "Undertone0809",
          "body": "I got the same error. Change other environment and still happend. I have created a new conda environment.\r\n\r\n```text\r\n\r\n  File \"D:\\Projects\\CogitAGI\\promptulate\\promptulate\\llms\\_litellm.py\", line 5, in <module>\r\n    import litellm\r\n  File \"D:\\Application\\Conda\\envs\\pne\\lib\\site-packages\\litellm\\__i",
          "created_at": "2024-04-29T14:19:21Z"
        }
      ]
    },
    {
      "issue_number": 10427,
      "title": "[Bug]: cache_control_injection_points not working with VertexAI ",
      "body": "### What happened?\n\nUsing `cache_control_injection_points` with Claude Sonnet 3.7 on VertexAI returns an error. Vertex's documentation says it supports Claude's caching syntax: https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching\n\nAm I doing something wrong here with this config?\n\nIn `litellm_config.yaml`:\n```  - model_name: claude-sonnet-3.7\n    litellm_params:\n      model: vertex_ai/claude-3-7-sonnet@20250219\n      cache_control_injection_points:\n        - location: message\n          role: system\n```\n\nResponse from LiteLLM Proxy:\n```\n400 litellm.BadRequestError: VertexAIException BadRequestError - b'{\"type\":\"error\",\"error\":{\"type\":\"invalid_request_error\",\"message\":\"cache_control_injection_points: Extra inputs are not permitted\"}}'. Received Model Group=claude-sonnet-3.7\nAvailable Model Group Fallbacks=None\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.67.4-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "gabehollombe",
      "author_type": "User",
      "created_at": "2025-04-30T03:21:06Z",
      "updated_at": "2025-06-04T12:40:15Z",
      "closed_at": null,
      "labels": [
        "bug",
        "april 2025"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10427/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10427",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10427",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:56.622075",
      "comments": [
        {
          "author": "ketangangal",
          "body": "Hi @ishaan-jaff , I want to work on this Bug!",
          "created_at": "2025-06-04T12:40:13Z"
        }
      ]
    },
    {
      "issue_number": 11401,
      "title": "[Feature]: Support for Black Forest Labs' flux-kontext-pro model in LiteLLM's /images/edits endpoint",
      "body": "### The Feature\n\nHi @krrishdholakia and @ishaan-jaff!\n\nI'd like to request the addition of the new `flux-kontext-pro` model from Black Forest Labs as a supported provider for the [/images/edits](https://docs.litellm.ai/docs/image_edits) endpoint. \n\nBlack Forest Labs links:\n\n- [Flux Kontext](https://flux-kontext.app/)\n- [Dashboard](https://dashboard.bfl.ai/)\n- [API Docs](https://docs.bfl.ml/kontext/kontext_image_editing)\n\n### Motivation, pitch\n\nAfter testing, I‚Äôve found that `flux-kontext-pro` delivers significantly better quality than `gpt-image-1`, especially in preserving the original faces in edited images (which is a major pain point with other models). It also provides faster inference times, making it a strong candidate for production use.\n\nAdding support for this model would be highly valuable for users who need higher fidelity and speed in image editing tasks.\n\nRelated issues:\n\n- [#6066](https://github.com/BerriAI/litellm/issues/6066)\n- [#6149](https://github.com/BerriAI/litellm/issues/6149)\n\nThanks for considering this!\n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "mvrodrig",
      "author_type": "User",
      "created_at": "2025-06-04T12:37:22Z",
      "updated_at": "2025-06-04T12:37:22Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11401/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11401",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11401",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:56.784785",
      "comments": []
    },
    {
      "issue_number": 11229,
      "title": "[Bug]: Bedrock Deployed Models AI21 Jamba 1.5 are not working with",
      "body": "### What happened?\n\nBoth Bedrock AI21 Jamba 1.5 models are not working with LiteLLM: Models:\nv1.71.2 bedrock/ai21.jamba-1-5-mini-v1:0 and bedrock/ai21.jamba-1-5-large-v1:0\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.71.2\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "shagunb-acn",
      "author_type": "User",
      "created_at": "2025-05-29T08:37:54Z",
      "updated_at": "2025-06-04T11:47:45Z",
      "closed_at": "2025-06-04T11:47:12Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11229/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11229",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11229",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:56.784807",
      "comments": [
        {
          "author": "AnilAren",
          "body": "Yep this is a valid bug",
          "created_at": "2025-05-29T10:38:35Z"
        },
        {
          "author": "AnilAren",
          "body": "This Bug is closed as part of [Merged PR](https://github.com/BerriAI/litellm/pull/11233)\n\n@ishaan-jaff  please close this BUG",
          "created_at": "2025-06-03T09:00:30Z"
        },
        {
          "author": "shagunb-acn",
          "body": "Bug has been closed with version v1.72.0",
          "created_at": "2025-06-04T11:47:44Z"
        }
      ]
    },
    {
      "issue_number": 8918,
      "title": "[Bug]: Prisma package outdated and wolfi distribution unknown warning",
      "body": "### What happened?\n\nPrisma package outdated and wolfi distribution unknown warning. \n\n### Relevant log output\n\n```shell\nprisma:warn Prisma doesn't know which engines to download for the Linux distro \"wolfi\". Falling back to Prisma engines built \"debian\".\nPlease report your experience by creating an issue at https://github.com/prisma/prisma/issues so we can add your distro to the list of known supported distros.\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.61.20\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "subnet-dev",
      "author_type": "User",
      "created_at": "2025-03-01T17:03:35Z",
      "updated_at": "2025-06-04T11:03:23Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8918/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8918",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8918",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:56.957970",
      "comments": [
        {
          "author": "lipanpan-hub",
          "body": "```\nvagrant@u11:/opt/docker/litellm_V1_0$ docker logs -f litellm_v1_0-litellm-1\nprisma:warn Prisma doesn't know which engines to download for the Linux distro \"wolfi\". Falling back to Prisma engines built \"debian\".\nPlease report your experience by creating an issue at https://github.com/prisma/prism",
          "created_at": "2025-05-06T03:59:45Z"
        }
      ]
    },
    {
      "issue_number": 10388,
      "title": "[Bug]: No clear way to get openrouter usage correctly",
      "body": "### What happened?\n\nIn the current version, when using openrouter provider we are lacking the following parts:\n- option to get the generationId (this can be used to retrieve the request cost)\n- option to support {usage: true} [flag](https://openrouter.ai/docs/use-cases/usage-accounting)\n- option to support the accurate billing using litellm proxy\n\nThese changes are blocking from shipping openrouter as a general provider through litellm proxy.\n\nKey notes this was tested and verified on acompletion with stream set to True\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.67.4.post1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "matannahmani",
      "author_type": "User",
      "created_at": "2025-04-28T19:14:41Z",
      "updated_at": "2025-06-04T10:55:11Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10388/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10388",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10388",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:57.138449",
      "comments": [
        {
          "author": "mattius459",
          "body": "Did you ever get this solved or find a solution? We are in the same situation. The metadata that we receive by default through litellm is not correct and we would much rather get the object exactly as it comes from openrouter.",
          "created_at": "2025-06-04T10:55:10Z"
        }
      ]
    },
    {
      "issue_number": 11396,
      "title": "[Feature]: Filter null attributes",
      "body": "### The Feature\n\nHere is current Response sample:\n`{\n    \"id\": \"chatcmpl-565d891b-a42e-4c39-8d14-82a1f5208885\",\n    \"created\": 1734366691,\n    \"model\": \"claude-3-sonnet-20240229\",\n    \"object\": \"chat.completion\",\n    \"system_fingerprint\": null,\n    \"choices\": [\n        {\n            \"finish_reason\": \"stop\",\n            \"index\": 0,\n            \"message\": {\n                \"content\": \"Hello! As an AI language model, I don't have feelings, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\",\n                \"role\": \"assistant\",\n                \"tool_calls\": null,\n                \"function_call\": null\n            }\n        }\n    ],\n    \"usage\": {\n        \"completion_tokens\": 43,\n        \"prompt_tokens\": 13,\n        \"total_tokens\": 56,\n        \"completion_tokens_details\": null,\n        \"prompt_tokens_details\": {\n            \"audio_tokens\": null,\n            \"cached_tokens\": 0\n        },\n        \"cache_creation_input_tokens\": 0,\n        \"cache_read_input_tokens\": 0\n    }\n}`\n\nThere are multiple null attributes such as \"function_call\": null, can we have an option to skip such attributes?\n\n### Motivation, pitch\n\n\"function_call\": null may leads to some error\n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "langpingxue",
      "author_type": "User",
      "created_at": "2025-06-04T08:22:17Z",
      "updated_at": "2025-06-04T10:41:14Z",
      "closed_at": "2025-06-04T10:41:14Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11396/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11396",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11396",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:57.297106",
      "comments": []
    },
    {
      "issue_number": 11372,
      "title": "[Bug]: Structured output - $ref Resolution Failure in anyOf/items Structures",
      "body": "### What happened?\n\n## Summary\nLiteLLM's `unpack_defs()` function fails to properly resolve `$ref` references when they appear inside `items` within `anyOf` structures, resulting in empty `items: {}` objects being sent to the LLM instead of the actual referenced schema definitions.\n\n## Environment\n- **LiteLLM Version**: 1.53.8 (and likely other recent versions)\n- **Provider**: VertexAI (confirmed), likely affects other providers too\n- **Python Version**: 3.12\n- **Pydantic Version**: 2.x\n\n## Bug Description\nLiteLLM's `unpack_defs()` function in `litellm/utils.py` fails to properly resolve `$ref` references when they appear inside `items` within `anyOf` structures. This results in malformed schemas being sent to LLM providers, causing incorrect structured output.\n\n## Reproduction Case\n\n### 1. Pydantic Model Structure\n```python\nfrom pydantic import BaseModel\nfrom typing import List, Optional\n\nclass VatAmount(BaseModel):\n    vatRate: float\n    vatAmount: float\n\nclass ExpenseReceipt(BaseModel):\n    vatAmounts: Optional[List[VatAmount]] = None\n```\n\n### 2. Generated JSON Schema\nThe above models generate this JSON schema (simplified):\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"vatAmounts\": {\n      \"anyOf\": [\n        {\n          \"type\": \"array\",\n          \"items\": {\"$ref\": \"#/$defs/VatAmount\"}\n        },\n        {\"type\": \"null\"}\n      ],\n      \"title\": \"Vat Amounts\"\n    }\n  },\n  \"$defs\": {\n    \"VatAmount\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"vatRate\": {\"type\": \"number\"},\n        \"vatAmount\": {\"type\": \"number\"}\n      },\n      \"required\": [\"vatRate\", \"vatAmount\"],\n      \"title\": \"VatAmount\"\n    }\n  }\n}\n```\n\n### 3. Expected Behavior After LiteLLM Processing\nAfter `unpack_defs()` processes the schema, it should resolve the `$ref` to:\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"vatAmounts\": {\n      \"anyOf\": [\n        {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"vatRate\": {\"type\": \"number\"},\n              \"vatAmount\": {\"type\": \"number\"}\n            },\n            \"required\": [\"vatRate\", \"vatAmount\"],\n            \"title\": \"VatAmount\"\n          }\n        },\n        {\"type\": \"null\"}\n      ]\n    }\n  }\n}\n```\n\n### 4. Actual Behavior (Bug)\nLiteLLM's `unpack_defs()` function produces:\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"vatAmounts\": {\n      \"anyOf\": [\n        {\n          \"type\": \"array\",\n          \"items\": {}  // ‚ùå Empty object instead of resolved VatAmount schema\n        },\n        {\"type\": \"null\"}\n      ]\n    }\n  }\n}\n```\n\n## Root Cause Analysis\n\nThe `unpack_defs()` function in `litellm/utils.py` has recursive logic that handles simple `$ref` cases but fails when the reference appears in nested structures, specifically:\n- `$ref` inside `items` \n- `items` inside `anyOf`\n- The combination of both patterns\n\nThe function seems to process the `anyOf` array but doesn't properly traverse into the `items` objects within each `anyOf` element to resolve their `$ref` references.\n\n## Impact\n\n### Immediate Impact\n- **Malformed schemas sent to LLM providers** result in incorrect structured output\n- **Silent failures** - no error messages indicate the schema corruption\n- **Difficult to debug** - the issue only becomes apparent when examining LLM responses\n\n### Affected Use Cases\n- Optional lists of complex objects (`Optional[List[CustomModel]]`)\n- Union types containing arrays with references\n- Any Pydantic model using `$ref` within `anyOf`/`items` combinations\n\n### Severity\n**High** - This breaks structured output for common Pydantic patterns, affecting data extraction and API reliability.\n\n## Additional VertexAI Constraint\nVertexAI has an additional requirement that when `anyOf` is present in a schema object, it must be the only field (no additional `title`, `description`, etc. at the same level). This constraint should be handled by LiteLLM when targeting VertexAI models, but is a separate issue from the `$ref` resolution bug.\n\n## Reproduction Script\n```python\nimport json\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport litellm\nfrom litellm.utils import unpack_defs\n\nclass VatAmount(BaseModel):\n    vatRate: float\n    vatAmount: float\n\nclass TestModel(BaseModel):\n    vatAmounts: Optional[List[VatAmount]] = None\n\n# Generate schema\nschema = TestModel.model_json_schema()\nprint(\"Original schema:\")\nprint(json.dumps(schema, indent=2))\n\n# Process with LiteLLM's unpack_defs\nprocessed_schema = unpack_defs(schema)\nprint(\"\\nProcessed schema:\")\nprint(json.dumps(processed_schema, indent=2))\n\n# Check if items is empty (bug indicator)\nvat_amounts_items = processed_schema[\"properties\"][\"vatAmounts\"][\"anyOf\"][0][\"items\"]\nprint(f\"\\nBug present: {len(vat_amounts_items) == 0}\")\n```\n\n## Workaround\nUsers can work around this issue by manually resolving `$ref` references before passing the schema to LiteLLM:\n\n```python\ndef resolve_schema_refs(schema: dict) -> dict:\n    \"\"\"Manually resolve $refs to work around LiteLLM bug.\"\"\"\n    from copy import deepcopy\n    \n    resolved_schema = deepcopy(schema)\n    defs = resolved_schema.get('$defs', {})\n    \n    def resolve_refs_recursive(obj):\n        if isinstance(obj, dict):\n            # Handle direct $ref\n            if '$ref' in obj:\n                ref_key = obj['$ref']\n                if ref_key.startswith('#/$defs/'):\n                    ref_name = ref_key.split('$defs/')[-1]\n                    if ref_name in defs:\n                        return resolve_refs_recursive(defs[ref_name])\n            \n            # Handle anyOf items recursively\n            if 'anyOf' in obj:\n                for i, any_of_item in enumerate(obj['anyOf']):\n                    obj['anyOf'][i] = resolve_refs_recursive(any_of_item)\n            \n            # Handle items\n            if 'items' in obj:\n                obj['items'] = resolve_refs_recursive(obj['items'])\n            \n            # Handle properties\n            if 'properties' in obj:\n                for prop_name, prop_value in obj['properties'].items():\n                    obj['properties'][prop_name] = resolve_refs_recursive(prop_value)\n                    \n        elif isinstance(obj, list):\n            for i, item in enumerate(obj):\n                obj[i] = resolve_refs_recursive(item)\n                \n        return obj\n    \n    resolve_refs_recursive(resolved_schema)\n    \n    # Remove $defs since all refs are now resolved\n    if '$defs' in resolved_schema:\n        del resolved_schema['$defs']\n        \n    return resolved_schema\n\n# Usage:\nschema = MyModel.model_json_schema()\nresolved_schema = resolve_schema_refs(schema)\n# Now use resolved_schema with LiteLLM\n```\n\n## Suggested Fix\nThe `unpack_defs()` function in `litellm/utils.py` needs to be updated to:\n\n1. **Properly traverse nested structures** - ensure recursion goes into `items` within `anyOf`\n2. **Handle all `$ref` patterns** - not just simple direct references\n3. **Add comprehensive tests** for various `$ref` + `anyOf` + `items` combinations\n4. **Improve error handling** - provide clear messages when `$ref` resolution fails\n\n## Test Cases Needed\n```python\n# Test cases that should pass after fixing:\ntest_cases = [\n    \"Optional[List[CustomModel]]\",           # anyOf with items $ref\n    \"Union[List[ModelA], ModelB]\",           # anyOf with multiple item types\n    \"List[Union[ModelA, ModelB]]\",           # items with anyOf\n    \"Optional[Dict[str, CustomModel]]\",      # anyOf with additionalProperties $ref\n]\n```\n\nThis bug significantly impacts the reliability of structured output for real-world applications using complex Pydantic models.\n\nThanks to Claude Sonnet 4 for its active help in diagnosing the problem and writing most of this bug report that I would not have time to do otherwise.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.53.8\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "maximehardy",
      "author_type": "User",
      "created_at": "2025-06-03T18:01:23Z",
      "updated_at": "2025-06-04T08:41:57Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11372/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11372",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11372",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:57.297127",
      "comments": []
    },
    {
      "issue_number": 11392,
      "title": "[Bug]: AzureException BadRequestError ‚Äì Invalid 'messages[2].tool_calls[0].id' (length 41) when switching from Gemini‚ÜíAzure",
      "body": "### What happened?\n\nA bug happened!\n##  Bug Report\n\n### Description\nWhen using ChatLiteLLMRouter in  Langgraph to first generate a tool call via Gemini 2.5 Pro and then switching to Azure OpenAI for the final response, Azure returns:\nAzureException BadRequestError ‚Äì Invalid 'messages[2].tool_calls[0].id': string too long. Expected a string with maximum length 40, but got a string with length 41 instead.\n\n\nIn other words, the tool-call `id` that Gemini produced ended up being 41 characters long after the prefix \"call_\" was appended with the uuid, which exceeds Azure‚Äôs 40-character limit.\n\n### ‚ñ∂Ô∏è Steps to Reproduce\n1. Configure ChatLiteLLMRouter so that the Router has two models(gemini-pro-2.5 and Azure Open AI) in the same model group and set routing strategy to simple-shuffle:\n  (your code may look something like:  \n     ```python\n     from litellm import Router\n     from langchain_litellm import ChatLiteLLMRouter\n     ```\n2. Have Gemini generate a response that includes a tool call. In my case, that tool call's ID ended up as 41 characters.\n3. Then switch the same conversation over to Azure (e.g., using `AzureOpenAI(...)`) to get the ‚Äúfinal‚Äù answer or you could even pass that conversation to Azure.\n4. Azure rejects the request with exactly:\nBadRequestError ‚Äì Invalid 'messages[2].tool_calls[0].id': string too long. Expected a string with maximum length 40, but got a string with length 41 instead.\n\n\n###  Root Cause in Code\nThe `tool_call.id` is being set in this file for gemini:\n\n> **File**: [`litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py`](https://github.com/BerriAI/litellm/blob/main/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py#L785) \n\n```python\nid = f\"call_{str(uuid.uuid4())}\",  # Generates a 41-char string like 'call_84c85bfc-c197-...'\n```\n\n\n### üìã Environment\n- **Litellm version**: `1.71.1` \n- **Python version**: `3.12`\n- **OS/Platform**: macOS 15.5\n- **Models in use**:\n- Gemini 2.5 Pro \n- Azure OpenAI/o3   \n\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.71.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "ZPujan",
      "author_type": "User",
      "created_at": "2025-06-04T07:01:44Z",
      "updated_at": "2025-06-04T07:01:44Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11392/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11392",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11392",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:57.297135",
      "comments": []
    },
    {
      "issue_number": 11391,
      "title": "[Bug]: Proxy with ollama cannot run!!!",
      "body": "### What happened?\n\nollama is running and proxy is running:\n\nollama ps\nNAME                 ID              SIZE      PROCESSOR    UNTIL\ngranite3.3:latest    fd429f23b909    10 GB     100% GPU     10 hours from now\ndeepseek-r1:8b       6995872bfe4c    9.6 GB    100% GPU     9 hours from now\n\n\nlitellm --model \"ollama/granite3.3\" --api_base \"http://localhost:11434\" \n\nand run this code: \npython litellmTest.py\n\nimport openai # openai v1.0.0+\nclient = openai.OpenAI(api_key=\"anything\",base_url=\"http://localhost:4000\") # set proxy to base_url\n# request sent to model set on litellm proxy, `litellm --model`\nresponse = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"anthing else...\"\n    }\n])\n\nprint(response)\n\n\n\nTraceback (most recent call last):\n  File \"litellmTest.py\", line 4, in <module>\n    response = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages = [\n  File \"/home/yangjundian/.local/lib/python3.8/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/yangjundian/.local/lib/python3.8/site-packages/openai/resources/chat/completions/completions.py\", line 925, in create\n    return self._post(\n  File \"/home/yangjundian/.local/lib/python3.8/site-packages/openai/_base_client.py\", line 1242, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/home/yangjundian/.local/lib/python3.8/site-packages/openai/_base_client.py\", line 1037, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.InternalServerError: Error code: 500 - {'error': {'message': \"'NoneType' object has no attribute 'acompletion'\", 'type': 'None', 'param': 'None', 'code': '500'}}\n\n\n\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nVersion: 1.72.0\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "64933988",
      "author_type": "User",
      "created_at": "2025-06-04T06:52:36Z",
      "updated_at": "2025-06-04T06:58:44Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11391/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11391",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11391",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:57.297141",
      "comments": []
    },
    {
      "issue_number": 10667,
      "title": "[Feature]: return cache token usage in gemini streaming response",
      "body": "### The Feature\n\nhttps://developers.googleblog.com/en/gemini-2-5-models-now-support-implicit-caching/\n\n### Motivation, pitch\n\nImplicit caching (which is basically automatic if I understand it correctly, if you use it) allows cost savings\n\nDisabling caching or enabling caching and especially cache tracking\n\n### Are you a ML Ops Team?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "Classic298",
      "author_type": "User",
      "created_at": "2025-05-08T17:15:31Z",
      "updated_at": "2025-06-04T04:36:51Z",
      "closed_at": "2025-06-04T04:36:51Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 23,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10667/reactions",
        "total_count": 4,
        "+1": 4,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10667",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10667",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:59.132159",
      "comments": []
    },
    {
      "issue_number": 11370,
      "title": "[Bug]: litellm.supports_function_calling doesn't work with proxy models",
      "body": "### What happened?\n\nWhenever we are using supports function calling method it doesn't work when using the proxy models.\nPassing **model_info** doesn't seem to work either\n\n```\n- model_name: bedrock-claude-3-sonnet\n      litellm_params:\n        model: bedrock/converse/anthropic.claude-3-sonnet-20240229-v1:0\n        aws_region_name: us-east-1\n        aws_session_name: mlp-litellm-session\n        guardrailConfig: {\n          \"guardrailIdentifier\": \"PLACEHOLDER_GR_ID\", # The identifier (ID) for the guardrail.\n          \"guardrailVersion\": \"PLACEHOLDER_GR_VR\",    # The version of the guardrail.\n          \"trace\": \"disabled\",                        # The trace behavior for the guardrail. Can either be \"disabled\" or \"enabled\"\n        }\n      model_info:\n        supports_vision: True\n        supports_function_calling\n```\n\n```\nimport litellm\n\nlitellm.supports_function_calling(\n    \"litellm_proxy/bedrock-claude-3-sonnet\",\n)\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.72\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "pazevedo-hyland",
      "author_type": "User",
      "created_at": "2025-06-03T16:05:25Z",
      "updated_at": "2025-06-04T00:41:29Z",
      "closed_at": null,
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11370/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11370",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11370",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:59.132181",
      "comments": [
        {
          "author": "pazevedo-hyland",
          "body": "PR Here to solve it https://github.com/BerriAI/litellm/pull/11381/files#",
          "created_at": "2025-06-04T00:41:28Z"
        }
      ]
    },
    {
      "issue_number": 11386,
      "title": "[Bug]: mistral-medium-2505 breaking under tool call",
      "body": "### What happened?\n\nWhen you use tools with mistral-medium-2505 it errors out. Documentation says it does support tool calling, I think the format its being called in just needs to be updated.\n\nhttps://docs.mistral.ai/agents/function_calling/\n\n### Relevant log output\n\n```shell\nBadRequestError: litellm.BadRequestError: MistralException - Unexpected role 'user' after role 'tool'\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.72.0\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "JarettForzano",
      "author_type": "User",
      "created_at": "2025-06-04T00:10:29Z",
      "updated_at": "2025-06-04T00:10:29Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11386/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11386",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11386",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:59.357464",
      "comments": []
    },
    {
      "issue_number": 3508,
      "title": "[Feature]: Fallback Prompts - model-specific ",
      "body": "### The Feature\n\nAllow user to specify model specific prompts if fallbacks are triggered for a certain requests \r\n\r\ne.g. \r\n```python\r\n{\r\n  \"metadata\": {\r\n    \"fallback\": {\r\n      \"claude-3-opus\": \"Opus-specific system message\",\r\n      \"claude-3-sonnet\": \"Sonnet-specific system message\"\r\n      ...\r\n     }\r\n  }\r\n}\r\n```\n\n### Motivation, pitch\n\nUser would rather fail requests currently, than use fallbacks and get back bad answers \n\n### Twitter / LinkedIn details\n\ncc: @dkindlund",
      "state": "closed",
      "author": "krrishdholakia",
      "author_type": "User",
      "created_at": "2024-05-07T20:14:20Z",
      "updated_at": "2025-06-04T00:02:06Z",
      "closed_at": "2025-06-04T00:02:06Z",
      "labels": [
        "enhancement",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/3508/reactions",
        "total_count": 3,
        "+1": 3,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/3508",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/3508",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:59.357485",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-01-28T03:00:42Z"
        },
        {
          "author": "jonas-lyrebird-health",
          "body": "Is there any update on this? Is there any guidance with model specific fallback prompts? Maybe it is possible with Langfuse integration? @krrishdholakia ",
          "created_at": "2025-02-25T02:50:24Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Hey @jonas-lyrebird-health this can now be done - https://docs.litellm.ai/docs/proxy/reliability#control-fallback-prompts",
          "created_at": "2025-02-25T03:08:13Z"
        },
        {
          "author": "krrishdholakia",
          "body": "> Maybe it is possible with Langfuse integration\n\nhow would this work? @jonas-lyrebird-health ",
          "created_at": "2025-02-25T03:08:32Z"
        },
        {
          "author": "krrishdholakia",
          "body": "i believe the langfuse prompt_id / prompt management parameters would work in here",
          "created_at": "2025-02-25T03:08:56Z"
        }
      ]
    },
    {
      "issue_number": 6247,
      "title": "[Bug]: Problems with output parsing in presidio guardrail",
      "body": "### What happened?\r\n\r\nI have set up the litellm proxy locally together with microsoft presidio. The input analyzation and anonymization works, however I face two issues in the replacement of masked tokens in the LLM response. I tried to investigate the issue. As an example I send this request:\r\n\r\n```\r\n{\r\n    \"model\": \"gpt-3.5-turbo\",\r\n    \"messages\": [\r\n        {\r\n            \"role\": \"user\",\r\n            \"content\": \"Hi, my name is Mike. What is my name?\"\r\n        }\r\n    ],\r\n    \"guardrails\": [\r\n        \"presidio-pre-guard\"\r\n    ],\r\n    \"guardrail_config\": {\r\n        \"language\": \"en\"\r\n    }\r\n}\r\n```\r\n\r\nI defined presidio in the config.yaml as follows:\r\n```\r\nguardrails:\r\n  - guardrail_name: \"presidio-pre-guard\"\r\n    litellm_params:\r\n      guardrail: presidio\r\n      mode: \"pre_call\"\r\n      output_parse_pii: true\r\n```\r\n\r\n**The replacement values are not correctly extracted**\r\nI added print statements to the log output below, which show that incorrect replacement values are extracted and stored in the self.pii_tokens variable: pii tokens collected: {'PERSON': 'Mike. Wh'}.\r\n\r\nMaybe the problem is here that that the response of the anonymizer call is used for extracting the replacement values. Items start and end information of the anonymization responds refer to the start and endposition of the anonymization in the text, e.g. <Person>. Instead the start and end values of the analyze call refer to the start and endposition of the pii in the text, e.g. Mike, so maybe this can be used?\r\n\r\n**The extracted pii tokens in the async_pre_call_hook are not available anymore in the async_post_call_success_hook**\r\nThe logs below show that the pii tokens are empty in the async_pre_call_hook. As a result, only the actual response of the llm is returned, which contain the masked tokens: \"Your name is PERSON.\" Maybe the problem is here that in init_guardrails.py two callbacks with different _OPTIONAL_PresidioPIIMasking objects are created for the anonymization and the output parsing?\r\n\r\nThanks in advance for any help :)\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n...\r\n\r\n22:31:16 - LiteLLM Proxy:DEBUG: presidio.py:159 - Making request to: http://localhost:5002/analyze with payload: {'text': 'Hi, my name is Mike. What is my name?', 'language': 'en'}\r\n\r\n#########\r\nAnalyze result: [{'analysis_explanation': None, 'end': 19, 'entity_type': 'PERSON', 'recognition_metadata': {'recognizer_identifier': 'SpacyRecognizer_139711657644240', 'recognizer_name': 'SpacyRecognizer'}, 'score': 0.85, 'start': 15}]  \r\n#########\r\n\r\n22:31:16 - LiteLLM Proxy:DEBUG: presidio.py:176 - Making request to: http://localhost:5001/anonymize\r\n22:31:16 - LiteLLM Proxy:DEBUG: presidio.py:189 - redacted_text: {'text': 'Hi, my name is <PERSON>. What is my name?', 'items': [{'start': 15, 'end': 23, 'entity_type': 'PERSON', 'text': '<PERSON>', 'operator': 'replace'}]}\r\n22:31:16 - LiteLLM Proxy:INFO: presidio.py:264 - Presidio PII Masking: Redacted pii message: [{'role': 'user', 'content': 'Hi, my name is <PERSON>. What is my name?'}]\r\n\r\n#########\r\npii tokens collected: {'<PERSON>': 'Mike. Wh'}\r\n#########\r\n\r\n...\r\n\r\n22:31:17 - LiteLLM:DEBUG: custom_guardrail.py:23 - inside should_run_guardrail for guardrail=presidio-pre-guard event_type= GuardrailEventHooks.post_call guardrail_supported_event_hooks= post_call requested_guardrails= ['presidio-pre-guard']\r\n22:31:17 - LiteLLM Proxy:DEBUG: presidio.py:331 - PII Masking Args: self.output_parse_pii=True; type of response=<class 'litellm.types.utils.ModelResponse'>\r\n\r\n#########\r\nAvailable pii tokens: {}\r\n#########\r\n\r\n22:31:17 - LiteLLM Proxy:DEBUG: presidio.py:347 - self.pii_tokens: {}; initial response: Your name is <PERSON>.        \r\n22:31:17 - LiteLLM:DEBUG: custom_guardrail.py:23 - inside should_run_guardrail for guardrail=presidio-pre-guard event_type= GuardrailEventHooks.post_call guardrail_supported_event_hooks= pre_call requested_guardrails= ['presidio-pre-guard']\r\n\r\n...\r\n```\r\n\r\n\r\n### Twitter / LinkedIn details\r\n\r\n_No response_",
      "state": "closed",
      "author": "taimans-git",
      "author_type": "User",
      "created_at": "2024-10-15T21:00:27Z",
      "updated_at": "2025-06-04T00:02:05Z",
      "closed_at": "2025-06-04T00:02:05Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/6247/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "krrishdholakia"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/6247",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/6247",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:59.628864",
      "comments": [
        {
          "author": "pilganchuk",
          "body": "Had same issue, any solution?",
          "created_at": "2024-12-30T15:33:35Z"
        },
        {
          "author": "krrishdholakia",
          "body": "missed this. will pick up this issue today ",
          "created_at": "2024-12-30T15:49:05Z"
        },
        {
          "author": "szymon-klimowicz-DS",
          "body": "Hello, any updates in regards to this issue? Many thanks",
          "created_at": "2025-02-26T11:39:40Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Hi @szymon-klimowicz-DS haven't had a chance to work on this yet. A PR here is welcome! ",
          "created_at": "2025-02-26T15:24:03Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-05-28T00:02:03Z"
        }
      ]
    },
    {
      "issue_number": 8820,
      "title": "docker-0.15.17-deepseek-olllam+mxbai-embed-large",
      "body": "Can Docker start Wren version 0.15.17, and use the ollama+mxbai-embed-large vector model? \nIf supported, how should the config.yaml configuration file be written correctly?\nIf Docker starts, how to enable litellm._turn_on_debug()?\nthinks",
      "state": "closed",
      "author": "synjonestj",
      "author_type": "User",
      "created_at": "2025-02-26T02:49:01Z",
      "updated_at": "2025-06-04T00:01:56Z",
      "closed_at": "2025-06-04T00:01:56Z",
      "labels": [
        "stale"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8820/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8820",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8820",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:31:59.905183",
      "comments": [
        {
          "author": "synjonestj",
          "body": "![Image](https://github.com/user-attachments/assets/e94f395a-00a1-438a-8bcc-7bc0b46c217f)\n\n![Image](https://github.com/user-attachments/assets/74a43395-9c4d-486f-a00b-d61e791b5c1f)",
          "created_at": "2025-02-26T03:19:33Z"
        },
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-05-28T00:01:48Z"
        }
      ]
    },
    {
      "issue_number": 8821,
      "title": "[Bug]: chat/completions Incorrect error message format when Invalid model name",
      "body": "### What happened?\n\nAccording to the documentation, message should be of type str, but a dictionary was received.\n\n### Relevant log output\n\n```json\n{\n    \"error\": {\n        \"message\": {\n            \"error\": \"/chat/completions: Invalid model name passed in model=DeepSeek-R1x. Call `/v1/models` to view available models for your key.\"**\n        },\n        \"type\": \"None\",\n        \"param\": \"None\",\n        \"code\": \"400\"\n    }\n}\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.61.1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "whozwhat",
      "author_type": "User",
      "created_at": "2025-02-26T03:40:54Z",
      "updated_at": "2025-06-04T00:01:55Z",
      "closed_at": "2025-06-04T00:01:55Z",
      "labels": [
        "bug",
        "stale"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8821/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8821",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8821",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:32:00.169687",
      "comments": [
        {
          "author": "github-actions[bot]",
          "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.",
          "created_at": "2025-05-28T00:01:46Z"
        }
      ]
    },
    {
      "issue_number": 8961,
      "title": "[Bug]: Pass \"thinking\" content back to Anthropic via Message in function calling mode",
      "body": "### What happened?\n\nAccording to Anthropic documentation, we are required to pass \"thinking\" back to `message.[].content` to generate the NEXT action when in function calling model. However, this doesn't seem to work with the latest version of litellm.\n\n<img width=\"737\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e7957ee1-7172-435c-a093-8f628dfa62ed\" />\n\nSee repro script below\n\n### Relevant log output\n\n```shell\n# Repro script:\n\nkwargs = {\n  \"messages\": [\n    {\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"You are OpenHands agent, a helpful AI assistant that can interact with a computer to solve tasks.\\n<IMPORTANT>\\n* If user provides a path, you should NOT assume it's relative to the current working directory. Instead, you should explore the file system to find the file before working on it.\\n* When configuring git credentials, use \\\"openhands\\\" as the user.name and \\\"openhands@all-hands.dev\\\" as the user.email by default, unless explicitly instructed otherwise.\\n* You MUST NOT include comments in the code unless they are necessary to describe non-obvious behavior.\\n* If the user asks you to edit a file, you should edit the file directly, do NOT create a new file with the updated content unless the user explicitly instructs you to do so.\\n* When you are doing global search-and-replace, consider using `sed` instead of running file editor multiple times.\\n* Only use GITHUB_TOKEN and other credentials in ways that the user has asked for and would expect. Do NOT make potentially dangerous changes (e.g. pushing to main, deleting a repository) unless explicitly asked to do so.\\n* Use APIs to work with GitHub or other platforms, unless the user asks otherwise or your task requires browsing.\\n* If you've made repeated attempts to solve a problem, but the tests won't pass or the user says it's still broken, reflect on 5-7 different possible sources of the problem. Assess the likelihood of these options, and proceed with fixing the most likely one.\\n</IMPORTANT>\",\n          \"cache_control\": {\n            \"type\": \"ephemeral\"\n          }\n        }\n      ],\n      \"role\": \"system\"\n    },\n    {\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"<uploaded_files>\\n/workspace/scikit-learn__scikit-learn__0.21\\n</uploaded_files>\\nI've uploaded a python code repository in the directory scikit-learn__scikit-learn__0.21. Consider the following issue description:\\n\\n<issue_description>\\nPipeline should implement __len__\\n#### Description\\r\\n\\r\\nWith the new indexing support `pipe[:len(pipe)]` raises an error.\\r\\n\\r\\n#### Steps/Code to Reproduce\\r\\n\\r\\n\\r\\nfrom sklearn import svm\\r\\nfrom sklearn.datasets import samples_generator\\r\\nfrom sklearn.feature_selection import SelectKBest\\r\\nfrom sklearn.feature_selection import f_regression\\r\\nfrom sklearn.pipeline import Pipeline\\r\\n\\r\\n# generate some data to play with\\r\\nX, y = samples_generator.make_classification(\\r\\n    n_informative=5, n_redundant=0, random_state=42)\\r\\n\\r\\nanova_filter = SelectKBest(f_regression, k=5)\\r\\nclf = svm.SVC(kernel='linear')\\r\\npipe = Pipeline([('anova', anova_filter), ('svc', clf)])\\r\\n\\r\\nlen(pipe)\\r\\n\\r\\n\\r\\n#### Versions\\r\\n\\r\\n\\r\\nSystem:\\r\\n    python: 3.6.7 | packaged by conda-forge | (default, Feb 19 2019, 18:37:23)  [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\\r\\nexecutable: /Users/krisz/.conda/envs/arrow36/bin/python\\r\\n   machine: Darwin-18.2.0-x86_64-i386-64bit\\r\\n\\r\\nBLAS:\\r\\n    macros: HAVE_CBLAS=None\\r\\n  lib_dirs: /Users/krisz/.conda/envs/arrow36/lib\\r\\ncblas_libs: openblas, openblas\\r\\n\\r\\nPython deps:\\r\\n       pip: 19.0.3\\r\\nsetuptools: 40.8.0\\r\\n   sklearn: 0.21.dev0\\r\\n     numpy: 1.16.2\\r\\n     scipy: 1.2.1\\r\\n    Cython: 0.29.6\\r\\n    pandas: 0.24.1\\r\\n\\n\\n</issue_description>\\n\\nCan you help me implement the necessary changes to the repository so that the requirements specified in the <issue_description> are met?\\nI've already taken care of all changes to any of the test files described in the <issue_description>. This means you DON'T have to modify the testing logic or any of the tests in any way!\\nAlso the development Python environment is already set up for you (i.e., all dependencies already installed), so you don't need to install other packages.\\nYour task is to make the minimal changes to non-test files in the /workspace directory to ensure the <issue_description> is satisfied.\\nFollow these steps to resolve the issue:\\n1. As a first step, it might be a good idea to explore the repo to familiarize yourself with its structure.\\n2. Create a script to reproduce the error and execute it with `python <filename.py>` using the BashTool, to confirm the error\\n3. Edit the sourcecode of the repo to resolve the issue\\n4. Rerun your reproduce script and confirm that the error is fixed!\\n5. Think about edgecases, add comprehensive tests for them in your reproduce script, and run them to make sure your fix handles them as well\\n6. Once you are done with the initial implementation, please carefully re-read the problem description and check the difference between the current code and the base commit a62775e99f2a5ea3d51db7160fad783f6cd8a4c5. Do you think that the issue has been completely and comprehensively solved? Write tests to check the correctness of the solution, specifically focusing on tests that may point out any remaining problems that are not yet solved. Run all of the tests in the repo and check if any of them fail, and if they do fix the code. Repeat this process of carefully reading the problem description and current implementation, testing, and fixing any problems until you are confident that the current implementation is correct. Find and run any tests in the repo that are related to:\\n   - The issue you are fixing\\n   - The files you modified\\n   - The functions you changed\\n   Make sure all these tests pass with your changes.\\nYour thinking should be thorough and so it's fine if it's very long.\\n\"\n        }\n      ],\n      \"role\": \"user\"\n    },\n    {\n      \"content\": [\n        {\n          \"type\": \"thinking\",\n          \"thinking\": \"Alright, let's get started with resolving this issue about implementing `__len__` for the Pipeline class in scikit-learn.\\n\\n### Step 1: Explore the repository structure to understand it better\\n\\nLet's first explore the repository structure to get a better understanding of the codebase:\\n\\n\\nls -la /workspace/scikit-learn__scikit-learn__0.21\\n\\n\\nThis will give us an overview of the top-level directories and files. From there, we can delve deeper into relevant parts of the code.\\n\\nLet's look at the main organization of the repository first, then we'll locate the Pipeline class, understand how it's currently implemented, and then add the `__len__` method to it.\\n\\nNext, let's look at the pipeline module, as the issue mentions `pipe[:len(pipe)]`, indicating that there is indexing support for Pipeline, but it lacks a `__len__` method.\\n\\nSince the issue mentions that Pipeline supports indexing but not `__len__`, I'll check the Pipeline class to understand its implementation.\\n\\nThe Pipeline class is likely to be in sklearn/pipeline.py or within a pipeline directory. Let's look for it:\\n\\n\\nfind /workspace/scikit-learn__scikit-learn__0.21 -name \\\"*pipeline*\\\" -type f | grep -v __pycache__ | grep -v \\\"\\\\.pyc$\\\"\\n\\n\\nThis will help us locate the file containing the Pipeline class.\\n\\nOnce we've located the file, we'll examine it to understand:\\n1. How indexing is currently implemented\\n2. Where we need to add the `__len__` method\\n3. How the length of a pipeline should be determined\\n\\nThen, we'll implement the `__len__` method, test our changes to ensure they fix the issue, and verify that our changes don't break existing functionality.\\n\\nLet's begin by exploring the repository.\\n\\n### Step 2: Reproduce the error\\n\\nAfter exploring the repository, I'll create a script to reproduce the error described in the issue. This script will use the code provided in the \\\"Steps/Code to Reproduce\\\" section of the issue description.\\n\\nThen I'll run the script to confirm the error occurs as described, which will help us understand what needs to be fixed.\\n\\n### Step 3: Fix the issue\\n\\nOnce I've understood the problem and located the relevant files, I'll modify the code to add the `__len__` method to the Pipeline class. This method should return the number of steps in the pipeline, which is consistent with what a user would expect when calling `len(pipe)`.\\n\\n### Step 4: Test the fix\\n\\nAfter implementing the fix, I'll rerun the reproduction script to verify that the error is resolved and `len(pipe)` works correctly.\\n\\n### Step 5: Consider edge cases\\n\\nI'll think about potential edge cases like:\\n- Empty pipelines\\n- Very long pipelines\\n- Any other special cases where the `__len__` implementation might behave unexpectedly\\n\\nI'll add tests for these cases to ensure our implementation handles them correctly.\\n\\n### Step 6: Comprehensive testing\\n\\nI'll run the existing tests to ensure our changes don't break any existing functionality, and I'll also write additional tests to specifically test our new `__len__` method.\\n\\nLet's start with exploring the repository.\\n\\nLet's start by exploring the repository structure:\\n\\n\\nls -la /workspace/scikit-learn__scikit-learn__0.21\\n\\n\\nNow, let's find the Pipeline class:\\n\\n\\nfind /workspace/scikit-learn__scikit-learn__0.21 -name \\\"*pipeline*\\\" -type f | grep -v __pycache__ | grep -v \\\"\\\\.pyc$\\\"\\n\\n\\nBased on the repository structure, let's look at the pipeline implementation:\\n\\n\\ncat /workspace/scikit-learn__scikit-learn__0.21/sklearn/pipeline.py\\n\\n\\nLet's check if there are tests for the Pipeline class:\\n\\n\\nfind /workspace/scikit-learn__scikit-learn__0.21 -name \\\"*pipeline*test*\\\" -type f | grep -v __pycache__ | grep -v \\\"\\\\.pyc$\\\"\\n\\n\\nNow, let's create a script to reproduce the error. I'll create a file in the workspace directory and add the code from the issue description:\\n\\n\\ncat > /workspace/reproduce_pipeline_len_error.py << 'EOF'\\nfrom sklearn import svm\\nfrom sklearn.datasets import samples_generator\\nfrom sklearn.feature_selection import SelectKBest\\nfrom sklearn.feature_selection import f_regression\\nfrom sklearn.pipeline import Pipeline\\n\\n# generate some data to play with\\nX, y = samples_generator.make_classification(\\n    n_informative=5, n_redundant=0, random_state=42)\\n\\nanova_filter = SelectKBest(f_regression, k=5)\\nclf = svm.SVC(kernel='linear')\\npipe = Pipeline([('anova', anova_filter), ('svc', clf)])\\n\\nprint(\\\"Trying to get length of pipeline...\\\")\\ntry:\\n    length = len(pipe)\\n    print(f\\\"Length of pipeline: {length}\\\")\\nexcept Exception as e:\\n    print(f\\\"Error: {type(e).__name__}: {e}\\\")\\nEOF\\n\\n\\nLet's run this to confirm the error:\\n\\n\\npython /workspace/reproduce_pipeline_len_error.py\\n\\n\\nNow, let's implement the `__len__` method in the Pipeline class. First, I'll examine the Pipeline class to understand how it's structured and where to add the `__len__` method.\\n\\nFrom the error we saw above and looking at the pipeline.py file, we should add a `__len__` method to the Pipeline class.\\n\\nLet's modify the pipeline.py file:\\n\\n\\ncp /workspace/scikit-learn__scikit-learn__0.21/sklearn/pipeline.py /workspace/pipeline.py.backup\\n\\n\\nNow, let's edit the file to add the `__len__` method:\\n\\n\\nsed -n -e '/class Pipeline/,/^class /p' /workspace/scikit-learn__scikit-learn__0.21/sklearn/pipeline.py\\n\\n\\nThis will show us the Pipeline class definition so we can understand where to add the `__len__` method.\\n\\nBased on what we see, I'll add the `__len__` method to the Pipeline class:\\n\\n\\nsed -i -e '/def __getitem__.*:/a\\\\\\n    def __len__(self):\\\\\\n        \\\"\\\"\\\"Returns the length of the Pipeline.\\\\\\n\\\\\\n        Returns\\\\\\n        -------\\\\\\n        length : int\\\\\\n            The number of steps in the pipeline.\\\\\\n        \\\"\\\"\\\"\\\\\\n        return len(self.steps)\\n' /workspace/scikit-learn__scikit-learn__0.21/sklearn/pipeline.py\\n\\n\\nLet's verify that the changes were applied correctly:\\n\\n\\ngrep -A 10 \\\"__len__\\\" /workspace/scikit-learn__scikit-learn__0.21/sklearn/pipeline.py\\n\\n\\nNow, let's run our script again to see if the error is fixed:\\n\\n\\npython /workspace/reproduce_pipeline_len_error.py\\n\\n\\nIf the error is fixed, let's test some edge cases by modifying our script:\\n\\n\\ncat > /workspace/test_pipeline_len.py << 'EOF'\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn import svm\\nfrom sklearn.feature_selection import SelectKBest, f_regression\\nfrom sklearn.datasets import samples_generator\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Test case 1: Normal pipeline\\nanova_filter = SelectKBest(f_regression, k=5)\\nclf = svm.SVC(kernel='linear')\\npipe = Pipeline([('anova', anova_filter), ('svc', clf)])\\nprint(f\\\"Length of normal pipeline: {len(pipe)}\\\")\\n\\n# Test case 2: Empty pipeline\\nempty_pipe = Pipeline([])\\nprint(f\\\"Length of empty pipeline: {len(empty_pipe)}\\\")\\n\\n# Test case 3: Long pipeline\\nX, y = samples_generator.make_classification(\\n    n_informative=5, n_redundant=0, random_state=42)\\n\\nsteps = []\\nfor i in range(10):\\n    steps.append((f'scaler_{i}', StandardScaler()))\\nlong_pipe = Pipeline(steps)\\nprint(f\\\"Length of long pipeline: {len(long_pipe)}\\\")\\n\\n# Test case 4: Slicing with len\\npipe = Pipeline([('anova', anova_filter), ('svc', clf)])\\nsliced_pipe = pipe[:len(pipe)]\\nprint(f\\\"Original pipe steps: {[name for name, _ in pipe.steps]}\\\")\\nprint(f\\\"Sliced pipe steps: {[name for name, _ in sliced_pipe.steps]}\\\")\\nEOF\\n\\n\\nLet's run this test script:\\n\\n\\npython /workspace/test_pipeline_len.py\\n\\n\\nLet's also run the tests for the pipeline module to make sure we haven't broken anything:\\n\\n\\ncd /workspace/scikit-learn__scikit-learn__0.21\\npython -m pytest sklearn/tests/test_pipeline.py -v\\n\\n\\nAfter testing the changes, let's update our script once more to test the edge case where we use `len(pipe)` in slicing:\\n\\n\\ncat >> /workspace/test_pipeline_len.py << 'EOF'\\n\\n# Test case 5: Various slicing operations with len\\npipe = Pipeline([\\n    ('step1', StandardScaler()),\\n    ('step2', SelectKBest(f_regression, k=5)),\\n    ('step3', svm.SVC(kernel='linear'))\\n])\\nprint(f\\\"\\\\nOriginal pipe length: {len(pipe)}\\\")\\n\\n# Slice from beginning to end\\nslice1 = pipe[:len(pipe)]\\nprint(f\\\"pipe[:len(pipe)] steps: {[name for name, _ in slice1.steps]}\\\")\\n\\n# Slice from beginning to middle\\nslice2 = pipe[:len(pipe)//2]\\nprint(f\\\"pipe[:len(pipe)//2] steps: {[name for name, _ in slice2.steps]}\\\")\\n\\n# Slice from middle to end\\nslice3 = pipe[len(pipe)//2:]\\nprint(f\\\"pipe[len(pipe)//2:] steps: {[name for name, _ in slice3.steps]}\\\")\\n\\n# Slice a subset in the middle\\nif len(pipe) >= 3:\\n    slice4 = pipe[1:len(pipe)-1]\\n    print(f\\\"pipe[1:len(pipe)-1] steps: {[name for name, _ in slice4.steps]}\\\")\\nEOF\\n\\n\\nLet's run this updated script:\\n\\n\\npython /workspace/test_pipeline_len.py\\n\\n\\nLet's also check all the tests in the sklearn directory to ensure our changes don't break anything:\\n\\n\\ncd /workspace/scikit-learn__scikit-learn__0.21\\npython -m pytest -xvs sklearn/tests/test_pipeline.py\\n\\n\\nBefore we conclude, let's verify that our changes to scikit-learn match the expected functionality:\\n\\n1. We have implemented the `__len__` method in the Pipeline class\\n2. The method returns the number of steps in the pipeline\\n3. We have tested that it works for normal, empty, and long pipelines\\n4. We have also tested that slicing operations using `len(pipe)` work correctly\\n\\nSince the issue description mentioned that the repository already supports indexing but not `__len__`, we didn't have to implement indexing. We only needed to add the `__len__` method to make `len(pipe)` work, which we've done.\\n\\nOur changes are minimal and focused on solving the specific issue described.\\n\\nFurthermore, to ensure we've done a thorough job, let's run all sklearn tests to make sure we haven't broken anything:\\n\\n\\ncd /workspace/scikit-learn__scikit-learn__0.21\\npython -m pytest sklearn/tests/\\n\\n\\nLet's check the full implementation of the Pipeline to understand where `__len__` fits in compared to other methods like `__getitem__`:\\n\\n\\nhead -n 300 /workspace/scikit-learn__scikit-learn__0.21/sklearn/pipeline.py\\n\\n\\nLet's more specifically check the surrounding methods to ensure our addition fits well with the rest of the class:\\n\\n\\ngrep -A 20 \\\"__getitem__\\\" /workspace/scikit-learn__scikit-learn__0.21/sklearn/pipeline.py\\n\\n\\nNow, let's also test our changes against the base commit a62775e99f2a5ea3d51db7160fad783f6cd8a4c5, as mentioned in the requirements:\\n\\n\\ncd /workspace/scikit-learn__scikit-learn__0.21\\ngit status\\n\\n\\nHmm, we don't have access to the git repository directly. Let's focus on thoroughly testing our changes then.\\n\\nLet's run the full test suite for the sklearn module to make sure our changes don't break any existing functionality:\\n\\n\\ncd /workspace/scikit-learn__scikit-learn__0.21\\npython -m pytest -xvs sklearn/tests/test_pipeline.py\\n\\n\\nGreat, now let's create a more comprehensive test script to ensure our `__len__` implementation works in various scenarios:\\n\\n\\ncat > /workspace/comprehensive_pipeline_len_test.py << 'EOF'\\n\\\"\\\"\\\"\\nComprehensive tests for the __len__ method of Pipeline.\\n\\\"\\\"\\\"\\nfrom sklearn.pipeline import Pipeline, FeatureUnion\\nfrom sklearn import svm\\nfrom sklearn.feature_selection import SelectKBest, f_regression\\nfrom sklearn.datasets import samples_generator\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.decomposition import PCA\\n\\n# Test case 1: Normal pipeline\\nprint(\\\"Test case 1: Normal pipeline\\\")\\nanova_filter = SelectKBest(f_regression, k=5)\\nclf = svm.SVC(kernel='linear')\\npipe = Pipeline([('anova', anova_filter), ('svc', clf)])\\nprint(f\\\"Length of normal pipeline: {len(pipe)}\\\")\\nassert len(pipe) == 2, \\\"Length of normal pipeline should be 2\\\"\\n\\n# Test case 2: Empty pipeline\\nprint(\\\"\\\\nTest case 2: Empty pipeline\\\")\\nempty_pipe = Pipeline([])\\nprint(f\\\"Length of empty pipeline: {len(empty_pipe)}\\\")\\nassert len(empty_pipe) == 0, \\\"Length of empty pipeline should be 0\\\"\\n\\n# Test case 3: Long pipeline\\nprint(\\\"\\\\nTest case 3: Long pipeline\\\")\\nX, y = samples_generator.make_classification(\\n    n_informative=5, n_redundant=0, random_state=42)\\n\\nsteps = []\\nfor i in range(10):\\n    steps.append((f'scaler_{i}', StandardScaler()))\\nlong_pipe = Pipeline(steps)\\nprint(f\\\"Length of long pipeline: {len(long_pipe)}\\\")\\nassert len(long_pipe) == 10, \\\"Length of long pipeline should be 10\\\"\\n\\n# Test case 4: Pipeline with FeatureUnion\\nprint(\\\"\\\\nTest case 4: Pipeline with FeatureUnion\\\")\\nunion = FeatureUnion([\\n    ('pca', PCA(n_components=1)),\\n    ('select', SelectKBest(k=1))\\n])\\npipe_with_union = Pipeline([\\n    ('scaler', StandardScaler()),\\n    ('union', union),\\n    ('svc', svm.SVC(kernel='linear'))\\n])\\nprint(f\\\"Length of pipeline with FeatureUnion: {len(pipe_with_union)}\\\")\\nassert len(pipe_with_union) == 3, \\\"Length of pipeline with FeatureUnion should be 3\\\"\\n\\n# Test case 5: Various slicing operations with len\\nprint(\\\"\\\\nTest case 5: Various slicing operations with len\\\")\\npipe = Pipeline([\\n    ('step1', StandardScaler()),\\n    ('step2', SelectKBest(f_regression, k=5)),\\n    ('step3', svm.SVC(kernel='linear'))\\n])\\nprint(f\\\"Original pipe length: {len(pipe)}\\\")\\nassert len(pipe) == 3, \\\"Original pipe length should be 3\\\"\\n\\n# Slice from beginning to end\\nslice1 = pipe[:len(pipe)]\\nprint(f\\\"pipe[:len(pipe)] steps: {[name for name, _ in slice1.steps]}\\\")\\nassert len(slice1) == 3, \\\"Length of pipe[:len(pipe)] should be 3\\\"\\n\\n# Slice from beginning to middle\\nslice2 = pipe[:len(pipe)//2]\\nprint(f\\\"pipe[:len(pipe)//2] steps: {[name for name, _ in slice2.steps]}\\\")\\nassert len(slice2) == 1, \\\"Length of pipe[:len(pipe)//2] should be 1\\\"\\n\\n# Slice from middle to end\\nslice3 = pipe[len(pipe)//2:]\\nprint(f\\\"pipe[len(pipe)//2:] steps: {[name for name, _ in slice3.steps]}\\\")\\nassert len(slice3) == 2, \\\"Length of pipe[len(pipe)//2:] should be 2\\\"\\n\\n# Slice a subset in the middle\\nslice4 = pipe[1:len(pipe)-1]\\nprint(f\\\"pipe[1:len(pipe)-1] steps: {[name for name, _ in slice4.steps]}\\\")\\nassert len(slice4) == 1, \\\"Length of pipe[1:len(pipe)-1] should be 1\\\"\\n\\nprint(\\\"\\\\nAll tests passed!\\\")\\nEOF\\n\\n\\nLet's run this comprehensive test:\\n\\n\\npython /workspace/comprehensive_pipeline_len_test.py\\n\\n\\nLet's also run specific tests from the scikit-learn test suite that relate to Pipeline indexing to ensure our changes integrate well:\\n\\n\\ncd /workspace/scikit-learn__scikit-learn__0.21\\npython -m pytest -xvs sklearn/tests/test_pipeline.py::test_pipeline_indexing\\n\\n\\nOh, there might not be a specific test named `test_pipeline_indexing`. Let's look at the available tests in the test_pipeline.py file:\\n\\n\\ngrep -r \\\"def test_\\\" /workspace/scikit-learn__scikit-learn__0.21/sklearn/tests/test_pipeline.py\\n\\n\\nLet's run the tests that seem related to pipeline indexing or slicing:\\n\\n\\ncd /workspace/scikit-learn__scikit-learn__0.21\\npython -m pytest -xvs sklearn/tests/test_pipeline.py::test_pipeline_indexing || python -m pytest -xvs sklearn/tests/test_pipeline.py | grep -i \\\"index\\\"\\n\\n\\nSince we might not have direct access to specific test names, let's examine the pipeline test file to identify relevant tests:\\n\\n\\ncat /workspace/scikit-learn__scikit-learn__0.21/sklearn/tests/test_pipeline.py | grep -i \\\"index\\\\|slicing\\\\|getitem\\\"\\n\\n\\nBased on our findings, let's run specific tests that are relevant to our changes:\\n\\n\\ncd /workspace/scikit-learn__scikit-learn__0.21\\npython -m pytest -xvs sklearn/tests/test_pipeline.py::test_pipeline_indexing || python -m pytest -xvs sklearn/tests/test_pipeline.py\\n\\n\\nLet's check if the test_pipeline.py file contains any tests for slicing functionality:\\n\\n\\ncat /workspace/scikit-learn__scikit-learn__0.21/sklearn/tests/test_pipeline.py | grep -A 20 -B 20 \\\"slicing\\\\|__getitem__\\\"\\n\\n\\nLet's modify our comprehensive test script to include more edge cases and ensure it aligns with any existing tests:\\n\\n\\ncat >> /workspace/comprehensive_pipeline_len_test.py << 'EOF'\\n\\n# Test case 6: Testing on pipeline with make_pipeline\\nprint(\\\"\\\\nTest case 6: Testing on pipeline with make_pipeline\\\")\\nfrom sklearn.pipeline import make_pipeline\\n\\npipe = make_pipeline(StandardScaler(), PCA(n_components=2), SelectKBest(k=1))\\nprint(f\\\"Length of make_pipeline: {len(pipe)}\\\")\\nassert len(pipe) == 3, \\\"Length of make_pipeline should be 3\\\"\\n\\n# Test case 7: Testing on nested pipelines\\nprint(\\\"\\\\nTest case 7: Testing on nested pipelines\\\")\\ninner_pipe = Pipeline([('scaler', StandardScaler()), ('pca', PCA(n_components=2))])\\nouter_pipe = Pipeline([('inner', inner_pipe), ('svc', svm.SVC())])\\nprint(f\\\"Length of outer pipeline: {len(outer_pipe)}\\\")\\nassert len(outer_pipe) == 2, \\\"Length of outer pipeline should be 2\\\"\\n\\n# Test case 8: Testing __len__ with negative indexing\\nprint(\\\"\\\\nTest case 8: Testing __len__ with negative indexing\\\")\\npipe = Pipeline([\\n    ('step1', StandardScaler()),\\n    ('step2', PCA(n_components=2)),\\n    ('step3', SelectKBest(k=1))\\n])\\nneg_slice = pipe[-len(pipe):]\\nprint(f\\\"pipe[-len(pipe):] steps: {[name for name, _ in neg_slice.steps]}\\\")\\nassert len(neg_slice) == 3, \\\"Length of pipe[-len(pipe):] should be 3\\\"\\n\\nneg_slice2 = pipe[:-len(pipe)]\\nprint(f\\\"pipe[:-len(pipe)] steps: {[name for name, _ in neg_slice2.steps]}\\\")\\nassert len(neg_slice2) == 0, \\\"Length of pipe[:-len(pipe)] should be 0\\\"\\n\\n# Test case 9: Testing complex slicing expressions with len\\nprint(\\\"\\\\nTest case 9: Testing complex slicing expressions with len\\\")\\npipe = Pipeline([\\n    ('step1', StandardScaler()),\\n    ('step2', PCA(n_components=2)),\\n    ('step3', SelectKBest(k=1)),\\n    ('step4', svm.SVC())\\n])\\ncomplex_slice = pipe[len(pipe)//4:len(pipe)//4*3]\\nprint(f\\\"pipe[len(pipe)//4:len(pipe)//4*3] steps: {[name for name, _ in complex_slice.steps]}\\\")\\nassert len(complex_slice) == 2, \\\"Length of complex slice should be 2\\\"\\n\\nprint(\\\"\\\\nAll additional tests passed!\\\")\\nEOF\\n\\n\\nLet's run the updated comprehensive test:\\n\\n\\npython /workspace/comprehensive_pipeline_len_test.py\\n\\n\\nLet's also verify that our implementation doesn't cause any regressions in the scikit-learn test suite:\\n\\n\\ncd /workspace/scikit-learn__scikit-learn__0.21\\npython -m pytest -xvs sklearn/tests/test_pipeline.py\\n\\n\\nNow, let's also run another specific test related to pipeline functionality:\\n\\n\\ncd /workspace/scikit-learn__scikit-learn__0.21\\npython -m pytest -xvs sklearn/tests/test_metaestimators.py | grep -i \\\"pipeline\\\"\\n\\n\\nLet's get a deeper understanding of how the Pipeline class is used in tests by examining test files containing pipeline tests:\\n\\n\\ncd /workspace/scikit-learn__scikit-learn__0.21\\ngrep -r \\\"Pipeline\\\" --include=\\\"*.py\\\" sklearn/tests/ | head -n 20\\n\\n\\nLet's also look for any tests related to pipeline length or slicing:\\n\\n\\ncd /workspace/scikit-learn__scikit-learn__0.21\\ngrep -r \\\"len(\\\" --include=\\\"*.py\\\" sklearn/tests/ | grep -i \\\"pipeline\\\" | head -n 20\\n\\n\\nWith all our tests passing, it appears we've successfully implemented the `__len__` method for the Pipeline class, and it behaves as expected in various scenarios.\\n\\nLet's summarize what we've done:\\n\\n1. We located the Pipeline class in sklearn/pipeline.py\\n2. We identified that it was missing a `__len__` method, which was causing the error described in the issue\\n3. We implemented the `__len__` method to return the length of the pipeline's steps list\\n4. We verified our fix with a reproduction script and comprehensive tests\\n5. We confirmed that existing scikit-learn tests still pass with our changes\\n\\nOur implementation is minimal and focused on the specific issue, adding only the `__len__` method without changing any existing functionality.\\n\\nAt this point we can conclude that the issue has been successfully resolved. The Pipeline class now supports `len(pipe)`, which allows expressions like `pipe[:len(pipe)]` to work correctly.\\n\\nLet's make sure we've run the specific test for pipeline indexing:\\n\\n\\ncd /workspace/scikit-learn__scikit-learn__0.21\\npython -m pytest -xvs sklearn/tests/test_pipeline.py | grep -i \\\"indexing\\\"\\n\\n\\nLet's also check if there are any tests related to `__len__` or length in the pipeline tests:\\n\\n\\ncd /workspace/scikit-learn__scikit-learn__0.21\\ngrep -r \\\"__len__\\\\|len(\\\" --include=\\\"*.py\\\" sklearn/tests/test_pipeline.py\\n\\n\\nWith all our checks and tests, we're confident that our implementation of the `__len__` method in the Pipeline class correctly resolves the issue described. The method returns the number of steps in the pipeline, which is the expected behavior for `len(pipe)`.\\n\\nI believe our solution is complete and correct. The changes we made are minimal and focused on the specific issue, and we've thoroughly tested them to ensure they work as expected in various scenarios.\\n\\nLet's complete the task by running our comprehensive test one more time:\\n\\n\\npython /workspace/comprehensive_pipeline_len_test.py\\n\\n\\nI think we're good to go. Our implementation of the `__len__` method in the Pipeline class resolves the issue described, allowing expressions like `pipe[:len(pipe)]` to work correctly.\\n\\nLet's do a final check on the changes we made. The diff should show just the addition of the `__len__` method to the Pipeline class:\\n\\n\\ncd /workspace/scikit-learn__scikit-learn__0.21\\ndiff /workspace/pipeline.py.backup sklearn/pipeline.py\\n\\n\\nWith all our checks and tests passing, we've successfully resolved the issue described in the problem statement.\\n\\nAlright, let's summarize our solution once more:\\n\\nThe issue was that Pipeline in scikit-learn didn't implement the `__len__` method, which was causing `pipe[:len(pipe)]` to raise an error, even though Pipeline supported indexing.\\n\\nTo resolve this, we added a `__len__` method to the Pipeline class that returns the number of steps in the pipeline. This allows expressions like `len(pipe)` and `pipe[:len(pipe)]` to work correctly, providing a more consistent and intuitive API for users.\\n\\nWe thoroughly tested our solution with:\\n1. A script to reproduce the original error and confirm our fix\\n2. Comprehensive tests for various scenarios, including edge cases\\n3. Running the existing scikit-learn test suite to ensure no regressions\\n\\nThese tests confirm that our solution correctly resolves the issue while maintaining compatibility with the rest of the codebase.\\n\\nAnd that concludes our implementation.\",\n          \"signature\": \"ErUBCkYIARgCIkDnKEqW20HOxB9jS85ReLznKuKSV88wnAJf/M+y9ge7U+GOWAHd/trgmvpgHpJmOBkeUWUftR5g+81TNmAsDd2EEgxQHv0uMQBMvelCR+saDFvnSSfaNOj3NTVYpyIwG1ddKj5AVnXe382HERyVchmOTdn1ebogEFpXlRE66Mckq57xLSygusq6MwAV/6RvKh28HEuwcXNQn+BPIxQDVCwGGUh5g4fpZ6IgGk3mRg==\"\n        },\n        {\n          \"type\": \"text\",\n          \"text\": \"I'll help you implement the `__len__` method for Pipeline in scikit-learn to fix the issue described. Let's start by exploring the repository and understanding the problem better.\\n\\n## Step 1: Explore the repository structure\\n\\nLet's first look at the repository structure:\"\n        }\n      ],\n      \"role\": \"assistant\",\n      \"tool_calls\": [\n        {\n          \"id\": \"toolu_01WfsUZxRn8dsTyN6q6F79pN\",\n          \"type\": \"function\",\n          \"function\": {\n            \"name\": \"execute_bash\",\n            \"arguments\": \"{\\\"command\\\": \\\"ls -la /workspace/scikit-learn__scikit-learn__0.21\\\"}\"\n          }\n        }\n      ]\n    },\n    {\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"total 152\\ndrwxr-xr-x 12 root root  4096 Mar  3 20:25 .\\ndrwxrwxr-x  3 root root  4096 Mar  3 20:25 ..\\ndrwxr-xr-x  2 root root  4096 Mar  3 20:25 .circleci\\n-rw-r--r--  1 root root   672 Mar  3 20:25 .codecov.yml\\n-rw-r--r--  1 root root   126 Mar  3 20:25 .coveragerc\\ndrwxr-xr-x  8 root root  4096 Mar  3 20:25 .git\\n-rw-r--r--  1 root root    31 Mar  3 20:25 .gitattributes\\n-rw-r--r--  1 root root   891 Mar  3 20:25 .gitignore\\n-rw-r--r--  1 root root    86 Mar  3 20:25 .landscape.yml\\n-rw-r--r--  1 root root  7263 Mar  3 20:25 .mailmap\\n-rw-r--r--  1 root root  1234 Mar  3 20:25 .travis.yml\\n-rw-r--r--  1 root root  2107 Mar  3 20:25 CONTRIBUTING.md\\n-rw-r--r--  1 root root  1559 Mar  3 20:25 COPYING\\n-rw-r--r--  1 root root  1860 Mar  3 20:25 ISSUE_TEMPLATE.md\\n-rw-r--r--  1 root root   243 Mar  3 20:25 MANIFEST.in\\n-rw-r--r--  1 root root  1390 Mar  3 20:25 Makefile\\n-rw-r--r--  1 root root  1198 Mar  3 20:25 PULL_REQUEST_TEMPLATE.md\\n-rw-r--r--  1 root root  5816 Mar  3 20:25 README.rst\\n-rw-r--r--  1 root root  4694 Mar  3 20:25 appveyor.yml\\n-rw-r--r--  1 root root  2226 Mar  3 20:25 azure-pipelines.yml\\ndrwxr-xr-x  2 root root  4096 Mar  3 20:25 benchmarks\\ndrwxr-xr-x  3 root root  4096 Mar  3 20:25 build\\ndrwxr-xr-x  6 root root  4096 Mar  3 20:25 build_tools\\n-rw-r--r--  1 root root  2327 Mar  3 20:25 conftest.py\\ndrwxr-xr-x 14 root root  4096 Mar  3 20:25 doc\\ndrwxr-xr-x 29 root root  4096 Mar  3 20:25 examples\\n-rw-r--r--  1 root root   149 Mar  3 20:25 lgtm.yml\\ndrwxr-xr-x  2 root root  4096 Mar  3 20:25 maint_tools\\ndrwxr-xr-x  2 root root  4096 Mar  3 20:25 scikit_learn.egg-info\\n-rw-r--r--  1 root root   664 Mar  3 20:25 setup.cfg\\n-rwxr-xr-x  1 root root 11442 Mar  3 20:25 setup.py\\n-rw-r--r--  1 root root   292 Mar  3 20:25 site.cfg\\ndrwxr-xr-x 29 root root  4096 Mar  3 20:25 sklearn\\n[The command completed with exit code 0.]\\n[Current working directory: /workspace/scikit-learn__scikit-learn__0.21]\\n[Python interpreter: /opt/miniconda3/envs/testbed/bin/python]\\n[Command finished with exit code 0]\"\n        }\n      ],\n      \"role\": \"tool\",\n      \"cache_control\": {\n        \"type\": \"ephemeral\"\n      },\n      \"tool_call_id\": \"toolu_01WfsUZxRn8dsTyN6q6F79pN\",\n      \"name\": \"execute_bash\"\n    }\n  ],\n  \"tools\": [\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"execute_bash\",\n        \"description\": \"Execute a bash command in the terminal.\\n* Long running commands: For commands that may run indefinitely, it should be run in the background and the output should be redirected to a file, e.g. command = `python3 app.py > server.log 2>&1 &`.\\n* Interact with running process: If a bash command returns exit code `-1`, this means the process is not yet finished. By setting `is_input` to `true`, the assistant can interact with the running process and send empty `command` to retrieve any additional logs, or send additional text (set `command` to the text) to STDIN of the running process, or send command like `C-c` (Ctrl+C), `C-d` (Ctrl+D), `C-z` (Ctrl+Z) to interrupt the process.\\n* One command at a time: You can only execute one bash command at a time. If you need to run multiple commands sequentially, you can use `&&` or `;` to chain them together.\\n\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"command\": {\n              \"type\": \"string\",\n              \"description\": \"The bash command to execute. Can be empty string to view additional logs when previous exit code is `-1`. Can be `C-c` (Ctrl+C) to interrupt the currently running process. Note: You can only execute one bash command at a time. If you need to run multiple commands sequentially, you can use `&&` or `;` to chain them together.\"\n            },\n            \"is_input\": {\n              \"type\": \"string\",\n              \"description\": \"If True, the command is an input to the running process. If False, the command is a bash command to be executed in the terminal. Default is False.\",\n              \"enum\": [\n                \"true\",\n                \"false\"\n              ]\n            }\n          },\n          \"required\": [\n            \"command\"\n          ]\n        }\n      }\n    },\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"think\",\n        \"description\": \"Use the tool to think about something. It will not obtain new information or make any changes to the repository, but just log the thought. Use it when complex reasoning or brainstorming is needed.\\n\\nCommon use cases:\\n1. When exploring a repository and discovering the source of a bug, call this tool to brainstorm several unique ways of fixing the bug, and assess which change(s) are likely to be simplest and most effective.\\n2. After receiving test results, use this tool to brainstorm ways to fix failing tests.\\n3. When planning a complex refactoring, use this tool to outline different approaches and their tradeoffs.\\n4. When designing a new feature, use this tool to think through architecture decisions and implementation details.\\n5. When debugging a complex issue, use this tool to organize your thoughts and hypotheses.\\n\\nThe tool simply logs your thought process for better transparency and does not execute any code or make changes.\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"thought\": {\n              \"type\": \"string\",\n              \"description\": \"The thought to log.\"\n            }\n          },\n          \"required\": [\n            \"thought\"\n          ]\n        }\n      }\n    },\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"finish\",\n        \"description\": \"Signals the completion of the current task or conversation.\\n\\nUse this tool when:\\n- You have successfully completed the user's requested task\\n- You cannot proceed further due to technical limitations or missing information\\n\\nThe message should include:\\n- A clear summary of actions taken and their results\\n- Any next steps for the user\\n- Explanation if you're unable to complete the task\\n- Any follow-up questions if more information is needed\\n\\nThe task_completed field should be set to True if you believed you have completed the task, and False otherwise.\\n\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"required\": [\n            \"message\",\n            \"task_completed\"\n          ],\n          \"properties\": {\n            \"message\": {\n              \"type\": \"string\",\n              \"description\": \"Final message to send to the user\"\n            },\n            \"task_completed\": {\n              \"type\": \"string\",\n              \"enum\": [\n                \"true\",\n                \"false\",\n                \"partial\"\n              ],\n              \"description\": \"Whether you have completed the task.\"\n            }\n          }\n        }\n      }\n    },\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"str_replace_editor\",\n        \"description\": \"Custom editing tool for viewing, creating and editing files in plain-text format\\n* State is persistent across command calls and discussions with the user\\n* If `path` is a file, `view` displays the result of applying `cat -n`. If `path` is a directory, `view` lists non-hidden files and directories up to 2 levels deep\\n* The `create` command cannot be used if the specified `path` already exists as a file\\n* If a `command` generates a long output, it will be truncated and marked with `<response clipped>`\\n* The `undo_edit` command will revert the last edit made to the file at `path`\\n\\nNotes for using the `str_replace` command:\\n* The `old_str` parameter should match EXACTLY one or more consecutive lines from the original file. Be mindful of whitespaces!\\n* If the `old_str` parameter is not unique in the file, the replacement will not be performed. Make sure to include enough context in `old_str` to make it unique\\n* The `new_str` parameter should contain the edited lines that should replace the `old_str`\\n\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"command\": {\n              \"description\": \"The commands to run. Allowed options are: `view`, `create`, `str_replace`, `insert`, `undo_edit`.\",\n              \"enum\": [\n                \"view\",\n                \"create\",\n                \"str_replace\",\n                \"insert\",\n                \"undo_edit\"\n              ],\n              \"type\": \"string\"\n            },\n            \"path\": {\n              \"description\": \"Absolute path to file or directory, e.g. `/workspace/file.py` or `/workspace`.\",\n              \"type\": \"string\"\n            },\n            \"file_text\": {\n              \"description\": \"Required parameter of `create` command, with the content of the file to be created.\",\n              \"type\": \"string\"\n            },\n            \"old_str\": {\n              \"description\": \"Required parameter of `str_replace` command containing the string in `path` to replace.\",\n              \"type\": \"string\"\n            },\n            \"new_str\": {\n              \"description\": \"Optional parameter of `str_replace` command containing the new string (if not given, no string will be added). Required parameter of `insert` command containing the string to insert.\",\n              \"type\": \"string\"\n            },\n            \"insert_line\": {\n              \"description\": \"Required parameter of `insert` command. The `new_str` will be inserted AFTER the line `insert_line` of `path`.\",\n              \"type\": \"integer\"\n            },\n            \"view_range\": {\n              \"description\": \"Optional parameter of `view` command when `path` points to a file. If none is given, the full file is shown. If provided, the file will be shown in the indicated line number range, e.g. [11, 12] will show lines 11 and 12. Indexing at 1 to start. Setting `[start_line, -1]` shows all lines from `start_line` to the end of the file.\",\n              \"items\": {\n                \"type\": \"integer\"\n              },\n              \"type\": \"array\"\n            }\n          },\n          \"required\": [\n            \"command\",\n            \"path\"\n          ]\n        }\n      }\n    }\n  ]\n}\n\nresponse = litellm.completion(\n    model = 'anthropic/claude-3-7-sonnet-20250219',\n    api_key=API_KEY,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 30720, # 32768 - 2048\n    },\n    max_completion_tokens=32768,\n    **kwargs\n)\n\nOutput\n    raise exception_type(\n          ^^^^^^^^^^^^^^^\n  File \"/home/xingyaow/.cache/pypoetry/virtualenvs/openhands-ai-msAbIsPU-py3.12/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2202, in exception_type\n    raise e\n  File \"/home/xingyaow/.cache/pypoetry/virtualenvs/openhands-ai-msAbIsPU-py3.12/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 527, in exception_type\n    raise BadRequestError(\nlitellm.exceptions.BadRequestError: litellm.BadRequestError: AnthropicException - {\"type\":\"error\",\"error\":{\"type\":\"invalid_request_error\",\"message\":\"messages.1.content.0.type: Expected `thinking` or `redacted_thinking`, but found `text`. When `thinking` is enabled, a final `assistant` message must start with a thinking block (preceeding the lastmost set of `tool_use` and `tool_result` blocks). We recommend you include thinking blocks from previous turns. To avoid this requirement, disable `thinking`. Please consult our documentation at https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking\"}}\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.61.20\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "xingyaoww",
      "author_type": "User",
      "created_at": "2025-03-03T21:09:46Z",
      "updated_at": "2025-06-03T21:28:09Z",
      "closed_at": "2025-03-05T05:12:18Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 14,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8961/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8961",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8961",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:32:00.405805",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Hi @xingyaoww i think your error happens as your thinking block also contains the tool calls, though i have to check. \n\nI can confirm tool calling with thinking does pass our testing - https://github.com/BerriAI/litellm/blob/b9bddac77691747b3130c2de341f05e42891900d/tests/local_testing/test_function_",
          "created_at": "2025-03-03T21:43:29Z"
        },
        {
          "author": "xingyaoww",
          "body": "I just checked again, the same error also happened in the \"main\" branch too üò¢ ",
          "created_at": "2025-03-03T21:54:09Z"
        },
        {
          "author": "krrishdholakia",
          "body": "what's the raw request sent by litellm to anthropic? @xingyaoww \n\n(run `litellm._turn_on_debug()`) ",
          "created_at": "2025-03-03T21:55:45Z"
        },
        {
          "author": "xingyaoww",
          "body": "I asked OpenHands to take a look at this issue -- here's what I've got, hopefully it is somewhat helpful\n\n---\n\n## Root Cause Analysis\n\nAfter examining the code, I believe the issue is related to how thinking blocks are handled when using function calling with Anthropic models.\n\n### The Problem\n\nWhen",
          "created_at": "2025-03-03T21:56:20Z"
        },
        {
          "author": "xingyaoww",
          "body": "@krrishdholakia here it is - it doesn't have the \"thinking\" content\n\n```POST Request Sent from LiteLLM:\n\ncurl -X POST \\\nhttps://api.anthropic.com/v1/messages \\\n-H 'anthropic-version: *****' -H 'x-api-key: sk-ant-api03-********************************************' -H 'accept: *****' -H 'content-type:",
          "created_at": "2025-03-03T21:58:06Z"
        }
      ]
    },
    {
      "issue_number": 11376,
      "title": "[Bug]: Groq Response Broken for Compound Beta & Compound Beta Mini Model",
      "body": "### What happened?\n\nI was able to successfully hit the groq compound beta model but the litellm response is not complete as it does not include the response.choices[0].message.executed_tools section. See the attached API request and response for reference\n\n[compound_beta_completion.json](https://github.com/user-attachments/files/20579793/compound_beta_completion.json)\n[compound_beta_request.json](https://github.com/user-attachments/files/20579794/compound_beta_request.json)\n\n### Relevant log output\n\n```shell\n(Pdb) p response.choices[0].message.executed_tools\n*** AttributeError: 'Message' object has no attribute 'executed_tools'\n\n(Pdb) p response.choices[0].message\nMessage(content=\"I'm Compound-Beta, a system built by Groq. I'm here to help with any questions or tasks you may have. How can I assist you today?\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None, reasoning='<Think>\\n\\n</Think>')\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.72.0\n\n### Twitter / LinkedIn details\n\n@amolsingh01",
      "state": "open",
      "author": "amolpsingh",
      "author_type": "User",
      "created_at": "2025-06-03T20:16:54Z",
      "updated_at": "2025-06-03T20:16:54Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11376/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11376",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11376",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:32:00.617807",
      "comments": []
    },
    {
      "issue_number": 9797,
      "title": "[Bug]: TogetherAi tool call support issues",
      "body": "### What happened?\n\nWhen using the together ai models, I bump into issues with tool call support. Even though they support tool calls, it seems that litellm does raise an exception.\n\n## Example\n\n### Litellm\n```python\ntools = [\n  {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"get_current_weather\",\n      \"description\": \"Get the current weather in a given location\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"location\": {\n            \"type\": \"string\",\n            \"description\": \"The city and state, e.g. San Francisco, CA\"\n          },\n          \"unit\": {\n            \"type\": \"string\",\n            \"enum\": [\n              \"celsius\",\n              \"fahrenheit\"\n            ]\n          }\n        }\n      }\n    }\n  }\n]\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant that can access external functions. The responses from these function calls will be appended to this dialogue. Please provide responses based on the information from these function calls.\"},\n    {\"role\": \"user\", \"content\": \"What is the current temperature of New York, San Francisco and Chicago?\"}\n]\n\nres = litellm.completion(\n\tmodel=\"together_ai/meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n\tmessages=messages,\n\ttools=tools,\n\ttool_choice=\"required\", \n)\nprint(res.model_dump_json(indent=2))\n```\n\nOutput: \n```bash\nUnsupportedParamsError: litellm.UnsupportedParamsError: together_ai does not support parameters: ['tools', 'tool_choice'], for model=meta-llama/Llama-4-Scout-17B-16E-Instruct. To drop these, set `litellm.drop_params=True` or for proxy:\n\n`litellm_settings:\n drop_params: true`\n. \n If you want to use these params dynamically send allowed_openai_params=['tools', 'tool_choice'] in your request.\n```\n\n### Raw together API\n\n```python\nimport os\nimport openai\n\nclient = openai.OpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ['TOGETHER_API_KEY'],\n)\ntools = [\n  {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"get_current_weather\",\n      \"description\": \"Get the current weather in a given location\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"location\": {\n            \"type\": \"string\",\n            \"description\": \"The city and state, e.g. San Francisco, CA\"\n          },\n          \"unit\": {\n            \"type\": \"string\",\n            \"enum\": [\n              \"celsius\",\n              \"fahrenheit\"\n            ]\n          }\n        }\n      }\n    }\n  }\n]\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant that can access external functions. The responses from these function calls will be appended to this dialogue. Please provide responses based on the information from these function calls.\"},\n    {\"role\": \"user\", \"content\": \"What is the current temperature of New York, San Francisco and Chicago?\"}\n]\n\nres = client.chat.completions.create(\n\tmodel=\"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n    messages=messages,\n    tools=tools,\n    tool_choice=\"required\",\n)\nprint(res.model_dump_json(indent=2))\n```\n\nOutput:\n\n```bash\n{\n  \"id\": \"np73UjW-6UHjtw-92c78f9bfc8b9218\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"tool_calls\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": null,\n        \"refusal\": null,\n        \"role\": \"assistant\",\n        \"annotations\": null,\n        \"audio\": null,\n        \"function_call\": null,\n        \"tool_calls\": [\n          {\n            \"id\": \"call_89mc1zzegksw4nrfgynt9c4e\",\n            \"function\": {\n              \"arguments\": \"{\\\"location\\\":\\\"New York\\\",\\\"unit\\\":\\\"celsius\\\"}\",\n              \"name\": \"get_current_weather\"\n            },\n            \"type\": \"function\",\n            \"index\": 0\n          },\n          {\n            \"id\": \"call_4fihkdf2iylzrzfmjz4wdeu9\",\n            \"function\": {\n              \"arguments\": \"{\\\"location\\\":\\\"San Francisco\\\",\\\"unit\\\":\\\"celsius\\\"}\",\n              \"name\": \"get_current_weather\"\n            },\n            \"type\": \"function\",\n            \"index\": 1\n          },\n          {\n            \"id\": \"call_u6nyd7v84d2nae646i48qsil\",\n            \"function\": {\n              \"arguments\": \"{\\\"location\\\":\\\"Chicago\\\",\\\"unit\\\":\\\"celsius\\\"}\",\n              \"name\": \"get_current_weather\"\n            },\n            \"type\": \"function\",\n            \"index\": 2\n          }\n        ]\n      },\n      \"seed\": null\n    }\n  ],\n  \"created\": 1744008315,\n  \"model\": \"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n  \"object\": \"chat.completion\",\n  \"service_tier\": null,\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 85,\n    \"prompt_tokens\": 245,\n    \"total_tokens\": 330,\n    \"completion_tokens_details\": null,\n    \"prompt_tokens_details\": null\n  },\n  \"prompt\": []\n}\n```\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\n1.65.4.post1\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "Somtom",
      "author_type": "User",
      "created_at": "2025-04-07T06:50:29Z",
      "updated_at": "2025-06-03T19:38:25Z",
      "closed_at": "2025-05-30T22:10:01Z",
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9797/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "krrishdholakia"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9797",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9797",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:32:00.617847",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "We should just allow all tool calling to tgai",
          "created_at": "2025-04-12T16:06:34Z"
        },
        {
          "author": "Somtom",
          "body": "@krrishdholakia is there a way to support fixing this? I am happy to contribute but might need some pointers on where to look :) ",
          "created_at": "2025-04-24T07:59:30Z"
        },
        {
          "author": "krrishdholakia",
          "body": "Hey @Somtom this was already fixed - it looks like tgai does only allow tool calling on specific models, so i updated our list ",
          "created_at": "2025-05-30T22:08:48Z"
        },
        {
          "author": "krrishdholakia",
          "body": "https://github.com/BerriAI/litellm/blob/b13d1b3ff1d22086b1194d5e89e97b25cf448588/model_prices_and_context_window.json#L11906\n\nI migrated the call to use our model cost map - and we can now just update the model cost map with the flag for any tgai model that needs it - so users don't need to keep bum",
          "created_at": "2025-05-30T22:09:55Z"
        },
        {
          "author": "chandlj",
          "body": "@krrishdholakia This doesn't seem to be fixed. I added a couple of models to the json files and I still got the same error",
          "created_at": "2025-06-03T16:45:14Z"
        }
      ]
    },
    {
      "issue_number": 7569,
      "title": "[Bug]: OpenMeter integration fails due to incorrect CloudEvent subject field format",
      "body": "### What happened?\n\nThe openmeter integration doesn't work correctly, the cloudevent subject field transmitted by litellm is invalid and the openmeter server throws a status code 400.\r\n\r\nThe cloudEvent config on the openmeter end is configured correctly (as shown in the official [openmeter docs](https://docs.litellm.ai/docs/observability/openmeter)), as demonstrated by the following test curl that runs successfully:\r\n```\r\ncurl -X POST https://openmeter.cloud/api/v1/events \\\r\n  -H 'Content-Type: application/cloudevents+json' \\\r\n  -H 'Authorization: Bearer <OPENMETER_PLACEHOLDER_API_KEY>' \\\r\n  --data-raw '\r\n  {\r\n    \"specversion\" : \"1.0\",\r\n    \"type\": \"litellm_tokens\",\r\n    \"id\": \"3ed4f15f-8dcf-45cb-8ec3-0922f5a67ab7\",\r\n    \"time\": \"2025-01-05T10:15:57.783Z\",\r\n    \"source\": \"my-app\",\r\n    \"subject\": \"customer-1\",\r\n    \"data\": {\r\n      \"completion_tokens\": \"123\",\r\n      \"cost\": \"cost\",\r\n      \"model\": \"model\",\r\n      \"prompt_tokens\": \"prompt_tokens\",\r\n      \"total_tokens\": \"total_tokens\"\r\n    }\r\n  }\r\n'\r\n```\r\n\r\nThe problem appears to be in the extraction of the user field inside the openmeter code: https://github.com/BerriAI/litellm/blob/9cfd7c790a0169d69b192c3ac426346ae5115c01/litellm/integrations/openmeter.py#L68\r\nwhere the user OpenAI spec field is extracted and used as the subject for the cloudevent event.\r\n\r\nThanks for the great project!\n\n### Relevant log output\n\n```shell\nlitellm-1                   | {\"message\": \"LiteLLM.LoggingError: [Non-Blocking] Exception occurred while success logging Traceback (most recent call last):\\n  File \\\"/usr/local/lib/python3.13/site-packages/litellm/integrations/openmeter.py\\\", line 124, in async_log_success_event\\n    await self.async_http_handler.post(\\n    ...<3 lines>...\\n    )\\n  File \\\"/usr/local/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py\\\", line 219, in post\\n    raise e\\n  File \\\"/usr/local/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py\\\", line 177, in post\\n    response.raise_for_status()\\n    ~~~~~~~~~~~~~~~~~~~~~~~~~^^\\n  File \\\"/usr/local/lib/python3.13/site-packages/httpx/_models.py\\\", line 761, in raise_for_status\\n    raise HTTPStatusError(message, request=request, response=self)\\nhttpx.HTTPStatusError: Client error '400 Bad Request' for url 'https://openmeter.cloud/api/v1/events'\\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \\\"/usr/local/lib/python3.13/site-packages/litellm/litellm_core_utils/litellm_logging.py\\\", line 1602, in async_success_handler\\n    await callback.async_log_success_event(\\n    ...<6 lines>...\\n    )\\n  File \\\"/usr/local/lib/python3.13/site-packages/litellm/integrations/openmeter.py\\\", line 130, in async_log_success_event\\n    raise Exception(f\\\"OpenMeter logging error: {e.response.text}\\\")\\nException: OpenMeter logging error: {\\\"type\\\":\\\"about:blank\\\",\\\"title\\\":\\\"Bad Request\\\",\\\"status\\\":400,\\\"detail\\\":\\\"request body has an error: doesn't match schema #/components/schemas/Event: Error at \\\\\\\"/subject\\\\\\\": value must be a string\\\"}\\n\\n\", \"level\": \"ERROR\", \"timestamp\": \"2025-01-05T03:36:32.704192\"}\n```\n\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.56.9\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "enricobellato",
      "author_type": "User",
      "created_at": "2025-01-05T10:31:54Z",
      "updated_at": "2025-06-03T19:31:28Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/7569/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/7569",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/7569",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:32:00.877164",
      "comments": [
        {
          "author": "mkagit",
          "body": "Is it working? I'm trying to set Openmeter too",
          "created_at": "2025-02-25T14:42:33Z"
        },
        {
          "author": "mkagit",
          "body": "```\n2025-02-25 11:53:40 INFO:     172.17.0.1:58130 - \"GET /health/services?service=openmeter HTTP/1.1\" 200 OK\n2025-02-25 11:53:40 14:53:40 - LiteLLM Proxy:ERROR: proxy_track_cost_callback.py:135 - Error in tracking cost callback - User API key and team id and user id missing from custom callback.\n20",
          "created_at": "2025-02-25T14:54:44Z"
        },
        {
          "author": "enricobellato",
          "body": "Still experiencing issues. Would appreciate if a LiteLLM (@ishaan-jaff) or Openmeter maintainer (@hekike) could investigate. This feature would be really valuable for tracking LLM usage costs.",
          "created_at": "2025-03-01T20:37:51Z"
        },
        {
          "author": "mkagit",
          "body": "## OpenMeter Integration: \"Error at \\\"/subject\\\": value must be a string\"\n\n**Description:**\n\nI'm encountering an issue with the OpenMeter integration in LiteLLM.  I consistently receive a `400 Bad Request` error from OpenMeter with the message: `\"Error at \\\"/subject\\\": value must be a string\"`. This",
          "created_at": "2025-03-05T15:36:22Z"
        },
        {
          "author": "mohittalele",
          "body": "@mkagit @enricobellato facing same issue. Did you guys find some solution ?",
          "created_at": "2025-06-03T14:05:50Z"
        }
      ]
    },
    {
      "issue_number": 11369,
      "title": "[Bug]: NameError: ModelResponseStream is not defined in vertex_and_google_ai_studio_gemini.py causing LiteLLM startup failure on main branch",
      "body": "### What happened?\n\n**Description:**  \nWhen running the latest LiteLLM code from the main branch, the pod fails to start with the following traceback:\n\n```\nNameError: name 'ModelResponseStream' is not defined\n```\n\nThis error occurs in the file `litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py` at the definition of the `ModelResponseIterator` class, specifically in the method `handle_valid_json_chunk`.\n\n**Impact:**  \nThis prevents the LiteLLM server from starting, making it unusable with the current main branch code.\n\n**Steps to Reproduce:**  \n1. Pull the latest LiteLLM code from the main branch.  \n2. Build and run the LiteLLM server (e.g., in Kubernetes pod or locally).  \n3. Observe the startup failure with the above traceback.\n\n**Expected Behavior:**  \nLiteLLM should start successfully without `NameError` exceptions.\n\n**Additional Notes:**  \n- The error indicates that the class or type `ModelResponseStream` is referenced but not imported or defined in the source file.  \n- This appears to be a recent regression or missing import in the main branch.  \n- Using a stable release tag or commit prior to this issue avoids the problem.\n\n**Environment:**  \n- LiteLLM version: main branch as of [date]  \n- Python version: 3.13 (as per traceback)  \n- Deployment: Kubernetes pod (or specify your environment)\n\n**Request:**  \nPlease fix the missing definition or import of `ModelResponseStream` in `vertex_and_google_ai_studio_gemini.py` to restore startup functionality.\n\n\n### Relevant log output\n\n```shell\nTraceback (most recent call last):\n  File \"/usr/bin/litellm\", line 5, in <module>\n    from litellm import run_server\n  File \"/usr/lib/python3.13/site-packages/litellm/__init__.py\", line 919, in <module>\n    from .llms.vertex_ai.gemini.vertex_and_google_ai_studio_gemini import (\n    ...<2 lines>...\n    )\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1732, in <module>\n    class ModelResponseIterator:\n    ...<195 lines>...\n                raise RuntimeError(f\"Error parsing chunk: {e},\\nReceived chunk: {chunk}\")\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1838, in ModelResponseIterator\n    def handle_valid_json_chunk(self, chunk: str) -> Optional[ModelResponseStream]:\n                                                              ^^^^^^^^^^^^^^^^^^^\nNameError: name 'ModelResponseStream' is not defined\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.72.1\n\n### Twitter / LinkedIn details\n\n@JosephBenraz  /  https://www.linkedin.com/in/josephbenraz/",
      "state": "closed",
      "author": "Joseph1977",
      "author_type": "User",
      "created_at": "2025-06-03T15:24:21Z",
      "updated_at": "2025-06-03T18:16:56Z",
      "closed_at": "2025-06-03T18:16:55Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11369/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11369",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11369",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:32:01.094612",
      "comments": [
        {
          "author": "Joseph1977",
          "body": "@krrishdholakia i fixed couldn't push/open PR, but i saw you just pushed a new fix, if thats the case LMK so i can close it or feel free to close this issue.",
          "created_at": "2025-06-03T16:32:28Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "fixed on main ",
          "created_at": "2025-06-03T18:16:55Z"
        }
      ]
    },
    {
      "issue_number": 11368,
      "title": "[Bug]: user_id is not unique, user_email is used as primary key",
      "body": "### What happened?\n\nUsing the REST api, it is possible to create users using just `user_id` with no email provided, however, you can then keep creating users with the same `user_id`, this doesn't create new users but keeps creating new keys for the same `user_id`.\n\nIt is also impossible to give a user that was added with just `user_id` access to the UI.\n\nOn that note, it is impossible to set a password and give them access to the UI using the REST api, you have to invite them manually and send them an invite link after which they have to set a password themselves\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.71.1-stable\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "AntoonBeres",
      "author_type": "User",
      "created_at": "2025-06-03T15:06:17Z",
      "updated_at": "2025-06-03T15:06:17Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11368/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11368",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11368",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:32:01.283469",
      "comments": []
    },
    {
      "issue_number": 11367,
      "title": "[Feature]: Feature Request: Enable System Prompt Editing via Web UI",
      "body": "### The Feature\n\nThe suggested change should be integrated into the model settings interface. Navigate to the 'Models' tab, and then locate the configuration area for a specific model, where the model name and other parameters are displayed. The change can be implemented in this section.\n\n### Motivation, pitch\n\nCurrently, LiteLLM lacks the ability to directly edit system prompts. This is a critical and commonly needed function. This feature requests the addition of a web UI element allowing quick and easy configuration of system prompts. Providing this functionality via the UI would significantly improve the user experience. Current solutions for prompt management, callbacks are really difficult, inefficient, and prone to errors.\n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "zee1717gh",
      "author_type": "User",
      "created_at": "2025-06-03T14:42:52Z",
      "updated_at": "2025-06-03T14:42:52Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11367/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11367",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11367",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:32:01.283497",
      "comments": []
    },
    {
      "issue_number": 11366,
      "title": "[Bug]: CustomPromptManagement.get_chat_completion_prompt not invoked (v1.71) & UI prompt management suggestion",
      "body": "### What happened?\n\nI'm experiencing an issue where the CustomPromptManagement.get_chat_completion_prompt method is not being invoked in LiteLLM Proxy (v1.71) when handling chat/completions requests. The custom manager instance initializes correctly, but the method itself is never called, preventing dynamic prompt modification.\n\nThis highlights a broader challenge with current custom prompt methods, which often lead to debugging complexities. It would be highly beneficial to have a built-in feature in the LiteLLM UI allowing direct editing of system prompts for each configured model. This would significantly simplify prompt management and reduce reliance on custom code that currently proves problematic.\n\n### Relevant log output\n\n```shell\ncustom_prompt_manager.py content:\npython\n\n    from litellm.integrations.custom_prompt_management import CustomPromptManagement\n\n    class TestPromptManager(CustomPromptManagement):\n        def __init__(self):\n            super().__init__()\n            print(\"‚úÖ [TestPromptManager] Initializing prompt manager!\")\n\n        def get_chat_completion_prompt(\n            self,\n            model,\n            messages,\n            non_default_params,\n            prompt_id,\n            prompt_variables,\n            dynamic_callback_params\n        ):\n            print(\"üö®üö®üö® get_chat_completion_prompt WAS INVOKED üö®üö®üö®\")\n            print(f\"Model: {model}\")\n            print(f\"Messages: {messages}\")\n            return model, messages, non_default_params\n\n    test_prompt_manager = TestPromptManager()\n    \n\n2.  **`config.yaml` relevant section:**\n\n    \n    model_list:\n      - model_name: moderator\n        litellm_params:\n          model: openai/gpt-3.5-turbo\n          api_key: os.environ/OPENAI_API_KEY\n          max_tokens: 25\n\n    litellm_settings:\n      callbacks: custom_prompt_manager.test_prompt_manager\n      debug: True\n      detailed_debug: True\n    \n\n3.  **Environment:**\n    *   LiteLLM Version: `1.71`\n    *   Deployment: Docker container\n    *   Python version: `3.13.3`\n\n4.  **Actions:**\n    *   `custom_prompt_manager.py` placed in `/app` within container.\n    *   Confirmed successful import and initialization of `test_prompt_manager` (log `‚úÖ [TestPromptManager] Initializing prompt manager!` appears on startup).\n    *   Sent `POST` request to `/v1/chat/completions?model=moderator`.\n\n#### **Expected Behavior:**\n\nThe `get_chat_completion_prompt` method should be invoked, and `üö®üö®üö® get_chat_completion_prompt WAS INVOKED üö®üö®üö®` should appear in logs.\n\n#### **Actual Behavior:**\n\nThe `get_chat_completion_prompt` method is **not invoked**. The log message does not appear. The proxy processes the request without applying custom prompt logic.\n\n#### **Additional Context:**\n\n*   This occurs even with a minimal `TestPromptManager`.\n*   `config.yaml` setup for `callbacks` follows documentation.\n*   Tested with `openai/gpt-3.5-turbo` backend.\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.71\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "zee1717gh",
      "author_type": "User",
      "created_at": "2025-06-03T14:31:32Z",
      "updated_at": "2025-06-03T14:31:32Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11366/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11366",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11366",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:32:01.283504",
      "comments": []
    },
    {
      "issue_number": 11126,
      "title": "[Bug]: unable to pass `input_type` parameter to Azuree AI Cohere Embed v4 embedding model",
      "body": "### What happened?\n\nWhat happened?\nAccording to the documentation [here](https://docs.litellm.ai/docs/embedding/supported_embedding#provider-specific-params):\n\n> Any non-openai params, will be treated as provider-specific params, and sent in the request body as kwargs to the provider.\n\nI expect the following to work.\n\n```\nresponse = embedding(\n                model=\"azure_ai/embed-v-4-0\",\n                input=[\"Hello World\"],\n                input_type=\"search_document\",\n                dimensions=1536,  # Specify dimensions explicitly\n            )\n```\n\nInstead I see the following error:\n\n```\nlitellm.APIError: APIError: Azure_aiException - Embeddings.create() got an unexpected keyword argument 'input_type'\n```\n\n### Relevant log output\n\n```shell\nlitellm.APIError: APIError: Azure_aiException - Embeddings.create() got an unexpected keyword argument 'input_type'\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.71.0\n\n### Twitter / LinkedIn details\n\n@htutlynnaung",
      "state": "closed",
      "author": "HtutLynn",
      "author_type": "User",
      "created_at": "2025-05-24T18:03:23Z",
      "updated_at": "2025-06-03T14:24:14Z",
      "closed_at": "2025-06-03T14:24:14Z",
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11126/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11126",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11126",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:32:01.283512",
      "comments": []
    },
    {
      "issue_number": 11330,
      "title": "[Bug]: LiteLLM Router Issue: \"No deployments available\" with Healthy Ollama Endpoints",
      "body": "### What happened?\n\n## Issue Summary\n\nLiteLLM proxy consistently returns \"No deployments available for selected model\" error despite:\n- Health checks showing endpoints as healthy\n- Direct Ollama API calls working perfectly\n- Proper model configuration and networking\n\n## Environment\n\n- **LiteLLM Image**: `ghcr.io/berriai/litellm:main-latest`\n- **Ollama Image**: `ollama/ollama:latest`\n- **Model**: `nomic-embed-text` (274MB embedding model)\n- **Network**: Docker Compose with dedicated bridge network\n- **Configuration**: YAML-based config file\n\n## Configuration\n\n### docker-compose.yml\n```yaml\nservices:\n  ollama:\n    image: ollama/ollama:latest\n    environment:\n      - OLLAMA_CONTEXT_LENGTH=8192\n      - OLLAMA_KEEP_ALIVE=10m\n      - OLLAMA_HOST=0.0.0.0\n    ports:\n      - \"127.0.0.1:11434:11434\"\n    networks:\n      - ollama-litellm\n\n  litellm:\n    image: ghcr.io/berriai/litellm:main-latest\n    environment:\n      - LITELLM_MASTER_KEY=sk-test-key-12345\n      - OLLAMA_BASE_URL=http://ollama:11434\n    volumes:\n      - ./config.yaml:/app/config.yaml\n    command: [\"--config\", \"/app/config.yaml\", \"--port\", \"4000\", \"--host\", \"0.0.0.0\"]\n    ports:\n      - \"127.0.0.1:4001:4000\"\n    networks:\n      - ollama-litellm\n    depends_on:\n      ollama:\n        condition: service_healthy\n\nnetworks:\n  ollama-litellm:\n    driver: bridge\n```\n\n### LiteLLM config.yaml\n```yaml\nmodel_list:\n  - model_name: nomic-embed\n    litellm_params:\n      api_base: http://ollama:11434\n      model: ollama/nomic-embed-text\n    model_info:\n      mode: embedding\n      input_cost_per_token: 0\n      output_cost_per_token: 0\n      max_tokens: 2048\n\nrouter_settings:\n  routing_strategy: \"simple\"\n  enable_pre_call_checks: false\n\ngeneral_settings:\n  master_key: sk-test-key-12345\n```\n\n## What Works\n\n‚úÖ **Direct Ollama API calls**:\n```bash\ncurl -X POST http://localhost:11434/api/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"nomic-embed-text\", \"prompt\": \"test\"}'\n# Returns: {\"embedding\": [0.32019859552383423, ...]} (768 dimensions)\n```\n\n‚úÖ **Container networking**:\n```bash\ndocker exec litellm curl -s http://ollama:11434/api/tags\n# Returns: {\"models\":[{\"name\":\"nomic-embed-text:latest\",...}]}\n```\n\n‚úÖ **LiteLLM health checks**:\n```bash\ncurl -H \"Authorization: Bearer sk-test-key-12345\" http://localhost:4001/health\n# Returns: {\"healthy_endpoints\":[{\"api_base\":\"http://ollama:11434\",\"model\":\"ollama/nomic-embed-text\"}],\"healthy_count\":2,\"unhealthy_count\":0}\n```\n\n‚úÖ **Model registration**:\n```bash\ncurl -H \"Authorization: Bearer sk-test-key-12345\" http://localhost:4001/models\n# Returns: {\"data\":[{\"id\":\"nomic-embed\",\"object\":\"model\"}]}\n```\n\n## What Fails\n\n‚ùå **Embedding requests through LiteLLM**:\n```bash\ncurl -X POST http://localhost:4001/v1/embeddings \\\n  -H \"Authorization: Bearer sk-test-key-12345\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"nomic-embed\", \"input\": \"test\"}'\n\n# Error Response:\n{\n  \"error\": {\n    \"message\": \"No deployments available for selected model, Try again in 5 seconds. Passed model=nomic-embed. pre-call-checks=False, cooldown_list=[]\",\n    \"type\": \"None\", \n    \"param\": \"None\",\n    \"code\": \"429\"\n  }\n}\n```\n\n## Error Analysis\n\nThe error occurs in LiteLLM's router logic (`litellm/router.py`):\n```\nRouterRateLimitError: No deployments available for selected model, Try again in 5 seconds. \nPassed model=nomic-embed. pre-call-checks=False, cooldown_list=[]\n```\n\n## Troubleshooting Attempts\n\n1. **Configuration variations tried**:\n   - Different model_name formats: `nomic-embed`, `ollama/nomic-embed-text`\n   - Various `litellm_params` configurations\n   - With/without `litellm_provider` field\n   - Different `api_base` vs `base_url` parameters\n\n2. **Network configurations tried**:\n   - Default Docker Compose network\n   - Dedicated bridge network\n   - Added `OLLAMA_BASE_URL` environment variable\n\n3. **Model pre-loading**:\n   - Pre-warmed model with direct Ollama calls\n   - Verified model is loaded and responsive\n\n4. **LiteLLM variants**:\n   - Tried `ghcr.io/berriai/litellm:main-latest` (basic)\n   - Tried `ghcr.io/berriai/litellm-database:main-latest` (with DB)\n\n## Key Observations\n\n1. **Inconsistent behavior**: Health checks show \"healthy\" but router shows \"no deployments\"\n2. **Perfect connectivity**: All container-to-container communication works\n3. **Model availability**: Ollama serves the model correctly\n4. **Configuration loading**: LiteLLM recognizes and loads the model config\n\n## Potential Root Causes\n\n1. **Router logic bug**: Issue in LiteLLM's deployment availability checking\n2. **Timing issue**: Race condition between health checks and request routing\n3. **Model state synchronization**: Mismatch between health status and router state\n4. **Configuration interpretation**: Router not properly interpreting the deployment config\n\n## Expected Behavior\n\nLiteLLM should route embedding requests to the healthy Ollama endpoint and return successful embedding responses, similar to direct Ollama API calls.\n\n## Impact\n\nThis issue blocks the integration of local embeddings with LiteLLM proxy, forcing applications to either:\n- Use costly external embedding APIs\n- Bypass LiteLLM entirely for embeddings\n- Use separate routing logic for embedding vs completion requests\n\n## Environment Details\n\n- **Platform**: Linux x86_64 (WSL2)\n- **Docker**: Docker Compose\n- **Date**: June 2, 2025\n- **LiteLLM Version**: main-latest (as of June 2025)\n- **Ollama Version**: latest\n\n## Reproduction Steps\n\n1. Set up the provided Docker Compose configuration\n2. Start services and pull `nomic-embed-text` model in Ollama\n3. Verify health checks pass and models are listed\n4. Attempt embedding request through LiteLLM proxy\n5. Observe \"No deployments available\" error despite healthy status\n\nThis issue appears to be a bug in LiteLLM's router logic rather than a configuration problem, as evidenced by the contradiction between health checks and actual request routing.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nmain-latest (as of June 2, 2025)\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "doublefx",
      "author_type": "User",
      "created_at": "2025-06-02T18:08:23Z",
      "updated_at": "2025-06-03T13:49:27Z",
      "closed_at": "2025-06-03T13:49:26Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11330/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11330",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11330",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:32:01.283519",
      "comments": [
        {
          "author": "doublefx",
          "body": "My bad, it took me few days to get it:\n\n`routing_strategy: \"simple\"` is not one of the defined strategy!",
          "created_at": "2025-06-03T13:49:26Z"
        }
      ]
    },
    {
      "issue_number": 8878,
      "title": "[Bug]: router cache MOVED Error with Redis Cluster",
      "body": "### What happened?\n\n#### **Configuration Used:**  \nDocumentation: https://docs.litellm.ai/docs/proxy/caching#redis-cluster\n\n```yaml\nlitellm_settings:\n  cache: True\n  cache_params:\n    type: redis\n    namespace: \"litellm.caching.caching\"\n    redis_startup_nodes:\n      - host: os.environ/REDIS_HOST\n        port: os.environ/REDIS_PORT\nrouter_settings:\n  routing_strategy: usage-based-routing-v2\n  redis_host: os.environ/REDIS_HOST\n  redis_port: os.environ/REDIS_PORT\n  redis_password: os.environ/REDIS_PASSWORD\n  timeout: 600\n  num_retries: 2\n```\n\n#### **Expected Behavior:**  \nRedis caching should function without triggering `MOVED` errors, and cache increments should not fail.  \n\n#### **Steps to Reproduce:**  \n1. Configure LiteLLM with Redis cluster caching (see the configuration above).  \n2. Run requests that trigger cache incrementation.  \n3. Check the logs for `MOVED` errors.  \n\n#### **Environment:**  \n- LiteLLM Version: 1.61.19\n- Redis Version: 7.4.2\n- Redis Configuration: Clustered (redis-cluster bitnami helm chart)\n\nWould it be possible to improve Redis error handling to prevent these failures?\n\n\n### Relevant log output\n\n```shell\n22:24:17 - LiteLLM Router:INFO: router.py:1019 - litellm.acompletion(model=hosted_vllm/mistralai/Mistral-Small-24B-Instruct-2501) 200 OK\nINFO:     172.16.6.127:61733 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n22:24:17 - LiteLLM:ERROR: redis_cache.py:584 - LiteLLM Redis Caching: async async_increment() - Got exception from REDIS MOVED 8116 172.16.7.254:6379, Writing value=1\n22:24:17 - LiteLLM Router:ERROR: router.py:3546 - litellm.router.Router::deployment_callback_on_success(): Exception occured - MOVED 8116 172.16.7.254:6379\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 3531, in deployment_callback_on_success\n    await self.cache.async_increment_cache(\n    ...<4 lines>...\n    )\n  File \"/usr/lib/python3.13/site-packages/litellm/caching/dual_cache.py\", line 374, in async_increment_cache\n    raise e  # don't log if exception is raised\n    ^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/caching/dual_cache.py\", line 365, in async_increment_cache\n    result = await self.redis_cache.async_increment(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/caching/redis_cache.py\", line 589, in async_increment\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/caching/redis_cache.py\", line 547, in async_increment\n    result = await _redis_client.incrbyfloat(name=key, amount=value)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/redis/asyncio/client.py\", line 550, in execute_command\n    return await conn.retry.call_with_retry(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/redis/asyncio/retry.py\", line 59, in call_with_retry\n    return await do()\n           ^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/redis/asyncio/client.py\", line 524, in _send_command_parse_response\n    return await self.parse_response(conn, command_name, **options)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/redis/asyncio/client.py\", line 571, in parse_response\n    response = await connection.read_response()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/redis/asyncio/connection.py\", line 537, in read_response\n    raise response from None\nredis.exceptions.ResponseError: MOVED 8116 172.16.7.254:6379\n22:24:17 - LiteLLM:ERROR: redis_cache.py:584 - LiteLLM Redis Caching: async async_increment() - Got exception from REDIS MOVED 5948 172.16.7.254:6379, Writing value=301\n22:24:17 - LiteLLM:ERROR: lowest_tpm_rpm_v2.py:311 - litellm.proxy.hooks.lowest_tpm_rpm_v2.py::async_log_success_event(): Exception occured - MOVED 5948 172.16.7.254:6379\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/router_strategy/lowest_tpm_rpm_v2.py\", line 300, in async_log_success_event\n    await self.router_cache.async_increment_cache(\n    ...<4 lines>...\n    )\n  File \"/usr/lib/python3.13/site-packages/litellm/caching/dual_cache.py\", line 374, in async_increment_cache\n    raise e  # don't log if exception is raised\n    ^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/caching/dual_cache.py\", line 365, in async_increment_cache\n    result = await self.redis_cache.async_increment(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/caching/redis_cache.py\", line 589, in async_increment\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/caching/redis_cache.py\", line 547, in async_increment\n    result = await _redis_client.incrbyfloat(name=key, amount=value)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/redis/asyncio/client.py\", line 550, in execute_command\n    return await conn.retry.call_with_retry(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/redis/asyncio/retry.py\", line 59, in call_with_retry\n    return await do()\n           ^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/redis/asyncio/client.py\", line 524, in _send_command_parse_response\n    return await self.parse_response(conn, command_name, **options)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/redis/asyncio/client.py\", line 571, in parse_response\n    response = await connection.read_response()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/redis/asyncio/connection.py\", line 537, in read_response\n    raise response from None\nredis.exceptions.ResponseError: MOVED 5948 172.16.7.254:6379\n22:24:18 - LiteLLM Router:INFO: router.py:1019 - litellm.acompletion(model=hosted_vllm/mistralai/Mistral-Small-24B-Instruct-2501) 200 OK\nINFO:     172.16.6.127:61727 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n22:24:18 - LiteLLM:ERROR: redis_cache.py:584 - LiteLLM Redis Caching: async async_increment() - Got exception from REDIS MOVED 8116 172.16.7.254:6379, Writing value=1\n22:24:18 - LiteLLM Router:ERROR: router.py:3546 - litellm.router.Router::deployment_callback_on_success(): Exception occured - MOVED 8116 172.16.7.254:6379\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 3531, in deployment_callback_on_success\n    await self.cache.async_increment_cache(\n    ...<4 lines>...\n    )\n  File \"/usr/lib/python3.13/site-packages/litellm/caching/dual_cache.py\", line 374, in async_increment_cache\n    raise e  # don't log if exception is raised\n    ^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/caching/dual_cache.py\", line 365, in async_increment_cache\n    result = await self.redis_cache.async_increment(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/caching/redis_cache.py\", line 589, in async_increment\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/caching/redis_cache.py\", line 547, in async_increment\n    result = await _redis_client.incrbyfloat(name=key, amount=value)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/redis/asyncio/client.py\", line 550, in execute_command\n    return await conn.retry.call_with_retry(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/redis/asyncio/retry.py\", line 59, in call_with_retry\n    return await do()\n           ^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/redis/asyncio/client.py\", line 524, in _send_command_parse_response\n    return await self.parse_response(conn, command_name, **options)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/redis/asyncio/client.py\", line 571, in parse_response\n    response = await connection.read_response()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/redis/asyncio/connection.py\", line 537, in read_response\n    raise response from None\nredis.exceptions.ResponseError: MOVED 8116 172.16.7.254:6379\n22:24:18 - LiteLLM:ERROR: redis_cache.py:584 - LiteLLM Redis Caching: async async_increment() - Got exception from REDIS MOVED 5948 172.16.7.254:6379, Writing value=326\n22:24:18 - LiteLLM:ERROR: lowest_tpm_rpm_v2.py:311 - litellm.proxy.hooks.lowest_tpm_rpm_v2.py::async_log_success_event(): Exception occured - MOVED 5948 172.16.7.254:6379\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/router_strategy/lowest_tpm_rpm_v2.py\", line 300, in async_log_success_event\n    await self.router_cache.async_increment_cache(\n    ...<4 lines>...\n    )\n  File \"/usr/lib/python3.13/site-packages/litellm/caching/dual_cache.py\", line 374, in async_increment_cache\n    raise e  # don't log if exception is raised\n    ^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/caching/dual_cache.py\", line 365, in async_increment_cache\n    result = await self.redis_cache.async_increment(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/caching/redis_cache.py\", line 589, in async_increment\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/caching/redis_cache.py\", line 547, in async_increment\n    result = await _redis_client.incrbyfloat(name=key, amount=value)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/redis/asyncio/client.py\", line 550, in execute_command\n    return await conn.retry.call_with_retry(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/redis/asyncio/retry.py\", line 59, in call_with_retry\n    return await do()\n           ^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/redis/asyncio/client.py\", line 524, in _send_command_parse_response\n    return await self.parse_response(conn, command_name, **options)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/redis/asyncio/client.py\", line 571, in parse_response\n    response = await connection.read_response()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/redis/asyncio/connection.py\", line 537, in read_response\n    raise response from None\nredis.exceptions.ResponseError: MOVED 5948 172.16.7.254:6379\n22:24:18 - LiteLLM Router:INFO: router.py:1019 - litellm.acompletion(model=hosted_vllm/mistralai/Mistral-Small-24B-Instruct-2501) 200 OK\nINFO:     172.16.6.127:61761 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n22:24:18 - LiteLLM:ERROR: redis_cache.py:584 - LiteLLM Redis Caching: async async_increment() - Got exception from REDIS MOVED 8116 172.16.7.254:6379, Writing value=1\n22:24:18 - LiteLLM Router:ERROR: router.py:3546 - litellm.router.Router::deployment_callback_on_success(): Exception occured - MOVED 8116 172.16.7.254:6379\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/router.py\", line 3531, in deployment_callback_on_success\n    await self.cache.async_increment_cache(\n    ...<4 lines>...\n    )\n  File \"/usr/lib/python3.13/site-packages/litellm/caching/dual_cache.py\", line 374, in async_increment_cache\n    raise e  # don't log if exception is raised\n    ^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/caching/dual_cache.py\", line 365, in async_increment_cache\n    result = await self.redis_cache.async_increment(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/caching/redis_cache.py\", line 589, in async_increment\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/caching/redis_cache.py\", line 547, in async_increment\n    result = await _redis_client.incrbyfloat(name=key, amount=value)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/redis/asyncio/client.py\", line 550, in execute_command\n    return await conn.retry.call_with_retry(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/redis/asyncio/retry.py\", line 59, in call_with_retry\n    return await do()\n           ^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/redis/asyncio/client.py\", line 524, in _send_command_parse_response\n    return await self.parse_response(conn, command_name, **options)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/redis/asyncio/client.py\", line 571, in parse_response\n    response = await connection.read_response()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/redis/asyncio/connection.py\", line 537, in read_response\n    raise response from None\nredis.exceptions.ResponseError: MOVED 8116 172.16.7.254:6379\n22:24:18 - LiteLLM:ERROR: redis_cache.py:584 - LiteLLM Redis Caching: async async_increment() - Got exception from REDIS MOVED 5948 172.16.7.254:6379, Writing value=329\n22:24:18 - LiteLLM:ERROR: lowest_tpm_rpm_v2.py:311 - litellm.proxy.hooks.lowest_tpm_rpm_v2.py::async_log_success_event(): Exception occured - MOVED 5948 172.16.7.254:6379\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/router_strategy/lowest_tpm_rpm_v2.py\", line 300, in async_log_success_event\n    await self.router_cache.async_increment_cache(\n    ...<4 lines>...\n    )\n  File \"/usr/lib/python3.13/site-packages/litellm/caching/dual_cache.py\", line 374, in async_increment_cache\n    raise e  # don't log if exception is raised\n    ^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/caching/dual_cache.py\", line 365, in async_increment_cache\n    result = await self.redis_cache.async_increment(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/caching/redis_cache.py\", line 589, in async_increment\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/caching/redis_cache.py\", line 547, in async_increment\n    result = await _redis_client.incrbyfloat(name=key, amount=value)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/redis/asyncio/client.py\", line 550, in execute_command\n    return await conn.retry.call_with_retry(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/redis/asyncio/retry.py\", line 59, in call_with_retry\n    return await do()\n           ^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/redis/asyncio/client.py\", line 524, in _send_command_parse_response\n    return await self.parse_response(conn, command_name, **options)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/redis/asyncio/client.py\", line 571, in parse_response\n    response = await connection.read_response()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/redis/asyncio/connection.py\", line 537, in read_response\n    raise response from None\nredis.exceptions.ResponseError: MOVED 5948 172.16.7.254:6379\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.61.19\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "subnet-dev",
      "author_type": "User",
      "created_at": "2025-02-27T22:41:45Z",
      "updated_at": "2025-06-03T13:36:24Z",
      "closed_at": null,
      "labels": [
        "bug",
        "march 2025"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/8878/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "ishaan-jaff"
      ],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/8878",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/8878",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:32:01.488037",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "cc: @ishaan-jaff for redis cluster issue",
          "created_at": "2025-02-28T07:16:14Z"
        },
        {
          "author": "subnet-dev",
          "body": "It appears that the bug is not limited to the cache but also involves the router.\n\n```bash\n[pod/litellm-deployment-64fb86bc77-wpk8g/litellm] 08:56:55 - LiteLLM Router:ERROR: router.py:3546 - litellm.router.Router::deployment_callback_on_success(): Exception occured - MOVED 4289 172.16.5.19:6379\n[pod",
          "created_at": "2025-02-28T09:00:55Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "can you bump to latest ? ",
          "created_at": "2025-03-10T15:16:52Z"
        },
        {
          "author": "ishaan-jaff",
          "body": "added to march 2025 roadmap ",
          "created_at": "2025-03-10T15:18:41Z"
        },
        {
          "author": "subnet-dev",
          "body": "Could you please provide an update on the status of this issue?",
          "created_at": "2025-06-03T13:36:23Z"
        }
      ]
    },
    {
      "issue_number": 11363,
      "title": "[Bug]: Prisma doesn't know which engines to download for the Linux distro \"wolfi\"",
      "body": "### What happened?\n\nprisma:warn Prisma doesn't know which engines to download for the Linux distro \"wolfi\". Falling back to Prisma engines built \"debian\".\nPlease report your experience by creating an issue at https://github.com/prisma/prisma/issues so we can add your distro to the list of known supported distros.\nprisma:warn Prisma doesn't know which engines to download for the Linux distro \"wolfi\". Falling back to Prisma engines built \"debian\".\nPlease report your experience by creating an issue at https://github.com/prisma/prisma/issues so we can add your distro to the list of known supported distros.\nPrisma schema loaded from schema.prisma\nDatasource \"client\": PostgreSQL database \"litellm_db\", schema \"public\" at \"postgres:5432\"\nThe database is already in sync with the Prisma schema.\nRunning generate... (Use --skip-generate to skip the generators)\nRunning generate... - Prisma Client Python (v0.11.0)\nSome types are disabled by default due to being incompatible with Mypy, it is highly recommended\nto use Pyright instead and configure Prisma Python to use recursive types. To re-enable certain types:\ngenerator client {\n  provider             = \"prisma-client-py\"\n  recursive_type_depth = -1\n}\nIf you need to use Mypy, you can also disable this message by explicitly setting the default value:\ngenerator client {\n  provider             = \"prisma-client-py\"\n  recursive_type_depth = 5\n}\nFor more information see: https://prisma-client-py.readthedocs.io/en/stable/reference/limitations/#default-type-limitations\n‚úî Generated Prisma Client Python (v0.11.0) to ./../../prisma in 549ms\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.72\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "ocristaldo",
      "author_type": "User",
      "created_at": "2025-06-03T13:18:00Z",
      "updated_at": "2025-06-03T13:18:00Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11363/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11363",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11363",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:32:01.697685",
      "comments": []
    },
    {
      "issue_number": 11361,
      "title": "[Feature]: Ability to provide a custom Email template",
      "body": "### The Feature\n\nI would like the ability to provide custom Email templates.  Currently all emails state they are coming from the Litellm team.\n\n### Motivation, pitch\n\nThis would allow me to further customize the enterprise version of the product and allow use to really own it and solve the issue where it looks like all emails are coming from the litellm team.\n\n\n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "ncecere",
      "author_type": "User",
      "created_at": "2025-06-03T12:37:31Z",
      "updated_at": "2025-06-03T12:40:33Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11361/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11361",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11361",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:32:01.697707",
      "comments": [
        {
          "author": "ncecere",
          "body": "Simple things like being abble to set the Subject and Signature would be good starting points",
          "created_at": "2025-06-03T12:40:32Z"
        }
      ]
    },
    {
      "issue_number": 11357,
      "title": "[Feature]: Add the prediction argument to OpenAI ChatCompletions",
      "body": "### The Feature\n\nThis feature adds a new ChatCompletion argument that could be used in OpenAI models.\n\n### Motivation, pitch\n\nThe predictions arguments allow us to prefix the output generation, which could reduce a lot the inference time from some predictions.\nBellow is the description of this param given by the OpenAI team\n\n> Configuration for a [Predicted Output](https://platform.openai.com/docs/guides/predicted-outputs), which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content.\n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "SSamDav",
      "author_type": "User",
      "created_at": "2025-06-03T10:55:29Z",
      "updated_at": "2025-06-03T10:55:29Z",
      "closed_at": null,
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11357/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11357",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11357",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:32:01.871121",
      "comments": []
    },
    {
      "issue_number": 11354,
      "title": "[Feature]: Support pydantic RootModels for response formats",
      "body": "### The Feature\n\n`response_format` parameter for `acompletion` etc should accept pydantic `RootModel`s (it already supports `BaseModel`s).\n\n([RootModel docs](https://docs.pydantic.dev/latest/api/root_model/))\n\n### Motivation, pitch\n\nRootModels allow greater flexibility over the response format, and in our case was necessary to use to get the output we wanted.\n\nNotably, following type of schema is not representable with BaseModels alone:\n\n```python\nclass ExampleSchema(RootModel):\n    root: dict[str, list[str]]\n```\n\nWe've confirmed that this returns the expected responses with Anthropic models when we manually pass the following as the response_type to `litellm.acompletion`:\n\n```python\n{\n    \"type\": \"json_schema\",\n    \"json_schema\": {\"schema\": {**ExampleSchema.model_json_schema()}, \"strict\": True}\n}\n```\n\n### Proposed approach to this feature\n\nThe change would mainly happen here:\n\nhttps://github.com/BerriAI/litellm/blob/v1.72.0.rc1/litellm/llms/base_llm/base_utils.py#L139-L172\n\n~~Annoyingly, `is_basemodel_type`, `to_strict_json_schema` are relevant functions here and they're just openai library internal functions ([source for them here](https://github.com/openai/openai-python/blob/v1.83.0/src/openai/lib/_pydantic.py#L132-L135)), so it's not super trivial, they'd need some local equivalents.~~\n\n~~At the very least, the most complex function there, `_ensure_strict_json_schema` (called by `to_strict_json_schema`), should be compatible with RootModel, making it possible to achieve this change by only adding equivalents of the must smaller and simpler `is_basemodel_type` and `to_strict_json_schema` functions.~~\n\nNevermind:\n\n```python\n>>> class ExampleSchema(RootModel):\n...     root: dict[str, list[str]]\n... \n>>> _parsing._completions.is_basemodel_type(ExampleSchema)\nTrue\n>>> _pydantic.to_strict_json_schema(ExampleSchema)\n{'additionalProperties': {'items': {'type': 'string'}, 'type': 'array'}, 'title': 'ExampleSchema', 'type': 'object'}\n```\n\nEdit: Hmm, I think this may not even be needed in the first place? Will give it one more check, and probably close this. Sorry!\n\nEdit: Yeah, sorry, I was mistakenly checking if the type hinting would match with `isinstance`, not `issubclass`. This should indeed work. My bad!",
      "state": "closed",
      "author": "aveao",
      "author_type": "User",
      "created_at": "2025-06-03T09:59:24Z",
      "updated_at": "2025-06-03T10:11:34Z",
      "closed_at": "2025-06-03T10:11:02Z",
      "labels": [
        "enhancement"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/11354/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/11354",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/11354",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:32:01.871145",
      "comments": []
    },
    {
      "issue_number": 10010,
      "title": "[Bug]: Bedrock pass-through endpoint fails with \"Too little data for declared Content-Length\" error",
      "body": "### What happened?\n\n## Description\n### Issue Description\nWhen using the LiteLLM proxy's Bedrock pass-through endpoint, requests fail with the error: \nToo little data for declared Content-Length. This occurs because of a mismatch between the Content-Length header calculated \nduring AWS SigV4 authentication and the actual JSON data being sent in the HTTP request.\n\n### Environment\n‚Ä¢ LiteLLM version: 1.66.0\n‚Ä¢ Python version: 3.11\n‚Ä¢ Operating system: Linux\n‚Ä¢ AWS Bedrock model: anthropic.claude-v2\n\n### Steps to Reproduce\n1. Set up LiteLLM proxy with Bedrock pass-through endpoint\n2. Make a request to the Bedrock endpoint, for example:\njson\n{\n  \"prompt\": \"\\n\\nHuman: Tell me a short joke\\n\\nAssistant:\",\n  \"max_tokens_to_sample\": 50,\n  \"temperature\": 0.7,\n  \"top_p\": 0.9\n}\n\n3. Observe the error in the logs: Too little data for declared Content-Length\n\n\n### Root Cause Analysis\nThe issue occurs because:\n1. During AWS SigV4 authentication, the Content-Length header is calculated based on the JSON-serialized request body (121 \nbytes)\n2. When the actual HTTP request is made using the json parameter, httpx re-serializes the JSON, resulting in a different \nstring (114 bytes)\n3. This mismatch between the Content-Length header (121) and the actual data size (114) causes the error\n\n\n### Relevant log output\n\n```shell\n### Error Logs\nLiteLLM Proxy:DEBUG: pass_through_endpoints.py:688 - Defaulting to target being a url.\nLiteLLM Proxy:DEBUG: pass_through_endpoints.py:373 - Pass through endpoint sending request to \nURL https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-v2/invoke\nheaders: {'Content-Type': 'application/json', 'X-Amz-Date': '20250414T174255Z', 'Authorization': 'AWS4-HMAC-SHA256 Credential=AKIAQ6RMICMJ524AJ6B6/20250414/us-east-1/bedrock/aws4_request, SignedHeaders=content-type;host;x-amz-date, Signature=218015e95666d5b125c2e46ff224f12ebcc7f65c41e421f1a6d4de2e9dba4a58', 'Content-Length': '121'}\nbody: {'prompt': '\\n\\nHuman: Tell me a short joke\\n\\nAssistant:', 'max_tokens_to_sample': 50, 'temperature': 0.7, 'top_p': 0.9}\n\nLiteLLM Proxy:ERROR: pass_through_endpoints.py:580 - litellm.proxy.proxy_server.pass_through_endpoint(): Exception occured - Too little data for declared Content-Length\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.66.0\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "open",
      "author": "RohitPanda",
      "author_type": "User",
      "created_at": "2025-04-15T08:39:53Z",
      "updated_at": "2025-06-03T08:35:42Z",
      "closed_at": null,
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/10010/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/10010",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/10010",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:32:01.871153",
      "comments": [
        {
          "author": "wirjo",
          "body": "+1",
          "created_at": "2025-04-15T13:45:43Z"
        },
        {
          "author": "martin-rounds-ai",
          "body": "+1",
          "created_at": "2025-05-26T17:18:57Z"
        },
        {
          "author": "chuangfengwang",
          "body": "+1 but `Too much` when using litellm SDK, in tornado 6.4.2.\nBedrockException - Too much data for declared Content-Length\n",
          "created_at": "2025-05-29T10:38:59Z"
        },
        {
          "author": "brunonunes",
          "body": "+1",
          "created_at": "2025-05-29T17:56:32Z"
        },
        {
          "author": "kaka-srp",
          "body": "+1",
          "created_at": "2025-06-03T08:35:40Z"
        }
      ]
    },
    {
      "issue_number": 9787,
      "title": "[Bug]:  model_cost_map_url is forced downloaded causes slowdown in server startup and dev server refresh",
      "body": "### What happened?\n\nThis file is download every time client is instantiated.\n\"https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json\"\ncaused by\nhttps://github.com/BerriAI/litellm/blob/e67d16d5bd3387fd7eccca19594e8f8d3f89f144/litellm/__init__.py#L312\n\nsoln:  allow user to disable the behavior, use local file, cache file\n\n### Relevant log output\n\n```shell\n-\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.65.4\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "Lodimup",
      "author_type": "User",
      "created_at": "2025-04-06T03:45:08Z",
      "updated_at": "2025-06-03T08:27:27Z",
      "closed_at": "2025-04-06T15:38:03Z",
      "labels": [
        "bug",
        "mlops user request"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9787/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9787",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9787",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:32:02.040108",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Hi @Lodimup this is already supported - https://docs.litellm.ai/docs/completion/token_usage#9-register_model\n\n<img width=\"994\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/0908dd63-0db2-4d94-b3d4-2b9cdfb601bc\" />",
          "created_at": "2025-04-06T15:38:01Z"
        },
        {
          "author": "tylerseymour",
          "body": "@krrishdholakia Hey, thank you for all the work you put into LiteLLM.\n\nWas hoping you'd be open to further discussion on this approach.  To my experience, import-time network I/O is generally discouraged because it:\n\n1. makes startup nondeterministic\n2. ties performance to external connectivity\n3. s",
          "created_at": "2025-06-03T08:27:26Z"
        }
      ]
    },
    {
      "issue_number": 9939,
      "title": "[Bug]: Conversation Titles Not Generated When Using Claude Sonnet 3.7 Thinking Model",
      "body": "### What happened?\n\n#### Description\nI've successfully integrated Anthropic's Claude Sonnet 3.7 thinking model into LiteLLM and am using it with Open WebUI. While I've enabled the thinking functionality by passing a JSON configuration through LiteLLM params, conversation titles are not being generated as expected.\n\n#### Steps to Reproduce\n1. Configured LiteLLM with Claude Sonnet 3.7 thinking model\n2. Enabled thinking via JSON params (LiteLLM params)\n3. Initiated conversations in Open WebUI\n4. Observed that conversations remain untitled despite the thinking model being active\n\n#### Expected Behavior\nConversations should receive automatic titles based on the initial prompt/content, as happens with other models.\n\n#### Current Behavior\nNo titles are generated for new conversations.\n\n#### Additional Context\nLiteLLM version:  v1.65.4 (nightly)\nThis problem is not occurred when using other thinking models such as Grok's. \n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nghcr.io/berriai/litellm:main-v1.65.4-nightly\n\n### Twitter / LinkedIn details\n\n_No response_",
      "state": "closed",
      "author": "okaeiz",
      "author_type": "User",
      "created_at": "2025-04-12T12:07:23Z",
      "updated_at": "2025-06-03T07:27:15Z",
      "closed_at": "2025-04-12T15:15:41Z",
      "labels": [
        "bug"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/BerriAI/litellm/issues/9939/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/BerriAI/litellm/issues/9939",
      "api_url": "https://api.github.com/repos/BerriAI/litellm/issues/9939",
      "repository": "BerriAI/litellm",
      "extraction_date": "2025-06-21T23:32:02.223156",
      "comments": [
        {
          "author": "krrishdholakia",
          "body": "Hey @okaeiz this seems like a bug for openwebui.\n\nIf you see a litellm related error in your server logs feel free to reopen the ticket. ",
          "created_at": "2025-04-12T15:15:39Z"
        },
        {
          "author": "okaeiz",
          "body": "Looking at the container logs, it seems like there's a similar problem to #9001 and #8951. Here you can see the complete container logs in the [Open WebUI discussion](https://github.com/open-webui/open-webui/discussions/12779).\n\nI'd appreciate it if you took a look at this @krrishdholakia ",
          "created_at": "2025-04-13T09:55:18Z"
        },
        {
          "author": "krrishdholakia",
          "body": "@okaeiz can you share the error you see on litellm server logs?\n\nAnd confirm this persists on v1.66.0-stable",
          "created_at": "2025-04-13T17:27:35Z"
        },
        {
          "author": "okaeiz",
          "body": "The issue is still present on `v1.66.0-stable`. The exceptions are apparently raised in fastapi code:\n```bash\n    res = await generate_title(\n\n                ‚îî <function generate_title at 0x7b9f7206e5c0>\n\n> File \"/app/backend/open_webui/routers/tasks.py\", line 235, in generate_title\n\n    return awa",
          "created_at": "2025-04-17T10:05:48Z"
        },
        {
          "author": "arunbugkiller",
          "body": "@okaeiz were you able to solve this issue? I suggest using a smaller model for \"Title Generation\" tasks. \n\nAlso, in OpenWebUI if you are using \"Thinking\" models are you able to view the thinking process? If yes, how did you enable that. ",
          "created_at": "2025-06-03T05:21:10Z"
        }
      ]
    }
  ]
}