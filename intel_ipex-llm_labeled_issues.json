{
  "repository": "intel/ipex-llm",
  "repository_info": {
    "repo": "intel/ipex-llm",
    "stars": 8042,
    "language": "Python",
    "description": "Accelerate local LLM inference and finetuning (LLaMA, Mistral, ChatGLM, Qwen, DeepSeek, Mixtral, Gemma, Phi, MiniCPM, Qwen-VL, MiniCPM-V, etc.) on Intel XPU (e.g., local PC with iGPU and NPU, discrete",
    "url": "https://github.com/intel/ipex-llm",
    "topics": [
      "gpu",
      "llm",
      "pytorch",
      "transformers"
    ],
    "created_at": "2016-08-29T07:59:50Z",
    "updated_at": "2025-06-21T13:29:17Z",
    "search_query": "ollama language:python stars:>2",
    "total_issues_estimate": 102,
    "labeled_issues_estimate": 81,
    "labeling_rate": 80.0,
    "sample_labeled": 28,
    "sample_total": 35,
    "has_issues": true,
    "repo_id": 66823715,
    "default_branch": "main",
    "size": 237782
  },
  "extraction_date": "2025-06-22T00:32:47.673714",
  "extraction_type": "LABELED_ISSUES_ONLY",
  "total_labeled_issues": 500,
  "issues": [
    {
      "issue_number": 13226,
      "title": "Training doesn't start QLoRA fine tune BMG",
      "body": "**Describe the bug**\nI want to run the simplest example of fine tune on a B580\nI am following this page https://github.com/intel/ipex-llm/blob/main/python/llm/example/GPU/LLM-Finetuning/QLoRA/simple-example/README.md and I struggled a lot with the libraries and finally made the code run I guess but getting an issue and it doesn't allow to start the training.\n\nOutput:\n(fine-tune) PS C:\\llm\\ipex-llm\\python\\llm\\example\\GPU\\LLM-Finetuning\\QLoRA\\simple-example> & C:/Users/Carlos/miniforge3/envs/fine-tune/python.exe c:/llm/ipex-llm/python/llm/example/GPU/LLM-Finetuning/QLoRA/simple-example/qlora_finetuning.py  --repo-id-or-model-path meta-llama/Llama-2-7b-hf\nC:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\transformers\\deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n  warnings.warn(\n2025-06-17 19:06:43,080 - INFO - PyTorch version 2.6.0+xpu available.\nMap: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5176/5176 [00:04<00:00, 1292.62 examples/s]\nC:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nLoading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  6.79it/s]\n2025-06-17 19:06:58,960 - INFO - Converting the current model to nf4 format......\nC:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\transformers\\utils\\import_utils.py:533: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.\n  warnings.warn(\nValue: False\nmax_steps is given, it will override any value given in num_train_epochs\nC:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n  0%|                                                                                                                                                       | 0/200 [00:00<?, ?it/s]C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\transformers\\trainer.py:3206: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  ctx_manager = torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n2025-06-17 19:07:11,544 - ERROR - \n\n****************************Usage Error************************\nNF3, NF4, FP4 and FP8 quantization are currently not supported on CPU\n2025-06-17 19:07:11,544 - ERROR -\n\n****************************Call Stack*************************\nTraceback (most recent call last):\n  File \"c:\\llm\\ipex-llm\\python\\llm\\example\\GPU\\LLM-Finetuning\\QLoRA\\simple-example\\qlora_finetuning.py\", line 114, in <module>\n    result = trainer.train()\n             ^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\transformers\\trainer.py\", line 1885, in train\n    return inner_training_loop(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\transformers\\trainer.py\", line 2216, in _inner_training_loop\n    tr_loss_step = self.training_step(model, inputs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\transformers\\trainer.py\", line 3238, in training_step\n    loss = self.compute_loss(model, inputs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\transformers\\trainer.py\", line 3264, in compute_loss\n    outputs = model(**inputs)\n              ^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\accelerate\\utils\\operations.py\", line 817, in forward\n    return model_forward(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\accelerate\\utils\\operations.py\", line 805, in __call__\n    return convert_to_fp32(self.model_forward(*args, **kwargs))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\torch\\amp\\autocast_mode.py\", line 44, in decorate_autocast\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\peft\\peft_model.py\", line 1430, in forward\n    return self.base_model(\n           ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py\", line 179, in forward\n    return self.model.forward(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\", line 1164, in forward\n    outputs = self.model(\n              ^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\", line 957, in forward\n    layer_outputs = self._gradient_checkpointing_func(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\torch\\_compile.py\", line 32, in inner\n    return disable_fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 745, in _fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\torch\\utils\\checkpoint.py\", line 489, in checkpoint\n    return CheckpointFunction.apply(function, preserve, *args)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\torch\\autograd\\function.py\", line 575, in apply\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\torch\\utils\\checkpoint.py\", line 264, in forward\n    outputs = run_function(*args)\n              ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\", line 713, in forward\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n                                                          ^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\", line 615, in forward\n    query_states = self.q_proj(hidden_states)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl    \n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\ipex_llm\\transformers\\qlora.py\", line 118, in forward\n    result = self.base_layer.forward(x)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\ipex_llm\\transformers\\low_bit_linear.py\", line 729, in forward   \n    invalidInputError(self.qtype not in [NF3, NF4, FP8E4, FP4, FP8E5,\n  File \"C:\\Users\\Carlos\\miniforge3\\envs\\fine-tune\\Lib\\site-packages\\ipex_llm\\utils\\common\\log4Error.py\", line 32, in invalidInputError\n    raise RuntimeError(errMsg)\nRuntimeError: NF3, NF4, FP4 and FP8 quantization are currently not supported on CPU\n  0%|          | 0/200 [00:00<?, ?it/s]\n\nSetup:\n\nconda create -n fine-tune python=3.11\nconda activate fine-tune\npip install --pre --upgrade ipex-llm[xpu_2.6] --extra-index-url https://download.pytorch.org/whl/xpu\npip install transformers==4.41.0 \"trl==0.8.6\" datasets==2.19.0\npip install peft==0.11.0\npip install bitsandbytes==0.43.1 scipy==1.13.0\npip install accelerate==0.27.2\n\nBut now I am getting this issue:\n\n**How to reproduce**\nSteps to reproduce the error:\n1. ... setup the env and install all dependencies above\n2. ... Run this command qlora_finetuning.py  --repo-id-or-model-path meta-llama/Llama-2-7b-hf\n\n**Environment information**\nIf possible, please attach the output of the environment check script, using:\n- https://github.com/intel/ipex-llm/blob/main/python/llm/scripts/env-check.bat, or\n- https://github.com/intel/ipex-llm/blob/main/python/llm/scripts/env-check.sh\n\n-----------------------------------------------------------------\nSystem Information\n\nOS Name:                       Microsoft Windows 11 Pro\nOS Version:                    10.0.26100 N/A Build 26100\nOS Manufacturer:               Microsoft Corporation\nOS Configuration:              Standalone Workstation\nOS Build Type:                 Multiprocessor Free\nRegistered Owner:              Carlos\nRegistered Organization:       N/A\nProduct ID:                    00331-20316-09538-AA687\nOriginal Install Date:         12/2/2025, 17:54:03\nSystem Boot Time:              17/6/2025, 10:08:58\nSystem Manufacturer:           AZW\nSystem Model:                  GTi14\nSystem Type:                   x64-based PC\nProcessor(s):                  1 Processor(s) Installed.\n                               [01]: Intel64 Family 6 Model 170 Stepping 4 GenuineIntel ~3100 Mhz\nBIOS Version:                  American Megatrends International, LLC. GTi14T201, 23/9/2024\nWindows Directory:             C:\\WINDOWS\nSystem Directory:              C:\\WINDOWS\\system32\nBoot Device:                   \\Device\\HarddiskVolume1\nSystem Locale:                 en-us;English (United States)\nInput Locale:                  es;Spanish (Traditional Sort)\nTime Zone:                     (UTC-06:00) Central America\nTotal Physical Memory:         32 438 MB\nAvailable Physical Memory:     20 664 MB\nVirtual Memory: Max Size:      51 894 MB\nVirtual Memory: Available:     36 383 MB\nVirtual Memory: In Use:        15 511 MB\nPage File Location(s):         C:\\pagefile.sys\nDomain:                        WORKGROUP\nLogon Server:                  \\\\DESKTOP-4JPKTVJ\nHotfix(s):                     3 Hotfix(s) Installed.\n                               [01]: KB5054979\n                               [02]: KB5058411\n                               [03]: KB5059502\nNetwork Card(s):               3 NIC(s) Installed.\n                               [01]: Intel(R) Ethernet Controller I226-V\n                                     Connection Name: Ethernet\n                                     Status:          Media disconnected\n                               [02]: Intel(R) Ethernet Controller I226-V\n                                     Connection Name: Ethernet 2\n                                     Status:          Media disconnected\n                               [03]: Bluetooth Device (Personal Area Network)\n                                     Connection Name: Bluetooth Network Connection\n                                     Status:          Media disconnected\nVirtualization-based security: Status: Running\n                               Required Security Properties:\n                                     Base Virtualization Support\n                               Available Security Properties:\n                                     Base Virtualization Support\n                                     DMA Protection\n                                     UEFI Code Readonly\n                                     SMM Security Mitigations 1.0\n                                     Mode Based Execution Control\n                                     APIC Virtualization\n                               Services Configured:\n                                     Hypervisor enforced Code Integrity\n                               Services Running:\n                                     Hypervisor enforced Code Integrity\n                                     Hypervisor-Enforced Paging Translation\n                               App Control for Business policy: Enforced\n                               App Control for Business user mode policy: Off\n                               Security Features Enabled:\nHyper-V Requirements:          A hypervisor has been detected. Features required for Hyper-V will not be displayed.\n-----------------------------------------------------------------\n'xpu-smi' is not recognized as an internal or external command,\noperable program or batch file.\nxpu-smi is not installed properly.\n",
      "state": "open",
      "author": "carlosrojasg",
      "author_type": "User",
      "created_at": "2025-06-18T01:18:16Z",
      "updated_at": "2025-06-21T19:57:52Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13226/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13226",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13226",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:25.112864",
      "comments": [
        {
          "author": "Uxito-Ada",
          "body": "Hi @carlosrojasg ,\n\n```bash\nNF3, NF4, FP4 and FP8 quantization are currently not supported on CPU\n```\n\nAs shown above, the model was running on CPU instead of XPU, where NF4, 4bit type of QLoRA, is not supported on CPU.\n\nI guess you may annotate the necessary [line](https://github.com/intel/ipex-llm",
          "created_at": "2025-06-18T01:52:23Z"
        },
        {
          "author": "carlosrojasg",
          "body": "> Hi [@carlosrojasg](https://github.com/carlosrojasg) ,\n> \n> NF3, NF4, FP4 and FP8 quantization are currently not supported on CPU\n> \n> As shown above, the model was running on CPU instead of XPU, where NF4, 4bit type of QLoRA, is not supported on CPU.\n> \n> I guess you may annotate the necessary [li",
          "created_at": "2025-06-18T02:54:40Z"
        },
        {
          "author": "Uxito-Ada",
          "body": "Hi @carlosrojasg ,\n\nThe model.parameters().device is managed by torch, which ipex-llm low bit does not operate. By contrast, the log tells that it fell to [LowBitLinear's CPU logic](https://github.com/intel/ipex-llm/blob/main/python/llm/src/ipex_llm/transformers/low_bit_linear.py#L727) because [the ",
          "created_at": "2025-06-19T02:26:01Z"
        }
      ]
    },
    {
      "issue_number": 13223,
      "title": "ollama ipex-llm crashes when running tinyllama llama 3.1b and qwen 2.5",
      "body": "**Note to Maintainers:** I previously filed this bug as issue #13221. It was closed, but no comments or explanation were visible on the issue page, preventing me from understanding the resolution or next steps. I am re-filing this detailed report to ensure it is seen and addressed. I hope I have not failed to follow a necessary protocol of some kind.  Please advise me if I have.\n\n1. Problem Description:\n\nWhen attempting to run any model (e.g., tinyllama, llama3.1, qwen2.5:7b) using the ollama-ipex-llm optimized builds on an Intel Arc A770 discrete GPU, the llama runner process consistently terminates with a SYCL error during the ggml_sycl_op_mul_mat operation. intel_gpu_top shows blitter activity, indicating data transfer to the GPU, but no significant \"Render/3D\" or \"Compute\" utilization occurs before the crash. This issue persists across different IPEX-LLM build versions and various models.\n\n2. Steps to Reproduce:\n\nOperating System: Clean installation of Ubuntu 24.04 LTS (Noble Numbat).\n\nKernel Version: 6.11.0-26-generic.\n\nIntel GPU Driver Installation:\n\nintel-opencl-icd was installed via sudo apt install intel-opencl-icd.\nLatest discrete GPU drivers (25.18.33578.6) were manually downloaded from Intel's website and installed.\nIPEX-LLM Ollama Setup (Primary Test: Build 2.3.0b20250612):\n\nDownloaded ollama-ipex-llm-2.3.0b20250612-ubuntu.tgz.\nExtracted to ~/ollama_ipex/ollama-ipex-llm-2.3.0b20250612-ubuntu/.\nCreated /usr/local/bin/start_ollama_ipex.sh wrapper script and /etc/systemd/system/ollama.service systemd unit file.\nThe start_ollama_ipex.sh script included all export variables from the IPEX-LLM's start-ollama.sh (e.g., OLLAMA_NUM_GPU=999, ZES_ENABLE_SYSMAN=1, SYCL_CACHE_PERSISTENT=1, OLLAMA_KEEP_ALIVE=10m).\nSYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1 was tested (uncommented, then re-commented), but showed no change in behavior.\nThe systemd service was configured to run as user michael-fothergill and point to /usr/local/bin/start_ollama_ipex.sh.\nModel Run Attempts (Build 2.3.0b20250612):\n\nollama pull tinyllama\nollama run tinyllama \"Hello, what is your purpose?\"\nResult: SYCL error termination.\nollama pull llama3.1\nollama run llama3.1 \"Write a short, evocative poem...\"\nResult: SYCL error termination.\nollama pull qwen2.5:7b\nollama run qwen2.5:7b \"What is the capital of France?\"\nResult: SYCL error termination.\nIPEX-LLM Ollama Setup (Secondary Test: Older Build 2.3.0b20250429):\n\nDownloaded ollama-ipex-llm-2.3.0b20250429-ubuntu.tgz.\nExtracted to ~/ollama_ipex_older/ollama-ipex-llm-2.3.0b20250429-ubuntu/.\nUpdated start_ollama_ipex.sh to point to the ollama binary in this older directory.\nSYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1 was kept commented out for this test.\nsudo systemctl daemon-reload && sudo systemctl restart ollama.\nModel Run Attempt (Older Build 2.3.0b20250429):\n\nollama run tinyllama \"Hello, what is your purpose?\"\nResult: SYCL error termination (same as newer build).\n3. Expected Behavior:\n\nThe IPEX-LLM optimized Ollama should leverage the Intel Arc A770 discrete GPU for inference, resulting in high \"Render/3D\" or \"Compute\" utilization in intel_gpu_top and successful text generation.\n\n4. Actual Behavior:\n\nThe llama runner process consistently terminates with the following SYCL error during matrix multiplication (ggml_sycl_op_mul_mat), regardless of the model size (tinyllama, llama3.1, qwen2.5:7b) or IPEX-LLM build version tested:\n\nError: llama runner process has terminated: error:CHECK_TRY_ERROR(op(ctx, src0, src1, dst, src0_dd_i, src1_ddf_i, src1_ddq_i, dst_dd_i, dev[i].row_low, dev[i].row_high, src1_ncols, src1_padded_col_size, stream)): Meet error in this line code!\n  in function ggml_sycl_op_mul_mat at /home/runner/_work/llm.cpp/llm.cpp/ollama-llama-cpp/ggml/src/ggml-sycl/ggml-sycl.cpp:3260\n/home/runner/_work/llm.cpp/llm.cpp/ollama-llama-cpp/ggml/src/ggml-sycl/../ggml-sycl/common.hpp:117: SYCL error\n(Note: The line number may vary slightly, e.g., 3277 vs 3260, but the core error remains the same.)\n\nintel_gpu_top shows blitter activity (data transfer) but no significant \"Render/3D\" or \"Compute\" engine utilization before the crash.\n\n5. System Information:\n\nGPU: Intel Arc A770 (Desktop)\nOperating System: Ubuntu 24.04 LTS (Noble Numbat)\nKernel Version: 6.11.0-26-generic\nIntel GPU Driver Version: 25.18.33578.6 (manually installed from Intel's website). Confirmed intel-opencl-icd also installed.\nsycl-ls Output:\n[level_zero:gpu][level_zero:0] Intel(R) oneAPI Unified Runtime over Level-Zero, Intel(R) Arc(TM) A770 Graphics 12.55.8 [1.6.33578.600000]\n[opencl:cpu][opencl:0] Intel(R) OpenCL, AMD Ryzen 7 8700F 8-Core Processor        OpenCL 3.0 (Build 0) [2025.19.4.0.18_160000.xmain-hotfix]\nIPEX-LLM Ollama Builds Tested:\nollama-ipex-llm-2.3.0b20250612-ubuntu.tgz\nollama-ipex-llm-2.3.0b20250429-ubuntu.tgz\nRelevant Environment Variables set in /usr/local/bin/start_ollama_ipex.sh (consistent across tests):\nBash\n\nexport OLLAMA_NUM_GPU=999\nexport no_proxy=localhost,127.0.0.1\nexport ZES_ENABLE_SYSMAN=1\nexport SYCL_CACHE_PERSISTENT=1\nexport OLLAMA_",
      "state": "open",
      "author": "funky-gibbon",
      "author_type": "User",
      "created_at": "2025-06-16T13:10:06Z",
      "updated_at": "2025-06-20T17:08:03Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13223/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13223",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13223",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:25.320169",
      "comments": [
        {
          "author": "Ellie-Williams-007",
          "body": "Hey man I used to get this error before, so I think I can do sth to help. Btw can you send me your error logs?",
          "created_at": "2025-06-17T01:50:18Z"
        },
        {
          "author": "funky-gibbon",
          "body": "Many thanks for taking the time to comment on the post.   You are most kind. I have the error logs.  They are from the ollama-ipex-llm optimised build not from previous non-IPEX-LLM attempts.  Here is the output from 'journalctl -u ollama.service -n 50 --no-pager' capuring recent attempts:\n\nbase) mi",
          "created_at": "2025-06-17T15:24:48Z"
        },
        {
          "author": "funky-gibbon",
          "body": "I made an error.  I forgot to convert the logs into markdown format.  Here is the corrected log output:\n\njournalctl -u ollama.service -n 50 --no-pager Jun 14 12:45:06 michael-fothergill-B650-EAGLE-AX start_ollama_ipex.sh[7900]: r14 0x16 Jun 14 12:45:06 michael-fothergill-B650-EAGLE-AX start_ollama_i",
          "created_at": "2025-06-17T15:35:11Z"
        },
        {
          "author": "Ellie-Williams-007",
          "body": "I think it's caused by AMD cpu, they only support intel cpu (I mentioned it to the intel team before). Also `msg=\"no compatible GPUs were discovered\"` is not relevant, just a useless log. ",
          "created_at": "2025-06-18T01:34:44Z"
        },
        {
          "author": "funky-gibbon",
          "body": "That sounds odd to me.  Are there any people you know in the intel team I could email and ask about that personally or a user group I could join run by Intel that I could politely ask that question to?     You make it sound like I would be better buying an AMD graphics card and not bothering with an",
          "created_at": "2025-06-18T10:37:09Z"
        }
      ]
    },
    {
      "issue_number": 13173,
      "title": "B60 Performance Issue with INT4",
      "body": "**Describe the bug**\nB60 Performance Issue with INT4, use the latest b3 image with vllm.\n\n**How to reproduce**\nStart vLLM with 1/2/4 cards and 32B/70B model, you will find the performance is so bad vs multiple A770.\n\n\n\n",
      "state": "open",
      "author": "RobinJing",
      "author_type": "User",
      "created_at": "2025-05-21T08:59:38Z",
      "updated_at": "2025-06-20T15:35:56Z",
      "closed_at": null,
      "labels": [
        "user issue",
        "multi-arc"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 17,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13173/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "gc-fu"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13173",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13173",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:25.562309",
      "comments": [
        {
          "author": "gc-fu",
          "body": "Hi, I am investigating this issue.",
          "created_at": "2025-05-22T01:07:50Z"
        },
        {
          "author": "gc-fu",
          "body": "Hi, I could not reproduce this issue on our machines.\n\nModel: DeepSeek-R1-Distill-Qwen-32B\n\nFor Arc A770 sym_int4 four cards, we get performance for batch size 1:\n```\n============ Serving Benchmark Result ============\nSuccessful requests:                     1\nBenchmark duration (s):                ",
          "created_at": "2025-05-23T02:16:45Z"
        },
        {
          "author": "savvadesogle",
          "body": "> Hi, I could not reproduce this issue on our machines.\n> \nDo you can to share you software config, please? \nOS ver, vllm image version (intelanalitics/...)\n\nHost's system driver version and guc (a770)? \n\nAnd vllm start params (+ container)\n\nI have ubuntu 22.04 lts, 6.5 kernel (exactly that recomend",
          "created_at": "2025-06-16T07:33:42Z"
        },
        {
          "author": "gc-fu",
          "body": "> > Hi, I could not reproduce this issue on our machines.\n> \n> Do you can to share you software config, please? OS ver, vllm image version (intelanalitics/...)\n> \n> Host's system driver version and guc (a770)?\n> \n> And vllm start params (+ container)\n> \n> I have ubuntu 22.04 lts, 6.5 kernel (exactly",
          "created_at": "2025-06-17T02:08:55Z"
        },
        {
          "author": "savvadesogle",
          "body": "> ii intel-i915-dkms 1.23.10.54.231129.55+i87-1 all Out of tree i915 driver.\n\nHello\nI have \ndpkg -l | grep i915\n`ii  intel-i915-dkms 1.23.10.92.231129.101+i141-1 all Out of tree i915 driver.`\nnot **54**,  does it matter?\nfrom \n```\necho \"deb [arch=amd64 signed-by=/usr/share/keyrings/intel-graphics.gp",
          "created_at": "2025-06-17T14:29:21Z"
        }
      ]
    },
    {
      "issue_number": 13208,
      "title": "Loading GGUF model failed",
      "body": "**Describe the bug**\nWhen following [README](https://github.com/intel/ipex-llm/tree/main/python/llm/example/GPU/HuggingFace/Advanced-Quantizations/GGUF) to run Llama-2-Q4_0.gguf model inference, an error raised:\n\n![Image](https://github.com/user-attachments/assets/9e5ff4d7-d408-4b83-97ff-2d877d3d8f39)\n\nI've tried ipex-llm 2.1.0 and 2.3.0-nightly, both raise this error\n\n**How to reproduce**\nSteps to reproduce the error:\n1. Following the readme, run `python generate.py --model llama-2-7b-chat.Q4_0.gguf --prompt \"What is AI?\"`\n\n**Screenshots**\nIf applicable, add screenshots to help explain the problem\n\n**Environment information**\nIf possible, please attach the output of the environment check script, using:\n- https://github.com/intel/ipex-llm/blob/main/python/llm/scripts/env-check.bat, or\n- https://github.com/intel/ipex-llm/blob/main/python/llm/scripts/env-check.sh\n\n```\n-----------------------------------------------------------------\nPYTHON_VERSION=3.9.0\n-----------------------------------------------------------------\ntransformers=4.36.0\n-----------------------------------------------------------------\ntorch=2.1.0a0+cxx11.abi\n-----------------------------------------------------------------\nipex-llm Version: 2.1.0\n-----------------------------------------------------------------\nipex=2.1.10+xpu\n-----------------------------------------------------------------\nCPU Information: \nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        39 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               32\nOn-line CPU(s) list:                  0-31\nVendor ID:                            GenuineIntel\nModel name:                           13th Gen Intel(R) Core(TM) i9-13900KF\nCPU family:                           6\nModel:                                183\nThread(s) per core:                   2\nCore(s) per socket:                   24\nSocket(s):                            1\nStepping:                             1\nCPU max MHz:                          5800.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             5990.40\n-----------------------------------------------------------------\nTotal CPU Memory: 94.1262 GB\n-----------------------------------------------------------------\nOperating System: \nUbuntu 22.04.5 LTS \\n \\l\n\n-----------------------------------------------------------------\nLinux bjldempd1 6.8.0-51-generic #52~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Dec  9 15:00:52 UTC 2 x86_64 x86_64 x86_64 GNU/Linux\n-----------------------------------------------------------------\nCLI:\n    Version: 1.2.22.20231025\n    Build ID: 00000000\n\nService:\n    Version: 1.2.22.20231025\n    Build ID: 00000000\n    Level Zero Version: 1.15.5\n-----------------------------------------------------------------\n  Driver Version                                  2023.16.10.0.17_160000\n  Driver Version                                  2023.16.10.0.17_160000\n  Driver UUID                                     0f7477e2-3499-aea8-e286-b6eec4040fe4\n  Driver Version                                  535.154.05\n  Driver UUID                                     32342e33-352e-3330-3837-320000000000\n  Driver Version                                  24.35.30872\n-----------------------------------------------------------------\nDriver related package version:\nii  intel-level-zero-gpu                             1.3.29735.27-914~22.04                  amd64        Intel(R) Graphics Compute Runtime for oneAPI Level Zero.\nii  level-zero-dev                                   1.13.1-719~22.04                        amd64        Intel(R) Graphics Compute Runtime for oneAPI Level Zero.\n-----------------------------------------------------------------\nigpu not detected\n-----------------------------------------------------------------\nxpu-smi is properly installed. \n-----------------------------------------------------------------\n+-----------+--------------------------------------------------------------------------------------+\n| Device ID | Device Information                                                                   |\n+-----------+--------------------------------------------------------------------------------------+\n| 0         | Device Name: Intel(R) Arc(TM) A770 Graphics                                          |\n|           | Vendor Name: Intel(R) Corporation                                                    |\n|           | SOC UUID: 00000000-0000-0003-0000-000856a08086                                       |\n|           | PCI BDF Address: 0000:03:00.0                                                        |\n|           | DRM Device: /dev/dri/card1                                                           |\n|           | Function Type: physical                                                              |\n+-----------+--------------------------------------------------------------------------------------+\nGPU0 Memory size=16G\nGPU1 Memory size=32G\n-----------------------------------------------------------------\n03:00.0 VGA compatible controller: Intel Corporation Device 56a0 (rev 08) (prog-if 00 [VGA controller])\n\tSubsystem: Device 1ef7:1997\n\tFlags: bus master, fast devsel, latency 0, IRQ 173, IOMMU group 16\n\tMemory at 80000000 (64-bit, non-prefetchable) [size=16M]\n\tMemory at 4c00000000 (64-bit, prefetchable) \u001b[1;33m[size=16G]\u001b[0m\n\tExpansion ROM at 81000000 [disabled] [size=2M]\n\tCapabilities: <access denied>\n\tKernel driver in use: i915\n\tKernel modules: i915, xe\n--\n07:00.0 VGA compatible controller: NVIDIA Corporation GA102 [GeForce RTX 3090] (rev a1) (prog-if 00 [VGA controller])\n\tSubsystem: Micro-Star International Co., Ltd. [MSI] GA102 [GeForce RTX 3090]\n\tFlags: bus master, fast devsel, latency 0, IRQ 196, IOMMU group 19\n\tMemory at 82000000 (32-bit, non-prefetchable) [size=16M]\n\tMemory at 4000000000 (64-bit, prefetchable) \u001b[1;33m[size=32G]\u001b[0m\n\tMemory at 4800000000 (64-bit, prefetchable) [size=32M]\n\tI/O ports at 3000 [size=128]\n\tExpansion ROM at 83000000 [virtual] [disabled] [size=512K]\n\tCapabilities: <access denied>\n-----------------------------------------------------------------\n```\n\n**Additional context**\nSimply debugging shows that `model.model.layers[0].self_attn.q_proj` has wrong shape\n\n![Image](https://github.com/user-attachments/assets/a035a367-1e31-4e56-b402-3c6232b03a3f)\n\n",
      "state": "closed",
      "author": "kyang-06",
      "author_type": "User",
      "created_at": "2025-06-06T02:21:24Z",
      "updated_at": "2025-06-20T11:19:00Z",
      "closed_at": "2025-06-20T11:19:00Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13208/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "qiyuangong"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13208",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13208",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:25.773505",
      "comments": [
        {
          "author": "kyang-06",
          "body": "Hi team. After deeply debugging, I found that the error is caused by unexpected shape of quantized linear weight. For example,   `self_attn.o_proj.weight` should have a shape of `(4096, 2176)` for `sym_int4` quantization, but got `(8912896,)`. \n\n[generate.py](https://github.com/intel/ipex-llm/blob/m",
          "created_at": "2025-06-06T09:41:25Z"
        },
        {
          "author": "qiyuangong",
          "body": "> Hi team. After deeply debugging, I found that the error is caused by unexpected shape of quantized linear weight. For example, `self_attn.o_proj.weight` should have a shape of `(4096, 2176)` for `sym_int4` quantization, but got `(8912896,)`.\n> \n> [generate.py](https://github.com/intel/ipex-llm/blo",
          "created_at": "2025-06-11T02:29:18Z"
        },
        {
          "author": "Uxito-Ada",
          "body": "Hi @kyang-06 ,\n\nTks for your patience.\n\nAfter days of breaking down, it is shown that the root cause is [merge_linear](https://github.com/intel/ipex-llm/blob/main/python/llm/src/ipex_llm/transformers/models/common.py#L23), called by [merge_qkv_base](https://github.com/intel/ipex-llm/blob/main/python",
          "created_at": "2025-06-20T06:48:25Z"
        },
        {
          "author": "kyang-06",
          "body": "That's great! I have confirmed that it works.",
          "created_at": "2025-06-20T11:19:00Z"
        }
      ]
    },
    {
      "issue_number": 13183,
      "title": "A770 can't run deepseek R1 Q4 with flashmoe",
      "body": "**Describe the bug**\nA770 can't run deepseek R1 Q4 with flashmoe\n\n**How to reproduce**\nSteps to reproduce the error:\n1. install the gpu driver following the instruction (https://dgpu-docs.intel.com/driver/client/overview.html)\n2. download the gguf [DeepSeek-R1-Q4_K_M.gguf] which includs 9 files.\n3. ./flash-moe -m /PATH/TO/DeepSeek-R1-Q4_K_M-00001-of-00009.gguf --prompt \"What's AI?\" -no-cnv\n4. then there is the error message:\n\n**Screenshots**\n\n./flash-moe -m /home/deepseek/文档/deepseek/DeepSeek-R1-Q4_K_M-00001-of-00009.gguf --prompt \"What's AI?\" -no-cnv\nterminate called after throwing an instance of 'std::filesystem::__cxx11::filesystem_error'\n  what():  filesystem error: Cannot convert character sequence: Invalid or incomplete multibyte or wide character\n./flash-moe: 第 25 行：  8026 已中止               （核心已转储） LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$(cd \"$(dirname \"$0\")\";pwd) $(cd \"$(dirname \"$0\")\";pwd)/llama-cli-bin -t $CORES -e -ngl 999 --color --no-context-shift -ot exps=CPU \"$@\"\n\n\n**Environment information**\nubuntu 22.04.05\n384G ddr5\n2 A770 gpus\n\n**Additional context**\nAdd any other context about the problem here.\n",
      "state": "open",
      "author": "luningxie",
      "author_type": "User",
      "created_at": "2025-05-22T11:45:01Z",
      "updated_at": "2025-06-20T05:33:34Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13183/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "rnwang04"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13183",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13183",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:26.009071",
      "comments": [
        {
          "author": "rnwang04",
          "body": "Hi @luningxie , it looks like a path related error. Could you please try to put your DeepSeek model to a full English path (like /home/deepseek/deepseek/DeepSeek-R1-Q4_K_M-00001-of-00009.gguf) and try flashmoe again ?",
          "created_at": "2025-05-23T02:21:43Z"
        },
        {
          "author": "luningxie",
          "body": "thank you for the help.  prob solved and there is a new one:\n\nOMP: Warning #65: KMP_AFFINITY: syntax error, not using affinity.\nOMP: Warning #62: KMP_AFFINITY: proclist not specified with explicit affinity type, using \"none\".\nmain: llama threadpool init, n_threads = 96\n\nsystem_info: n_threads = 96 (",
          "created_at": "2025-05-23T05:20:59Z"
        },
        {
          "author": "rnwang04",
          "body": "Hi @luningxie , based on your error message, it seems there exists two issues. \n### 1. OMP: Warning https://github.com/intel/ipex-llm/pull/65: KMP_AFFINITY: syntax error, not using affinity.\n\nThis is most likely a failure of CORES recognition.\nIn flashmoe script, we use `CORES=$(lscpu | grep \"Core(s",
          "created_at": "2025-05-23T05:57:24Z"
        },
        {
          "author": "luningxie",
          "body": "Thank you very much!\n\nI have formatted the pc and reinstalled ubuntu 22.04.02. Now the \"KMP_AFFINITY: syntax error, not using affinity\" is solved.\n\nbut \"The program was built for 1 devices Build program log for 'Intel(R) Arc(TM) A770 Graphics'\" problem remained the same.\n\nnot in any conda env,  neit",
          "created_at": "2025-05-24T14:02:13Z"
        },
        {
          "author": "rnwang04",
          "body": "Hi @luningxie , sadly we never got this error on our machine when running flashmoe cli + DeepSeek Q4K.\nAre you using the latest version flashmoe (like [llama-cpp-ipex-llm-2.3.0b20250430-ubuntu-core.tgz](https://github.com/ipex-llm/ipex-llm/releases/download/v2.3.0-nightly/llama-cpp-ipex-llm-2.3.0b20",
          "created_at": "2025-05-26T05:57:40Z"
        }
      ]
    },
    {
      "issue_number": 13227,
      "title": "Getting a error while trying to start a chat with a ollama model",
      "body": "I used this quick setup guide for ubuntu.\n\nhttps://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_portable_zip_quickstart.md#linux-quickstart\n\n\nThis is my full log file\nhttps://pastebin.com/UkHVNSKu",
      "state": "open",
      "author": "Mailootje",
      "author_type": "User",
      "created_at": "2025-06-19T23:59:39Z",
      "updated_at": "2025-06-20T01:28:58Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13227/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13227",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13227",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:26.199619",
      "comments": [
        {
          "author": "rnwang04",
          "body": "Hi @Mailootje , could you please provide us with more machine info ? You could check it with [env-check.sh](https://github.com/intel/ipex-llm/blob/main/python/llm/scripts/env-check.sh) in https://github.com/intel/ipex-llm/tree/main/python/llm/scripts and show us  the output.",
          "created_at": "2025-06-20T01:28:36Z"
        }
      ]
    },
    {
      "issue_number": 13225,
      "title": "Crashes with DeepSeek-R1-0528-Qwen3-8B-UD-Q8_K_XL (maybe others)",
      "body": "**Describe the bug**\nllama.cpp crashes with assertion failure and longjmp stack frame errors when processing large context with the DeepSeek-R1-0528-Qwen3-8B-UD-Q8_K_XL model on Intel A770 GPU. The crash occurs in the SYCL scaled dot-product attention (SDP) XMX kernel after prompt processing completes.\n\n**How to reproduce**\nSteps to reproduce the error:\n1. Launch the DeepSeek-R1-0528-Qwen3-8B-UD-Q8_K_XL model using llama.cpp portable\n2. Load a large context (specific size not mentioned, but appears to be >185 tokens based on the log)\n3. Process the prompt through completion\n4. The crash occurs after prompt processing finishes with assertion failure in sdp_xmx_kernel.cpp:439\n\n**Screenshots**\n```\nslot update_slots: id  0 | task 0 | prompt processing progress, n_past = 185, n_tokens = 185, progress = 1.000000\nslot update_slots: id  0 | task 0 | prompt done, n_past = 185, n_tokens = 185\nllama-server-bin: /home/runner/_work/llm.cpp/llm.cpp/llm.cpp/bigdl-core-xe/llama_backend/sdp_xmx_kernel.cpp:439: auto ggml_sycl_op_sdp_xmx_casual(fp16 *, fp16 *, fp16 *, fp16 *, fp16 *, float *, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, float *, float, sycl::queue &)::(anonymous class)::operator()() const: Assertion `false' failed.\n*** longjmp causes uninitialized stack frame ***: terminated\n[Multiple longjmp errors repeated]\n```\n\n**Environment information**\nGPU: Intel A770\nModel: DeepSeek-R1-0528-Qwen3-8B-UD-Q8_K_XL\nRuntime: llama.cpp portable\n\n**Additional context**\nThe A770 performs well with llama.cpp portable in general, but this specific crash occurs consistently when processing large contexts with this model as the initial prompt. Interestingly, if the conversation is started with a simple message (e.g., \"hi\") and the large context is provided as a subsequent prompt, the model handles it without issues. This suggests the crash may be related to initial memory allocation, context initialization, or the KV cache setup when processing large contexts from a cold start. The error appears to be related to the SYCL implementation of the scaled dot-product attention kernel for Intel XMX (Xe Matrix Extensions), possibly involving uninitialized state when handling large initial contexts.\n",
      "state": "open",
      "author": "markussiebert",
      "author_type": "User",
      "created_at": "2025-06-17T19:46:27Z",
      "updated_at": "2025-06-19T08:04:29Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13225/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13225",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13225",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:26.406460",
      "comments": [
        {
          "author": "cyita",
          "body": "Hi @markussiebert, thank you for the information. We will work on reproducing this error.",
          "created_at": "2025-06-18T02:09:31Z"
        },
        {
          "author": "cyita",
          "body": "Hi @markussiebert, could you please provide the output of `lspci -nn |grep  -Ei 'VGA|DISPLAY’`?",
          "created_at": "2025-06-18T10:25:33Z"
        },
        {
          "author": "markussiebert",
          "body": "```03:00.0 VGA compatible controller [0300]: Intel Corporation DG2 [Arc A770] [8086:56a0] (rev 08)```\n\nAnother model: jan-nano-4b-Q8_0\n\n```\nmain: server is listening on http://127.0.0.1:5802 - starting the main loop\nsrv  update_slots: all slots are idle\nsrv  log_server_r: request: GET /health 127.0.",
          "created_at": "2025-06-18T22:26:10Z"
        },
        {
          "author": "rnwang04",
          "body": "Hi @markussiebert , sadly we can  not reproduce this error on our A770 machine with https://github.com/ipex-llm/ipex-llm/releases/download/v2.3.0-nightly/llama-cpp-ipex-llm-2.3.0b20250612-ubuntu-core.tgz. And below is my log & cmd.\n\n```bash\nexport ONEAPI_DEVICE_SELECTOR=level_zero:0\nexport SYCL_PI_L",
          "created_at": "2025-06-19T08:04:29Z"
        }
      ]
    },
    {
      "issue_number": 13074,
      "title": "The NPU version of llama.cpp did not return an appropriate response on Intel Core Ultra 7 268V",
      "body": "**Describe the bug**\nHere is the English translation of your text:\n\nI tried running the NPU version of llama.cpp on Windows 11, but I did not receive an appropriate response.  \n\nModel: DeepSeek-R1-Distill-Qwen-7B-Q6_K.gguf  \nExecutable: llama-cpp-ipex-llm-2.2.0-win-npu.zip  \nCPU: Intel Core Ultra 7 268V  \nNPU: Intel AI Boost  \nNPU Driver: 32.0.100.3967  \n\nThe command itself was referenced in the following URL:  \nhttps://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/npu_quickstart.md  \n\nCould the issue be with the method of execution? Or is it that the model is not supported?\n\n** Logs **\n```\nPS C:\\Users\\kotau\\Downloads\\llama-cpp-ipex-llm-2.2.0-win-npu> .\\llama-cli-npu.exe -m ..\\DeepSeek-R1-Distill-Qwen-7B-Q6_K.gguf -n 32 --prompt \"What is AI?\"\nbuild: 1 (3ac676a) with MSVC 19.39.33519.0 for x64\nllama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from ..\\DeepSeek-R1-Distill-Qwen-7B-Q6_K.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 7B\nllama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen\nllama_model_loader: - kv   4:                         general.size_label str              = 7B\nllama_model_loader: - kv   5:                          qwen2.block_count u32              = 28\nllama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072\nllama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 3584\nllama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 18944\nllama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 28\nllama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 4\nllama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-r1-qwen\nllama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151387]  = [\"ﾄ ﾄ\", \"ﾄﾄ ﾄﾄ\", \"i n\", \"ﾄ t\",...\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151646\nllama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643\nllama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\nllama_model_loader: - kv  24:               general.quantization_version u32              = 2\nllama_model_loader: - kv  25:                          general.file_type u32              = 18\nllama_model_loader: - type  f32:  141 tensors\nllama_model_loader: - type q6_K:  198 tensors\nllm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 22\nllm_load_vocab: token to piece cache size = 0.9310 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = qwen2\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 152064\nllm_load_print_meta: n_merges         = 151387\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 131072\nllm_load_print_meta: n_embd           = 3584\nllm_load_print_meta: n_layer          = 28\nllm_load_print_meta: n_head           = 28\nllm_load_print_meta: n_head_kv        = 4\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 7\nllm_load_print_meta: n_embd_k_gqa     = 512\nllm_load_print_meta: n_embd_v_gqa     = 512\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 18944\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 2\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 131072\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = ?B\nllm_load_print_meta: model ftype      = Q6_K\nllm_load_print_meta: model params     = 7.62 B\nllm_load_print_meta: model size       = 5.82 GiB (6.56 BPW)\nllm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 7B\nllm_load_print_meta: BOS token        = 151646 '<・彙egin笆｛f笆《entence・・'\nllm_load_print_meta: EOS token        = 151643 '<・彳nd笆｛f笆《entence・・'\nllm_load_print_meta: PAD token        = 151643 '<・彳nd笆｛f笆《entence・・'\nllm_load_print_meta: LF token         = 148848 'ﾃ・ｬ'\nllm_load_print_meta: EOG token        = 151643 '<・彳nd笆｛f笆《entence・・'\nllm_load_print_meta: max token length = 256\nllm_load_tensors: ggml ctx size =    0.15 MiB\nllm_load_tensors:        CPU buffer size =  5958.79 MiB\n........................................................................................\nDirectory created: \"C:\\\\Users\\\\kotau\\\\Downloads\\\\llama-cpp-ipex-llm-2.2.0-win-npu\\\\NPU_models\\\\qwen2-28-3584-152064-Q4_0\"\nDirectory created: \"C:\\\\Users\\\\kotau\\\\Downloads\\\\llama-cpp-ipex-llm-2.2.0-win-npu\\\\NPU_models\\\\qwen2-28-3584-152064-Q4_0\\\\model_weights\"\nConverting GGUF model to Q4_0 NPU model...\nModel weights saved to C:\\Users\\kotau\\Downloads\\llama-cpp-ipex-llm-2.2.0-win-npu\\NPU_models\\qwen2-28-3584-152064-Q4_0\\model_weights\nllama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from ..\\DeepSeek-R1-Distill-Qwen-7B-Q6_K.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 7B\nllama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen\nllama_model_loader: - kv   4:                         general.size_label str              = 7B\nllama_model_loader: - kv   5:                          qwen2.block_count u32              = 28\nllama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072\nllama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 3584\nllama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 18944\nllama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 28\nllama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 4\nllama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-r1-qwen\nllama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151387]  = [\"ﾄ ﾄ\", \"ﾄﾄ ﾄﾄ\", \"i n\", \"ﾄ t\",...\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151646\nllama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643\nllama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\nllama_model_loader: - kv  24:               general.quantization_version u32              = 2\nllama_model_loader: - kv  25:                          general.file_type u32              = 18\nllama_model_loader: - type  f32:  141 tensors\nllama_model_loader: - type q6_K:  198 tensors\nllm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 22\nllm_load_vocab: token to piece cache size = 0.9310 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = qwen2\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 152064\nllm_load_print_meta: n_merges         = 151387\nllm_load_print_meta: vocab_only       = 1\nllm_load_print_meta: model type       = ?B\nllm_load_print_meta: model ftype      = all F32\nllm_load_print_meta: model params     = 7.62 B\nllm_load_print_meta: model size       = 5.82 GiB (6.56 BPW)\nllm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 7B\nllm_load_print_meta: BOS token        = 151646 '<・彙egin笆｛f笆《entence・・'\nllm_load_print_meta: EOS token        = 151643 '<・彳nd笆｛f笆《entence・・'\nllm_load_print_meta: PAD token        = 151643 '<・彳nd笆｛f笆《entence・・'\nllm_load_print_meta: LF token         = 148848 'ﾃ・ｬ'\nllm_load_print_meta: EOG token        = 151643 '<・彳nd笆｛f笆《entence・・'\nllm_load_print_meta: max token length = 256\nllama_model_load: vocab only - skipping tensors\nModel saved to C:\\Users\\kotau\\Downloads\\llama-cpp-ipex-llm-2.2.0-win-npu\\NPU_models\\qwen2-28-3584-152064-Q4_0//decoder_layer_0.blob\nModel saved to C:\\Users\\kotau\\Downloads\\llama-cpp-ipex-llm-2.2.0-win-npu\\NPU_models\\qwen2-28-3584-152064-Q4_0//decoder_layer_1.blob\nllama_new_context_with_model: n_ctx      = 1024\nllama_new_context_with_model: n_batch    = 1024\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 0.0\nllama_new_context_with_model: freq_scale = 1\n・ｿusing              犹もｸ巵ｸ｣犹・ <think>ﾘｵﾙ・ｯ  Di盻・  2犹もｸ巵ｸ｣犹・ 1  2\n\nllm_perf_print:        load time =   53917.00 ms\nllm_perf_print: prompt eval time =    3467.00 ms /     7 tokens (  495.29 ms per token,     2.02 tokens per second)\nllm_perf_print:        eval time =    2704.00 ms /    31 runs   (   87.23 ms per token,    11.46 tokens per second)\nllm_perf_print:       total time =   60146.00 ms /    38 tokens\n```",
      "state": "open",
      "author": "kotauchisunsun",
      "author_type": "User",
      "created_at": "2025-04-14T15:18:36Z",
      "updated_at": "2025-06-19T02:44:17Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13074/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13074",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13074",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:26.622328",
      "comments": [
        {
          "author": "plusbang",
          "body": "Hi, please use 32.0.100.3104 NPU driver as our document(https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/llama_cpp_npu_portable_zip_quickstart.md#prerequisites) described.",
          "created_at": "2025-04-15T02:17:04Z"
        },
        {
          "author": "kotauchisunsun",
          "body": "@plusbang \nThank you for your reply! I uninstalled the 32.0.100.3967 driver that was installed on my PC.\nThen, I tried to install the 32.0.100.3104 NPU driver.\nHowever, the driver version shown in Device Manager is 32.0.100.3717.\nIt seems that the PC originally came with a later version than 32.0.10",
          "created_at": "2025-04-16T06:46:27Z"
        },
        {
          "author": "buikhoa40",
          "body": "Just curious why driver version has to be exactly the same, is there some compatibility problem of the dll files provided by the driver?",
          "created_at": "2025-04-16T13:35:33Z"
        },
        {
          "author": "KrisNathan",
          "body": "Same problem here when I use DeepSeek-R1-Distill-Qwen-7B-Q6_K.gguf model.\n\nInterestingly, the garbled output issue doesn't seem to happen on DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M.gguf\n\nCPU: Intel Core Ultra 7 258V\nNPU Driver: 32.0.100.3764 (from device manager)\n\nI'm well aware that the driver version",
          "created_at": "2025-06-18T08:41:22Z"
        },
        {
          "author": "cyita",
          "body": "> Same problem here when I use DeepSeek-R1-Distill-Qwen-7B-Q6_K.gguf model.\n> \n> Interestingly, the garbled output issue doesn't seem to happen on DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M.gguf\n> \n> CPU: Intel Core Ultra 7 258V NPU Driver: 32.0.100.3764 (from device manager)\n> \n> I'm well aware that the ",
          "created_at": "2025-06-19T02:15:08Z"
        }
      ]
    },
    {
      "issue_number": 13224,
      "title": "qwen32B hung when running 20K/12K w/ 4 GPU",
      "body": "**Describe the bug**\nwhen run DeepSeek-R1-Distill-Qwen-32B on 4 B60 GPU with 20K/12K, it will hang there even concurrency 1\n\n**How to reproduce**\nSteps to reproduce the error:\n1. MAX_NUM_BATCHED_TOKENS=${MAX_NUM_BATCHED_TOKENS:-40000}\nMAX_MODEL_LEN=${MAX_MODEL_LEN:-40000}\nexport VLLM_USE_V1=1\npython -m ipex_llm.vllm.xpu.entrypoints.openai.api_server \\\n  --served-model-name $SERVED_MODEL_NAME \\\n  --port $PORT \\\n  --model $MODEL_PATH \\\n  --trust-remote-code \\\n  --block-size 64 \\\n  --gpu-memory-utilization 0.95 \\\n  --device xpu \\\n  --dtype float16 \\\n  --enforce-eager \\\n  --load-in-low-bit $LOAD_IN_LOW_BIT \\\n  --max-model-len $MAX_MODEL_LEN \\\n  --max-num-batched-tokens $MAX_NUM_BATCHED_TOKENS \\\n  --max-num-seqs $MAX_NUM_SEQS \\\n  --tensor-parallel-size $TENSOR_PARALLEL_SIZE \\\n  --distributed-executor-backend ray\n\n2. for client\ninput_length=20480\noutput_length=12288\n\nfor bsize in 1 2 4 8 10; do\n  echo \"benchmark serving bs${bsize}\"\n  python /llm/vllm/benchmarks/benchmark_serving.py \\\n  --model ${modelname} \\\n  --served-model-name ${servedname} \\\n  --dataset-name random \\\n  --trust_remote_code \\\n  --ignore-eos \\\n  --num_prompt $bsize \\\n  --random-input-len=$input_length \\\n  --random-output-len=$output_length \\\n  --port 8000\n\n**Screenshots**\n\n<img width=\"1915\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f69d919a-1e92-4924-9004-b22cb173a582\" />\n\n**Environment information**\nroot@w05:/home/intel/ipex-llm/python/llm/scripts# bash env-check.sh \n-----------------------------------------------------------------\nPYTHON_VERSION=3.11.13\n-----------------------------------------------------------------\n[W617 13:56:29.995243467 OperatorEntry.cpp:154] Warning: Warning only once for all operators,  other operators may also be overridden.\n  Overriding a previously registered kernel for the same operator and the same dispatch key\n  operator: aten::_validate_compressed_sparse_indices(bool is_crow, Tensor compressed_idx, Tensor plain_idx, int cdim, int dim, int nnz) -> ()\n    registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6\n  dispatch key: XPU\n  previous kernel: registered at /pytorch/build/aten/src/ATen/RegisterCPU.cpp:30477\n       new kernel: registered at /build/intel-pytorch-extension/build/Release/csrc/gpu/csrc/aten/generated/ATen/RegisterXPU.cpp:468 (function operator())\n[W617 13:56:31.952889367 OperatorEntry.cpp:154] Warning: Warning only once for all operators,  other operators may also be overridden.\n  Overriding a previously registered kernel for the same operator and the same dispatch key\n  operator: aten::_validate_compressed_sparse_indices(bool is_crow, Tensor compressed_idx, Tensor plain_idx, int cdim, int dim, int nnz) -> ()\n    registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6\n  dispatch key: XPU\n  previous kernel: registered at /pytorch/build/aten/src/ATen/RegisterCPU.cpp:30477\n       new kernel: registered at /build/intel-pytorch-extension/build/Release/csrc/gpu/csrc/aten/generated/ATen/RegisterXPU.cpp:468 (function operator())\ntransformers=4.52.4\n-----------------------------------------------------------------\n[W617 13:56:38.056986411 OperatorEntry.cpp:154] Warning: Warning only once for all operators,  other operators may also be overridden.\n  Overriding a previously registered kernel for the same operator and the same dispatch key\n  operator: aten::_validate_compressed_sparse_indices(bool is_crow, Tensor compressed_idx, Tensor plain_idx, int cdim, int dim, int nnz) -> ()\n    registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6\n  dispatch key: XPU\n  previous kernel: registered at /pytorch/build/aten/src/ATen/RegisterCPU.cpp:30477\n       new kernel: registered at /build/intel-pytorch-extension/build/Release/csrc/gpu/csrc/aten/generated/ATen/RegisterXPU.cpp:468 (function operator())\n[W617 13:56:41.725475380 OperatorEntry.cpp:154] Warning: Warning only once for all operators,  other operators may also be overridden.\n  Overriding a previously registered kernel for the same operator and the same dispatch key\n  operator: aten::_validate_compressed_sparse_indices(bool is_crow, Tensor compressed_idx, Tensor plain_idx, int cdim, int dim, int nnz) -> ()\n    registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6\n  dispatch key: XPU\n  previous kernel: registered at /pytorch/build/aten/src/ATen/RegisterCPU.cpp:30477\n       new kernel: registered at /build/intel-pytorch-extension/build/Release/csrc/gpu/csrc/aten/generated/ATen/RegisterXPU.cpp:468 (function operator())\ntorch=2.6.0+xpu\n-----------------------------------------------------------------\nipex-llm Version: 2.3.0b20250610\n-----------------------------------------------------------------\n[W617 13:56:49.740651507 OperatorEntry.cpp:154] Warning: Warning only once for all operators,  other operators may also be overridden.\n  Overriding a previously registered kernel for the same operator and the same dispatch key\n  operator: aten::_validate_compressed_sparse_indices(bool is_crow, Tensor compressed_idx, Tensor plain_idx, int cdim, int dim, int nnz) -> ()\n    registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6\n  dispatch key: XPU\n  previous kernel: registered at /pytorch/build/aten/src/ATen/RegisterCPU.cpp:30477\n       new kernel: registered at /build/intel-pytorch-extension/build/Release/csrc/gpu/csrc/aten/generated/ATen/RegisterXPU.cpp:468 (function operator())\n[W617 13:56:51.860952116 OperatorEntry.cpp:154] Warning: Warning only once for all operators,  other operators may also be overridden.\n  Overriding a previously registered kernel for the same operator and the same dispatch key\n  operator: aten::_validate_compressed_sparse_indices(bool is_crow, Tensor compressed_idx, Tensor plain_idx, int cdim, int dim, int nnz) -> ()\n    registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6\n  dispatch key: XPU\n  previous kernel: registered at /pytorch/build/aten/src/ATen/RegisterCPU.cpp:30477\n       new kernel: registered at /build/intel-pytorch-extension/build/Release/csrc/gpu/csrc/aten/generated/ATen/RegisterXPU.cpp:468 (function operator())\nipex=2.6.10+xpu\n-----------------------------------------------------------------\nCPU Information: \nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               64\nOn-line CPU(s) list:                  0-63\nVendor ID:                            GenuineIntel\nBIOS Vendor ID:                       Intel(R) Corporation\nModel name:                           Intel(R) Xeon(R) w7-3565X\nBIOS Model name:                      Intel(R) Xeon(R) w7-3565X  CPU @ 2.5GHz\nBIOS CPU family:                      179\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   32\nSocket(s):                            1\nStepping:                             8\n-----------------------------------------------------------------\nTotal CPU Memory: 247.097 GB\n-----------------------------------------------------------------\nOperating System: \nUbuntu 24.04.1 LTS \\n \\l\n\n-----------------------------------------------------------------\nLinux w05 6.14.0-15-generic #15-Ubuntu SMP PREEMPT_DYNAMIC Sun Apr  6 15:05:05 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux\n-----------------------------------------------------------------\nenv-check.sh: line 148: xpu-smi: command not found\n-----------------------------------------------------------------\nenv-check.sh: line 154: clinfo: command not found\n-----------------------------------------------------------------\nDriver related package version:\n-----------------------------------------------------------------\nigpu not detected\n-----------------------------------------------------------------\nxpu-smi is not installed. Please install xpu-smi according to README.md\n\n**Additional context**\nAdd any other context about the problem here.\n",
      "state": "open",
      "author": "aitss2017",
      "author_type": "User",
      "created_at": "2025-06-17T05:57:24Z",
      "updated_at": "2025-06-17T08:51:46Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13224/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hzjane"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13224",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13224",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:26.858468",
      "comments": [
        {
          "author": "hzjane",
          "body": "This issue is caused by bmg card 0xe211, maybe we need to upgrade [compute-runtime](https://github.com/intel/compute-runtime/releases) to latest in container to get the normal preformance.",
          "created_at": "2025-06-17T08:27:53Z"
        }
      ]
    },
    {
      "issue_number": 12963,
      "title": "Support for gemma3 from google",
      "body": "请更新ollama，已支持gemma3 \n\nError: llama runner process has terminated: this model is not supported by your version of Ollama. You may need to upgrade\n\n",
      "state": "open",
      "author": "bigtan",
      "author_type": "User",
      "created_at": "2025-03-12T14:29:08Z",
      "updated_at": "2025-06-17T01:44:33Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 62,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12963/reactions",
        "total_count": 12,
        "+1": 12,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12963",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12963",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:27.137450",
      "comments": []
    },
    {
      "issue_number": 13219,
      "title": "Linux 下ipex-ollama 启动模型乱码",
      "body": "系统：  Ubuntu 24.10 (GNU/Linux 6.11.0-26-generic x86_64)\n硬件配置： cpu： Ultra 9 285H  gpu：B580\n推理框架：ipex\n框架版本 ollama-intel-2.3.0b20250516-ubuntu-moe.tgz 2025-6-4 内部版本\n驱动安装版本 https://github.com/intel/compute-runtime/releases/tag/25.13.33276.16\n问题描述：大模型推理乱码\n\n<img width=\"851\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/787c3b6a-1114-4004-b41f-7fb0127e727e\" />\n\n安装oneapi Toolkit\n验证 驱动正常 \n\n<img width=\"446\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/7ef6372c-8789-471d-a6d0-b328f377ab26\" />\n\n[intel-ollama log.txt](https://github.com/user-attachments/files/20702781/intel-ollama.log.txt)\n",
      "state": "closed",
      "author": "qiflowy",
      "author_type": "User",
      "created_at": "2025-06-12T05:37:41Z",
      "updated_at": "2025-06-16T08:32:59Z",
      "closed_at": "2025-06-16T08:32:59Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13219/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13219",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13219",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:27.137475",
      "comments": [
        {
          "author": "cyita",
          "body": "Please run `ls-sycl-device` and provide the output. And you don't need to source oneAPI if you use this portable zip.",
          "created_at": "2025-06-12T10:50:43Z"
        },
        {
          "author": "qiflowy",
          "body": "![Image](https://github.com/user-attachments/assets/ba8c5912-939a-4b42-8f23-208a993079a6)",
          "created_at": "2025-06-13T02:42:42Z"
        },
        {
          "author": "cyita",
          "body": "Please enter the unzipped portable zip folder and run `ls-sycl-device`.",
          "created_at": "2025-06-13T02:54:52Z"
        },
        {
          "author": "qiflowy",
          "body": "test@ubuntu2410:~/ollama-intel-2.3.0b20250516-ubuntu$ ./ls-sycl-device\nterminate called after throwing an instance of 'sycl::_V1::exception'\n  what():  No device of requested type available. Please check https://software.intel.com/content/www/us/en/develop/articles/intel-oneapi-dpcpp-system-requirem",
          "created_at": "2025-06-13T03:19:37Z"
        },
        {
          "author": "qiflowy",
          "body": "test@ubuntu2410:~/ollama-intel-2.3.0b20250516-ubuntu$ ./ls-sycl-device\nFound 2 SYCL devices:\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\n|  |                   |                                       |       |compute|M",
          "created_at": "2025-06-13T07:01:10Z"
        }
      ]
    },
    {
      "issue_number": 13214,
      "title": "4xARC 770 on w5-3423 slow performance (half of what is reported here)",
      "body": "# Describe the bug\nI observe quite bad performance with my 4xARC770 setup with xeon w5-3423, 128 GB DDR5 and Asus w790 ACE. \nMain Question: I suppose with this setup i'm quite close to what you got. So what am i missing. What should be enabled in bios. **Could you share a bit more on how did you configure your system etc...?** \n\nThis is really getting frustrating i'm almost on the verge on selling it all. (before i tried with EPYC 7282 and i had more or less same numbers). \n\nGPUS are connected to PCIe 5 (does not really matter, as they are PCIe4 anyway). In theory they get good speed (all of them 18 GBPS, so inference should not be a problem)\n\n```\nsudo xpu-smi diag -d 0 --singletest 5\n\n+------------------+-------------------------------------------------------------------------------+\n| Device ID        | 0                                                                             |\n+------------------+-------------------------------------------------------------------------------+\n| Integration PCIe | Result: Fail                                                                  |\n|                  | Message: Fail to check PCIe bandwidth. Its bandwidth is 17.995 GBPS.          |\n|                  |   Unconfigured or invalid threshold. Fail on copy engine group 1.             |\n+------------------+-------------------------------------------------------------------------------+\n```\n\nI'm on **ubuntu 22.04** and i installed everything as in your tutorial. Out of tree, etc. Same kernel.... After quite some testing i even compiled out of tree driver myself from the backports repo.... Same results\n\nI locked freqs:\n```\n sudo xpu-smi config -d 0 -t 0 --frequencyrange 2400,2400\n sudo xpu-smi config -d 1 -t 0 --frequencyrange 2400,2400\n sudo xpu-smi config -d 2 -t 0 --frequencyrange 2400,2400\n sudo xpu-smi config -d 3 -t 0 --frequencyrange 2400,2400\n```\n\n```\nsudo cpupower frequency-set -g performance\nsudo cpupower frequency-set -d 4.0GHz\n```\n\nand still just a bit of improvement.\n\n## Tests with model=deepseek-ai/DeepSeek-R1-Distill-Qwen-32B on 4xARC sym-int4\nI get roughly half of what you are getting whatever i do\n\n```\n============ Serving Benchmark Result ============\nSuccessful requests:                     1\nBenchmark duration (s):                  35.40\nTotal input tokens:                      1024\nTotal generated tokens:                  512\nRequest throughput (req/s):              0.03\nOutput token throughput (tok/s):         14.46\nTotal Token throughput (tok/s):          43.39\n---------------Time to First Token----------------\nMean TTFT (ms):                          1332.15\nMedian TTFT (ms):                        1332.15\nP99 TTFT (ms):                           1332.15\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          66.67\nMedian TPOT (ms):                        66.67\nP99 TPOT (ms):                           66.67\n---------------Inter-token Latency----------------\nMean ITL (ms):                           66.67\nMedian ITL (ms):                         66.36\nP99 ITL (ms):                            74.45\n==================================================\n\n\n```\n\nAlso my GPUs don't go above 110W for a signle request (when i do 32 requrest then it goes up by a lot, but not for a single (the picture is from FP8 though))\n\nPicture attached\n\nMy tests were performed with a custom docker based on **intelanalytics/ipex-llm-serving-xpu:latest** (build today). But i tried a lot other builds and the performance does not really differ.\n\nOne observation: CCL_WORKER_COUNT=1 is the fastest\n\n```\nFROM intelanalytics/ipex-llm-serving-xpu:latest\n\nWORKDIR /temp\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nWORKDIR /llm\nRUN . /opt/intel/1ccl-wks/setvars.sh\n\nENTRYPOINT numactl -C 0-11 python -m ipex_llm.vllm.xpu.entrypoints.openai.api_server \\\n  --served-model-name ${served_model_name} \\\n  --quantization ${quantization} \\\n  --model $model \\\n  --port $port \\\n  --trust-remote-code \\\n  --block-size ${block_size} \\\n  --gpu-memory-utilization ${gpu_memory_utilization} \\\n  --device xpu \\\n  --dtype $dtype \\\n  --enforce-eager \\\n  --load-in-low-bit ${load_in_low_bit} \\\n  --max-model-len ${max_model_len} \\\n  --max-num-batched-tokens ${max_num_batched_tokens} \\\n  --max-num-seqs ${max_num_seqs} \\\n  --tensor-parallel-size ${tensor_parallel_size} \\\n  --pipeline-parallel-size ${pipeline_parallel_size} \\\n  --disable-async-output-proc \\\n  --distributed-executor-backend ray\n\n```\n\n\n```\nservices:\n  vllm-ipex:\n    image: intelanalytics/ipex-llm-serving-xpu-custom:latest\n    container_name: vllm-ipex\n    build:\n      dockerfile: ./dockerfile/dockerfile\n    volumes:\n      - \"/models/huggingface:/root/.cache/huggingface\"\n      - /cloud/custom/ipex-llm/:/ipex-llm\n      - /etc/timezone:/etc/timezone:ro\n      - /etc/localtime:/etc/localtime:ro\n    devices:\n      - /dev/dri:/dev/dri\n    privileged: true\n    tty: true\n    ports:\n      - 8000:8000\n    shm_size: \"32g\"\n    environment:\n      # Model selection\n      - quantization=None\n      - model=deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\n      - served_model_name=DeepSeek-R1-Distill-Qwen-32B\n\n      # Timezone and device\n      - TZ=Europe/Berlin\n      - DEVICE=Arc\n\n      # Intel/OneAPI/CCL/SYCL\n      - SYCL_CACHE_PERSISTENT=1\n      - CCL_WORKER_COUNT=1\n      - FI_PROVIDER=shm\n      - CCL_ATL_TRANSPORT=ofi\n      - CCL_ZE_IPC_EXCHANGE=sockets\n      - CCL_ATL_SHM=1\n      - USE_XETLA=OFF\n      - SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=2\n      - TORCH_LLM_ALLREDUCE=0\n      - VLLM_USE_V1=0\n      - CCL_SAME_STREAM=1\n      - CCL_BLOCKING_WAIT=0\n      - IPEX_LLM_LOWBIT=fp8\n\n      # vLLM/Serving\n      - port=8000\n      - gpu_memory_utilization=0.9\n      - dtype=float16\n      - block_size=8\n      - load_in_low_bit=sym_int4\n      - max_model_len=9000\n      - max_num_batched_tokens=9000\n      - max_num_seqs=32\n      - tensor_parallel_size=4\n      - pipeline_parallel_size=1\n      - enforce_eager=true\n\n    restart: unless-stopped\n\n    logging:\n      driver: json-file\n      options:\n        max-size: \"10mb\"\n        max-file: \"1\"\n\n```\n\n## ENV:\n```\nsudo ./env-check.sh\n-----------------------------------------------------------------\nPYTHON_VERSION=3.10.12\n-----------------------------------------------------------------\nTransformers is not installed.\n-----------------------------------------------------------------\nPyTorch is not installed.\n-----------------------------------------------------------------\nipex-llm ./env-check.sh: line 58: pip: command not found\n-----------------------------------------------------------------\nIPEX is not installed.\n-----------------------------------------------------------------\nCPU Information:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      52 bits physical, 57 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             24\nOn-line CPU(s) list:                0-23\nVendor ID:                          GenuineIntel\nModel name:                         Intel(R) Xeon(R) w5-3423\nCPU family:                         6\nModel:                              143\nThread(s) per core:                 2\nCore(s) per socket:                 12\nSocket(s):                          1\nStepping:                           8\nCPU max MHz:                        4200.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           4224.00\n-----------------------------------------------------------------\nTotal CPU Memory: 125.294 GB\nMemory Type: DDR5\n-----------------------------------------------------------------\nOperating System:\nUbuntu 22.04.5 LTS \\n \\l\n\n-----------------------------------------------------------------\nLinux aiflek 6.5.0-35-generic #35~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue May  7 09:00:52 UTC 2 x86_64 x86_64 x86_64 GNU/Linux\n-----------------------------------------------------------------\nCLI:\n    Version: 1.2.41.20250422\n    Build ID: 00000000\n\nService:\n    Version: 1.2.41.20250422\n    Build ID: 00000000\n    Level Zero Version: 1.21.1\n-----------------------------------------------------------------\n  Driver UUID                                     32352e31-332e-3333-3237-360000000000\n  Driver Version                                  25.13.33276\n  Driver UUID                                     32352e31-332e-3333-3237-360000000000\n  Driver Version                                  25.13.33276\n  Driver UUID                                     32352e31-332e-3333-3237-360000000000\n  Driver Version                                  25.13.33276\n  Driver UUID                                     32352e31-332e-3333-3237-360000000000\n  Driver Version                                  25.13.33276\n-----------------------------------------------------------------\nDriver related package version:\nii  intel-fw-gpu                                   2025.13.2-398~22.04                     all          Firmware package for Intel integrated and discrete GPUs\nii  intel-i915-dkms                                1.25.1.17.250113.16+i1-1                all          Out of tree i915 driver.\n-----------------------------------------------------------------\nigpu not detected\n-----------------------------------------------------------------\nxpu-smi is properly installed.\n-----------------------------------------------------------------\n+-----------+--------------------------------------------------------------------------------------+\n| Device ID | Device Information                                                                   |\n+-----------+--------------------------------------------------------------------------------------+\n| 0         | Device Name: Intel(R) Arc(TM) A770 Graphics                                          |\n|           | Vendor Name: Intel(R) Corporation                                                    |\n|           | SOC UUID: 00000000-0000-0018-0000-000856a08086                                       |\n|           | PCI BDF Address: 0000:18:00.0                                                        |\n|           | DRM Device: /dev/dri/card0                                                           |\n|           | Function Type: physical                                                              |\n+-----------+--------------------------------------------------------------------------------------+\n| 1         | Device Name: Intel(R) Arc(TM) A770 Graphics                                          |\n|           | Vendor Name: Intel(R) Corporation                                                    |\n|           | SOC UUID: 00000000-0000-0036-0000-000856a08086                                       |\n|           | PCI BDF Address: 0000:36:00.0                                                        |\n|           | DRM Device: /dev/dri/card1                                                           |\n|           | Function Type: physical                                                              |\n+-----------+--------------------------------------------------------------------------------------+\n| 2         | Device Name: Intel(R) Arc(TM) A770 Graphics                                          |\n|           | Vendor Name: Intel(R) Corporation                                                    |\n|           | SOC UUID: 00000000-0000-0054-0000-000856a08086                                       |\n|           | PCI BDF Address: 0000:54:00.0                                                        |\n|           | DRM Device: /dev/dri/card2                                                           |\n|           | Function Type: physical                                                              |\n+-----------+--------------------------------------------------------------------------------------+\n| 3         | Device Name: Intel(R) Arc(TM) A770 Graphics                                          |\n|           | Vendor Name: Intel(R) Corporation                                                    |\n|           | SOC UUID: 00000000-0000-0072-0000-000856a08086                                       |\n|           | PCI BDF Address: 0000:72:00.0                                                        |\n|           | DRM Device: /dev/dri/card3                                                           |\n|           | Function Type: physical                                                              |\n+-----------+--------------------------------------------------------------------------------------+\nGPU0 Memory size=16G\nGPU1 Memory size=16G\nGPU2 Memory size=16G\nGPU3 Memory size=16G\n-----------------------------------------------------------------\n18:00.0 VGA compatible controller: Intel Corporation Device 56a0 (rev 08) (prog-if 00 [VGA controller])\n        Subsystem: Device 172f:3937\n        Flags: bus master, fast devsel, latency 0, IRQ 91, NUMA node 0\n        Memory at 9e000000 (64-bit, non-prefetchable) [size=16M]\n        Memory at 2f800000000 (64-bit, prefetchable) [size=16G]\n        Expansion ROM at 9f000000 [disabled] [size=2M]\n        Capabilities: [40] Vendor Specific Information: Len=0c <?>\n        Capabilities: [70] Express Endpoint, MSI 00\n        Capabilities: [ac] MSI: Enable+ Count=1/1 Maskable+ 64bit+\n--\n36:00.0 VGA compatible controller: Intel Corporation Device 56a0 (rev 08) (prog-if 00 [VGA controller])\n        Subsystem: Intel Corporation Device 1020\n        Flags: bus master, fast devsel, latency 0, IRQ 94, NUMA node 0\n        Memory at a8000000 (64-bit, non-prefetchable) [size=16M]\n        Memory at 3f800000000 (64-bit, prefetchable) [size=16G]\n        Expansion ROM at a9000000 [disabled] [size=2M]\n        Capabilities: [40] Vendor Specific Information: Len=0c <?>\n        Capabilities: [70] Express Endpoint, MSI 00\n        Capabilities: [ac] MSI: Enable+ Count=1/1 Maskable+ 64bit+\n--\n54:00.0 VGA compatible controller: Intel Corporation Device 56a0 (rev 08) (prog-if 00 [VGA controller])\n        Subsystem: Intel Corporation Device 1020\n        Flags: bus master, fast devsel, latency 0, IRQ 97, NUMA node 0\n        Memory at b3000000 (64-bit, non-prefetchable) [size=16M]\n        Memory at 4f800000000 (64-bit, prefetchable) [size=16G]\n        Expansion ROM at b4000000 [disabled] [size=2M]\n        Capabilities: [40] Vendor Specific Information: Len=0c <?>\n        Capabilities: [70] Express Endpoint, MSI 00\n        Capabilities: [ac] MSI: Enable+ Count=1/1 Maskable+ 64bit+\n--\n72:00.0 VGA compatible controller: Intel Corporation Device 56a0 (rev 08) (prog-if 00 [VGA controller])\n        Subsystem: Intel Corporation Device 1020\n        Flags: bus master, fast devsel, latency 0, IRQ 100, NUMA node 0\n        Memory at bd000000 (64-bit, non-prefetchable) [size=16M]\n        Memory at 5f800000000 (64-bit, prefetchable) [size=16G]\n        Expansion ROM at be000000 [disabled] [size=2M]\n        Capabilities: [40] Vendor Specific Information: Len=0c <?>\n        Capabilities: [70] Express Endpoint, MSI 00\n        Capabilities: [ac] MSI: Enable+ Count=1/1 Maskable+ 64bit+\n-----------------------------------------------------------------\n\n```\n\n![Image](https://github.com/user-attachments/assets/eb132f5e-079f-493b-9885-ae97f1a14886)",
      "state": "open",
      "author": "flekol",
      "author_type": "User",
      "created_at": "2025-06-07T19:22:21Z",
      "updated_at": "2025-06-16T03:21:51Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13214/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hzjane"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13214",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13214",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:27.377281",
      "comments": [
        {
          "author": "savvadesogle",
          "body": "> SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=2\n\nTry  to change 2 to 0 (may be it brings to you some t/s)\n\nps And i have same problem with compute and power 😭\nB16, b19, b20, b21.. better single thread only with b12-usm. \n\n",
          "created_at": "2025-06-07T22:29:15Z"
        },
        {
          "author": "hzjane",
          "body": "It seems that there is nothing wrong with your test related configuration and OS kernel. Maybe this error is caused by the newer driver:\n```bash\nDriver related package version:\nii  intel-fw-gpu                                   2025.13.2-398~22.04                     all          Firmware package fo",
          "created_at": "2025-06-09T03:22:54Z"
        },
        {
          "author": "flekol",
          "body": "Hi @hzjane, no change, i tested it with 4 other \"official\" out of tree drivers. Same numbers or maybe even a bit lower on some.\n\ni reinstalled ubuntu, first tried with 5.15 kernel and to my surprise VLLM always crashed.\nLlama was running with the same speed as with the previous kernel and divers.\n\nT",
          "created_at": "2025-06-14T19:36:25Z"
        },
        {
          "author": "savvadesogle",
          "body": "> 5.15 kernel\n\nhttps://cdrdv2-public.intel.com/828236/828236_Installation%20BKC%20and%20AI%20Benchmark%20UG%20on%20Intel%20Xeon_ARC%20A770_rev2.2.pdf\n\nDid you read this document (user guide 828236, v2.2)\n",
          "created_at": "2025-06-14T19:51:26Z"
        },
        {
          "author": "flekol",
          "body": "Thanks a lot!\n\nI have not seen this document yet. But I think I tried all of them. I found another guide once upon a time at intel. And it always is like 2x the performance that I'm getting. \n\nReally really weird. The weird thing is that Llama.cpp provides sometimes better performance even on multi ",
          "created_at": "2025-06-14T20:08:35Z"
        }
      ]
    },
    {
      "issue_number": 13192,
      "title": "Ollama-ipex cannot  support Gemma3 4B、Gemma3 12B、 phi4-mini 3.8B，but the official Ollama can.",
      "body": " Please provide support.\nOllama-ipex cannot  support Gemma3 4B、Gemma3 12B multimodal large model.\nAnd phi4-mini 3.8B  LLM cannot support, but official Ollama can.",
      "state": "open",
      "author": "suxm795",
      "author_type": "User",
      "created_at": "2025-05-27T03:22:50Z",
      "updated_at": "2025-06-16T01:42:56Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 13,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13192/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13192",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13192",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:27.575759",
      "comments": [
        {
          "author": "kv9898",
          "body": "[Here](https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_portable_zip_quickstart.md) it says the currently Ollama Portable Zip is based on Ollama v0.6.2, but with the latest [v2.2.0 release](https://github.com/ipex-llm/ipex-llm/releases/tag/v2.2.0), ollama's version is still ",
          "created_at": "2025-05-27T14:51:28Z"
        },
        {
          "author": "guidov",
          "body": "See #13184 ",
          "created_at": "2025-05-29T00:33:52Z"
        },
        {
          "author": "cyita",
          "body": "We have initially completed Ollama multimodal engine support, which should be able to support a series of multimodal models including Gemma 3.",
          "created_at": "2025-05-29T02:03:41Z"
        },
        {
          "author": "cyita",
          "body": "> [Here](https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_portable_zip_quickstart.md) it says the currently Ollama Portable Zip is based on Ollama v0.6.2, but with the latest [v2.2.0 release](https://github.com/ipex-llm/ipex-llm/releases/tag/v2.2.0), ollama's version is stil",
          "created_at": "2025-05-29T02:07:30Z"
        },
        {
          "author": "kv9898",
          "body": "> > [Here](https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_portable_zip_quickstart.md) it says the currently Ollama Portable Zip is based on Ollama v0.6.2, but with the latest [v2.2.0 release](https://github.com/ipex-llm/ipex-llm/releases/tag/v2.2.0), ollama's version is st",
          "created_at": "2025-05-29T03:44:08Z"
        }
      ]
    },
    {
      "issue_number": 12839,
      "title": "ollama + deepseek v2: The number of work-items in each dimension of a work-group cannot exceed {512, 512, 512} for this device",
      "body": "The number of work-items in each dimension of a work-group cannot exceed {512, 512, 512} for this device\nException caught at file:/home/runner/_work/llm.cpp/llm.cpp/ollama-llama-cpp/ggml/src/ggml-sycl/ggml-sycl.cpp, line:4463\n\nusing this container, running on NixOS https://github.com/mattcurf/ollama-intel-gpu\n\npodman build -t \"ollama-intel-gpu\" .\n\npodman run --rm -p 127.0.0.1:11434:11434 -v /home/stereomato/models:/mnt -v ollama-volume:/root/.ollama -e OLLAMA_NUM_PARALLEL=1 -e OLLAMA_MAX_LOADED_MODELS=1 -e OLLAMA_FLASH_ATTENTION=1 -e OLLAMA_NUM_GPU=999 -e DEVICE=iGPU --device /dev/dri --name=ollama-intel-gpu\n\npodman exec -it ollama-intel-gpu bash\n\n./ollama pull deepseek-v2:16b, but the q4_k_m 16b also exhibits the same issue\n\n./ollama run deepseek-v2 \"hello deepseek\"\n\nThen, I get the error in the title/first two lines of this bug report.\n\nHW:\nIntel i5-12500h,\nIntel Xe Graphics (Alder Lake)\n24GB of RAM\nup to date NixOS\n\n",
      "state": "open",
      "author": "stereomato",
      "author_type": "User",
      "created_at": "2025-02-17T18:35:14Z",
      "updated_at": "2025-06-16T01:38:47Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12839/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12839",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12839",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:27.801328",
      "comments": [
        {
          "author": "stereomato",
          "body": "nvm, this seems to be a memory limitation, derp. Is there a way to work around this? ",
          "created_at": "2025-02-17T18:40:19Z"
        },
        {
          "author": "qiuxin2012",
          "body": "You can try to tune `OLLAMA_NUM_GPU=999`, like `OLLAMA_NUM_GPU=18`. It means put 18 layers on GPU, rest layers on CPU.",
          "created_at": "2025-02-18T00:39:43Z"
        },
        {
          "author": "ytliew82",
          "body": "I am facing the similar issue while running ollama with deepseek-coder-v2 16b and olmoe 7b, both are mixture-of-experts (MoE) code language model,\n\nThe number of work-items in each dimension of a work-group cannot exceed {512, 512, 512} for this deviceException caught at file:/home/runner/_work/llm.",
          "created_at": "2025-03-15T15:16:44Z"
        },
        {
          "author": "stereomato",
          "body": "This is weird, I had a model be able to use a lot of memory, but with a MoE (10b) I get this? Weird.",
          "created_at": "2025-06-14T16:39:02Z"
        },
        {
          "author": "Ellie-Williams-007",
          "body": "still this error in latest version?",
          "created_at": "2025-06-16T01:38:46Z"
        }
      ]
    },
    {
      "issue_number": 13220,
      "title": "Ollama offload to the CPU then crash",
      "body": "**Describe the bug**\nIntel B580\nI use the docker container, latest version but the bug also reproduce for the precompile.\nI tried with multiple models, deepseek-r1:14b, gemma3:4b-it-fp16, same results, ollama offload the computation to the CPU then crash, sometimes freeze my pc and I need to restart. I can't read much from the logs.\n\n[log.txt](https://github.com/user-attachments/files/20709434/log.txt)\n\n\ndocker-compose file I use\n\n```\nname: 'ollama'\n\nservices:\n  intel-llm:\n    image: intelanalytics/ipex-llm-inference-cpp-xpu:latest\n    container_name: intel-llm\n    devices:\n      - /dev/dri\n    environment:\n      no_proxy: localhost,127.0.0.1\n      OLLAMA_HOST: 0.0.0.0\n      DEVICE: Arc\n      HOSTNAME: intel-llm\n      OLLAMA_NUM_GPU: \"999\"\n      ZES_ENABLE_SYSMAN: \"1\"\n    volumes:\n      - models:/root/.ollama/models\n    ports:\n      - \"11434:11434\"\n    restart: always\n    command: >\n      sh -c 'mkdir -p /llm/ollama && cd /llm/ollama && init-ollama && exec ./ollama serve'\n\n  openwebui:\n    image: ghcr.io/open-webui/open-webui:main\n    container_name: openwebui\n    environment:\n      OLLAMA_BASE_URL: http://intel-llm:11434\n    volumes:\n      - open-webui:/app/backend/data\n    ports:\n      - \"8080:8080\"\n    depends_on:\n      - intel-llm\n\nvolumes:\n  models:\n    driver: local\n    driver_opts:\n      o: bind\n      type: none\n      device: /path/to/models\n  open-webui:\n    driver: local\n    driver_opts:\n      o: bind\n      type: none\n      device: /path/to/open-webui\n\n```\n\n**How to reproduce**\nSteps to reproduce the error:\nNot sure exactly how to reproduce, the problem seems to appear when I ask something complex, but if I input something very simple first, like \"hello\", ollama load the model and then ask something more complex seems to help.\n\n\n**Environment information**\nArch linux but probably not relevant because I use the docker container\n```\n➜  Downloads ./env-check.sh\n-----------------------------------------------------------------\nPYTHON_VERSION=3.13.3\n-----------------------------------------------------------------\ntransformers=4.52.4\n-----------------------------------------------------------------\ntorch=2.7.0\n-----------------------------------------------------------------\nipex-llm WARNING: Package(s) not found: ipex-llm\n-----------------------------------------------------------------\nIPEX is not installed.\n-----------------------------------------------------------------\nCPU Information:\nArchitecture:                            x86_64\nCPU op-mode(s):                          32-bit, 64-bit\nAddress sizes:                           52 bits physical, 57 bits virtual\nByte Order:                              Little Endian\nCPU(s):                                  16\nOn-line CPU(s) list:                     0-15\nVendor ID:                               GenuineIntel\nModel name:                              Intel(R) Xeon(R) w3-2435\nCPU family:                              6\nModel:                                   143\nThread(s) per core:                      2\nCore(s) per socket:                      8\nSocket(s):                               1\nStepping:                                8\nCPU(s) scaling MHz:                      39%\nCPU max MHz:                             4500.0000\nCPU min MHz:                             800.0000\n-----------------------------------------------------------------\nTotal CPU Memory: 62.2652 GB\n-----------------------------------------------------------------\nOperating System:\n\\S{PRETTY_NAME} \\r (\\l)\n\n-----------------------------------------------------------------\nLinux filip-thinkstation 6.15.2-arch1-1 #1 SMP PREEMPT_DYNAMIC Tue, 10 Jun 2025 21:32:33 +0000 x86_64 GNU/Linux\n-----------------------------------------------------------------\nCLI:\n    Version: 1.2.35.20240423\n    Build ID: efa70d34\n\nService:\n    Version: 1.2.35.20240423\n    Build ID: efa70d34\n    Level Zero Version: 1.21.9\n-----------------------------------------------------------------\nfgrep: warning: fgrep is obsolescent; using grep -F\n-----------------------------------------------------------------\nDriver related package version:\n./env-check.sh: line 161: dpkg: command not found\n-----------------------------------------------------------------\n./env-check.sh: line 167: sycl-ls: command not found\nigpu not detected\n-----------------------------------------------------------------\nxpu-smi is properly installed.\n-----------------------------------------------------------------\nNo device discovered\nGPU0 Memory size=16G\n-----------------------------------------------------------------\n19:00.0 VGA compatible controller: Intel Corporation Battlemage G21 [Arc B580] (prog-if 00 [VGA controller])\n        Subsystem: Device 172f:4215\n        Flags: bus master, fast devsel, latency 0, IRQ 97, NUMA node 0\n        Memory at a6000000 (64-bit, non-prefetchable) [size=16M]\n        Memory at 201800000000 (64-bit, prefetchable) [size=16G]\n        Expansion ROM at a7000000 [disabled] [size=2M]\n        Capabilities: <access denied>\n        Kernel driver in use: xe\n        Kernel modules: xe\n-----------------------------------------------------------------\n```\n",
      "state": "open",
      "author": "FilipLaurentiu",
      "author_type": "User",
      "created_at": "2025-06-12T13:15:13Z",
      "updated_at": "2025-06-16T01:24:21Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13220/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13220",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13220",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:28.016737",
      "comments": [
        {
          "author": "Ellie-Williams-007",
          "body": "Hi I am also a Linux user. I think it's not due to offloading to cpu, but your sycl config error. Maybe you can activate oneapi and run `sycl-ls` to check.",
          "created_at": "2025-06-13T01:27:54Z"
        },
        {
          "author": "FilipLaurentiu",
          "body": "> Hi I am also a Linux user. I think it's not due to offloading to cpu, but your sycl config error. Maybe you can activate oneapi and run `sycl-ls` to check.\n\nwell..it shouldn't be relevant if I run it in a container. I use Arch and I can't find `sycl-ls`",
          "created_at": "2025-06-13T11:33:59Z"
        },
        {
          "author": "stigva",
          "body": "sir, I think you may need yo install a couple of missing packages on your Docker system : dpkg: command not found... sycl-ls: command not found....\nplease just enable a terminal session to your Docker and install the necessary. In the startup of llama, it should tell you if it has found some dedicat",
          "created_at": "2025-06-14T13:51:52Z"
        },
        {
          "author": "FilipLaurentiu",
          "body": "> sir, I think you may need yo install a couple of missing packages on your Docker system : dpkg: command not found... sycl-ls: command not found.... please just enable a terminal session to your Docker and install the necessary. In the startup of llama, it should tell you if it has found some dedic",
          "created_at": "2025-06-15T12:08:54Z"
        },
        {
          "author": "Ellie-Williams-007",
          "body": "This intel team has released a new linux portable zip release, give it a try to see if that works for you man.",
          "created_at": "2025-06-16T01:24:21Z"
        }
      ]
    },
    {
      "issue_number": 13195,
      "title": "OCR slows down with ollama-vision models",
      "body": "**Describe the bug**\nI use Ollama running minicpm to OCR image-based subtitles into text-based via SubtitleEdit 4.0.12. It starts with taking 0.5s to read the image and output text gradually slowing down. After ~40minutes of running it slowed down to 12s to process a single image. Pausing the OCR and restarting ollama-server reset the slowdown cycle and the process of slowdown started again from 0.5s\n\nGPU Utilization also drops from initial ~80% to ~8% with momentary spikes to ~20% when the output is created. \n\n**How to reproduce**\nSteps to reproduce the error:\n1. Load the model (minicpm-v:8b)\n2. Input image-based subtitles into Subtitle edit\n3. Select OCR method Ollama-vision, select minicpm-v:8b \n4. Start OCR\n\n**Screenshots**\n![Image](https://github.com/user-attachments/assets/2ef8ebed-366f-445a-815d-4f06eba37e8b)\n![Image](https://github.com/user-attachments/assets/36798963-0293-41ff-bd22-ef8cfc4af742)\n![Image](https://github.com/user-attachments/assets/a7c3c691-6e7a-4927-b353-af6e5632782c)\n\nUsing latest nightly build of portable ipex-llm-ollama\n\n\n",
      "state": "closed",
      "author": "ksof1",
      "author_type": "User",
      "created_at": "2025-05-28T20:48:04Z",
      "updated_at": "2025-06-15T18:16:16Z",
      "closed_at": "2025-06-15T18:16:16Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13195/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13195",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13195",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:28.275194",
      "comments": [
        {
          "author": "ksof1",
          "body": "Appears to be fixed in latest nightly build. ",
          "created_at": "2025-06-15T18:16:16Z"
        }
      ]
    },
    {
      "issue_number": 13213,
      "title": "Ollama run crashes using `ipex-llm-inference-cpp-xpu` docker image - A750 - Seg Fault",
      "body": "**Describe the bug**\nWhen I try to use the latest docker image `ipex-llm-inference-cpp-xpu` to run ollama, there will be a seg fault whenever I try to run any model. \n\n```\nSIGBUS: bus error\nPC=0x7a7e89d52e07 m=3 sigcode=2 addr=0x7a7d52fae020\nsignal arrived during cgo execution\n```\n\nI have followed the instructions detailed here: https://github.com/intel/ipex-llm/blob/main/docs/mddocs/DockerGuides/docker_cpp_xpu_quickstart.md\nThe container has been started with the exact docker run command detailed in the example. \n\nThe GPU seems to be detected inside the container correctly: \n```\nroot@host:/llm/scripts# sycl-ls\n[level_zero:gpu][level_zero:0] Intel(R) oneAPI Unified Runtime over Level-Zero, Intel(R) Arc(TM) A750 Graphics 12.55.8 [1.6.32224.500000]\n[opencl:cpu][opencl:0] Intel(R) OpenCL, Intel(R) Core(TM) i7-6950X CPU @ 3.00GHz OpenCL 3.0 (Build 0) [2024.18.12.0.05_160000]\n[opencl:gpu][opencl:1] Intel(R) OpenCL Graphics, Intel(R) Arc(TM) A750 Graphics OpenCL 3.0 NEO  [24.52.32224.5]\n```\n\nThe GPU is currently in an older X99-based system (with no resizeable BAR support sadly).\nAny help would be greatly appreicated - would love to get this working and test out what Arc can do with LLMs! \n\n**Environment information**\n```\n-----------------------------------------------------------------\nPYTHON_VERSION=3.13.3\n-----------------------------------------------------------------\nTransformers is not installed.\n-----------------------------------------------------------------\nPyTorch is not installed.\n-----------------------------------------------------------------\nipex-llm WARNING: Package(s) not found: ipex-llm\n-----------------------------------------------------------------\nIPEX is not installed.\n-----------------------------------------------------------------\nCPU Information:\nArchitecture:                            x86_64\nCPU op-mode(s):                          32-bit, 64-bit\nAddress sizes:                           46 bits physical, 48 bits virtual\nByte Order:                              Little Endian\nCPU(s):                                  20\nOn-line CPU(s) list:                     0-19\nVendor ID:                               GenuineIntel\nModel name:                              Intel(R) Core(TM) i7-6950X CPU @ 3.00GHz\nCPU family:                              6\nModel:                                   79\nThread(s) per core:                      2\nCore(s) per socket:                      10\nSocket(s):                               1\nStepping:                                1\nCPU(s) scaling MHz:                      40%\nCPU max MHz:                             4000.0000\nCPU min MHz:                             1200.0000\n-----------------------------------------------------------------\nTotal CPU Memory: 62.7016 GB\nMemory Type: DDR4\n-----------------------------------------------------------------\nOperating System:\n\\S{PRETTY_NAME} \\r (\\l)\n\n-----------------------------------------------------------------\nLinux host 6.12.32-1-lts #1 SMP PREEMPT_DYNAMIC Wed, 04 Jun 2025 14:14:48 +0000 x86_64 GNU/Linux\n-----------------------------------------------------------------\nWARNING: Small BAR detected for device 0000:03:00.0\nCLI:\n    Version: 1.2.35.20240423\n    Build ID: efa70d34\n\nService:\n    Version: 1.2.35.20240423\n    Build ID: efa70d34\n    Level Zero Version: 1.21.9\n-----------------------------------------------------------------\nfgrep: warning: fgrep is obsolescent; using grep -F\nWARNING: Small BAR detected for device 0000:03:00.0\n-----------------------------------------------------------------\nDriver related package version:\n./env-check.sh: line 161: dpkg: command not found\n-----------------------------------------------------------------\n./env-check.sh: line 167: sycl-ls: command not found\nigpu not detected\n-----------------------------------------------------------------\nxpu-smi is properly installed.\n-----------------------------------------------------------------\nWARNING: Small BAR detected for device 0000:03:00.0\nNo device discovered\nGPU0 Memory ize=256M\n-----------------------------------------------------------------\n03:00.0 VGA compatible controller: Intel Corporation DG2 [Arc A750] (rev 08) (prog-if 00 [VGA controller])\n\tSubsystem: Intel Corporation Device 1021\n\tFlags: bus master, fast devsel, latency 0, IRQ 97, NUMA node 0\n\tMemory at fa000000 (64-bit, non-prefetchable) [size=16M]\n\tMemory at c0000000 (64-bit, prefetchable) [size=256M]\n\tExpansion ROM at fb000000 [disabled] [size=2M]\n\tCapabilities: [40] Vendor Specific Information: Len=0c <?>\n\tCapabilities: [70] Express Endpoint, IntMsgNum 0\n\tCapabilities: [ac] MSI: Enable+ Count=1/1 Maskable+ 64bit+\n```\n\n**Additional context**\nThe full output from trying to run Llama 3.2 1B:\n\n```\nroot@host:/llm/scripts# bash start-ollama.sh\nroot@host:/llm/scripts# 2025/06/08 01:18:23 routes.go:1230: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:2048 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:localhost,127.0.0.1]\"\ntime=2025-06-08T01:18:23.065+08:00 level=INFO source=images.go:432 msg=\"total blobs: 5\"\ntime=2025-06-08T01:18:23.065+08:00 level=INFO source=images.go:439 msg=\"total unused blobs removed: 0\"\ntime=2025-06-08T01:18:23.065+08:00 level=INFO source=routes.go:1297 msg=\"Listening on 127.0.0.1:11434\"\ntime=2025-06-08T01:18:23.065+08:00 level=INFO source=routes.go:1298 msg=\"Version intel-ollama-0.6.2\"\ntime=2025-06-08T01:18:23.065+08:00 level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\ntime=2025-06-08T01:18:23.065+08:00 level=INFO source=gpu.go:218 msg=\"using Intel GPU\"\ntime=2025-06-08T01:18:23.087+08:00 level=INFO source=types.go:130 msg=\"inference compute\" id=0 library=oneapi variant=\"\" compute=\"\" driver=0.0 name=\"Intel(R) Arc(TM) A750 Graphics\" total=\"7.9 GiB\" available=\"7.5 GiB\"\ntime=2025-06-08T01:18:39.159+08:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=llama.vision.block_count default=0\ntime=2025-06-08T01:18:39.160+08:00 level=INFO source=sched.go:715 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 gpu=0 parallel=1 available=8096684441 required=\"1.6 GiB\"\ntime=2025-06-08T01:18:39.175+08:00 level=INFO source=server.go:107 msg=\"system memory\" total=\"62.7 GiB\" free=\"61.2 GiB\" free_swap=\"4.0 GiB\"\ntime=2025-06-08T01:18:39.175+08:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=llama.vision.block_count default=0\ntime=2025-06-08T01:18:39.175+08:00 level=INFO source=server.go:154 msg=offload library=oneapi layers.requested=-1 layers.model=17 layers.offload=17 layers.split=\"\" memory.available=\"[7.5 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"1.6 GiB\" memory.required.partial=\"1.6 GiB\" memory.required.kv=\"64.0 MiB\" memory.required.allocations=\"[1.6 GiB]\" memory.weights.total=\"986.2 MiB\" memory.weights.repeating=\"986.2 MiB\" memory.weights.nonrepeating=\"266.2 MiB\" memory.graph.full=\"254.5 MiB\" memory.graph.partial=\"464.0 MiB\"\nllama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from /root/.ollama/models/blobs/sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct\nllama_model_loader: - kv   3:                           general.finetune str              = Instruct\nllama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\nllama_model_loader: - kv   5:                         general.size_label str              = 1B\nllama_model_loader: - kv   6:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   7:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\nllama_model_loader: - kv   8:                          llama.block_count u32              = 16\nllama_model_loader: - kv   9:                       llama.context_length u32              = 131072\nllama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048\nllama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192\nllama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000\nllama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64\nllama_model_loader: - kv  17:               llama.attention.value_length u32              = 64\nllama_model_loader: - kv  18:                          general.file_type u32              = 7\nllama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\nllama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64\nllama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\nllama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\nllama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\nllama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009\nllama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\nllama_model_loader: - kv  29:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   34 tensors\nllama_model_loader: - type q8_0:  113 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q8_0\nprint_info: file size   = 1.22 GiB (8.50 BPW)\nload: special tokens cache size = 256\nload: token to piece cache size = 0.7999 MB\nprint_info: arch             = llama\nprint_info: vocab_only       = 1\nprint_info: model type       = ?B\nprint_info: model params     = 1.24 B\nprint_info: general.name     = Llama 3.2 1B Instruct\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 128256\nprint_info: n_merges         = 280147\nprint_info: BOS token        = 128000 '<|begin_of_text|>'\nprint_info: EOS token        = 128009 '<|eot_id|>'\nprint_info: EOT token        = 128009 '<|eot_id|>'\nprint_info: EOM token        = 128008 '<|eom_id|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: EOG token        = 128008 '<|eom_id|>'\nprint_info: EOG token        = 128009 '<|eot_id|>'\nprint_info: max token length = 256\nllama_model_load: vocab only - skipping tensors\ntime=2025-06-08T01:18:39.506+08:00 level=INFO source=server.go:430 msg=\"starting llama server\" cmd=\"/usr/local/lib/python3.11/dist-packages/bigdl/cpp/libs/ollama/ollama-lib runner --model /root/.ollama/models/blobs/sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 --ctx-size 2048 --batch-size 512 --n-gpu-layers 999 --threads 10 --parallel 1 --port 42041\"\ntime=2025-06-08T01:18:39.507+08:00 level=INFO source=sched.go:450 msg=\"loaded runners\" count=1\ntime=2025-06-08T01:18:39.507+08:00 level=INFO source=server.go:605 msg=\"waiting for llama runner to start responding\"\ntime=2025-06-08T01:18:39.507+08:00 level=INFO source=server.go:639 msg=\"waiting for server to become available\" status=\"llm server error\"\ntime=2025-06-08T01:18:39.559+08:00 level=INFO source=runner.go:883 msg=\"starting go runner\"\nload_backend: loaded SYCL backend from /usr/local/lib/python3.11/dist-packages/bigdl/cpp/libs/ollama/libggml-sycl.so\nload_backend: loaded CPU backend from /usr/local/lib/python3.11/dist-packages/bigdl/cpp/libs/ollama/libggml-cpu-haswell.so\ntime=2025-06-08T01:18:39.592+08:00 level=INFO source=ggml.go:109 msg=system CPU.0.LLAMAFILE=1 CPU.0.OPENMP=1 CPU.0.AARCH64_REPACK=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)\ntime=2025-06-08T01:18:39.593+08:00 level=INFO source=runner.go:944 msg=\"Server listening on 127.0.0.1:42041\"\nllama_model_load_from_file_impl: using device SYCL0 (Intel(R) Arc(TM) A750 Graphics) - 7721 MiB free\nllama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from /root/.ollama/models/blobs/sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45 (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct\nllama_model_loader: - kv   3:                           general.finetune str              = Instruct\nllama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\nllama_model_loader: - kv   5:                         general.size_label str              = 1B\nllama_model_loader: - kv   6:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   7:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\nllama_model_loader: - kv   8:                          llama.block_count u32              = 16\nllama_model_loader: - kv   9:                       llama.context_length u32              = 131072\nllama_model_loader: - kv  10:                     llama.embedding_length u32              = 2048\nllama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192\nllama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000\nllama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  16:                 llama.attention.key_length u32              = 64\nllama_model_loader: - kv  17:               llama.attention.value_length u32              = 64\nllama_model_loader: - kv  18:                          general.file_type u32              = 7\nllama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\nllama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64\nllama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\nllama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\nllama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\nllama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009\nllama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\nllama_model_loader: - kv  29:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   34 tensors\nllama_model_loader: - type q8_0:  113 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q8_0\nprint_info: file size   = 1.22 GiB (8.50 BPW)\ntime=2025-06-08T01:18:39.759+08:00 level=INFO source=server.go:639 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nload: special tokens cache size = 256\nload: token to piece cache size = 0.7999 MB\nprint_info: arch             = llama\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 2048\nprint_info: n_layer          = 16\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 64\nprint_info: n_swa            = 0\nprint_info: n_embd_head_k    = 64\nprint_info: n_embd_head_v    = 64\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 512\nprint_info: n_embd_v_gqa     = 512\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-05\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: n_ff             = 8192\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 0\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 500000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 1B\nprint_info: model params     = 1.24 B\nprint_info: general.name     = Llama 3.2 1B Instruct\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 128256\nprint_info: n_merges         = 280147\nprint_info: BOS token        = 128000 '<|begin_of_text|>'\nprint_info: EOS token        = 128009 '<|eot_id|>'\nprint_info: EOT token        = 128009 '<|eot_id|>'\nprint_info: EOM token        = 128008 '<|eom_id|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: EOG token        = 128008 '<|eom_id|>'\nprint_info: EOG token        = 128009 '<|eot_id|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: offloading 16 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 17/17 layers to GPU\nload_tensors:   CPU_Mapped model buffer size =   266.16 MiB\nload_tensors:        SYCL0 model buffer size =  1252.41 MiB\nSIGBUS: bus error\nPC=0x7a7e89d52e07 m=3 sigcode=2 addr=0x7a7d52fae020\nsignal arrived during cgo execution\n\ngoroutine 41 gp=0xc000504fc0 m=3 mp=0xc0000ad008 [syscall]:\nruntime.cgocall(0x1101b60, 0xc0000b9880)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/cgocall.go:167 +0x4b fp=0xc0000b9858 sp=0xc0000b9820 pc=0x48ea4b\ngithub.com/ollama/ollama/llama._Cfunc_llama_model_load_from_file(0x7a7e20000d50, {0x0, 0x0, 0x3e7, 0x1, 0x0, 0x0, 0x1101370, 0xc0005917d0, 0x0, ...})\n\t_cgo_gotypes.go:867 +0x47 fp=0xc0000b9880 sp=0xc0000b9858 pc=0x826e87\ngithub.com/ollama/ollama/llama.LoadModelFromFile.func4(...)\n\t/home/runner/_work/llm.cpp/llm.cpp/ollama-internal/llama/llama.go:311\ngithub.com/ollama/ollama/llama.LoadModelFromFile({0x7ffea80f7395, 0x62}, {0x3e7, 0x0, 0x1, 0x0, {0x0, 0x0, 0x0}, 0xc00005cbd0, ...})\n\t/home/runner/_work/llm.cpp/llm.cpp/ollama-internal/llama/llama.go:311 +0x4d7 fp=0xc0000b9d70 sp=0xc0000b9880 pc=0x829b57\ngithub.com/ollama/ollama/runner/llamarunner.(*Server).loadModel(0xc000126000, {0x3e7, 0x0, 0x1, 0x0, {0x0, 0x0, 0x0}, 0xc00005cbd0, 0x0, ...}, ...)\n\t/home/runner/_work/llm.cpp/llm.cpp/ollama-internal/runner/llamarunner/runner.go:765 +0x10d fp=0xc0000b9ee8 sp=0xc0000b9d70 pc=0x8e2b6d\ngithub.com/ollama/ollama/runner/llamarunner.Execute.gowrap1()\n\t/home/runner/_work/llm.cpp/llm.cpp/ollama-internal/runner/llamarunner/runner.go:918 +0x115 fp=0xc0000b9fe0 sp=0xc0000b9ee8 pc=0x8e46b5\nruntime.goexit({})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc0000b9fe8 sp=0xc0000b9fe0 pc=0x4993a1\ncreated by github.com/ollama/ollama/runner/llamarunner.Execute in goroutine 1\n\t/home/runner/_work/llm.cpp/llm.cpp/ollama-internal/runner/llamarunner/runner.go:918 +0xd8a\n\ngoroutine 1 gp=0xc000002380 m=nil [IO wait]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:435 +0xce fp=0xc00051d5a8 sp=0xc00051d588 pc=0x491d4e\nruntime.netpollblock(0xc00051d5f8?, 0x42b826?, 0x0?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/netpoll.go:575 +0xf7 fp=0xc00051d5e0 sp=0xc00051d5a8 pc=0x456d17\ninternal/poll.runtime_pollWait(0x7a7e8a2c7eb0, 0x72)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/netpoll.go:351 +0x85 fp=0xc00051d600 sp=0xc00051d5e0 pc=0x490f65\ninternal/poll.(*pollDesc).wait(0xc000124100?, 0x900000036?, 0x0)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc00051d628 sp=0xc00051d600 pc=0x5182c7\ninternal/poll.(*pollDesc).waitRead(...)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/internal/poll/fd_poll_runtime.go:89\ninternal/poll.(*FD).Accept(0xc000124100)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/internal/poll/fd_unix.go:620 +0x295 fp=0xc00051d6d0 sp=0xc00051d628 pc=0x51d695\nnet.(*netFD).accept(0xc000124100)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/net/fd_unix.go:172 +0x29 fp=0xc00051d788 sp=0xc00051d6d0 pc=0x5904a9\nnet.(*TCPListener).accept(0xc0000d5580)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/net/tcpsock_posix.go:159 +0x1b fp=0xc00051d7d8 sp=0xc00051d788 pc=0x5a5e1b\nnet.(*TCPListener).Accept(0xc0000d5580)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/net/tcpsock.go:380 +0x30 fp=0xc00051d808 sp=0xc00051d7d8 pc=0x5a4cd0\nnet/http.(*onceCloseListener).Accept(0xc000126120?)\n\t<autogenerated>:1 +0x24 fp=0xc00051d820 sp=0xc00051d808 pc=0x7bc244\nnet/http.(*Server).Serve(0xc000034800, {0x1568278, 0xc0000d5580})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/net/http/server.go:3424 +0x30c fp=0xc00051d950 sp=0xc00051d820 pc=0x793b0c\ngithub.com/ollama/ollama/runner/llamarunner.Execute({0xc000034120, 0xe, 0xe})\n\t/home/runner/_work/llm.cpp/llm.cpp/ollama-internal/runner/llamarunner/runner.go:945 +0x1249 fp=0xc00051dd08 sp=0xc00051d950 pc=0x8e42a9\ngithub.com/ollama/ollama/runner.Execute({0xc000034110?, 0x0?, 0x0?})\n\t/home/runner/_work/llm.cpp/llm.cpp/ollama-internal/runner/runner.go:22 +0xd4 fp=0xc00051dd30 sp=0xc00051dd08 pc=0x946b74\ngithub.com/ollama/ollama/cmd.NewCLI.func2(0xc000116f00?, {0x139a140?, 0x4?, 0x139a144?})\n\t/home/runner/_work/llm.cpp/llm.cpp/ollama-internal/cmd/cmd.go:1327 +0x45 fp=0xc00051dd58 sp=0xc00051dd30 pc=0x10941a5\ngithub.com/spf13/cobra.(*Command).execute(0xc0004d6f08, {0xc000000ee0, 0xe, 0xe})\n\t/root/go/pkg/mod/github.com/spf13/cobra@v1.7.0/command.go:940 +0x85c fp=0xc00051de78 sp=0xc00051dd58 pc=0x609abc\ngithub.com/spf13/cobra.(*Command).ExecuteC(0xc0004ac908)\n\t/root/go/pkg/mod/github.com/spf13/cobra@v1.7.0/command.go:1068 +0x3a5 fp=0xc00051df30 sp=0xc00051de78 pc=0x60a305\ngithub.com/spf13/cobra.(*Command).Execute(...)\n\t/root/go/pkg/mod/github.com/spf13/cobra@v1.7.0/command.go:992\ngithub.com/spf13/cobra.(*Command).ExecuteContext(...)\n\t/root/go/pkg/mod/github.com/spf13/cobra@v1.7.0/command.go:985\nmain.main()\n\t/home/runner/_work/llm.cpp/llm.cpp/ollama-internal/main.go:12 +0x4d fp=0xc00051df50 sp=0xc00051df30 pc=0x109450d\nruntime.main()\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:283 +0x28b fp=0xc00051dfe0 sp=0xc00051df50 pc=0x45e2ab\nruntime.goexit({})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc00051dfe8 sp=0xc00051dfe0 pc=0x4993a1\n\ngoroutine 2 gp=0xc000002e00 m=nil [force gc (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:435 +0xce fp=0xc0000a6fa8 sp=0xc0000a6f88 pc=0x491d4e\nruntime.goparkunlock(...)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:441\nruntime.forcegchelper()\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:348 +0xb3 fp=0xc0000a6fe0 sp=0xc0000a6fa8 pc=0x45e5f3\nruntime.goexit({})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc0000a6fe8 sp=0xc0000a6fe0 pc=0x4993a1\ncreated by runtime.init.7 in goroutine 1\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:336 +0x1a\n\ngoroutine 3 gp=0xc000003340 m=nil [GC sweep wait]:\nruntime.gopark(0x1?, 0x0?, 0x0?, 0x0?, 0x0?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:435 +0xce fp=0xc0000a7780 sp=0xc0000a7760 pc=0x491d4e\nruntime.goparkunlock(...)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:441\nruntime.bgsweep(0xc0000d2000)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgcsweep.go:316 +0xdf fp=0xc0000a77c8 sp=0xc0000a7780 pc=0x448d3f\nruntime.gcenable.gowrap1()\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:204 +0x25 fp=0xc0000a77e0 sp=0xc0000a77c8 pc=0x43d1a5\nruntime.goexit({})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc0000a77e8 sp=0xc0000a77e0 pc=0x4993a1\ncreated by runtime.gcenable in goroutine 1\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:204 +0x66\n\ngoroutine 4 gp=0xc000003500 m=nil [GC scavenge wait]:\nruntime.gopark(0x10000?, 0x1556200?, 0x0?, 0x0?, 0x0?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:435 +0xce fp=0xc0000a7f78 sp=0xc0000a7f58 pc=0x491d4e\nruntime.goparkunlock(...)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:441\nruntime.(*scavengerState).park(0x202bb80)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgcscavenge.go:425 +0x49 fp=0xc0000a7fa8 sp=0xc0000a7f78 pc=0x446789\nruntime.bgscavenge(0xc0000d2000)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgcscavenge.go:658 +0x59 fp=0xc0000a7fc8 sp=0xc0000a7fa8 pc=0x446d19\nruntime.gcenable.gowrap2()\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:205 +0x25 fp=0xc0000a7fe0 sp=0xc0000a7fc8 pc=0x43d145\nruntime.goexit({})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc0000a7fe8 sp=0xc0000a7fe0 pc=0x4993a1\ncreated by runtime.gcenable in goroutine 1\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:205 +0xa5\n\ngoroutine 5 gp=0xc000003dc0 m=nil [finalizer wait]:\nruntime.gopark(0x1b8?, 0xc000002380?, 0x1?, 0x23?, 0xc0000a6688?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:435 +0xce fp=0xc0000a6630 sp=0xc0000a6610 pc=0x491d4e\nruntime.runfinq()\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mfinal.go:196 +0x107 fp=0xc0000a67e0 sp=0xc0000a6630 pc=0x43c167\nruntime.goexit({})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc0000a67e8 sp=0xc0000a67e0 pc=0x4993a1\ncreated by runtime.createfing in goroutine 1\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mfinal.go:166 +0x3d\n\ngoroutine 6 gp=0xc0001fe8c0 m=nil [chan receive]:\nruntime.gopark(0xc00024b680?, 0xc000118018?, 0x60?, 0x87?, 0x5771e8?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:435 +0xce fp=0xc0000a8718 sp=0xc0000a86f8 pc=0x491d4e\nruntime.chanrecv(0xc00005a380, 0x0, 0x1)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/chan.go:664 +0x445 fp=0xc0000a8790 sp=0xc0000a8718 pc=0x42e3a5\nruntime.chanrecv1(0x0?, 0x0?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/chan.go:506 +0x12 fp=0xc0000a87b8 sp=0xc0000a8790 pc=0x42df32\nruntime.unique_runtime_registerUniqueMapCleanup.func2(...)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1796\nruntime.unique_runtime_registerUniqueMapCleanup.gowrap1()\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1799 +0x2f fp=0xc0000a87e0 sp=0xc0000a87b8 pc=0x4402ef\nruntime.goexit({})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc0000a87e8 sp=0xc0000a87e0 pc=0x4993a1\ncreated by unique.runtime_registerUniqueMapCleanup in goroutine 1\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1794 +0x79\n\ngoroutine 7 gp=0xc0001fefc0 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:435 +0xce fp=0xc0000a8f38 sp=0xc0000a8f18 pc=0x491d4e\nruntime.gcBgMarkWorker(0xc00005b960)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1423 +0xe9 fp=0xc0000a8fc8 sp=0xc0000a8f38 pc=0x43f609\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x25 fp=0xc0000a8fe0 sp=0xc0000a8fc8 pc=0x43f4e5\nruntime.goexit({})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc0000a8fe8 sp=0xc0000a8fe0 pc=0x4993a1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 8 gp=0xc0001ff180 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:435 +0xce fp=0xc0000a9738 sp=0xc0000a9718 pc=0x491d4e\nruntime.gcBgMarkWorker(0xc00005b960)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1423 +0xe9 fp=0xc0000a97c8 sp=0xc0000a9738 pc=0x43f609\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x25 fp=0xc0000a97e0 sp=0xc0000a97c8 pc=0x43f4e5\nruntime.goexit({})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc0000a97e8 sp=0xc0000a97e0 pc=0x4993a1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 9 gp=0xc0001ff340 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:435 +0xce fp=0xc0000a9f38 sp=0xc0000a9f18 pc=0x491d4e\nruntime.gcBgMarkWorker(0xc00005b960)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1423 +0xe9 fp=0xc0000a9fc8 sp=0xc0000a9f38 pc=0x43f609\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x25 fp=0xc0000a9fe0 sp=0xc0000a9fc8 pc=0x43f4e5\nruntime.goexit({})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc0000a9fe8 sp=0xc0000a9fe0 pc=0x4993a1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 10 gp=0xc0001ff500 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:435 +0xce fp=0xc0000a2738 sp=0xc0000a2718 pc=0x491d4e\nruntime.gcBgMarkWorker(0xc00005b960)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1423 +0xe9 fp=0xc0000a27c8 sp=0xc0000a2738 pc=0x43f609\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x25 fp=0xc0000a27e0 sp=0xc0000a27c8 pc=0x43f4e5\nruntime.goexit({})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc0000a27e8 sp=0xc0000a27e0 pc=0x4993a1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 18 gp=0xc000504000 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:435 +0xce fp=0xc00050a738 sp=0xc00050a718 pc=0x491d4e\nruntime.gcBgMarkWorker(0xc00005b960)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1423 +0xe9 fp=0xc00050a7c8 sp=0xc00050a738 pc=0x43f609\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x25 fp=0xc00050a7e0 sp=0xc00050a7c8 pc=0x43f4e5\nruntime.goexit({})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc00050a7e8 sp=0xc00050a7e0 pc=0x4993a1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 19 gp=0xc0005041c0 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:435 +0xce fp=0xc00050af38 sp=0xc00050af18 pc=0x491d4e\nruntime.gcBgMarkWorker(0xc00005b960)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1423 +0xe9 fp=0xc00050afc8 sp=0xc00050af38 pc=0x43f609\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x25 fp=0xc00050afe0 sp=0xc00050afc8 pc=0x43f4e5\nruntime.goexit({})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc00050afe8 sp=0xc00050afe0 pc=0x4993a1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 34 gp=0xc000102380 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:435 +0xce fp=0xc000506738 sp=0xc000506718 pc=0x491d4e\nruntime.gcBgMarkWorker(0xc00005b960)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1423 +0xe9 fp=0xc0005067c8 sp=0xc000506738 pc=0x43f609\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x25 fp=0xc0005067e0 sp=0xc0005067c8 pc=0x43f4e5\nruntime.goexit({})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc0005067e8 sp=0xc0005067e0 pc=0x4993a1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 35 gp=0xc000102540 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:435 +0xce fp=0xc000506f38 sp=0xc000506f18 pc=0x491d4e\nruntime.gcBgMarkWorker(0xc00005b960)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1423 +0xe9 fp=0xc000506fc8 sp=0xc000506f38 pc=0x43f609\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x25 fp=0xc000506fe0 sp=0xc000506fc8 pc=0x43f4e5\nruntime.goexit({})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc000506fe8 sp=0xc000506fe0 pc=0x4993a1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 36 gp=0xc000102700 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:435 +0xce fp=0xc000507738 sp=0xc000507718 pc=0x491d4e\nruntime.gcBgMarkWorker(0xc00005b960)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1423 +0xe9 fp=0xc0005077c8 sp=0xc000507738 pc=0x43f609\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x25 fp=0xc0005077e0 sp=0xc0005077c8 pc=0x43f4e5\nruntime.goexit({})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc0005077e8 sp=0xc0005077e0 pc=0x4993a1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 37 gp=0xc0001028c0 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:435 +0xce fp=0xc000507f38 sp=0xc000507f18 pc=0x491d4e\nruntime.gcBgMarkWorker(0xc00005b960)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1423 +0xe9 fp=0xc000507fc8 sp=0xc000507f38 pc=0x43f609\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x25 fp=0xc000507fe0 sp=0xc000507fc8 pc=0x43f4e5\nruntime.goexit({})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc000507fe8 sp=0xc000507fe0 pc=0x4993a1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 38 gp=0xc000102a80 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:435 +0xce fp=0xc000508738 sp=0xc000508718 pc=0x491d4e\nruntime.gcBgMarkWorker(0xc00005b960)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1423 +0xe9 fp=0xc0005087c8 sp=0xc000508738 pc=0x43f609\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x25 fp=0xc0005087e0 sp=0xc0005087c8 pc=0x43f4e5\nruntime.goexit({})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc0005087e8 sp=0xc0005087e0 pc=0x4993a1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 39 gp=0xc000102c40 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:435 +0xce fp=0xc000508f38 sp=0xc000508f18 pc=0x491d4e\nruntime.gcBgMarkWorker(0xc00005b960)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1423 +0xe9 fp=0xc000508fc8 sp=0xc000508f38 pc=0x43f609\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x25 fp=0xc000508fe0 sp=0xc000508fc8 pc=0x43f4e5\nruntime.goexit({})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc000508fe8 sp=0xc000508fe0 pc=0x4993a1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 40 gp=0xc000102e00 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:435 +0xce fp=0xc000509738 sp=0xc000509718 pc=0x491d4e\nruntime.gcBgMarkWorker(0xc00005b960)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1423 +0xe9 fp=0xc0005097c8 sp=0xc000509738 pc=0x43f609\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x25 fp=0xc0005097e0 sp=0xc0005097c8 pc=0x43f4e5\nruntime.goexit({})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc0005097e8 sp=0xc0005097e0 pc=0x4993a1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 11 gp=0xc0001ff6c0 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:435 +0xce fp=0xc0000a2f38 sp=0xc0000a2f18 pc=0x491d4e\nruntime.gcBgMarkWorker(0xc00005b960)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1423 +0xe9 fp=0xc0000a2fc8 sp=0xc0000a2f38 pc=0x43f609\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x25 fp=0xc0000a2fe0 sp=0xc0000a2fc8 pc=0x43f4e5\nruntime.goexit({})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc0000a2fe8 sp=0xc0000a2fe0 pc=0x4993a1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 12 gp=0xc0001ff880 m=nil [GC worker (idle)]:\nruntime.gopark(0x3a09a573313?, 0x0?, 0x0?, 0x0?, 0x0?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:435 +0xce fp=0xc0000a3738 sp=0xc0000a3718 pc=0x491d4e\nruntime.gcBgMarkWorker(0xc00005b960)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1423 +0xe9 fp=0xc0000a37c8 sp=0xc0000a3738 pc=0x43f609\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x25 fp=0xc0000a37e0 sp=0xc0000a37c8 pc=0x43f4e5\nruntime.goexit({})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc0000a37e8 sp=0xc0000a37e0 pc=0x4993a1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 13 gp=0xc0001ffa40 m=nil [GC worker (idle)]:\nruntime.gopark(0x20d9b60?, 0x1?, 0x1b?, 0x59?, 0x0?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:435 +0xce fp=0xc0000a3f38 sp=0xc0000a3f18 pc=0x491d4e\nruntime.gcBgMarkWorker(0xc00005b960)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1423 +0xe9 fp=0xc0000a3fc8 sp=0xc0000a3f38 pc=0x43f609\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x25 fp=0xc0000a3fe0 sp=0xc0000a3fc8 pc=0x43f4e5\nruntime.goexit({})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc0000a3fe8 sp=0xc0000a3fe0 pc=0x4993a1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 20 gp=0xc000504380 m=nil [GC worker (idle)]:\nruntime.gopark(0x20d9b60?, 0x1?, 0xb4?, 0xd4?, 0x0?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:435 +0xce fp=0xc00050b738 sp=0xc00050b718 pc=0x491d4e\nruntime.gcBgMarkWorker(0xc00005b960)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1423 +0xe9 fp=0xc00050b7c8 sp=0xc00050b738 pc=0x43f609\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x25 fp=0xc00050b7e0 sp=0xc00050b7c8 pc=0x43f4e5\nruntime.goexit({})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc00050b7e8 sp=0xc00050b7e0 pc=0x4993a1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 21 gp=0xc000504540 m=nil [GC worker (idle)]:\nruntime.gopark(0x20d9b60?, 0x1?, 0x35?, 0x11?, 0x0?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:435 +0xce fp=0xc00050bf38 sp=0xc00050bf18 pc=0x491d4e\nruntime.gcBgMarkWorker(0xc00005b960)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1423 +0xe9 fp=0xc00050bfc8 sp=0xc00050bf38 pc=0x43f609\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x25 fp=0xc00050bfe0 sp=0xc00050bfc8 pc=0x43f4e5\nruntime.goexit({})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc00050bfe8 sp=0xc00050bfe0 pc=0x4993a1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 22 gp=0xc000504700 m=nil [GC worker (idle)]:\nruntime.gopark(0x20d9b60?, 0x1?, 0x48?, 0xf2?, 0x0?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:435 +0xce fp=0xc00050c738 sp=0xc00050c718 pc=0x491d4e\nruntime.gcBgMarkWorker(0xc00005b960)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1423 +0xe9 fp=0xc00050c7c8 sp=0xc00050c738 pc=0x43f609\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x25 fp=0xc00050c7e0 sp=0xc00050c7c8 pc=0x43f4e5\nruntime.goexit({})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc00050c7e8 sp=0xc00050c7e0 pc=0x4993a1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 23 gp=0xc0005048c0 m=nil [GC worker (idle)]:\nruntime.gopark(0x20d9b60?, 0x1?, 0xab?, 0xb2?, 0x0?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:435 +0xce fp=0xc00050cf38 sp=0xc00050cf18 pc=0x491d4e\nruntime.gcBgMarkWorker(0xc00005b960)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1423 +0xe9 fp=0xc00050cfc8 sp=0xc00050cf38 pc=0x43f609\nruntime.gcBgMarkStartWorkers.gowrap1()\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x25 fp=0xc00050cfe0 sp=0xc00050cfc8 pc=0x43f4e5\nruntime.goexit({})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc00050cfe8 sp=0xc00050cfe0 pc=0x4993a1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:1339 +0x105\n\ngoroutine 42 gp=0xc000505180 m=nil [sync.WaitGroup.Wait]:\nruntime.gopark(0x0?, 0x0?, 0x60?, 0x0?, 0x0?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:435 +0xce fp=0xc000509e18 sp=0xc000509df8 pc=0x491d4e\nruntime.goparkunlock(...)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:441\nruntime.semacquire1(0xc000126008, 0x0, 0x1, 0x0, 0x18)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/sema.go:188 +0x21d fp=0xc000509e80 sp=0xc000509e18 pc=0x4717bd\nsync.runtime_SemacquireWaitGroup(0x0?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/sema.go:110 +0x25 fp=0xc000509eb8 sp=0xc000509e80 pc=0x493745\nsync.(*WaitGroup).Wait(0x0?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/sync/waitgroup.go:118 +0x48 fp=0xc000509ee0 sp=0xc000509eb8 pc=0x4a4e08\ngithub.com/ollama/ollama/runner/llamarunner.(*Server).run(0xc000126000, {0x156a540, 0xc0001a0eb0})\n\t/home/runner/_work/llm.cpp/llm.cpp/ollama-internal/runner/llamarunner/runner.go:317 +0x47 fp=0xc000509fb8 sp=0xc000509ee0 pc=0x8df6e7\ngithub.com/ollama/ollama/runner/llamarunner.Execute.gowrap2()\n\t/home/runner/_work/llm.cpp/llm.cpp/ollama-internal/runner/llamarunner/runner.go:925 +0x28 fp=0xc000509fe0 sp=0xc000509fb8 pc=0x8e4568\nruntime.goexit({})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc000509fe8 sp=0xc000509fe0 pc=0x4993a1\ncreated by github.com/ollama/ollama/runner/llamarunner.Execute in goroutine 1\n\t/home/runner/_work/llm.cpp/llm.cpp/ollama-internal/runner/llamarunner/runner.go:925 +0xe65\n\ngoroutine 43 gp=0xc000505340 m=nil [IO wait]:\nruntime.gopark(0x51b8c5?, 0xc000124180?, 0x40?, 0x1a?, 0xb?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:435 +0xce fp=0xc000161948 sp=0xc000161928 pc=0x491d4e\nruntime.netpollblock(0x4b50d8?, 0x42b826?, 0x0?)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/netpoll.go:575 +0xf7 fp=0xc000161980 sp=0xc000161948 pc=0x456d17\ninternal/poll.runtime_pollWait(0x7a7e8a2c7d98, 0x72)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/netpoll.go:351 +0x85 fp=0xc0001619a0 sp=0xc000161980 pc=0x490f65\ninternal/poll.(*pollDesc).wait(0xc000124180?, 0xc000156000?, 0x0)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc0001619c8 sp=0xc0001619a0 pc=0x5182c7\ninternal/poll.(*pollDesc).waitRead(...)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/internal/poll/fd_poll_runtime.go:89\ninternal/poll.(*FD).Read(0xc000124180, {0xc000156000, 0x1000, 0x1000})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/internal/poll/fd_unix.go:165 +0x27a fp=0xc000161a60 sp=0xc0001619c8 pc=0x5195ba\nnet.(*netFD).Read(0xc000124180, {0xc000156000?, 0xc000161ad0?, 0x518785?})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/net/fd_posix.go:55 +0x25 fp=0xc000161aa8 sp=0xc000161a60 pc=0x58e505\nnet.(*conn).Read(0xc0000aa208, {0xc000156000?, 0x0?, 0x0?})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/net/net.go:194 +0x45 fp=0xc000161af0 sp=0xc000161aa8 pc=0x59c8a5\nnet/http.(*connReader).Read(0xc000120510, {0xc000156000, 0x1000, 0x1000})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/net/http/server.go:798 +0x159 fp=0xc000161b40 sp=0xc000161af0 pc=0x7889b9\nbufio.(*Reader).fill(0xc000430240)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/bufio/bufio.go:113 +0x103 fp=0xc000161b78 sp=0xc000161b40 pc=0x5b4023\nbufio.(*Reader).Peek(0xc000430240, 0x4)\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/bufio/bufio.go:152 +0x53 fp=0xc000161b98 sp=0xc000161b78 pc=0x5b4153\nnet/http.(*conn).serve(0xc000126120, {0x156a508, 0xc000120420})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/net/http/server.go:2137 +0x785 fp=0xc000161fb8 sp=0xc000161b98 pc=0x78e7a5\nnet/http.(*Server).Serve.gowrap3()\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/net/http/server.go:3454 +0x28 fp=0xc000161fe0 sp=0xc000161fb8 pc=0x793f08\nruntime.goexit({})\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc000161fe8 sp=0xc000161fe0 pc=0x4993a1\ncreated by net/http.(*Server).Serve in goroutine 1\n\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/net/http/server.go:3454 +0x485\n\nrax    0x7a7d52fae000\nrbx    0x234ce900\nrcx    0x7a7d52fae000\nrdx    0x7a7d52faff80\nrdi    0x7a7d52fae020\nrsi    0x7a7e234c99c0\nrbp    0x7a7e2cbfc3f0\nrsp    0x7a7e2cbfc208\nr8     0x0\nr9     0x1003c5000\nr10    0x1\nr11    0x246\nr12    0x7a7e234c9920\nr13    0x7a7d52fae000\nr14    0x7a7e2340cc20\nr15    0x7a7e234cb930\nrip    0x7a7e89d52e07\nrflags 0x10207\ncs     0x33\nfs     0x0\ngs     0x0\ntime=2025-06-08T01:18:40.024+08:00 level=INFO source=server.go:639 msg=\"waiting for server to become available\" status=\"llm server error\"\ntime=2025-06-08T01:18:40.274+08:00 level=ERROR source=sched.go:456 msg=\"error loading llama server\" error=\"llama runner process has terminated: exit status 2\"\ntime=2025-06-08T01:18:45.293+08:00 level=WARN source=sched.go:647 msg=\"gpu VRAM usage didn't recover within timeout\" seconds=5.018448494 model=/root/.ollama/models/blobs/sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45\ntime=2025-06-08T01:18:45.543+08:00 level=WARN source=sched.go:647 msg=\"gpu VRAM usage didn't recover within timeout\" seconds=5.268760856 model=/root/.ollama/models/blobs/sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45\ntime=2025-06-08T01:18:45.793+08:00 level=WARN source=sched.go:647 msg=\"gpu VRAM usage didn't recover within timeout\" seconds=5.518197234 model=/root/.ollama/models/blobs/sha256-74701a8c35f6c8d9a4b91f3f3497643001d63e0c7a84e085bed452548fa88d45\n```\n",
      "state": "open",
      "author": "sbonner0",
      "author_type": "User",
      "created_at": "2025-06-07T17:55:53Z",
      "updated_at": "2025-06-14T13:33:24Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13213/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13213",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13213",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:28.495146",
      "comments": [
        {
          "author": "Ellie-Williams-007",
          "body": "The reason is that your CPU is too old (i7-6950) — ipex-llm ollama only supports 13th Gen CPUs and newer.",
          "created_at": "2025-06-09T01:38:59Z"
        },
        {
          "author": "sbonner0",
          "body": "> The reason is that your CPU is too old (i7-6950) — ipex-llm ollama only supports 13th Gen CPUs and newer.\n\nThanks for the reply @Ellie-Williams-007 ! \nInteresting, is this requirement due to the intel version of Ollama?\nI have been running Ollama on this exact machine with an nvidia GPU in for ove",
          "created_at": "2025-06-11T07:52:39Z"
        },
        {
          "author": "cyita",
          "body": "> > The reason is that your CPU is too old (i7-6950) — ipex-llm ollama only supports 13th Gen CPUs and newer.\n> \n> Thanks for the reply [@Ellie-Williams-007](https://github.com/Ellie-Williams-007) ! Interesting, is this requirement due to the intel version of Ollama? I have been running Ollama on th",
          "created_at": "2025-06-12T10:34:40Z"
        },
        {
          "author": "stigva",
          "body": "\"bus error\" yes, I know that too. Also tried similar X99 ASRock 18-core rig. I learned some simple lessons... get a CPU with internal graphics (so the dedicated GPU dont need to do more than what you set it to do)... get a new CPU (reasons obvious here)... stick with Windows because there is literal",
          "created_at": "2025-06-14T13:22:17Z"
        }
      ]
    },
    {
      "issue_number": 13221,
      "title": "ollama ipex-llm crashes when running tinyllama llama 3.1b and qwen 2.5",
      "body": "**Describe the bug**\nA clear and concise description of what the bug or error is.\n\n**How to reproduce**\nSteps to reproduce the error:\n1. ...\n2. ...\n3. ...\n4. ...\n\n**Screenshots**\nIf applicable, add screenshots to help explain the problem\n\n**Environment information**\nIf possible, please attach the output of the environment check script, using:\n- https://github.com/intel/ipex-llm/blob/main/python/llm/scripts/env-check.bat, or\n- https://github.com/intel/ipex-llm/blob/main/python/llm/scripts/env-check.sh\n\n**Additional context**\nAdd any other context about the problem here.\n",
      "state": "closed",
      "author": "funky-gibbon",
      "author_type": "User",
      "created_at": "2025-06-14T12:45:53Z",
      "updated_at": "2025-06-14T12:58:10Z",
      "closed_at": "2025-06-14T12:53:41Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13221/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13221",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13221",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:28.716742",
      "comments": [
        {
          "author": "funky-gibbon",
          "body": "1. Problem Description:\n\nWhen attempting to run any model (e.g., tinyllama, llama3.1, qwen2.5:7b) using the ollama-ipex-llm optimized builds on an Intel Arc A770 discrete GPU, the llama runner process consistently terminates with a SYCL error during the ggml_sycl_op_mul_mat operation. intel_gpu_top ",
          "created_at": "2025-06-14T12:53:41Z"
        },
        {
          "author": "funky-gibbon",
          "body": "I forgot to add that the machine I have has an amd 8700f cpu with 32GB RAM and a b650 Auros Eagle AX V2 motherboard and an intel A 770 16GB graphics card.   Cheers MF ",
          "created_at": "2025-06-14T12:58:10Z"
        }
      ]
    },
    {
      "issue_number": 13218,
      "title": "段错误 核心已转移",
      "body": "Ubuntu 24.04 LTS \nGPU: Intel Arc A770 *4\nCPU: Intel(R) Xeon(R) w9-3575X\n\n用的是llama-cpp-ipex-llm-2.3.0b20250424-ubuntu-xeon\n\n原本使用llama-server llama-cli 和flash-moe都正常，直到我安装了anaconda，第二天这三个命令都无法使用率，也不确定是什么问题\n\n![Image](https://github.com/user-attachments/assets/d6a4ca90-152e-4060-885f-adbe4134be78)",
      "state": "open",
      "author": "drew-ye",
      "author_type": "User",
      "created_at": "2025-06-11T03:44:17Z",
      "updated_at": "2025-06-13T09:18:33Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13218/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13218",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13218",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:28.930027",
      "comments": [
        {
          "author": "cyita",
          "body": "Please try to run portable zip w/o conda.",
          "created_at": "2025-06-12T02:13:15Z"
        },
        {
          "author": "drew-ye",
          "body": "> 请尝试在没有 conda 的情况下运行便携式 zip。\n试过的，不行，重装系统了\n",
          "created_at": "2025-06-13T08:32:37Z"
        },
        {
          "author": "qiyuangong",
          "body": "> > 请尝试在没有 conda 的情况下运行便携式 zip。\n> > 试过的，不行，重装系统了\n\nHi @drew-ye \n不需要重新安装系统，麻烦检查下level-zero和driver版本\n\n```bash\ndpkg -l | grep i915\ndpkg -l | grep level-zero\n```\n",
          "created_at": "2025-06-13T09:18:33Z"
        }
      ]
    },
    {
      "issue_number": 13217,
      "title": "multi-modal model chunked-prefill report error on ARC",
      "body": "```\n#!/bin/bash\nMODEL_PATH=${MODEL_PATH:-\"/llm/models/Qwen2-VL-7B-Instruct\"}\nSERVED_MODEL_NAME=${SERVED_MODEL_NAME:-\"Qwen2-VL-7B-Instruct\"}\nTENSOR_PARALLEL_SIZE=${TENSOR_PARALLEL_SIZE:-1}\n\nMAX_NUM_SEQS=${MAX_NUM_SEQS:-256}\nMAX_NUM_BATCHED_TOKENS=${MAX_NUM_BATCHED_TOKENS:-2048}\nMAX_MODEL_LEN=${MAX_MODEL_LEN:-16384}\nLOAD_IN_LOW_BIT=${LOAD_IN_LOW_BIT:-\"fp8\"}\nPORT=${PORT:-8000}\n\necho \"Starting service with model: $MODEL_PATH\"\necho \"Served model name: $SERVED_MODEL_NAME\"\necho \"Tensor parallel size: $TENSOR_PARALLEL_SIZE\"\necho \"Max num sequences: $MAX_NUM_SEQS\"\necho \"Max num batched tokens: $MAX_NUM_BATCHED_TOKENS\"\necho \"Max model length: $MAX_MODEL_LEN\"\necho \"Load in low bit: $LOAD_IN_LOW_BIT\"\necho \"Port: $PORT\"\n\nexport USE_XETLA=OFF\nexport SYCL_CACHE_PERSISTENT=1\nexport SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=2\nexport FI_PROVIDER=shm\nexport TORCH_LLM_ALLREDUCE=0\n\nexport CCL_WORKER_COUNT=2        # On BMG, set CCL_WORKER_COUNT=1; otherwise, internal-oneccl will not function properly\nexport CCL_ATL_TRANSPORT=ofi\nexport CCL_ZE_IPC_EXCHANGE=sockets\nexport CCL_ATL_SHM=1\nexport CCL_SAME_STREAM=1\nexport CCL_BLOCKING_WAIT=0\n# export CCL_DG2_USM=1         # Needed on Core to enable USM (Shared Memory GPUDirect). Xeon supports P2P and doesn't need this.\n\nexport VLLM_USE_V1=0       # Used to select between V0 and V1 engine\nexport IPEX_LLM_LOWBIT=$LOAD_IN_LOW_BIT        # Ensures low-bit info is used for MoE; otherwise, IPEX's default MoE will be used\n\nsource /opt/intel/1ccl-wks/setvars.sh\n\npython -m ipex_llm.vllm.xpu.entrypoints.openai.api_server \\\n  --served-model-name $SERVED_MODEL_NAME \\\n  --port $PORT \\\n  --model $MODEL_PATH \\\n  --trust-remote-code \\\n  --block-size 8 \\\n  --gpu-memory-utilization 0.95 \\\n  --device xpu \\\n  --dtype float16 \\\n  --enforce-eager \\\n  --enable-chunked-prefill \\\n --load-in-low-bit $LOAD_IN_LOW_BIT \\\n  --max-model-len $MAX_MODEL_LEN \\\n  --max-num-batched-tokens $MAX_NUM_BATCHED_TOKENS \\\n  --max-num-seqs $MAX_NUM_SEQS \\\n  --tensor-parallel-size $TENSOR_PARALLEL_SIZE \\\n  --disable-async-output-proc \\\n  --distributed-executor-backend ray\n```\n\n测试脚本：\npython vllm_online_benchmark_multimodal.py --model-name Qwen2-VL-7B-Instruct --image-url 40_20240903093021_78830772282673.jpg\n\n报错日志\n```\nINFO 06-09 13:50:30 [engine.py:310] Added request chatcmpl-fcc96ec1870e430998e0a7dfb64271a0.\nERROR 06-09 13:50:31 [worker_base.py:620] Error executing method 'execute_model'. This might cause deadlock in distributed execution.\nERROR 06-09 13:50:31 [worker_base.py:620] Traceback (most recent call last):\nERROR 06-09 13:50:31 [worker_base.py:620]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.8.3+ipexllm.xpu-py3.11-linux-x86_64.egg/vllm/worker/worker_base.py\", line 612, in execute_method\nERROR 06-09 13:50:31 [worker_base.py:620]     return run_method(self, method, args, kwargs)\nERROR 06-09 13:50:31 [worker_base.py:620]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-09 13:50:31 [worker_base.py:620]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.8.3+ipexllm.xpu-py3.11-linux-x86_64.egg/vllm/utils.py\", line 2349, in run_method\nERROR 06-09 13:50:31 [worker_base.py:620]     return func(*args, **kwargs)\nERROR 06-09 13:50:31 [worker_base.py:620]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 06-09 13:50:31 [worker_base.py:620]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.8.3+ipexllm.xpu-py3.11-linux-x86_64.egg/vllm/worker/worker_base.py\", line 420, in execute_model\nERROR 06-09 13:50:31 [worker_base.py:620]     output = self.model_runner.execute_model(\nERROR 06-09 13:50:31 [worker_base.py:620]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-09 13:50:31 [worker_base.py:620]   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 06-09 13:50:31 [worker_base.py:620]     return func(*args, **kwargs)\nERROR 06-09 13:50:31 [worker_base.py:620]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 06-09 13:50:31 [worker_base.py:620]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.8.3+ipexllm.xpu-py3.11-linux-x86_64.egg/vllm/worker/xpu_model_runner.py\", line 973, in execute_model\nERROR 06-09 13:50:31 [worker_base.py:620]     hidden_or_intermediate_states = model_executable(\nERROR 06-09 13:50:31 [worker_base.py:620]                                     ^^^^^^^^^^^^^^^^^\nERROR 06-09 13:50:31 [worker_base.py:620]   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\nERROR 06-09 13:50:31 [worker_base.py:620]     return self._call_impl(*args, **kwargs)\nERROR 06-09 13:50:31 [worker_base.py:620]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-09 13:50:31 [worker_base.py:620]   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\nERROR 06-09 13:50:31 [worker_base.py:620]     return forward_call(*args, **kwargs)\nERROR 06-09 13:50:31 [worker_base.py:620]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-09 13:50:31 [worker_base.py:620]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.8.3+ipexllm.xpu-py3.11-linux-x86_64.egg/vllm/model_executor/models/qwen2_vl.py\", line 1414, in forward\nERROR 06-09 13:50:31 [worker_base.py:620]     inputs_embeds = self.get_input_embeddings_v0(\nERROR 06-09 13:50:31 [worker_base.py:620]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-09 13:50:31 [worker_base.py:620]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.8.3+ipexllm.xpu-py3.11-linux-x86_64.egg/vllm/model_executor/models/qwen2_vl.py\", line 1352, in get_input_embeddings_v0\nERROR 06-09 13:50:31 [worker_base.py:620]     inputs_embeds = merge_multimodal_embeddings(\nERROR 06-09 13:50:31 [worker_base.py:620]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-09 13:50:31 [worker_base.py:620]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.8.3+ipexllm.xpu-py3.11-linux-x86_64.egg/vllm/model_executor/models/utils.py\", line 481, in merge_multimodal_embeddings\nERROR 06-09 13:50:31 [worker_base.py:620]     return _merge_multimodal_embeddings(\nERROR 06-09 13:50:31 [worker_base.py:620]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-09 13:50:31 [worker_base.py:620]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.8.3+ipexllm.xpu-py3.11-linux-x86_64.egg/vllm/model_executor/models/utils.py\", line 397, in _merge_multimodal_embeddings\nERROR 06-09 13:50:31 [worker_base.py:620]     raise ValueError(\nERROR 06-09 13:50:31 [worker_base.py:620] ValueError: Attempted to assign 1989 = 1989 multimodal tokens to 1926 placeholders\nERROR 06-09 13:50:31 [engine.py:160] ValueError('Attempted to assign 1989 = 1989 multimodal tokens to 1926 placeholders')\nERROR 06-09 13:50:31 [engine.py:160] Traceback (most recent call last):\nERROR 06-09 13:50:31 [engine.py:160]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.8.3+ipexllm.xpu-py3.11-linux-x86_64.egg/vllm/engine/multiprocessing/engine.py\", line 158, in start\nERROR 06-09 13:50:31 [engine.py:160]     self.run_engine_loop()\nERROR 06-09 13:50:31 [engine.py:160]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.8.3+ipexllm.xpu-py3.11-linux-x86_64.egg/vllm/engine/multiprocessing/engine.py\", line 221, in run_engine_loop\nERROR 06-09 13:50:31 [engine.py:160]     request_outputs = self.engine_step()\nERROR 06-09 13:50:31 [engine.py:160]                       ^^^^^^^^^^^^^^^^^^\nERROR 06-09 13:50:31 [engine.py:160]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.8.3+ipexllm.xpu-py3.11-linux-x86_64.egg/vllm/engine/multiprocessing/engine.py\", line 247, in engine_step\nERROR 06-09 13:50:31 [engine.py:160]     raise e\nERROR 06-09 13:50:31 [engine.py:160]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.8.3+ipexllm.xpu-py3.11-linux-x86_64.egg/vllm/engine/multiprocessing/engine.py\", line 230, in engine_step\nERROR 06-09 13:50:31 [engine.py:160]     return self.engine.step()\nERROR 06-09 13:50:31 [engine.py:160]            ^^^^^^^^^^^^^^^^^^\nERROR 06-09 13:50:31 [engine.py:160]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.8.3+ipexllm.xpu-py3.11-linux-x86_64.egg/vllm/engine/llm_engine.py\", line 1430, in step\nERROR 06-09 13:50:31 [engine.py:160]     outputs = self.model_executor.execute_model(\nERROR 06-09 13:50:31 [engine.py:160]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-09 13:50:31 [engine.py:160]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.8.3+ipexllm.xpu-py3.11-linux-x86_64.egg/vllm/executor/ray_distributed_executor.py\", line 451, in execute_model\nERROR 06-09 13:50:31 [engine.py:160]     return super().execute_model(execute_model_req)\nERROR 06-09 13:50:31 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-09 13:50:31 [engine.py:160]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.8.3+ipexllm.xpu-py3.11-linux-x86_64.egg/vllm/executor/executor_base.py\", line 299, in execute_model\nERROR 06-09 13:50:31 [engine.py:160]     driver_outputs = self._driver_execute_model(execute_model_req)\nERROR 06-09 13:50:31 [engine.py:160]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-09 13:50:31 [engine.py:160]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.8.3+ipexllm.xpu-py3.11-linux-x86_64.egg/vllm/executor/ray_distributed_executor.py\", line 444, in _driver_execute_model\nERROR 06-09 13:50:31 [engine.py:160]     return self.driver_worker.execute_method(\"execute_model\",\nERROR 06-09 13:50:31 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-09 13:50:31 [engine.py:160]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.8.3+ipexllm.xpu-py3.11-linux-x86_64.egg/vllm/worker/worker_base.py\", line 621, in execute_method\nERROR 06-09 13:50:31 [engine.py:160]     raise e\nERROR 06-09 13:50:31 [engine.py:160]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.8.3+ipexllm.xpu-py3.11-linux-x86_64.egg/vllm/worker/worker_base.py\", line 612, in execute_method\nERROR 06-09 13:50:31 [engine.py:160]     return run_method(self, method, args, kwargs)\nERROR 06-09 13:50:31 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-09 13:50:31 [engine.py:160]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.8.3+ipexllm.xpu-py3.11-linux-x86_64.egg/vllm/utils.py\", line 2349, in run_method\nERROR 06-09 13:50:31 [engine.py:160]     return func(*args, **kwargs)\nERROR 06-09 13:50:31 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 06-09 13:50:31 [engine.py:160]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.8.3+ipexllm.xpu-py3.11-linux-x86_64.egg/vllm/worker/worker_base.py\", line 420, in execute_model\nERROR 06-09 13:50:31 [engine.py:160]     output = self.model_runner.execute_model(\nERROR 06-09 13:50:31 [engine.py:160]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-09 13:50:31 [engine.py:160]   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 06-09 13:50:31 [engine.py:160]     return func(*args, **kwargs)\nERROR 06-09 13:50:31 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 06-09 13:50:31 [engine.py:160]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.8.3+ipexllm.xpu-py3.11-linux-x86_64.egg/vllm/worker/xpu_model_runner.py\", line 973, in execute_model\nERROR 06-09 13:50:31 [engine.py:160]     hidden_or_intermediate_states = model_executable(\nERROR 06-09 13:50:31 [engine.py:160]                                     ^^^^^^^^^^^^^^^^^\nERROR 06-09 13:50:31 [engine.py:160]   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\nERROR 06-09 13:50:31 [engine.py:160]     return self._call_impl(*args, **kwargs)\nERROR 06-09 13:50:31 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-09 13:50:31 [engine.py:160]   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\nERROR 06-09 13:50:31 [engine.py:160]     return forward_call(*args, **kwargs)\nERROR 06-09 13:50:31 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-09 13:50:31 [engine.py:160]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.8.3+ipexllm.xpu-py3.11-linux-x86_64.egg/vllm/model_executor/models/qwen2_vl.py\", line 1414, in forward\nERROR 06-09 13:50:31 [engine.py:160]     inputs_embeds = self.get_input_embeddings_v0(\nERROR 06-09 13:50:31 [engine.py:160]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-09 13:50:31 [engine.py:160]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.8.3+ipexllm.xpu-py3.11-linux-x86_64.egg/vllm/model_executor/models/qwen2_vl.py\", line 1352, in get_input_embeddings_v0\nERROR 06-09 13:50:31 [engine.py:160]     inputs_embeds = merge_multimodal_embeddings(\nERROR 06-09 13:50:31 [engine.py:160]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-09 13:50:31 [engine.py:160]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.8.3+ipexllm.xpu-py3.11-linux-x86_64.egg/vllm/model_executor/models/utils.py\", line 481, in merge_multimodal_embeddings\nERROR 06-09 13:50:31 [engine.py:160]     return _merge_multimodal_embeddings(\nERROR 06-09 13:50:31 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-09 13:50:31 [engine.py:160]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.8.3+ipexllm.xpu-py3.11-linux-x86_64.egg/vllm/model_executor/models/utils.py\", line 397, in _merge_multimodal_embeddings\nERROR 06-09 13:50:31 [engine.py:160]     raise ValueError(\nERROR 06-09 13:50:31 [engine.py:160] ValueError: Attempted to assign 1989 = 1989 multimodal tokens to 1926 placeholders\nERROR 06-09 13:50:31 [serving_chat.py:883] Error in chat completion stream generator.\nERROR 06-09 13:50:31 [serving_chat.py:883] Traceback (most recent call last):\nERROR 06-09 13:50:31 [serving_chat.py:883]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.8.3+ipexllm.xpu-py3.11-linux-x86_64.egg/vllm/entrypoints/openai/serving_chat.py\", line 485, in chat_completion_stream_generator\nERROR 06-09 13:50:31 [serving_chat.py:883]     async for res in result_generator:\nERROR 06-09 13:50:31 [serving_chat.py:883]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.8.3+ipexllm.xpu-py3.11-linux-x86_64.egg/vllm/engine/multiprocessing/client.py\", line 664, in _process_request\nERROR 06-09 13:50:31 [serving_chat.py:883]     raise request_output\nERROR 06-09 13:50:31 [serving_chat.py:883] vllm.engine.multiprocessing.MQEngineDeadError: Engine loop is not running. Inspect the stacktrace to find the original error: ValueError('Attempted to assign 1989 = 1989 multimodal tokens to 1926 placeholders').\nCRITICAL 06-09 13:50:31 [launcher.py:116] MQLLMEngine is already dead, terminating server process\nINFO:     127.0.0.1:55246 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\nCRITICAL 06-09 13:50:31 [launcher.py:116] MQLLMEngine is already dead, terminating server process\nINFO:     127.0.0.1:55252 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\nCRITICAL 06-09 13:50:31 [launcher.py:116] MQLLMEngine is already dead, terminating server process\nINFO:     127.0.0.1:55268 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\nCRITICAL 06-09 13:50:31 [launcher.py:116] MQLLMEngine is already dead, terminating server process\nINFO:     127.0.0.1:55284 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\nCRITICAL 06-09 13:50:31 [launcher.py:116] MQLLMEngine is already dead, terminating server process\nINFO:     127.0.0.1:55286 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\nINFO:     Shutting down\nINFO:     Waiting for application shutdown.\nINFO:     Application shutdown complete.\n```",
      "state": "open",
      "author": "Zjq9409",
      "author_type": "User",
      "created_at": "2025-06-09T06:02:45Z",
      "updated_at": "2025-06-12T07:58:06Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13217/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hzjane"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13217",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13217",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:29.161684",
      "comments": [
        {
          "author": "hzjane",
          "body": "The b21 image actually exits bug on chunked-prefill and it will be fixed in next version. But it seems that the multimodal model can't not use chunked-prefill on v0 engine (Chunked-prefill is like enabling prefix_cache but without using the kv_cache only using the same first_token kernel. And the ke",
          "created_at": "2025-06-12T07:56:57Z"
        }
      ]
    },
    {
      "issue_number": 13212,
      "title": "ollama-ipex fails running any models - A380 - llama runner process terminated",
      "body": "When running ./ollama run <model>, it will fail after a few seconds. The GPU just barely starts to load up before it fails (watching intel_gpu_top). Error: `Error: llama runner process has terminated: exit status 2`\nLogs below of me trying to run 4 different models. This is the only thing in the log sticking out to me:\n```\nSIGBUS: bus error\nPC=0x7676b317af3d m=4 sigcode=2 addr=0x767514922ff0\nsignal arrived during cgo execution\n```\n\n\nSetup:\nDell T410 server, 2 x xeon processors, 128g ram\nproxmox, VM running fresh ubuntu 22.04 w pcie passthrough\nAsrock A380 low profile, updated firmware\nFollowed https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_portable_zip_quickstart.md\n\nI did find someone else who had similar issues, ended up solving by enabling resizeable bar.  That's not an option for me with this motherboard, hopefully that's not a dealbreaker.\n\n\n\n\n**Environment information**\n```\n(base) josh@ollama:~$ ./env-check.sh\n-----------------------------------------------------------------\nPYTHON_VERSION=3.13.2\n-----------------------------------------------------------------\nTransformers is not installed.\n-----------------------------------------------------------------\nPyTorch is not installed.\n-----------------------------------------------------------------\nipex-llm WARNING: Package(s) not found: ipex-llm\n-----------------------------------------------------------------\nIPEX is not installed.\n-----------------------------------------------------------------\nCPU Information:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        40 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               8\nOn-line CPU(s) list:                  0-7\nVendor ID:                            GenuineIntel\nModel name:                           QEMU Virtual CPU version 2.5+\nCPU family:                           15\nModel:                                107\nThread(s) per core:                   1\nCore(s) per socket:                   4\nSocket(s):                            2\nStepping:                             1\nBogoMIPS:                             6117.99\nFlags:                                fpu de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx lm constant_tsc nopl xtopology cpuid tsc_known_freq pni ssse3 cx16 sse4_1 sse4_2 x2apic popcnt aes hypervisor lahf_lm cpuid_fault pti\nHypervisor vendor:                    KVM\n-----------------------------------------------------------------\nTotal CPU Memory: 15.1406 GB\nMemory Type: -----------------------------------------------------------------\nOperating System:\nUbuntu 22.04.5 LTS \\n \\l\n\n-----------------------------------------------------------------\nLinux ollama 6.8.0-60-generic #63~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Apr 22 19:00:15 UTC 2 x86_64 x86_64 x86_64 GNU/Linux\n-----------------------------------------------------------------\nCLI:\n    Version: 1.2.41.20250422\n    Build ID: 00000000\n\nService:\n    Version: 1.2.41.20250422\n    Build ID: 00000000\n    Level Zero Version: 1.21.1\n-----------------------------------------------------------------\n  Driver UUID                                     32352e31-332e-3333-3237-360000000000\n  Driver Version                                  25.13.33276\n-----------------------------------------------------------------\nDriver related package version:\n-----------------------------------------------------------------\n./env-check.sh: line 167: sycl-ls: command not found\nigpu not detected\n-----------------------------------------------------------------\nxpu-smi is properly installed.\n-----------------------------------------------------------------\n+-----------+--------------------------------------------------------------------------------------+\n| Device ID | Device Information                                                                   |\n+-----------+--------------------------------------------------------------------------------------+\n| 0         | Device Name: Intel(R) Arc(TM) A380 Graphics                                          |\n|           | Vendor Name: Intel(R) Corporation                                                    |\n|           | SOC UUID: 00000000-0000-1006-0000-000556a58086                                       |\n|           | PCI BDF Address: 0000:06:10.0                                                        |\n|           | DRM Device: /dev/dri/card1                                                           |\n|           | Function Type: physical                                                              |\n+-----------+--------------------------------------------------------------------------------------+\nGPU0 Memory [size=4K\nGPU1 Memory size=16M\n-----------------------------------------------------------------\n00:01.0 VGA compatible controller: Device 1234:1111 (rev 02) (prog-if 00 [VGA controller])\n        Subsystem: Red Hat, Inc. Device 1100\n        Flags: bus master, fast devsel, latency 0\n        Memory at f0000000 (32-bit, prefetchable) [disabled] [size=16M]\n        Memory at fe814000 (32-bit, non-prefetchable) [disabled] [size=4K]\n        Expansion ROM at 000c0000 [disabled] [size=128K]\n        Kernel driver in use: bochs-drm\n        Kernel modules: bochs\n\n--\n06:10.0 VGA compatible controller: Intel Corporation Device 56a5 (rev 05) (prog-if 00 [VGA controller])\n        Subsystem: ASRock Incorporation Device 6006\n        Physical Slot: 16-2\n        Flags: bus master, fast devsel, latency 0, IRQ 43\n        Memory at fb000000 (64-bit, non-prefetchable) [size=16M]\n        Memory at c0000000 (64-bit, prefetchable) [size=256M]\n        Capabilities: <access denied>\n        Kernel driver in use: i915\n        Kernel modules: i915, xe\n```\n\n[ollama logs.txt](https://github.com/user-attachments/files/20632784/ollama.logs.txt)\n",
      "state": "open",
      "author": "jpmiller25",
      "author_type": "User",
      "created_at": "2025-06-06T18:29:32Z",
      "updated_at": "2025-06-11T17:08:19Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 13,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13212/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13212",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13212",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:29.360950",
      "comments": [
        {
          "author": "sbonner0",
          "body": "Hey just wanted to flag that I am having a similar issue: https://github.com/intel/ipex-llm/issues/13213\nI'm also running on a system without resizeable bar - Hope this isn't a terminal issue for LLM usage on Arc. ",
          "created_at": "2025-06-07T17:59:21Z"
        },
        {
          "author": "jpmiller25",
          "body": "@sbonner0 does your processor support AVX2?  It's looking to me like my issue is my xeon CPU's without AVX support.\nThere's been discussion and effort on ollama to be able to use non-avx cpu's. I'm trying to find out now if changes have made it upstream and integrated to the ipex version.",
          "created_at": "2025-06-07T19:37:59Z"
        },
        {
          "author": "jpmiller25",
          "body": "I'm using the ipex ollama portable version now, which I think is version 0.5.4. Some people are saying they were able to successfully run with GPU and no AVX support on ollama 0.5.9.",
          "created_at": "2025-06-07T19:39:39Z"
        },
        {
          "author": "jpmiller25",
          "body": "https://github.com/ollama/ollama/issues/7622#issuecomment-2524637378\nhttps://github.com/ollama/ollama/issues/2187",
          "created_at": "2025-06-07T19:40:00Z"
        },
        {
          "author": "jpmiller25",
          "body": "Well now I'm kind of doubting it's an AVX issue for me. I'm getting the same sigbus error using ollama-ipex portable and ollama on ipex-llama-cpp. I've also tried both binaries while running through intel-sde64 to emulate AVX support, and same issue.",
          "created_at": "2025-06-07T22:22:34Z"
        }
      ]
    },
    {
      "issue_number": 13197,
      "title": "Old version of Ollama",
      "body": "When I try to pull Qwen3 with this, the portable version of ollama seems to be out of date.\n\n\n```\nggml_sycl_init: found 1 SYCL devices:\npulling manifest \nError: pull model manifest: 412: \n\nThe model you are attempting to pull requires a newer version of Ollama.\n\nPlease download the latest version at:\n\n\thttps://ollama.com/download\n```\n\n",
      "state": "open",
      "author": "Teejer",
      "author_type": "User",
      "created_at": "2025-05-29T14:46:32Z",
      "updated_at": "2025-06-11T02:41:32Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13197/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13197",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13197",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:29.608199",
      "comments": [
        {
          "author": "rnwang04",
          "body": "Hi @Teejer , could you please upgrade ollama version first by `pip install ipex-llm[cpp]==2.3.0b20250529` and try it again ?",
          "created_at": "2025-05-30T03:56:43Z"
        },
        {
          "author": "thehoff",
          "body": "the 2.3.0 binaries from the repo do not see my A770's on power up, no GPU available, the 2.2.0 does.\n\nstill working out ipex-llm and ollama in python\n\n\n(ipex-llm-test1) thehoff@overmind:~/ipex-ollama/ipex-llm-test1$ pip install ipex-llm[cpp]==2.3.0b20250529\nCollecting ipex-llm==2.3.0b20250529 (from ",
          "created_at": "2025-06-01T07:49:33Z"
        },
        {
          "author": "thehoff",
          "body": "ollama-ipex-llm-2.3.0b20250415-ubuntu - zip/portable from github, no GPU available.\n\ntime=2025-06-02T02:31:58.125Z level=INFO source=routes.go:1297 msg=\"Listening on 127.0.0.1:11434 (version 0.0.0)\"\ntime=2025-06-02T02:31:58.125Z level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\ntime=202",
          "created_at": "2025-06-02T02:34:52Z"
        },
        {
          "author": "rnwang04",
          "body": "Hi @thehoff , `2.3.0rc1` is an older version, please try `pip install ipex-llm[cpp]==2.3.0b20250530` .\nThe information here is somewhat misleading, and we will consider optimizing this part. In fact, if you load the model, you should see that the model is loaded on the GPU.",
          "created_at": "2025-06-03T01:34:36Z"
        },
        {
          "author": "Ellie-Williams-007",
          "body": "> the 2.3.0 binaries from the repo do not see my A770's on power up, no GPU available, the 2.2.0 does.\n> \n> still working out ipex-llm and ollama in python\n> \n> (ipex-llm-test1) thehoff@overmind:~/ipex-ollama/ipex-llm-test1$ pip install ipex-llm[cpp]==2.3.0b20250529 Collecting ipex-llm==2.3.0b202505",
          "created_at": "2025-06-04T14:23:33Z"
        }
      ]
    },
    {
      "issue_number": 12858,
      "title": "No Qwen2.5-VL-7B-Instruct 4bit support",
      "body": "No Qwen2.5-VL-7B-Instruct 4bit support. Do we have plan to support?",
      "state": "open",
      "author": "aixiwangintel",
      "author_type": "User",
      "created_at": "2025-02-20T00:52:08Z",
      "updated_at": "2025-06-09T02:30:11Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12858/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12858",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12858",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:29.840604",
      "comments": [
        {
          "author": "xiangyuT",
          "body": "We now support IPEX-LLM-optimized Qwen2.5-VL models (INT4/FP8 qtype) with vLLM. You could try with [our docs for vLLM](https://github.com/intel/ipex-llm/blob/main/docs/mddocs/DockerGuides/vllm_docker_quickstart.md#multi-modal-model) to get started.",
          "created_at": "2025-06-09T02:30:10Z"
        }
      ]
    },
    {
      "issue_number": 12524,
      "title": "Can support ubuntu with npu?",
      "body": "I try run this project with ubuntu,but some dependence cant find(bigdl-core-npu,pipeline.so);\r\nHow can we run it with ubuntu?",
      "state": "open",
      "author": "shichang00",
      "author_type": "User",
      "created_at": "2024-12-11T06:39:44Z",
      "updated_at": "2025-06-07T13:46:36Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12524/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12524",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12524",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:30.055396",
      "comments": [
        {
          "author": "Oscilloscope98",
          "body": "Hi @dockerg,\r\n\r\nIPEX-LLM currently only supports Intel NPU on Windows. You could refer to [IPEX-LLM NPU QuickStart](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/npu_quickstart.md) for more information :)",
          "created_at": "2024-12-12T02:11:40Z"
        },
        {
          "author": "shichang00",
          "body": "> Hi @dockerg,\r\n> \r\n> IPEX-LLM currently only supports Intel NPU on Windows. You could refer to [IPEX-LLM NPU QuickStart](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/npu_quickstart.md) for more information :)\r\n\r\nIs there any plan for support on linux? @Oscilloscope98",
          "created_at": "2024-12-12T02:19:07Z"
        },
        {
          "author": "Oscilloscope98",
          "body": "Hi @dockerg,\r\n\r\nWe currently do not have plans to support Linux on Intel NPU. We will share here for any updates :)",
          "created_at": "2024-12-13T03:00:21Z"
        },
        {
          "author": "thesolomon-tech",
          "body": "Would be great if this was added. Is it that the NPU drivers are proprietary?",
          "created_at": "2025-02-20T06:49:10Z"
        },
        {
          "author": "Kepontry",
          "body": "Agree, I'm also looking forward to the linux version. Are there any updates on the plan?",
          "created_at": "2025-02-23T06:30:04Z"
        }
      ]
    },
    {
      "issue_number": 12190,
      "title": "Slow text generation on dual Arc A770's w/ vLLM",
      "body": "Hello!\r\n\r\nFollowed the quickstart guide regarding vLLM serving through the available Docker image.\r\nI'm using 2 x Arc A770's in my system.\r\nWhen configured and running on a single GPU, inference speed is fantastic and text generation speed is good (around 14-15t/s and 8-9t/s, respectively). When setting tensor_parallel_size and pipeline_parallel_size to 2 to scale to both GPUs, inference speed doubles, however text generation speed halves, down to 3-4t/s.\r\n\r\nBelow is my start-vllm-service.sh config:\r\n#!/bin/bash\r\nmodel=\"/llm/models/llama-3-1-instruct\"\r\nserved_model_name=\"Llama-3.1\"\r\n\r\nexport CCL_WORKER_COUNT=2\r\nexport FI_PROVIDER=shm\r\nexport CCL_ATL_TRANSPORT=ofi\r\nexport CCL_ZE_IPC_EXCHANGE=sockets\r\nexport CCL_ATL_SHM=1\r\n\r\nexport USE_XETLA=OFF\r\nexport SYCL_CACHE_PERSISTENT=1\r\nexport SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=2\r\nexport TORCH_LLM_ALLREDUCE=0\r\n\r\nsource /opt/intel/1ccl-wks/setvars.sh\r\n\r\npython -m ipex_llm.vllm.xpu.entrypoints.openai.api_server \\\r\n  --served-model-name $served_model_name \\\r\n  --port 8000 \\\r\n  --model $model \\\r\n  --trust-remote-code \\\r\n  --gpu-memory-utilization 0.9 \\\r\n  --device xpu \\\r\n  --dtype float16 \\\r\n  --enforce-eager \\\r\n  --load-in-low-bit sym_int4 \\\r\n  --max-model-len 8192 \\\r\n  --max-num-batched-tokens 10000 \\\r\n  --max-num-seqs 256 \\\r\n  --block-size 8 \\\r\n  --tensor-parallel-size 2\r\n  --pipeline-parallel-size 2\r\n\r\nMaybe I'm missing something, maybe I'm not. I did read to set the SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS to 1 for a performance boost, but set back to 2 during troubleshooting.\r\n\r\nThanks for taking the time to read!\r\nHoping someone has an answer.",
      "state": "closed",
      "author": "HumerousGorgon",
      "author_type": "User",
      "created_at": "2024-10-13T05:56:28Z",
      "updated_at": "2025-06-06T20:17:46Z",
      "closed_at": "2024-10-31T02:47:38Z",
      "labels": [
        "user issue",
        "multi-arc"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 38,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12190/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "gc-fu"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12190",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12190",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:30.400042",
      "comments": []
    },
    {
      "issue_number": 13176,
      "title": "[虚机跑vLLM] ipex-llm-serving-xpu 启动ray实例时卡死",
      "body": "**[硬件配置]**\n  2x EMR6530 (32C x 2S) + 4x Arc A770 (每个NUMA绑2卡)\n**[系统]**\n  ubuntu:22.04\n**[虚机]**\n  云宏的WindWare平台\n  ubuntu:22.04\n**[docker]**\n  ipex-llm-serving-xpu:2.2.0-b17\n  使用这个版本的原因是：\n    这台机器未知原因在ray启动实例时会报错，所以曾经找人帮忙修正了代码后才适配可用\n    使用更新的版本，如b18,b19均会导致旧问题出现，且之前修正的代码不再适用\n**[模型]**\n  DeepSeek-R1-Distill-Qwen-32B\n**[问题]**\n  虚机内启动服务时卡在ray启动实例，没有报错信息，持续24小时无变化\n  容器内使用sycl-ls可见4张卡\n  虚机内使用xpu-smi stats可见4张卡内存占用率达到67%\n  之前不搭建虚机时，裸机启动服务没有类似问题，且能正常benchmark\n\n**[容器启动脚本]** create_container.sh\n```bash\ndocker run -itd \\\n  --net=host \\\n  --device=/dev/dri \\\n  -v /dev/dri:/dev/dri \\\n  -v /root/models:/llm/models \\\n  -v /root/poc:/llm/poc \\\n  -w /llm/poc \\\n  -e no_proxy=localhost,127.0.0.1 \\\n  --entrypoint=\"/bin/bash\" \\\n  --shm-size=\"16g\" \\\n  --name=arc_vllm \\\n  intelanalytics/ipex-llm-serving-xpu:2.2.0-b17 \\\n  /llm/poc/start-vllm-serving.sh\n```\n\n**[服务启动脚本]** start-vllm-serving.sh\n  ```bash\n#!/bin/bash\nmodel=\"/llm/models/DeepSeek-R1-Distill-Qwen-32B\"\nserved_model_name=$model #\"DeepSeek-R1-Distill-Qwen-32B\"\nexport USE_XETLA=OFF\nexport SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1\nexport SYCL_CACHE_PERSISTENT=1\nexport TORCH_LLM_ALLREDUCE=0\nexport CCL_DG2_ALLREDUCE=1\n# Tensor parallel related arguments:\nexport CCL_WORKER_COUNT=4\nexport FI_PROVIDER=shm\nexport CCL_ATL_TRANSPORT=ofi\nexport CCL_ZE_IPC_EXCHANGE=sockets\nexport CCL_ATL_SHM=1\nsource /opt/intel/oneapi/setvars.sh\nsource /opt/intel/1ccl-wks/setvars.sh\npython -m ipex_llm.vllm.xpu.entrypoints.openai.api_server \\\n  --served-model-name $served_model_name \\\n  --port 8008 \\\n  --model $model \\\n  --trust-remote-code \\\n  --block-size 8 \\\n  --gpu-memory-utilization 0.95 \\\n  --device xpu \\\n  --dtype float16 \\\n  --enforce-eager \\\n  --load-in-low-bit fp8 \\\n  --max-model-len 10000 \\\n  --max-num-batched-tokens 10000 \\\n  --max-num-seqs 256 \\\n  --tensor-parallel-size 4 \\\n  --disable-async-output-proc \\\n  --distributed-executor-backend ray\n  ```\n\n  **[最终卡死的日志末尾信息]**\n\n<img width=\"829\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/949df953-ef83-430c-841d-0d669216b1e4\" />",
      "state": "closed",
      "author": "intelyoungway",
      "author_type": "User",
      "created_at": "2025-05-22T06:20:27Z",
      "updated_at": "2025-06-06T09:52:25Z",
      "closed_at": "2025-06-06T09:52:25Z",
      "labels": [
        "multi-arc"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13176/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "gc-fu"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13176",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13176",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:30.400062",
      "comments": [
        {
          "author": "gc-fu",
          "body": "Hi, I will investigate this issue.",
          "created_at": "2025-05-23T02:17:08Z"
        }
      ]
    },
    {
      "issue_number": 13201,
      "title": "Ollama version too old to use it with VSCode and Copilot",
      "body": "**Describe the bug**\nVSCode with copilot needs at least version 0.6.4 of Ollama according this issue\nhttps://github.com/microsoft/vscode-copilot-release/issues/8461\n\nI tried to install latest version of ollama by using `pip install --pre --upgrade 'ipex-llm[cpp]'` , also not sure which version of Ollama it uses, `./ollama -v` returns `0.0.0`\n\nWhen adding Ollama inside VSCode I got this error\n\n`Failed to register Ollama model: TypeError: Cannot read properties of undefined (reading 'includes')`\n\n",
      "state": "open",
      "author": "heyyo-droid",
      "author_type": "User",
      "created_at": "2025-06-01T09:45:03Z",
      "updated_at": "2025-06-05T12:13:29Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13201/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13201",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13201",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:30.579390",
      "comments": [
        {
          "author": "cyita",
          "body": "The ollama version is 0.6.2, it looks like copilot needs some api in newer ollama. We will notify you when we finish the update.",
          "created_at": "2025-06-03T02:00:17Z"
        },
        {
          "author": "jordang81",
          "body": "same here\n",
          "created_at": "2025-06-05T12:13:28Z"
        }
      ]
    },
    {
      "issue_number": 13131,
      "title": "Tensor parallel vllm fails on eGPU",
      "body": "**Describe the bug**\nWhen I try to run the vLLM container https://github.com/intel/ipex-llm/blob/main/docs/mddocs/DockerGuides/vllm_docker_quickstart.md with `tensor_parallel_size=2` on my 2 A770 system where the second A770 is connected to the m2 slot of mainboard I get\n\n\n```\n> dmesg\n[ 1470.497571] i915 0000:07:00.0: [drm] GPU HANG: ecode 12:10:85def5fa, in ray::WrapperWit [14631]\n[ 1470.497577] i915 0000:07:00.0: [drm] ray::IDLE[14631] context reset due to GPU hang\n[ 1470.612921] i915 0000:03:00.0: [drm] GPU HANG: ecode 12:10:85def5fa, in python [14382]\n[ 1470.612925] i915 0000:03:00.0: [drm] python[14382] context reset due to GPU hang\n```\n\nand vLLM crashes with `UR_RESULT_ERROR_DEVICE_LOST`.\n\nThe individual GPUs work when I set `ONEAPI_DEVICE_SELECTOR` and `tensor_parallel_size=1`\n\nIs this a driver problem? Should a second A770 connected as m2 eGPU work or is this unsupported?\n\n**How to reproduce**\nTough - you need my exotic setup\n\n**Environment information**\n```\n❯ sudo bash env-check.sh \n-----------------------------------------------------------------\nPYTHON_VERSION=3.13.3\n-----------------------------------------------------------------\nTransformers is not installed. \n-----------------------------------------------------------------\nPyTorch is not installed. \n-----------------------------------------------------------------\nipex-llm WARNING: Package(s) not found: ipex-llm\n-----------------------------------------------------------------\nIPEX is not installed. \n-----------------------------------------------------------------\nCPU Information: \nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               12\nOn-line CPU(s) list:                  0-11\nVendor ID:                            GenuineIntel\nBIOS Vendor ID:                       Intel(R) Corporation\nModel name:                           12th Gen Intel(R) Core(TM) i5-12500\nBIOS Model name:                      12th Gen Intel(R) Core(TM) i5-12500 To Be Filled By O.E.M. CPU @ 4.0GHz\nBIOS CPU family:                      205\nCPU family:                           6\nModel:                                151\nThread(s) per core:                   2\nCore(s) per socket:                   6\nSocket(s):                            1\nStepping:                             5\n-----------------------------------------------------------------\nTotal CPU Memory: 45.6102 GB\nMemory Type: DDR5 \n-----------------------------------------------------------------\nOperating System: \nUbuntu 25.04 \\n \\l\n\n-----------------------------------------------------------------\nLinux ailab 6.11.0-25-generic #25-Ubuntu SMP PREEMPT_DYNAMIC Fri Apr 11 23:29:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux\n-----------------------------------------------------------------\nCLI:\n    Version: 1.2.39.20241101\n    Build ID: 00000000\n\nService:\n    Version: 1.2.39.20241101\n    Build ID: 00000000\n    Level Zero Version: 1.20.2\n-----------------------------------------------------------------\n  Driver UUID                                     32352e30-392e-3332-3936-310000000000\n  Driver Version                                  25.09.32961\n  Driver UUID                                     32352e30-392e-3332-3936-310000000000\n  Driver Version                                  25.09.32961\n  Driver UUID                                     32352e30-392e-3332-3936-310000000000\n  Driver Version                                  25.09.32961\n-----------------------------------------------------------------\nDriver related package version:\nrc  intel-fw-gpu                                   2024.24.5-337~22.04                        all          Firmware package for Intel integrated and discrete GPUs\nii  intel-level-zero-gpu-raytracing                1.0.0-0ubuntu1~24.10~ppa4                  amd64        Level Zero Ray Tracing Support library\n-----------------------------------------------------------------\nenv-check.sh: line 167: sycl-ls: command not found\nigpu not detected\n-----------------------------------------------------------------\nxpu-smi is properly installed. \n-----------------------------------------------------------------\n+-----------+--------------------------------------------------------------------------------------+\n| Device ID | Device Information                                                                   |\n+-----------+--------------------------------------------------------------------------------------+\n| 0         | Device Name: Intel(R) Arc(TM) A770 Graphics                                          |\n|           | Vendor Name: Intel(R) Corporation                                                    |\n|           | SOC UUID: 00000000-0000-0003-0000-000856a08086                                       |\n|           | PCI BDF Address: 0000:03:00.0                                                        |\n|           | DRM Device: /dev/dri/card1                                                           |\n|           | Function Type: physical                                                              |\n+-----------+--------------------------------------------------------------------------------------+\n| 1         | Device Name: Intel(R) Arc(TM) A770 Graphics                                          |\n|           | Vendor Name: Intel(R) Corporation                                                    |\n|           | SOC UUID: 00000000-0000-0007-0000-000856a08086                                       |\n|           | PCI BDF Address: 0000:07:00.0                                                        |\n|           | DRM Device: /dev/dri/card0                                                           |\n|           | Function Type: physical                                                              |\n+-----------+--------------------------------------------------------------------------------------+\nGPU0 Memory size=16G\nGPU1 Memory size=16G\nGPU2 Memory size=16G\n-----------------------------------------------------------------\n03:00.0 VGA compatible controller: Intel Corporation DG2 [Arc A770] (rev 08) (prog-if 00 [VGA controller])\n        Subsystem: Intel Corporation Device 1020\n        Flags: bus master, fast devsel, latency 0, IRQ 188, IOMMU group 19\n        Memory at 73000000 (64-bit, non-prefetchable) [size=16M]\n        Memory at 5000000000 (64-bit, prefetchable) [size=16G]\n        Expansion ROM at 74000000 [disabled] [size=2M]\n        Capabilities: [40] Vendor Specific Information: Len=0c <?>\n        Capabilities: [70] Express Endpoint, IntMsgNum 0\n        Capabilities: [ac] MSI: Enable+ Count=1/1 Maskable+ 64bit+\n--\n07:00.0 VGA compatible controller: Intel Corporation DG2 [Arc A770] (rev 08) (prog-if 00 [VGA controller])\n        Subsystem: Device 172f:3937\n        Flags: bus master, fast devsel, latency 0, IRQ 192, IOMMU group 24\n        Memory at 71000000 (64-bit, non-prefetchable) [size=16M]\n        Memory at 4800000000 (64-bit, prefetchable) [size=16G]\n        Expansion ROM at 72000000 [disabled] [size=2M]\n        Capabilities: [40] Vendor Specific Information: Len=0c <?>\n        Capabilities: [70] Express Endpoint, IntMsgNum 0\n        Capabilities: [ac] MSI: Enable+ Count=1/1 Maskable+ 64bit+\n--\n7a:00.0 VGA compatible controller: Intel Corporation Battlemage G21 [Arc B580] (prog-if 00 [VGA controller])\n        Subsystem: Intel Corporation Device 1100\n        Flags: bus master, fast devsel, latency 0, IRQ 196, IOMMU group 39\n        Memory at 6f000000 (64-bit, non-prefetchable) [size=16M]\n        Memory at 4000000000 (64-bit, prefetchable) [size=16G]\n        Expansion ROM at 70000000 [disabled] [size=2M]\n        Capabilities: [40] Vendor Specific Information: Len=0c <?>\n        Capabilities: [70] Express Endpoint, IntMsgNum 0\n        Capabilities: [ac] MSI: Enable+ Count=1/1 Maskable+ 64bit+\n-----------------------------------------------------------------\n```\n",
      "state": "open",
      "author": "kirel",
      "author_type": "User",
      "created_at": "2025-05-03T20:48:39Z",
      "updated_at": "2025-06-04T01:03:59Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 13,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13131/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "gc-fu"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13131",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13131",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:30.790769",
      "comments": [
        {
          "author": "HumerousGorgon",
          "body": "vLLM freaks out if used with tensor parallel in any of the newer docker images past b12. I also had this problem. To be honest, I ended up testing out the llama.cpp portable build from the 2.3.0-nightly and it blew my socks off. Qwen3-30B-A3B-Q4 gguf from unSloth was running at 45 tokens per second ",
          "created_at": "2025-05-05T08:01:28Z"
        },
        {
          "author": "kirel",
          "body": "Do you run the portable \"bare metal\" or in a docker container? ",
          "created_at": "2025-05-05T14:56:03Z"
        },
        {
          "author": "HumerousGorgon",
          "body": "I've been running it bare metal and it's been going well. I did look at getting it working in a container but was getting shaderdumps all over the place.\nOne thing I will say is that the nightly releases are kinda slow to release to github, so I'm working on a docker image with the right stuff insta",
          "created_at": "2025-05-05T14:58:25Z"
        },
        {
          "author": "gc-fu",
          "body": "Hi, I wonder if you have installed the Intel out-of-tree driver?\n\nhttps://dgpu-docs.intel.com/driver/kernel-driver-types.html#out-of-tree-drivers\n\n\nAlso, can you post your start script for vLLM service?  I saw you have set some environment varibles.",
          "created_at": "2025-05-06T02:25:40Z"
        },
        {
          "author": "kirel",
          "body": "I did install that but since updated to plucky. That's probably a problem. How do I see if I use the out of tree driver?\n\nMy startup script (slightly extended from the one provided):\n```\n#!/bin/bash\nMODEL_PATH=${MODEL_PATH:-\"default_model_path\"}\nSERVED_MODEL_NAME=${SERVED_MODEL_NAME:-\"default_model_",
          "created_at": "2025-05-06T07:35:01Z"
        }
      ]
    },
    {
      "issue_number": 13193,
      "title": "enable rpc for multi nodes",
      "body": "For llama cpp sycl backend, please help to enable rpc for multiple nodes.\n\n-DGGML_RPC=ON\nPlease refer to :  https://github.com/ggml-org/llama.cpp/tree/master/tools/rpc",
      "state": "open",
      "author": "hli25",
      "author_type": "User",
      "created_at": "2025-05-28T02:50:21Z",
      "updated_at": "2025-05-29T01:54:03Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13193/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "ACupofAir"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13193",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13193",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:30.976856",
      "comments": [
        {
          "author": "ACupofAir",
          "body": "The native RPC of llama cpp is supported, but not released. We are developing a new RPC node for hybrid devices to enable one node with two backend type.",
          "created_at": "2025-05-28T07:12:51Z"
        },
        {
          "author": "hli25",
          "body": "cool, look forward to try the new release with RPC, thanks!",
          "created_at": "2025-05-29T01:54:03Z"
        }
      ]
    },
    {
      "issue_number": 12370,
      "title": "update to ollama 0.4.0",
      "body": "Hi, \r\n\r\nIs it possible that this project could be updated to support ollama 0.4.0. I want to try the new LLama Vision model but to run those models you need atleast version 0.4.0.\r\n\r\nThanks!",
      "state": "open",
      "author": "Matthww",
      "author_type": "User",
      "created_at": "2024-11-08T08:26:00Z",
      "updated_at": "2025-05-27T20:24:19Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 15,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12370/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12370",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12370",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:31.147291",
      "comments": [
        {
          "author": "user7z",
          "body": "i wonder why cant it just be integrated with upstream ollama ? just like nvidia & amd do",
          "created_at": "2024-11-08T19:31:28Z"
        },
        {
          "author": "tristan-k",
          "body": "+1",
          "created_at": "2024-11-09T17:41:32Z"
        },
        {
          "author": "rnwang04",
          "body": "Hi @Matthww , could you please let us know which exactly model do you want to run ?",
          "created_at": "2024-11-11T02:22:32Z"
        },
        {
          "author": "user7z",
          "body": "@Matthww this [one](https://ollama.com/library/llama3.2-vision)",
          "created_at": "2024-11-11T02:31:31Z"
        },
        {
          "author": "Matthww",
          "body": "> Hi @Matthww , could you please let us know which exactly model do you want to run ?\r\n\r\nHi @rnwang04 like @user7z mentioned I'm talking indeed about the Llama 3.2-Vision collection that can be found on ollama's model page: https://ollama.com/library/llama3.2-vision . \r\n\r\nOllama was updated accordin",
          "created_at": "2024-11-11T08:44:29Z"
        }
      ]
    },
    {
      "issue_number": 13010,
      "title": "Unable to use GLM model",
      "body": "**Describe the bug**\nError occurs when using the following GLM model\nhttps://www.modelscope.cn/models/ZhipuAI/glm-edge-1.5b-chat-gguf\nhttps://www.modelscope.cn/models/ZhipuAI/glm-edge-v-2b-gguf\n\n**Screenshots**\n![Image](https://github.com/user-attachments/assets/09f01536-7954-4157-99ac-aadb57f725eb)\n\nError messages:\nllama runner process has terminated: error loading model: missing tensor 'blk.0.attn_qkv.weight'\nllama_load_model_from_file: failed to load model\n\nllama_model_load: error loading model: missing tensor 'blk.0.attn_qkv.weight'\nllama_load_model_from_file: failed to load model\npanic: unable to load model: /root/.ollama/models/blobs/sha256-1d4816cb2da5ac2a5acfa7315049ac9826d52842df81ac567de64755986949fa\n\ngoroutine 20 [running]:\nollama/llama/runner.(*Server).loadModel(0xc0004b2120, {0x3e7, 0x0, 0x0, 0x0, {0x0, 0x0, 0x0}, 0xc000502dd0, 0x0}, ...)\n        ollama/llama/runner/runner.go:861 +0x4ee\ncreated by ollama/llama/runner.Execute in goroutine 1\n        ollama/llama/runner/runner.go:1001 +0xd0d\ntime=2025-03-26T11:22:38.876+08:00 level=ERROR source=sched.go:455 msg=\"error loading llama server\" error=\"llama runner process has terminated:\n error loading model: missing tensor 'blk.0.attn_qkv.weight'\"",
      "state": "open",
      "author": "RonkyTang",
      "author_type": "User",
      "created_at": "2025-03-26T08:58:02Z",
      "updated_at": "2025-05-27T03:31:37Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 30,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13010/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13010",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13010",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:31.367271",
      "comments": []
    },
    {
      "issue_number": 13191,
      "title": "Ollama-ipex cannot  support Gemma3 4B、Gemma3 12B、 phi4-mini 3.8B，but the official Ollama can.",
      "body": "**Describe the bug**\nA clear and concise description of what the bug or error is.\n\n**How to reproduce**\nSteps to reproduce the error:\n1. ...\n2. ...\n3. ...\n4. ...\n\n**Screenshots**\nIf applicable, add screenshots to help explain the problem\n\n**Environment information**\nIf possible, please attach the output of the environment check script, using:\n- https://github.com/intel/ipex-llm/blob/main/python/llm/scripts/env-check.bat, or\n- https://github.com/intel/ipex-llm/blob/main/python/llm/scripts/env-check.sh\n\n**Additional context**\nAdd any other context about the problem here.\n",
      "state": "closed",
      "author": "suxm795",
      "author_type": "User",
      "created_at": "2025-05-27T03:22:09Z",
      "updated_at": "2025-05-27T03:23:03Z",
      "closed_at": "2025-05-27T03:23:03Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13191/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13191",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13191",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:31.367290",
      "comments": []
    },
    {
      "issue_number": 13182,
      "title": "B60 Cannot Use FP16 Precision with GLM4-32B-0414",
      "body": "**Describe the bug**\nB60 Cannot Use FP16 Precision with GLM4-32B-0414\n**How to reproduce**\nDtype float32, lowbit fp16, the issue occurs.\n\n**Screenshots**\n\n![Image](https://github.com/user-attachments/assets/9d97c444-0976-484b-aa04-11225571a5cb)",
      "state": "open",
      "author": "RobinJing",
      "author_type": "User",
      "created_at": "2025-05-22T09:22:29Z",
      "updated_at": "2025-05-27T01:30:45Z",
      "closed_at": null,
      "labels": [
        "user issue",
        "multi-arc"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13182/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "xiangyuT"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13182",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13182",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:31.367297",
      "comments": [
        {
          "author": "hzjane",
          "body": "Currently, neither XPU nor CUDA support using float32 dtype when running weight as fp16.\n.",
          "created_at": "2025-05-27T01:26:09Z"
        }
      ]
    },
    {
      "issue_number": 12684,
      "title": "Will you support llama-cpp-python?",
      "body": "llama-cpp with ipex-llm backend cannot use llama-cpp-python( https://github.com/abetlen/llama-cpp-python/tree/main).\r\nWill you support llama-cpp-python in the future?",
      "state": "open",
      "author": "violet17",
      "author_type": "User",
      "created_at": "2025-01-09T08:12:57Z",
      "updated_at": "2025-05-24T04:32:21Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12684/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "MeouSker77"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12684",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12684",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:31.549946",
      "comments": [
        {
          "author": "nai-kon",
          "body": "ping\nIt will be great if llama-cpp-python with ipex will supported.",
          "created_at": "2025-05-24T04:32:20Z"
        }
      ]
    },
    {
      "issue_number": 13179,
      "title": "B60 Cannot Use FP16 Precision with MOE",
      "body": "**Describe the bug**\nB60 Cannot Use FP16 Precision with MOE\n\n**How to reproduce**\nLoad Qwen3-30B-A3B with dtype float16 and lowbit fp16, the issue occurs.\n\n**Screenshots**\n\n![Image](https://github.com/user-attachments/assets/97c46adc-d712-49b7-98ed-135e2e4e6d9a)",
      "state": "open",
      "author": "RobinJing",
      "author_type": "User",
      "created_at": "2025-05-22T07:33:58Z",
      "updated_at": "2025-05-23T02:10:45Z",
      "closed_at": null,
      "labels": [
        "user issue",
        "multi-arc"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13179/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "xiangyuT"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13179",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13179",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:31.741804",
      "comments": [
        {
          "author": "wrteww",
          "body": "B60?  ？？ B580 24g？",
          "created_at": "2025-05-23T00:32:15Z"
        }
      ]
    },
    {
      "issue_number": 13181,
      "title": "prefix-caching not work on multi-modal model",
      "body": "images: 0.8.3-b19\nserver command：\n```\npython -m ipex_llm.vllm.xpu.entrypoints.openai.api_server \\\n  --port $PORT \\\n  --model $MODEL_PATH \\\n  --trust-remote-code \\\n  --block-size 8 \\\n  --gpu-memory-utilization 0.95 \\\n  --device xpu \\\n  --dtype float16 \\\n  --enforce-eager \\\n  --no-enable-prefix-caching \\\n  --load-in-low-bit $LOAD_IN_LOW_BIT \\\n  --max-model-len $MAX_MODEL_LEN \\\n  --max-num-batched-tokens $MAX_NUM_BATCHED_TOKENS \\\n  --max-num-seqs $MAX_NUM_SEQS \\\n  --tensor-parallel-size $TENSOR_PARALLEL_SIZE \\\n  --disable-async-output-proc \\\n  --distributed-executor-backend ray\n```\n\ntest python code\n```\nimport requests\nimport json\nimport random\nimport base64\nimport os\nurl = \"http://127.0.0.1:8000/v1/chat/completions\"\nheaders = {\n    \"Content-Type\": \"application/json\"\n}\nfile_loc = './resized_image.jpg'\ntexts='''A serene, mystical forest at dawn, with golden sunlight piercing through the misty canopy. '''\n\nfrom PIL import Image\n\nfrom io import BytesIO\n\nTARGET_SIZE = (1080, 1440)\nwith Image.open(file_loc) as img:\n   img = img.resize(TARGET_SIZE)\n   buffered = BytesIO()\n   img.save(buffered, format=\"PNG\")\n   base64_image = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n\nimport time\nstart = time.time()\ndata = {\n    \"model\": \"/llm/models/Qwen2-VL-7B-Instruct/\",\n    \"temperature\": 0.0,\n    \"max_tokens\": 1,\n    \"messages\": [\n          {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n          {\n              \"role\": \"user\",\n              \"content\": [\n                  {\n                      \"type\": \"image_url\",\n                      \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n                  },\n                  {\n                      \"type\": \"text\",\n                      \"text\": texts\n                  },\n              ],\n          },\n      ],\n}\nresponse = requests.post(url, headers=headers, json=data)\nprint(response.text)\nprint(\"time cost is: \", time.time()-start)\n\nstart = time.time()\ndata = {\n    \"model\": \"/llm/models/Qwen2-VL-7B-Instruct/\",\n    \"temperature\": 0.0,\n    \"max_tokens\": 1,\n    \"messages\": [\n          {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n          {\n              \"role\": \"user\",\n              \"content\": [\n                  {\n                      \"type\": \"image_url\",\n                      \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n                  },\n                  {\n                      \"type\": \"text\",\n                      \"text\": texts\n                  },\n              ],\n          },\n      ],\n}\n\nresponse = requests.post(url, headers=headers, json=data)\nprint(response.text)\nprint(\"time cost is: \", time.time()-start)\n```\n\n在ARC上服务端启动脚本在加上 `--no-enable-prefix-caching \\` （不开启prefix-caching）或者去掉` --no-enable-prefix-caching \\`（开启prefix-caching）first token性能都是一样的。\ntime cost is:  1.450150728225708\n但是在NV上表现有区别：\ntime cost is:  0.5974080562591553（不开启prefix-caching）\ntime cost is:  0.14359188079833984（开启prefix-caching）\nNV上还会输出日志prefix cache的日志信息：\n```\nINFO 05-22 15:48:52 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 75.0%\n```\n",
      "state": "open",
      "author": "Zjq9409",
      "author_type": "User",
      "created_at": "2025-05-22T08:09:25Z",
      "updated_at": "2025-05-23T02:10:34Z",
      "closed_at": null,
      "labels": [
        "multi-arc"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13181/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hzjane"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13181",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13181",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:31.947870",
      "comments": []
    },
    {
      "issue_number": 13167,
      "title": "Qwen2.5-VL-32B-Instruct fp16 image recognition issue",
      "body": "**Describe the bug**\nusing intelanalytics/ipex-llm-serving-xpu:0.8.3-b18 to serve Qwen2.5-VL-32B-Instruct,  setting low bit to fp16 will not return image discription, while setting low bit to fp8 works fine.\n\n**How to reproduce**\nSteps to reproduce the error:\n#/bin/bash\nexport DOCKER_IMAGE=intelanalytics/ipex-llm-serving-xpu:0.8.3-b18\nexport CONTAINER_NAME=b18-test\n \nsudo docker run -itd \\\n        --net=host \\\n        --device=/dev/dri \\\n        --privileged \\\n        -v /your_model_path:/llm/models/ \\\n        --name=$CONTAINER_NAME \\\n        --shm-size=\"16g\" \\\n        --entrypoint /bin/bash \\\n        $DOCKER_IMAGE\n\ndocker exec -it b18-test /bin/bash\nexport MODEL_PATH=\"/llm/models/Qwen2.5-VL-32B-Instruct/\"\nexport SERVED_MODEL_NAME=\"Qwen2.5-VL-32B-Instruct\"\nexport TENSOR_PARALLEL_SIZE=8\nbash start-vllm-service.sh\n\ncurl http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"Qwen2.5-VL-32B-Instruct\",\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\n            \"type\": \"text\",\n            \"text\": \"图片里有什么?\"\n          },\n          {\n            \"type\": \"image_url\",\n            \"image_url\": {\n              \"url\": \"http://farm6.staticflickr.com/5268/5602445367_3504763978_z.jpg\"\n            }\n          }\n        ]\n      }\n    ],\n    \"max_tokens\": 128\n  }'\n**Screenshots**\n前面一张是fp8的，可以输出结果。后面那张是fp16的，不能输出识别结果。\n![Image](https://github.com/user-attachments/assets/076fa192-c5e8-423f-bda9-9646fcc89682)\n\n![Image](https://github.com/user-attachments/assets/abd577f5-3c04-41ce-9b12-40a182c9bb6f)\n\n**Environment information**\n8*arc A770\n\n**Additional context**\nAdd any other context about the problem here.\n",
      "state": "open",
      "author": "wluo7",
      "author_type": "User",
      "created_at": "2025-05-20T05:49:54Z",
      "updated_at": "2025-05-22T07:40:05Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13167/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hzjane"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13167",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13167",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:31.947892",
      "comments": [
        {
          "author": "hzjane",
          "body": "Will be fixed by this [pr](https://github.com/intel/ipex-llm/pull/13178)",
          "created_at": "2025-05-22T07:40:04Z"
        }
      ]
    },
    {
      "issue_number": 13171,
      "title": "What's the performance data of glm-edge-v 2B/5B",
      "body": "**Describe the bug**\nhttps://github.com/intel/ipex-llm/tree/main/python/llm/example/GPU/HuggingFace/Multimodal/glm-edge-v\nDo you happen to have the performance data on iGPU/NPU for GLM-Edge-V 2B/5B? \nYou can find Amy Zhao to share to me. Thank you very much!",
      "state": "open",
      "author": "zjamy",
      "author_type": "User",
      "created_at": "2025-05-21T03:24:50Z",
      "updated_at": "2025-05-22T02:07:02Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13171/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13171",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13171",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:32.165627",
      "comments": [
        {
          "author": "cyita",
          "body": "Let's talk offline.",
          "created_at": "2025-05-22T02:07:01Z"
        }
      ]
    },
    {
      "issue_number": 13134,
      "title": "IPEX-LLM Portable Zip Does Not See GPU/s",
      "body": "**Describe the bug**\nWhen running start-ollama.bat from the extracted zip I receive an error that no compatible GPUs are found. \n\nI have tried to set environment vars to just select a single GPU, same problem. oneAPI Level Zero 1.21.6, oneAPI Base Toolkit 2025.1 and ARC Drivers 32.0.101.6790 is installed.\n\nWindows 11 Pro\nCore Ultra 285k with iGPU disabled in BIOS\nDual ARC B580 LE GPUs (in x8/x8 mode)\n\nSame issue on both:\n\nollama-ipex-llm-2.3.0b20250415-win.zip\nollama-ipex-llm-2.3.0b20250429-win.zip\n\n**How to reproduce**\nSteps to reproduce the error:\n1. Extract the ipex-llm for gpu zip file\n2. Run start-ollama.bat\n\n**Screenshots**\n\n![Image](https://github.com/user-attachments/assets/dfd277b4-0a91-4e0d-8bb0-94cf744c6454)\n\n![Image](https://github.com/user-attachments/assets/fe470c14-84f6-4d53-9c23-be162252bc12)\n\n![Image](https://github.com/user-attachments/assets/7f91e55e-537c-40f9-88fd-7bef9b6ef3b9)\n\n**Output**\n\n```\n2025/05/06 09:13:22 routes.go:1230: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY:localhost,127.0.0.1 OLLAMA_CONTEXT_LENGTH:2048 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:10m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\\\Users\\\\phill\\\\.ollama\\\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]\"\ntime=2025-05-06T09:13:22.325-05:00 level=INFO source=images.go:432 msg=\"total blobs: 15\"\ntime=2025-05-06T09:13:22.325-05:00 level=INFO source=images.go:439 msg=\"total unused blobs removed: 0\"\n[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.\n\n[GIN-debug] [WARNING] Running in \"debug\" mode. Switch to \"release\" mode in production.\n - using env:   export GIN_MODE=release\n - using code:  gin.SetMode(gin.ReleaseMode)\n\n[GIN-debug] HEAD   /                         --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\n[GIN-debug] GET    /                         --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\n[GIN-debug] HEAD   /api/version              --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func3 (5 handlers)\n[GIN-debug] GET    /api/version              --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func4 (5 handlers)\n[GIN-debug] POST   /api/pull                 --> github.com/ollama/ollama/server.(*Server).PullHandler-fm (5 handlers)\n[GIN-debug] POST   /api/push                 --> github.com/ollama/ollama/server.(*Server).PushHandler-fm (5 handlers)\n[GIN-debug] HEAD   /api/tags                 --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (5 handlers)\n[GIN-debug] GET    /api/tags                 --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (5 handlers)\n[GIN-debug] POST   /api/show                 --> github.com/ollama/ollama/server.(*Server).ShowHandler-fm (5 handlers)\n[GIN-debug] DELETE /api/delete               --> github.com/ollama/ollama/server.(*Server).DeleteHandler-fm (5 handlers)\n[GIN-debug] POST   /api/create               --> github.com/ollama/ollama/server.(*Server).CreateHandler-fm (5 handlers)\n[GIN-debug] POST   /api/blobs/:digest        --> github.com/ollama/ollama/server.(*Server).CreateBlobHandler-fm (5 handlers)\n[GIN-debug] HEAD   /api/blobs/:digest        --> github.com/ollama/ollama/server.(*Server).HeadBlobHandler-fm (5 handlers)\n[GIN-debug] POST   /api/copy                 --> github.com/ollama/ollama/server.(*Server).CopyHandler-fm (5 handlers)\n[GIN-debug] GET    /api/ps                   --> github.com/ollama/ollama/server.(*Server).PsHandler-fm (5 handlers)\n[GIN-debug] POST   /api/generate             --> github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (5 handlers)\n[GIN-debug] POST   /api/chat                 --> github.com/ollama/ollama/server.(*Server).ChatHandler-fm (5 handlers)\n[GIN-debug] POST   /api/embed                --> github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (5 handlers)\n[GIN-debug] POST   /api/embeddings           --> github.com/ollama/ollama/server.(*Server).EmbeddingsHandler-fm (5 handlers)\n[GIN-debug] POST   /v1/chat/completions      --> github.com/ollama/ollama/server.(*Server).ChatHandler-fm (6 handlers)\n[GIN-debug] POST   /v1/completions           --> github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (6 handlers)\n[GIN-debug] POST   /v1/embeddings            --> github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (6 handlers)\n[GIN-debug] GET    /v1/models                --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (6 handlers)\n[GIN-debug] GET    /v1/models/:model         --> github.com/ollama/ollama/server.(*Server).ShowHandler-fm (6 handlers)\ntime=2025-05-06T09:13:22.326-05:00 level=INFO source=routes.go:1297 msg=\"Listening on 127.0.0.1:11434 (version 0.0.0)\"\ntime=2025-05-06T09:13:22.327-05:00 level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\ntime=2025-05-06T09:13:22.327-05:00 level=INFO source=gpu_windows.go:167 msg=packages count=1\ntime=2025-05-06T09:13:22.327-05:00 level=INFO source=gpu_windows.go:183 msg=\"efficiency cores detected\" maxEfficiencyClass=1\ntime=2025-05-06T09:13:22.327-05:00 level=INFO source=gpu_windows.go:214 msg=\"\" package=0 cores=24 efficiency=16 threads=24\ntime=2025-05-06T09:13:22.334-05:00 level=INFO source=gpu.go:377 msg=\"no compatible GPUs were discovered\"\ntime=2025-05-06T09:13:22.334-05:00 level=INFO source=types.go:130 msg=\"inference compute\" id=0 library=cpu variant=\"\" compute=\"\" driver=0.0 name=\"\" total=\"47.5 GiB\" available=\"34.9 GiB\"\n```",
      "state": "open",
      "author": "phillipscarroll",
      "author_type": "User",
      "created_at": "2025-05-06T14:28:27Z",
      "updated_at": "2025-05-21T08:12:58Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13134/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13134",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13134",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:32.373616",
      "comments": [
        {
          "author": "sgwhat",
          "body": "That's a normal behaviour. You will not see any intel gpu info until you loading a model.",
          "created_at": "2025-05-07T01:12:43Z"
        },
        {
          "author": "akiraaisha",
          "body": "> That's a normal behaviour. You will not see any intel gpu info until you loading a model.\n\nI'm using the same GPU, Intel B580. Just ignore that saying `\"no compatible GPUs were discovered\".`\n\n-  Open a new cmd at the same folder and run Ollama model, it should show your Intel Arc GPU.\n\n![Image](ht",
          "created_at": "2025-05-21T08:12:56Z"
        }
      ]
    },
    {
      "issue_number": 13122,
      "title": "Error running ipex-llm",
      "body": "**Describe the bug**\nAfter I run in a docker and use ./ollama run llama3.2 I get this:\n\n![Image](https://github.com/user-attachments/assets/f8133bd7-b76b-49d2-8fff-11cef1c01de9)\n\n**How to reproduce**\nSteps to reproduce the error:\n1. ...\n2. ...\n3. ...\n4. ...\n\n**Screenshots**\n\n![Image](https://github.com/user-attachments/assets/dc1888cf-f870-493c-81a6-2bb5f0b849be)\n\n**Environment information**\nIf possible, please attach the output of the environment check script, using:\n- https://github.com/intel/ipex-llm/blob/main/python/llm/scripts/env-check.bat, or\n- https://github.com/intel/ipex-llm/blob/main/python/llm/scripts/env-check.sh\n\n**Additional context**\nAdd any other context about the problem here.\n",
      "state": "closed",
      "author": "Njur",
      "author_type": "User",
      "created_at": "2025-04-29T09:25:00Z",
      "updated_at": "2025-05-21T06:18:10Z",
      "closed_at": "2025-05-21T02:02:43Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13122/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13122",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13122",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:32.555194",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Could you pls provide more detailed logs from `ollama serve` side?",
          "created_at": "2025-04-30T01:32:12Z"
        },
        {
          "author": "Njur",
          "body": "Thanks for replying.\nCan you use this?\n\n![Image](https://github.com/user-attachments/assets/f845a90c-c3ba-4bd8-b0c5-51f905eb78f3)",
          "created_at": "2025-04-30T08:07:52Z"
        },
        {
          "author": "sgwhat",
          "body": "Nope, I think that's from ollama client side, not useful to debug.",
          "created_at": "2025-04-30T08:36:49Z"
        },
        {
          "author": "Njur",
          "body": "Okay. If I change the Docker host in ipex-llm template in Unraid from 0.0.0.0:11434 to 192.168.1.2:11434 (Ollama server in the other docker on the machine) then the ipex-llm won't start and gives this error:\n\n![Image](https://github.com/user-attachments/assets/1b9eb0c3-0a18-426d-80d7-7f12b2102197)",
          "created_at": "2025-04-30T09:13:36Z"
        },
        {
          "author": "SiewwenL",
          "body": "Hello, I noticed an error: 'bind: address already in use.' Perhaps you could try running\n\nsudo lsof -i :11434\nsudo kill -9 <PID>\n",
          "created_at": "2025-05-02T04:18:05Z"
        }
      ]
    },
    {
      "issue_number": 13165,
      "title": "--override-tensor \"token_embd.weight=SYCL0\" results in endless repeated 3 as output",
      "body": "I have a 2 Arc A770 setup and wanted to test impact on performance of a coulple llama-server command line options but I get unexpected behavior when I run llama-server from Docker intelanalytics/ipex-llm-inference-cpp-xpu:2.3.0-SNAPSHOT with --override-tensor \"token_embd.weight=SYCL0\": I get endless repetition of \"3\" as output.",
      "state": "open",
      "author": "kirel",
      "author_type": "User",
      "created_at": "2025-05-19T09:58:40Z",
      "updated_at": "2025-05-21T01:37:42Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13165/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "rnwang04"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13165",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13165",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:32.812948",
      "comments": [
        {
          "author": "rnwang04",
          "body": "Hi @kirel , I have reproduced this issue with a Qwen Q4_0 model and fix it. You could try it again with `pip install ipex-llm[cpp]==2.3.0b20250521` (this will be released tonight).\nAt the same time, based on my observation, token embedding is not recommended to put on GPU in llama.cpp community. For",
          "created_at": "2025-05-21T01:37:41Z"
        }
      ]
    },
    {
      "issue_number": 13161,
      "title": "ollama on windows nightly build/portable zip",
      "body": "**Describe the bug**\nOn Windows with a B580 the Ollema build errors with `flag provided but not defined: -ngl`\n\n**How to reproduce**\nSteps to reproduce the error:\n1. Extract [ollama-ipex-llm-2.3.0b20250429-win.zip](https://github.com/ipex-llm/ipex-llm/releases/download/v2.3.0-nightly/ollama-ipex-llm-2.3.0b20250429-win.zip)\n2. Run the start-ollama.bat file\n3. Attempt to run a model `E:\\ollama-ipex-llm-win\\ollama.exe run gemma3:4b-it-fp16`\n\n**Screenshots**\n\n![Image](https://github.com/user-attachments/assets/c8981143-a400-49e1-baa2-6ad9d174f805)\n\n**Environment information**\n```log\n2025/05/18 14:37:09 routes.go:1230: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY:localhost,127.0.0.1 OLLAMA_CONTEXT_LENGTH:2048 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:true OLLAMA_KEEP_ALIVE:10m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:1 OLLAMA_MAX_QUEUE:5 OLLAMA_MODELS:E:\\\\ollama\\\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[* http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]\"\ntime=2025-05-18T14:37:09.077+10:00 level=INFO source=images.go:432 msg=\"total blobs: 106\"\ntime=2025-05-18T14:37:09.079+10:00 level=INFO source=images.go:439 msg=\"total unused blobs removed: 0\"\n[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.\n\n[GIN-debug] [WARNING] Running in \"debug\" mode. Switch to \"release\" mode in production.\n - using env:   export GIN_MODE=release\n - using code:  gin.SetMode(gin.ReleaseMode)\n\n[GIN-debug] HEAD   /                         --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\n[GIN-debug] GET    /                         --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\n[GIN-debug] HEAD   /api/version              --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func3 (5 handlers)\n[GIN-debug] GET    /api/version              --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func4 (5 handlers)\n[GIN-debug] POST   /api/pull                 --> github.com/ollama/ollama/server.(*Server).PullHandler-fm (5 handlers)\n[GIN-debug] POST   /api/push                 --> github.com/ollama/ollama/server.(*Server).PushHandler-fm (5 handlers)\n[GIN-debug] HEAD   /api/tags                 --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (5 handlers)\n[GIN-debug] GET    /api/tags                 --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (5 handlers)\n[GIN-debug] POST   /api/show                 --> github.com/ollama/ollama/server.(*Server).ShowHandler-fm (5 handlers)\n[GIN-debug] DELETE /api/delete               --> github.com/ollama/ollama/server.(*Server).DeleteHandler-fm (5 handlers)\n[GIN-debug] POST   /api/create               --> github.com/ollama/ollama/server.(*Server).CreateHandler-fm (5 handlers)\n[GIN-debug] POST   /api/blobs/:digest        --> github.com/ollama/ollama/server.(*Server).CreateBlobHandler-fm (5 handlers)\n[GIN-debug] HEAD   /api/blobs/:digest        --> github.com/ollama/ollama/server.(*Server).HeadBlobHandler-fm (5 handlers)\n[GIN-debug] POST   /api/copy                 --> github.com/ollama/ollama/server.(*Server).CopyHandler-fm (5 handlers)\n[GIN-debug] GET    /api/ps                   --> github.com/ollama/ollama/server.(*Server).PsHandler-fm (5 handlers)\n[GIN-debug] POST   /api/generate             --> github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (5 handlers)\n[GIN-debug] POST   /api/chat                 --> github.com/ollama/ollama/server.(*Server).ChatHandler-fm (5 handlers)\n[GIN-debug] POST   /api/embed                --> github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (5 handlers)\n[GIN-debug] POST   /api/embeddings           --> github.com/ollama/ollama/server.(*Server).EmbeddingsHandler-fm (5 handlers)\n[GIN-debug] POST   /v1/chat/completions      --> github.com/ollama/ollama/server.(*Server).ChatHandler-fm (6 handlers)\n[GIN-debug] POST   /v1/completions           --> github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (6 handlers)\n[GIN-debug] POST   /v1/embeddings            --> github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (6 handlers)\n[GIN-debug] GET    /v1/models                --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (6 handlers)\n[GIN-debug] GET    /v1/models/:model         --> github.com/ollama/ollama/server.(*Server).ShowHandler-fm (6 handlers)\ntime=2025-05-18T14:37:09.082+10:00 level=INFO source=routes.go:1297 msg=\"Listening on [::]:11434 (version 0.0.0)\"\ntime=2025-05-18T14:37:09.082+10:00 level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\ntime=2025-05-18T14:37:09.082+10:00 level=INFO source=gpu_windows.go:167 msg=packages count=1\ntime=2025-05-18T14:37:09.082+10:00 level=INFO source=gpu_windows.go:214 msg=\"\" package=0 cores=6 efficiency=0 threads=12\ntime=2025-05-18T14:37:09.267+10:00 level=INFO source=amd_hip_windows.go:103 msg=\"AMD ROCm reports no devices found\"\ntime=2025-05-18T14:37:09.267+10:00 level=INFO source=amd_windows.go:49 msg=\"no compatible amdgpu devices detected\"\ntime=2025-05-18T14:37:09.271+10:00 level=INFO source=types.go:130 msg=\"inference compute\" id=0 library=oneapi variant=\"\" compute=\"\" driver=0.0 name=\"\\xc0\" total=\"11.8 GiB\" available=\"9.5 GiB\"\n[GIN] 2025/05/18 - 14:37:19 | 200 |            0s |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/05/18 - 14:37:19 | 200 |     30.8947ms |       127.0.0.1 | POST     \"/api/show\"\ntime=2025-05-18T14:37:19.863+10:00 level=INFO source=server.go:107 msg=\"system memory\" total=\"31.1 GiB\" free=\"23.2 GiB\" free_swap=\"18.4 GiB\"\ntime=2025-05-18T14:37:19.866+10:00 level=INFO source=server.go:154 msg=offload library=oneapi layers.requested=-1 layers.model=35 layers.offload=31 layers.split=\"\" memory.available=\"[9.5 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"11.3 GiB\" memory.required.partial=\"9.4 GiB\" memory.required.kv=\"1.1 GiB\" memory.required.allocations=\"[9.4 GiB]\" memory.weights.total=\"6.0 GiB\" memory.weights.repeating=\"6.0 GiB\" memory.weights.nonrepeating=\"1.3 GiB\" memory.graph.full=\"517.0 MiB\" memory.graph.partial=\"1.0 GiB\" projector.weights=\"795.9 MiB\" projector.graph=\"1.0 GiB\"\ntime=2025-05-18T14:37:19.925+10:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=tokenizer.ggml.pretokenizer default=\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\\\r\\\\n\\\\p{L}\\\\p{N}]?\\\\p{L}+|\\\\p{N}{1,3}| ?[^\\\\s\\\\p{L}\\\\p{N}]+[\\\\r\\\\n]*|\\\\s*[\\\\r\\\\n]+|\\\\s+(?!\\\\S)|\\\\s+\"\ntime=2025-05-18T14:37:19.926+10:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=tokenizer.ggml.add_eot_token default=false\ntime=2025-05-18T14:37:19.928+10:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=tokenizer.ggml.pretokenizer default=\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\\\r\\\\n\\\\p{L}\\\\p{N}]?\\\\p{L}+|\\\\p{N}{1,3}| ?[^\\\\s\\\\p{L}\\\\p{N}]+[\\\\r\\\\n]*|\\\\s*[\\\\r\\\\n]+|\\\\s+(?!\\\\S)|\\\\s+\"\ntime=2025-05-18T14:37:19.931+10:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07\ntime=2025-05-18T14:37:19.931+10:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=gemma3.rope.local.freq_base default=10000\ntime=2025-05-18T14:37:19.931+10:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=gemma3.rope.global.freq_base default=1e+06\ntime=2025-05-18T14:37:19.931+10:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=gemma3.rope.freq_scale default=1\ntime=2025-05-18T14:37:19.931+10:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=gemma3.mm_tokens_per_image default=256\ntime=2025-05-18T14:37:19.937+10:00 level=INFO source=server.go:430 msg=\"starting llama server\" cmd=\"E:\\\\ollama-ipex-llm-win\\\\ollama-lib.exe runner --ollama-engine --model E:\\\\ollama\\\\models\\\\blobs\\\\sha256-2e1715faf889527461e76d271e827bbe03f3d22b4b86acf6146671d72eb6d11d --ctx-size 8192 --batch-size 512 -ngl 999 --threads 6 --parallel 1 --port 60759\"\ntime=2025-05-18T14:37:19.939+10:00 level=INFO source=sched.go:450 msg=\"loaded runners\" count=1\ntime=2025-05-18T14:37:19.939+10:00 level=INFO source=server.go:605 msg=\"waiting for llama runner to start responding\"\ntime=2025-05-18T14:37:19.940+10:00 level=INFO source=server.go:639 msg=\"waiting for server to become available\" status=\"llm server error\"\nflag provided but not defined: -ngl\nRunner usage\n  -batch-size int\n        Batch size (default 512)\n  -ctx-size int\n        Context (or KV cache) size (default 2048)\n  -flash-attn\n        Enable flash attention\n  -kv-cache-type string\n        quantization type for KV cache (default: f16)\n  -lora value\n        Path to lora layer file (can be specified multiple times)\n  -main-gpu int\n        Main GPU\n  -mlock\n        force system to keep model in RAM rather than swapping or compressing\n  -model string\n        Path to model binary file\n  -multiuser-cache\n        optimize input cache algorithm for multiple users\n  -n-gpu-layers int\n        Number of layers to offload to GPU\n  -no-mmap\n        do not memory-map model (slower load but may reduce pageouts if not using mlock)\n  -parallel int\n        Number of sequences to handle simultaneously (default 1)\n  -port int\n        Port to expose the server on (default 8080)\n  -tensor-split string\n        fraction of the model to offload to each GPU, comma-separated list of proportions\n  -threads int\n        Number of threads to use during generation (default 12)\n  -verbose\n        verbose output (default: disabled)\ntime=2025-05-18T14:37:20.190+10:00 level=ERROR source=sched.go:456 msg=\"error loading llama server\" error=\"llama runner process has terminated: exit status 2\"\n[GIN] 2025/05/18 - 14:37:20 | 500 |    551.1771ms |       127.0.0.1 | POST     \"/api/generate\"\n```\n\n**Additional Information**\n\n![Image](https://github.com/user-attachments/assets/eefe6f7c-4921-463b-95ab-f66db86f9e56)\n![Image](https://github.com/user-attachments/assets/5809c7b2-9967-485a-8f9b-a9d6598caf2d)",
      "state": "open",
      "author": "publicarray",
      "author_type": "User",
      "created_at": "2025-05-18T04:47:02Z",
      "updated_at": "2025-05-20T14:43:49Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13161/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13161",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13161",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:33.028239",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @publicarray, we currently do not support Gemma3, and our support is still working on it. We recommend switching to other models such as Qwen3 or DeepSeek-R1 in the meantime.",
          "created_at": "2025-05-19T08:15:20Z"
        },
        {
          "author": "publicarray",
          "body": "@sgwhat  I was under the impression that the  `fp16` version was working: https://github.com/intel/ipex-llm/issues/13129#issuecomment-2848582750",
          "created_at": "2025-05-20T14:39:47Z"
        },
        {
          "author": "publicarray",
          "body": "The docs have also been updated to reflect this https://github.com/intel/ipex-llm/commit/6f634b41da5cd8bf6c17231c788248d3fa6345d6",
          "created_at": "2025-05-20T14:43:49Z"
        }
      ]
    },
    {
      "issue_number": 13163,
      "title": "32B need to run on 2x B580 by vllm",
      "body": "move to another issue.",
      "state": "closed",
      "author": "biyuehuang",
      "author_type": "User",
      "created_at": "2025-05-19T07:07:08Z",
      "updated_at": "2025-05-20T02:15:58Z",
      "closed_at": "2025-05-20T02:15:57Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13163/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13163",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13163",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:33.206114",
      "comments": [
        {
          "author": "liu-shaojun",
          "body": "Hi,\n\nThanks for the message. Could you please provide more details about the issue?\n\nMore context (model name, error logs, what you’ve tried, etc.) would help us understand and assist you better.",
          "created_at": "2025-05-20T01:17:50Z"
        },
        {
          "author": "gc-fu",
          "body": "This has been solved offline.",
          "created_at": "2025-05-20T02:15:57Z"
        }
      ]
    },
    {
      "issue_number": 12873,
      "title": "Attempting to run vLLM on CPU results in an error almost immediately.",
      "body": "Hello!\n\nBasically what the title says! The moment I run 'bash start-vllm-service-sh' it freaks out and spits this out:\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/cpu/entrypoints/openai/api_server.py\", line 30, in <module>\n    from ipex_llm.vllm.cpu.engine import IPEXLLMAsyncLLMEngine as AsyncLLMEngine\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/cpu/engine/__init__.py\", line 16, in <module>\n    from .engine import IPEXLLMAsyncLLMEngine, IPEXLLMLLMEngine, IPEXLLMClass, run_mp_engine\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/cpu/engine/engine.py\", line 24, in <module>\n    from ipex_llm.vllm.cpu.model_convert import _ipex_llm_convert\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/cpu/model_convert.py\", line 20, in <module>\n    from vllm.model_executor.models.llama import LlamaMLP, LlamaAttention, LlamaForCausalLM\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.6.6.post1+cpu-py3.11-linux-x86_64.egg/vllm/model_executor/models/llama.py\", line 39, in <module>\n    from vllm.model_executor.layers.logits_processor import LogitsProcessor\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.6.6.post1+cpu-py3.11-linux-x86_64.egg/vllm/model_executor/layers/logits_processor.py\", line 11, in <module>\n    from vllm.model_executor.layers.vocab_parallel_embedding import (\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.6.6.post1+cpu-py3.11-linux-x86_64.egg/vllm/model_executor/layers/vocab_parallel_embedding.py\", line 136, in <module>\n    @torch.compile(dynamic=True)\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 2424, in fn\n    return compile(\n           ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 2447, in compile\n    return torch._dynamo.optimize(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\", line 716, in optimize\n    return _optimize(rebuild_ctx, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\", line 790, in _optimize\n    compiler_config=backend.get_compiler_config()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 2237, in get_compiler_config\n    from torch._inductor.compile_fx import get_patched_config_dict\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py\", line 49, in <module>\n    from torch._inductor.debug import save_args_for_compile_fx_inner\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/debug.py\", line 26, in <module>\n    from . import config, ir  # noqa: F811, this is needed\n    ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/ir.py\", line 77, in <module>\n    from .runtime.hints import ReductionHint\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/runtime/hints.py\", line 36, in <module>\n    attr_desc_fields = {f.name for f in fields(AttrsDescriptor)}\n                                        ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/dataclasses.py\", line 1246, in fields\n    raise TypeError('must be called with a dataclass type or instance') from None\nTypeError: must be called with a dataclass type or instance\nroot@neutronserver:/llm# nano start-vllm-service.sh\nroot@neutronserver:/llm# bash start-vllm-service.sh\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/cpu/entrypoints/openai/api_server.py\", line 30, in <module>\n    from ipex_llm.vllm.cpu.engine import IPEXLLMAsyncLLMEngine as AsyncLLMEngine\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/cpu/engine/__init__.py\", line 16, in <module>\n    from .engine import IPEXLLMAsyncLLMEngine, IPEXLLMLLMEngine, IPEXLLMClass, run_mp_engine\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/cpu/engine/engine.py\", line 24, in <module>\n    from ipex_llm.vllm.cpu.model_convert import _ipex_llm_convert\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/cpu/model_convert.py\", line 20, in <module>\n    from vllm.model_executor.models.llama import LlamaMLP, LlamaAttention, LlamaForCausalLM\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.6.6.post1+cpu-py3.11-linux-x86_64.egg/vllm/model_executor/models/llama.py\", line 39, in <module>\n    from vllm.model_executor.layers.logits_processor import LogitsProcessor\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.6.6.post1+cpu-py3.11-linux-x86_64.egg/vllm/model_executor/layers/logits_processor.py\", line 11, in <module>\n    from vllm.model_executor.layers.vocab_parallel_embedding import (\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.6.6.post1+cpu-py3.11-linux-x86_64.egg/vllm/model_executor/layers/vocab_parallel_embedding.py\", line 136, in <module>\n    @torch.compile(dynamic=True)\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 2424, in fn\n    return compile(\n           ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 2447, in compile\n    return torch._dynamo.optimize(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\", line 716, in optimize\n    return _optimize(rebuild_ctx, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\", line 790, in _optimize\n    compiler_config=backend.get_compiler_config()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 2237, in get_compiler_config\n    from torch._inductor.compile_fx import get_patched_config_dict\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py\", line 49, in <module>\n    from torch._inductor.debug import save_args_for_compile_fx_inner\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/debug.py\", line 26, in <module>\n    from . import config, ir  # noqa: F811, this is needed\n    ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/ir.py\", line 77, in <module>\n    from .runtime.hints import ReductionHint\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/runtime/hints.py\", line 36, in <module>\n    attr_desc_fields = {f.name for f in fields(AttrsDescriptor)}\n                                        ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/dataclasses.py\", line 1246, in fields\n    raise TypeError('must be called with a dataclass type or instance') from None\nTypeError: must be called with a dataclass type or instance\nroot@neutronserver:/llm# nano start-vllm-service.sh\nroot@neutronserver:/llm# bash start-vllm-service.sh\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/cpu/entrypoints/openai/api_server.py\", line 30, in <module>\n    from ipex_llm.vllm.cpu.engine import IPEXLLMAsyncLLMEngine as AsyncLLMEngine\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/cpu/engine/__init__.py\", line 16, in <module>\n    from .engine import IPEXLLMAsyncLLMEngine, IPEXLLMLLMEngine, IPEXLLMClass, run_mp_engine\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/cpu/engine/engine.py\", line 24, in <module>\n    from ipex_llm.vllm.cpu.model_convert import _ipex_llm_convert\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/cpu/model_convert.py\", line 20, in <module>\n    from vllm.model_executor.models.llama import LlamaMLP, LlamaAttention, LlamaForCausalLM\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.6.6.post1+cpu-py3.11-linux-x86_64.egg/vllm/model_executor/models/llama.py\", line 39, in <module>\n    from vllm.model_executor.layers.logits_processor import LogitsProcessor\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.6.6.post1+cpu-py3.11-linux-x86_64.egg/vllm/model_executor/layers/logits_processor.py\", line 11, in <module>\n    from vllm.model_executor.layers.vocab_parallel_embedding import (\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.6.6.post1+cpu-py3.11-linux-x86_64.egg/vllm/model_executor/layers/vocab_parallel_embedding.py\", line 136, in <module>\n    @torch.compile(dynamic=True)\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 2424, in fn\n    return compile(\n           ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 2447, in compile\n    return torch._dynamo.optimize(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\", line 716, in optimize\n    return _optimize(rebuild_ctx, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\", line 790, in _optimize\n    compiler_config=backend.get_compiler_config()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 2237, in get_compiler_config\n    from torch._inductor.compile_fx import get_patched_config_dict\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py\", line 49, in <module>\n    from torch._inductor.debug import save_args_for_compile_fx_inner\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/debug.py\", line 26, in <module>\n    from . import config, ir  # noqa: F811, this is needed\n    ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/ir.py\", line 77, in <module>\n    from .runtime.hints import ReductionHint\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/runtime/hints.py\", line 36, in <module>\n    attr_desc_fields = {f.name for f in fields(AttrsDescriptor)}\n                                        ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/dataclasses.py\", line 1246, in fields\n    raise TypeError('must be called with a dataclass type or instance') from None\nTypeError: must be called with a dataclass type or instance\n\nAny help would be greatly appreciated!\nThanks.",
      "state": "closed",
      "author": "HumerousGorgon",
      "author_type": "User",
      "created_at": "2025-02-23T03:14:29Z",
      "updated_at": "2025-05-19T02:31:50Z",
      "closed_at": "2025-05-19T02:31:50Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12873/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "gc-fu"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12873",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12873",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:33.470996",
      "comments": [
        {
          "author": "Airren",
          "body": "I encountered the same problem.\n\nThe configuration:\n\n![Image](https://github.com/user-attachments/assets/2245b686-5feb-4a5d-947f-2d964eea5f3c)\n\n![Image](https://github.com/user-attachments/assets/22284c4b-75b9-445a-8495-167a5e165edf)\n\nThe crash log:\n\n![Image](https://github.com/user-attachments/asse",
          "created_at": "2025-02-25T02:45:43Z"
        },
        {
          "author": "gc-fu",
          "body": "Hi, can you try to install `pip install triton==3.1.0` and see if this error persists?",
          "created_at": "2025-02-25T06:29:51Z"
        },
        {
          "author": "gc-fu",
          "body": "Tomorrow's image will include the fix, or you can fix by executing this command:`pip install triton==3.1.0`.",
          "created_at": "2025-02-25T07:01:24Z"
        },
        {
          "author": "Airren",
          "body": "> Hi, can you try to install `pip install triton==3.1.0` and see if this error persists?\n\nIt's worked for me 👍 ",
          "created_at": "2025-02-25T07:10:37Z"
        }
      ]
    },
    {
      "issue_number": 13158,
      "title": "Fatal Python error: Segmentation fault",
      "body": "**Describe the bug**\nThis error lead the vllm not work, but still up. Running 4 Arc, 45 error of 50 tests. But run single Arc, 50 test all work no error.\n\n**How to reproduce**\nSteps to reproduce the error:\nvllm input parameters\n```\nexport SHM_SIZE=\"32g\"\nexport DTYPE=\"float16\"\nexport QUANTIZATION=\"fp8\"\nexport MAX_MODEL_LEN=\"8192\"\nexport MAX_NUM_BATCHED_TOKENS=\"8192\"\nexport MAX_NUM_SEQS=\"256\"\nexport LLM_MODEL_ID=\"Qwen/Qwen2.5-Coder-7B-Instruct\"\nexport LLM_MODEL_LOCAL_PATH=\"/data/Qwen/Qwen2.5-Coder-7B-Instruct\"\nexport GPU_AFFINITY=\"1,2,3,4\"\nexport TENSOR_PARALLEL_SIZE=4\nexport TAG=1.2\n```\n```yaml\nservices:\n  vllm-service:\n    image: intelanalytics/ipex-llm-serving-xpu:2.2.0-b14\n    container_name: vllm-service\n    ports:\n      - \"${LLM_ENDPOINT_PORT:-8008}:80\"\n    privileged: true\n    ipc: host\n    devices:\n      - \"/dev/dri:/dev/dri\"\n    volumes:\n      - \"${MODEL_CACHE:-./data}:/data\"\n    shm_size: ${SHM_SIZE:-8g}\n    environment:\n      no_proxy: ${no_proxy}\n      http_proxy: ${http_proxy}\n      https_proxy: ${https_proxy}\n      HUGGINGFACE_HUB_CACHE: \"/data\"\n      LLM_MODEL_ID: ${LLM_MODEL_ID}\n      LLM_MODEL_LOCAL_PATH: ${LLM_MODEL_LOCAL_PATH}\n      VLLM_TORCH_PROFILER_DIR: \"/mnt\"\n      DTYPE: ${DTYPE:-float16}\n      QUANTIZATION: ${QUANTIZATION:-fp8}\n      MAX_MODEL_LEN: ${MAX_MODEL_LEN:-2048}\n      MAX_NUM_BATCHED_TOKENS: ${MAX_NUM_BATCHED_TOKENS:-4000}\n      MAX_NUM_SEQS: ${MAX_NUM_SEQS:-256}\n      TENSOR_PARALLEL_SIZE: ${TENSOR_PARALLEL_SIZE:-1}\n    healthcheck:\n      test: [\"CMD-SHELL\", \"curl -f http://vllm-service:80/health || exit 1\"]\n      interval: 10s\n      timeout: 10s\n      retries: 100\n    entrypoint: /bin/bash -c \"export CCL_WORKER_COUNT=2 &&\n                              export SYCL_CACHE_PERSISTENT=1 &&\n                              export FI_PROVIDER=shm &&\n                              export CCL_ATL_TRANSPORT=ofi &&\n                              export CCL_ZE_IPC_EXCHANGE=sockets &&\n                              export CCL_ATL_SHM=1 &&\n                              export USE_XETLA=OFF &&\n                              export SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=2 &&\n                              export TORCH_LLM_ALLREDUCE=0 &&\n                              export CCL_SAME_STREAM=1 &&\n                              export CCL_BLOCKING_WAIT=0 &&\n                              export ZE_AFFINITY_MASK=$GPU_AFFINITY &&\n                              python -m ipex_llm.vllm.xpu.entrypoints.openai.api_server \\\n                                --served-model-name $LLM_MODEL_ID \\\n                                --model $LLM_MODEL_LOCAL_PATH \\\n                                --port 80 \\\n                                --trust-remote-code \\\n                                --block-size 8 \\\n                                --gpu-memory-utilization 0.95 \\\n                                --device xpu \\\n                                --dtype $DTYPE \\\n                                --enforce-eager \\\n                                --load-in-low-bit $QUANTIZATION \\\n                                --max-model-len $MAX_MODEL_LEN \\\n                                --max-num-batched-tokens $MAX_NUM_BATCHED_TOKENS \\\n                                --max-num-seqs $MAX_NUM_SEQS \\\n                                --tensor-parallel-size $TENSOR_PARALLEL_SIZE \\\n                                --disable-async-output-proc \\\n                                --distributed-executor-backend ray\"\nnetworks:\n  default:\n    driver: bridge\n```\nmodel: Qwen/Qwen2.5-Coder-7B-Instruct\n```\nmodelscope download --model Qwen/Qwen2.5-Coder-7B-Instruct --local_dir ./data/Qwen/Qwen2.5-Coder-7B-Instruct\ndocker compose up -d\n```\n\n\n**Screenshots**\nIf applicable, add screenshots to help explain the problem\n\n![Image](https://github.com/user-attachments/assets/0aa4fd53-7c02-4a67-a022-56294f29659a)\n\n**Environment information**\nIf possible, please attach the output of the environment check script, using:\n- https://github.com/intel/ipex-llm/blob/main/python/llm/scripts/env-check.bat, or\n- https://github.com/intel/ipex-llm/blob/main/python/llm/scripts/env-check.sh\n```\n-----------------------------------------------------------------\nPYTHON_VERSION=3.10.12\n-----------------------------------------------------------------\nTransformers is not installed. \n-----------------------------------------------------------------\nPyTorch is not installed. \n-----------------------------------------------------------------\nipex-llm WARNING: Package(s) not found: ipex-llm\n-----------------------------------------------------------------\nIPEX is not installed. \n-----------------------------------------------------------------\nCPU Information: \nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        52 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               48\nOn-line CPU(s) list:                  0-47\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Silver 4410Y\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   12\nSocket(s):                            2\nStepping:                             8\nCPU max MHz:                          3900.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             4000.00\n-----------------------------------------------------------------\nTotal CPU Memory: 503.547 GB\nMemory Type: -----------------------------------------------------------------\nOperating System: \nUbuntu 22.04.1 LTS \\n \\l\n\n-----------------------------------------------------------------\nLinux arc003 6.8.0-49-generic #49~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Nov  6 17:42:15 UTC 2 x86_64 x86_64 x86_64 GNU/Linux\n-----------------------------------------------------------------\nCLI:\n    Version: 1.2.33.20250307\n    Build ID: 00000000\n\nService:\n    Version: 1.2.33.20250307\n    Build ID: 00000000\n    Level Zero Version: 1.14.0\n-----------------------------------------------------------------\n  Driver UUID                                     32332e34-332e-3237-3634-322e36390000\n  Driver Version                                  23.43.27642.69\n  Driver UUID                                     32332e34-332e-3237-3634-322e36390000\n  Driver Version                                  23.43.27642.69\n  Driver UUID                                     32332e34-332e-3237-3634-322e36390000\n  Driver Version                                  23.43.27642.69\n  Driver UUID                                     32332e34-332e-3237-3634-322e36390000\n  Driver Version                                  23.43.27642.69\n  Driver UUID                                     32332e34-332e-3237-3634-322e36390000\n  Driver Version                                  23.43.27642.69\n  Driver UUID                                     32332e34-332e-3237-3634-322e36390000\n  Driver Version                                  23.43.27642.69\n  Driver UUID                                     32332e34-332e-3237-3634-322e36390000\n  Driver Version                                  23.43.27642.69\n  Driver UUID                                     32332e34-332e-3237-3634-322e36390000\n  Driver Version                                  23.43.27642.69\n-----------------------------------------------------------------\nDriver related package version:\nii  intel-fw-gpu                               2025.13.2-398~22.04                     all          Firmware package for Intel integrated and discrete GPUs\nii  intel-i915-dkms                            1.23.10.92.231129.101+i141-1            all          Out of tree i915 driver.\nii  intel-level-zero-gpu                       1.3.27642.69-803.145~22.04              amd64        Intel(R) Graphics Compute Runtime for oneAPI Level Zero.\nii  level-zero-dev                             1.14.0-803.123~22.04                    amd64        Intel(R) Graphics Compute Runtime for oneAPI Level Zero.\n-----------------------------------------------------------------\n./env-check.sh: line 167: sycl-ls: command not found\nigpu not detected\n-----------------------------------------------------------------\nxpu-smi is properly installed. \n-----------------------------------------------------------------\n+-----------+--------------------------------------------------------------------------------------+\n| Device ID | Device Information                                                                   |\n+-----------+--------------------------------------------------------------------------------------+\n| 0         | Device Name: Intel(R) Arc(TM) A770 Graphics                                          |\n|           | Vendor Name: Intel(R) Corporation                                                    |\n|           | SOC UUID: 00000000-0000-0019-0000-000856a08086                                       |\n|           | PCI BDF Address: 0000:19:00.0                                                        |\n|           | DRM Device: /dev/dri/card0                                                           |\n|           | Function Type: physical                                                              |\n+-----------+--------------------------------------------------------------------------------------+\n| 1         | Device Name: Intel(R) Arc(TM) A770 Graphics                                          |\n|           | Vendor Name: Intel(R) Corporation                                                    |\n|           | SOC UUID: 00000000-0000-002c-0000-000856a08086                                       |\n|           | PCI BDF Address: 0000:2c:00.0                                                        |\n|           | DRM Device: /dev/dri/card2                                                           |\n|           | Function Type: physical                                                              |\n+-----------+--------------------------------------------------------------------------------------+\n| 2         | Device Name: Intel(R) Arc(TM) A770 Graphics                                          |\n|           | Vendor Name: Intel(R) Corporation                                                    |\n|           | SOC UUID: 00000000-0000-0052-0000-000856a08086                                       |\n|           | PCI BDF Address: 0000:52:00.0                                                        |\n|           | DRM Device: /dev/dri/card3                                                           |\n|           | Function Type: physical                                                              |\n+-----------+--------------------------------------------------------------------------------------+\n| 3         | Device Name: Intel(R) Arc(TM) A770 Graphics                                          |\n|           | Vendor Name: Intel(R) Corporation                                                    |\n|           | SOC UUID: 00000000-0000-0065-0000-000856a08086                                       |\n|           | PCI BDF Address: 0000:65:00.0                                                        |\n|           | DRM Device: /dev/dri/card4                                                           |\n|           | Function Type: physical                                                              |\n+-----------+--------------------------------------------------------------------------------------+\n| 4         | Device Name: Intel(R) Arc(TM) A770 Graphics                                          |\n|           | Vendor Name: Intel(R) Corporation                                                    |\n|           | SOC UUID: 00000000-0000-009b-0000-000856a08086                                       |\n|           | PCI BDF Address: 0000:9b:00.0                                                        |\n|           | DRM Device: /dev/dri/card5                                                           |\n|           | Function Type: physical                                                              |\n+-----------+--------------------------------------------------------------------------------------+\n| 5         | Device Name: Intel(R) Arc(TM) A770 Graphics                                          |\n|           | Vendor Name: Intel(R) Corporation                                                    |\n|           | SOC UUID: 00000000-0000-00ad-0000-000856a08086                                       |\n|           | PCI BDF Address: 0000:ad:00.0                                                        |\n|           | DRM Device: /dev/dri/card6                                                           |\n|           | Function Type: physical                                                              |\n+-----------+--------------------------------------------------------------------------------------+\n| 6         | Device Name: Intel(R) Arc(TM) A770 Graphics                                          |\n|           | Vendor Name: Intel(R) Corporation                                                    |\n|           | SOC UUID: 00000000-0000-00d1-0000-000856a08086                                       |\n|           | PCI BDF Address: 0000:d1:00.0                                                        |\n|           | DRM Device: /dev/dri/card7                                                           |\n|           | Function Type: physical                                                              |\n+-----------+--------------------------------------------------------------------------------------+\n| 7         | Device Name: Intel(R) Arc(TM) A770 Graphics                                          |\n|           | Vendor Name: Intel(R) Corporation                                                    |\n|           | SOC UUID: 00000000-0000-00e3-0000-000856a08086                                       |\n|           | PCI BDF Address: 0000:e3:00.0                                                        |\n|           | DRM Device: /dev/dri/card8                                                           |\n|           | Function Type: physical                                                              |\n+-----------+--------------------------------------------------------------------------------------+\nGPU0 Memory size=16M\nGPU1 Memory size=16G\nGPU2 Memory size=16G\nGPU3 Memory size=16G\nGPU4 Memory size=16G\nGPU5 Memory size=16G\nGPU6 Memory size=16G\nGPU7 Memory size=16G\nGPU8 Memory size=16G\n-----------------------------------------------------------------\n03:00.0 VGA compatible controller: ASPEED Technology, Inc. ASPEED Graphics Family (rev 52) (prog-if 00 [VGA controller])\n        DeviceName: Onboard VGA\n        Subsystem: ASPEED Technology, Inc. ASPEED Graphics Family\n        Flags: medium devsel, IRQ 16, NUMA node 0\n        Memory at 94000000 (32-bit, non-prefetchable) [size=16M]\n        Memory at 95000000 (32-bit, non-prefetchable) [size=256K]\n        I/O ports at 2000 [size=128]\n        Capabilities: [40] Power Management version 3\n        Capabilities: [50] MSI: Enable- Count=1/4 Maskable- 64bit+\n        Kernel driver in use: ast\n--\n19:00.0 VGA compatible controller: Intel Corporation Device 56a0 (rev 08) (prog-if 00 [VGA controller])\n        Subsystem: Device 1ef7:1334\n        Flags: bus master, fast devsel, latency 0, IRQ 228, NUMA node 0\n        Memory at 9e000000 (64-bit, non-prefetchable) [size=16M]\n        Memory at 5f800000000 (64-bit, prefetchable) [size=16G]\n        Expansion ROM at 9f000000 [disabled] [size=2M]\n        Capabilities: [40] Vendor Specific Information: Len=0c <?>\n        Capabilities: [70] Express Endpoint, MSI 00\n        Capabilities: [ac] MSI: Enable+ Count=1/1 Maskable+ 64bit+\n--\n2c:00.0 VGA compatible controller: Intel Corporation Device 56a0 (rev 08) (prog-if 00 [VGA controller])\n        Subsystem: Device 1ef7:1334\n        Flags: bus master, fast devsel, latency 0, IRQ 231, NUMA node 0\n        Memory at a8000000 (64-bit, non-prefetchable) [size=16M]\n        Memory at 6f800000000 (64-bit, prefetchable) [size=16G]\n        Expansion ROM at a9000000 [disabled] [size=2M]\n        Capabilities: [40] Vendor Specific Information: Len=0c <?>\n        Capabilities: [70] Express Endpoint, MSI 00\n        Capabilities: [ac] MSI: Enable+ Count=1/1 Maskable+ 64bit+\n--\n52:00.0 VGA compatible controller: Intel Corporation Device 56a0 (rev 08) (prog-if 00 [VGA controller])\n        Subsystem: Device 1ef7:1334\n        Flags: bus master, fast devsel, latency 0, IRQ 235, NUMA node 0\n        Memory at bc000000 (64-bit, non-prefetchable) [size=16M]\n        Memory at 8f800000000 (64-bit, prefetchable) [size=16G]\n        Expansion ROM at bd000000 [disabled] [size=2M]\n        Capabilities: [40] Vendor Specific Information: Len=0c <?>\n        Capabilities: [70] Express Endpoint, MSI 00\n        Capabilities: [ac] MSI: Enable+ Count=1/1 Maskable+ 64bit+\n--\n65:00.0 VGA compatible controller: Intel Corporation Device 56a0 (rev 08) (prog-if 00 [VGA controller])\n        Subsystem: Device 1ef7:1334\n        Flags: bus master, fast devsel, latency 0, IRQ 239, NUMA node 0\n        Memory at c6000000 (64-bit, non-prefetchable) [size=16M]\n        Memory at 9f800000000 (64-bit, prefetchable) [size=16G]\n        Expansion ROM at c7000000 [disabled] [size=2M]\n        Capabilities: [40] Vendor Specific Information: Len=0c <?>\n        Capabilities: [70] Express Endpoint, MSI 00\n        Capabilities: [ac] MSI: Enable+ Count=1/1 Maskable+ 64bit+\n--\n9b:00.0 VGA compatible controller: Intel Corporation Device 56a0 (rev 08) (prog-if 00 [VGA controller])\n        Subsystem: Device 1ef7:1334\n        Flags: bus master, fast devsel, latency 0, IRQ 243, NUMA node 1\n        Memory at d8000000 (64-bit, non-prefetchable) [size=16M]\n        Memory at cf800000000 (64-bit, prefetchable) [size=16G]\n        Expansion ROM at d9000000 [disabled] [size=2M]\n        Capabilities: [40] Vendor Specific Information: Len=0c <?>\n        Capabilities: [70] Express Endpoint, MSI 00\n        Capabilities: [ac] MSI: Enable+ Count=1/1 Maskable+ 64bit+\n--\nad:00.0 VGA compatible controller: Intel Corporation Device 56a0 (rev 08) (prog-if 00 [VGA controller])\n        Subsystem: Device 1ef7:1334\n        Flags: bus master, fast devsel, latency 0, IRQ 247, NUMA node 1\n        Memory at e0000000 (64-bit, non-prefetchable) [size=16M]\n        Memory at df800000000 (64-bit, prefetchable) [size=16G]\n        Expansion ROM at e1000000 [disabled] [size=2M]\n        Capabilities: [40] Vendor Specific Information: Len=0c <?>\n        Capabilities: [70] Express Endpoint, MSI 00\n        Capabilities: [ac] MSI: Enable+ Count=1/1 Maskable+ 64bit+\n--\nd1:00.0 VGA compatible controller: Intel Corporation Device 56a0 (rev 08) (prog-if 00 [VGA controller])\n        Subsystem: Device 1ef7:1334\n        Flags: bus master, fast devsel, latency 0, IRQ 251, NUMA node 1\n        Memory at f1000000 (64-bit, non-prefetchable) [size=16M]\n        Memory at ff800000000 (64-bit, prefetchable) [size=16G]\n        Expansion ROM at f2000000 [disabled] [size=2M]\n        Capabilities: [40] Vendor Specific Information: Len=0c <?>\n        Capabilities: [70] Express Endpoint, MSI 00\n        Capabilities: [ac] MSI: Enable+ Count=1/1 Maskable+ 64bit+\n--\ne3:00.0 VGA compatible controller: Intel Corporation Device 56a0 (rev 08) (prog-if 00 [VGA controller])\n        Subsystem: Device 1ef7:1334\n        Flags: bus master, fast devsel, latency 0, IRQ 255, NUMA node 1\n        Memory at f9000000 (64-bit, non-prefetchable) [size=16M]\n        Memory at 10f800000000 (64-bit, prefetchable) [size=16G]\n        Expansion ROM at fa000000 [disabled] [size=2M]\n        Capabilities: [40] Vendor Specific Information: Len=0c <?>\n        Capabilities: [70] Express Endpoint, MSI 00\n        Capabilities: [ac] MSI: Enable+ Count=1/1 Maskable+ 64bit+\n-----------------------------------------------------------------\n```\n\n**Additional context**\n```log\nINFO 05-14 02:49:05 __init__.py:180] Automatically detected platform xpu.\nWARNING 05-14 02:49:06 api_server.py:538] Torch Profiler is enabled in the API server. This should ONLY be used for local development!\nWARNING 05-14 02:49:06 api_server.py:893] Warning: Please use `ipex_llm.vllm.xpu.entrypoints.openai.api_server` instead of `vllm.entrypoints.openai.api_server` to start the API server\nINFO 05-14 02:49:06 api_server.py:837] vLLM API server version 0.6.6+ipexllm\nINFO 05-14 02:49:06 api_server.py:838] args: Namespace(host=None, port=80, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/data/Qwen/Qwen2.5-Coder-7B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='float16', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=8192, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend='ray', worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=8, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.95, num_gpu_blocks_override=None, max_num_batched_tokens=8192, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=True, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='xpu', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['Qwen/Qwen2.5-Coder-7B-Instruct'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=True, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, low_bit_model_path=None, low_bit_save_path=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, load_in_low_bit='fp8')\nINFO 05-14 02:49:06 api_server.py:197] Started engine process with PID 110\nWARNING 05-14 02:49:06 config.py:2289] Casting torch.bfloat16 to torch.float16.\nINFO 05-14 02:49:09 __init__.py:180] Automatically detected platform xpu.\nWARNING 05-14 02:49:11 api_server.py:538] Torch Profiler is enabled in the API server. This should ONLY be used for local development!\nWARNING 05-14 02:49:11 config.py:2289] Casting torch.bfloat16 to torch.float16.\nINFO 05-14 02:49:11 config.py:521] This model supports multiple tasks: {'embed', 'generate', 'reward', 'classify', 'score'}. Defaulting to 'generate'.\nINFO 05-14 02:49:15 config.py:521] This model supports multiple tasks: {'generate', 'classify', 'reward', 'score', 'embed'}. Defaulting to 'generate'.\nWARNING 05-14 02:49:15 ray_utils.py:239] No existing RAY instance detected. A new instance will be launched with current node resources.\n2025-05-14 02:49:17,922 INFO worker.py:1841 -- Started a local Ray instance.\nINFO 05-14 02:49:19 llm_engine.py:234] Initializing an LLM engine (v0.6.6+ipexllm) with config: model='/data/Qwen/Qwen2.5-Coder-7B-Instruct', speculative_config=None, tokenizer='/data/Qwen/Qwen2.5-Coder-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=xpu, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-Coder-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=True, \nINFO 05-14 02:49:19 ray_gpu_executor.py:123] use_ray_spmd_worker: False\n(WrapperWithLoadBit pid=652) INFO 05-14 02:49:22 __init__.py:180] Automatically detected platform xpu.\nINFO 05-14 02:49:24 xpu.py:27] Cannot use _Backend.FLASH_ATTN backend on XPU.\nINFO 05-14 02:49:24 selector.py:155] Using IPEX attention backend.\nWARNING 05-14 02:49:24 _ipex_ops.py:12] Import error msg: No module named 'intel_extension_for_pytorch'\nINFO 05-14 02:49:24 importing.py:14] Triton not installed or not compatible; certain GPU-related functions will not be available.\n(WrapperWithLoadBit pid=665) INFO 05-14 02:49:24 xpu.py:27] Cannot use _Backend.FLASH_ATTN backend on XPU.\n(WrapperWithLoadBit pid=665) INFO 05-14 02:49:24 selector.py:155] Using IPEX attention backend.\n(WrapperWithLoadBit pid=665) WARNING 05-14 02:49:24 _ipex_ops.py:12] Import error msg: No module named 'intel_extension_for_pytorch'\n(WrapperWithLoadBit pid=665) INFO 05-14 02:49:24 importing.py:14] Triton not installed or not compatible; certain GPU-related functions will not be available.\nINFO 05-14 02:49:24 shm_broadcast.py:255] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_2fc713d0'), local_subscribe_port=43547, remote_subscribe_port=None)\nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  9.06it/s]\nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  7.28it/s]\nLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:00<00:00,  7.10it/s]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:00<00:00,  8.41it/s]\n\n2025-05-14 02:49:25,171 - INFO - Converting the current model to fp8_e5m2 format......\n2025-05-14 02:49:25,172 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\n2025-05-14 02:49:26,786 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\n2025-05-14 02:49:27,667 - INFO - Loading model weights took 2.0819 GB\n(WrapperWithLoadBit pid=647) 2025-05-14 02:49:29,204 - INFO - Converting the current model to fp8_e5m2 format......\n(WrapperWithLoadBit pid=647) 2025-05-14 02:49:29,204 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\n(WrapperWithLoadBit pid=652) 2025-05-14 02:49:29,819 - INFO - Converting the current model to fp8_e5m2 format...... [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\n(WrapperWithLoadBit pid=647) 2025-05-14 02:49:34,226 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations [repeated 3x across cluster]\n(WrapperWithLoadBit pid=647) 2025-05-14 02:49:35,710 - INFO - Loading model weights took 2.0819 GB\n2025:05:14-02:49:37:(  110) |CCL_WARN| value of CCL_WORKER_COUNT changed to be 2 (default:1)\n2025:05:14-02:49:37:(  110) |CCL_WARN| value of CCL_ATL_TRANSPORT changed to be ofi (default:mpi)\n2025:05:14-02:49:37:(  110) |CCL_WARN| value of CCL_ATL_SHM changed to be 1 (default:0)\n2025:05:14-02:49:37:(  110) |CCL_WARN| value of CCL_LOCAL_RANK changed to be 0 (default:-1)\n2025:05:14-02:49:37:(  110) |CCL_WARN| value of CCL_LOCAL_SIZE changed to be 4 (default:-1)\n2025:05:14-02:49:37:(  110) |CCL_WARN| value of CCL_PROCESS_LAUNCHER changed to be none (default:hydra)\n2025:05:14-02:49:37:(  110) |CCL_WARN| value of CCL_ZE_IPC_EXCHANGE changed to be sockets (default:pidfd)\n(WrapperWithLoadBit pid=652) *** SIGSEGV received at time=1747162178 on cpu 18 ***\n(WrapperWithLoadBit pid=652) PC: @     0x7c17c042205e  (unknown)  smr_map_to_endpoint\n(WrapperWithLoadBit pid=652)     @     0x7c3db68a2733  (unknown)  (unknown)\n(WrapperWithLoadBit pid=652) [2025-05-14 02:49:38,328 E 652 652] logging.cc:484: *** SIGSEGV received at time=1747162178 on cpu 18 ***\n(WrapperWithLoadBit pid=652) [2025-05-14 02:49:38,328 E 652 652] logging.cc:484: PC: @     0x7c17c042205e  (unknown)  smr_map_to_endpoint\n(WrapperWithLoadBit pid=652) [2025-05-14 02:49:38,328 E 652 652] logging.cc:484:     @     0x7c3db68a2733  (unknown)  (unknown)\n(WrapperWithLoadBit pid=652) Fatal Python error: Segmentation fault\n(WrapperWithLoadBit pid=652) \n(WrapperWithLoadBit pid=652) Stack (most recent call first):\n(WrapperWithLoadBit pid=652)   File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 2806 in all_reduce\n(WrapperWithLoadBit pid=652)   File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 81 in wrapper\n(WrapperWithLoadBit pid=652)   File \"/usr/local/lib/python3.11/dist-packages/vllm/distributed/device_communicators/xpu_communicator.py\", line 19 in all_reduce\n(WrapperWithLoadBit pid=652)   File \"/usr/local/lib/python3.11/dist-packages/vllm/distributed/parallel_state.py\", line 345 in all_reduce\n(WrapperWithLoadBit pid=652)   File \"/usr/local/lib/python3.11/dist-packages/vllm/distributed/communication_op.py\", line 11 in tensor_model_parallel_all_reduce\n(WrapperWithLoadBit pid=652)   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/vocab_parallel_embedding.py\", line 419 in forward\n(WrapperWithLoadBit pid=652)   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750 in _call_impl\n(WrapperWithLoadBit pid=652)   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739 in _wrapped_call_impl\n(WrapperWithLoadBit pid=652)   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen2.py\", line 317 in get_input_embeddings\n(WrapperWithLoadBit pid=652)   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen2.py\", line 332 in forward\n(WrapperWithLoadBit pid=652)   File \"/usr/local/lib/python3.11/dist-packages/vllm/compilation/decorators.py\", line 168 in __call__\n(WrapperWithLoadBit pid=652)   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen2.py\", line 477 in forward\n(WrapperWithLoadBit pid=652)   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750 in _call_impl\n(WrapperWithLoadBit pid=652)   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739 in _wrapped_call_impl\n(WrapperWithLoadBit pid=652)   File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/xpu_model_runner.py\", line 949 in execute_model\n(WrapperWithLoadBit pid=652)   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116 in decorate_context\n(WrapperWithLoadBit pid=652)   File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/xpu_model_runner.py\", line 839 in profile_run\n(WrapperWithLoadBit pid=652)   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116 in decorate_context\n(WrapperWithLoadBit pid=652)   File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/xpu_worker.py\", line 106 in determine_num_available_blocks\n(WrapperWithLoadBit pid=652)   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116 in decorate_context\n(WrapperWithLoadBit pid=652)   File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/worker_base.py\", line 461 in execute_method\n(WrapperWithLoadBit pid=652)   File \"/usr/local/lib/python3.11/dist-packages/ray/util/tracing/tracing_helper.py\", line 463 in _resume_span\n(WrapperWithLoadBit pid=652)   File \"/usr/local/lib/python3.11/dist-packages/ray/_private/function_manager.py\", line 696 in actor_method_executor\n(WrapperWithLoadBit pid=652)   File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 935 in main_loop\n(WrapperWithLoadBit pid=652)   File \"/usr/local/lib/python3.11/dist-packages/ray/_private/workers/default_worker.py\", line 297 in <module>\n(WrapperWithLoadBit pid=652) \n(WrapperWithLoadBit pid=652) Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, uvloop.loop, ray._raylet, markupsafe._speedups, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, PIL._imaging, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, PIL._imagingft, msgspec._core, sentencepiece._sentencepiece, regex._regex, zmq.backend.cython._zmq, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, pyarrow.lib, pyarrow._json (total: 52)\n(raylet) A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff3c5176f98134daaa8c2456a501000000 Worker ID: b9be28cf125d015da221d7adadcb6a524901043d393dddb1ac1a3f38 Node ID: 13d4cdadeb61a0587c202b347d0135753701a7f7020c43436bbf524e Worker IP address: 172.19.0.2 Worker port: 39569 Worker PID: 652 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n(WrapperWithLoadBit pid=649) INFO 05-14 02:49:23 __init__.py:180] Automatically detected platform xpu. [repeated 3x across cluster]\n(WrapperWithLoadBit pid=647) INFO 05-14 02:49:24 xpu.py:27] Cannot use _Backend.FLASH_ATTN backend on XPU. [repeated 2x across cluster]\n(WrapperWithLoadBit pid=647) INFO 05-14 02:49:24 selector.py:155] Using IPEX attention backend. [repeated 2x across cluster]\n(WrapperWithLoadBit pid=647) WARNING 05-14 02:49:24 _ipex_ops.py:12] Import error msg: No module named 'intel_extension_for_pytorch' [repeated 2x across cluster]\n(WrapperWithLoadBit pid=647) INFO 05-14 02:49:24 importing.py:14] Triton not installed or not compatible; certain GPU-related functions will not be available. [repeated 2x across cluster]\n```",
      "state": "open",
      "author": "hualongfeng",
      "author_type": "User",
      "created_at": "2025-05-14T04:08:07Z",
      "updated_at": "2025-05-18T12:16:22Z",
      "closed_at": null,
      "labels": [
        "user issue",
        "multi-arc"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13158/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "liu-shaojun",
        "hzjane"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13158",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13158",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:33.739236",
      "comments": [
        {
          "author": "liu-shaojun",
          "body": "Running with a single GPU works fine, but encountering issues with multi-GPU. This could be related to OneCCL. From the script you provided, it looks like you haven’t sourced the OneCCL environment script (`setvars.sh`). Please refer to the documentation here:\nhttps://github.com/intel/ipex-llm/blob/",
          "created_at": "2025-05-15T03:08:23Z"
        },
        {
          "author": "hualongfeng",
          "body": "> source /opt/intel/1ccl-wks/setvars.sh &&\nHi @liu-shaojun \n![Image](https://github.com/user-attachments/assets/6a4806f9-a07e-4817-90ed-cc3eaaec0da5) \nI sourced the OneCCL environment script (setvars.sh), but the error still exist.\n![Image](https://github.com/user-attachments/assets/e1dfdef0-469a-46",
          "created_at": "2025-05-15T05:27:01Z"
        },
        {
          "author": "liu-shaojun",
          "body": "Hi,\n\nCould you please let us know whether your CPU is an Intel® Xeon® or Intel® Core™ processor?\n\nIn the meantime, we recommend trying our latest Docker image by running the following command:\n\n```\ndocker pull intelanalytics/ipex-llm-serving-xpu:0.8.3-b19\n```\n\nFor detailed steps and the latest confi",
          "created_at": "2025-05-16T02:12:11Z"
        },
        {
          "author": "hualongfeng",
          "body": "> Hi,\n> \n> Could you please let us know whether your CPU is an Intel® Xeon® or Intel® Core™ processor?\n> \n> In the meantime, we recommend trying our latest Docker image by running the following command:\n> \n> ```\n> docker pull intelanalytics/ipex-llm-serving-xpu:0.8.3-b19\n> ```\n> \n> For detailed step",
          "created_at": "2025-05-16T02:15:39Z"
        },
        {
          "author": "hualongfeng",
          "body": "\n> > In the meantime, we recommend trying our latest Docker image by running the following command:\n> > ```\n> > docker pull intelanalytics/ipex-llm-serving-xpu:0.8.3-b19\n> > ```\n\nHi @liu-shaojun I use the image`intelanalytics/ipex-llm-serving-xpu:0.8.3-b19`, but the error still exist.\n ```bash\n[W516",
          "created_at": "2025-05-16T03:27:47Z"
        }
      ]
    },
    {
      "issue_number": 12851,
      "title": "[SOLVED] How to use IPEX-LLM with a VS Code Extension on Win10 ?",
      "body": "Hi,\n\nI just got my B580 and it works fine with the IPEX-LLM Ollama portable zip installer, on Win10.   I wonder whether the IPEX-LLM can be used with a VS Code extension such as Continue.dev as described below:\n\nhttps://dev.to/lunaticprogrammer/using-deepseek-r1-in-visual-studio-code-for-free-2279\n\nAny advice would be greatly appreciated.\n\nAll the best.\n\n[SOLVED]  \n\n1. Start the Ollama Serve with start-ollama.bat\n2. Go into the ollama directory and run \"ollama pull <a model name>\"\n3. In VS Code, under the Continue UI, click the drop-down menu \"Add Chat Model\" > \"Ollama\" & \"autodetect\"\n\nThe local models will be shown.   Great job Intel !!   [No Docker](https://dev.to/itlackey/run-ollama-on-intel-arc-gpu-ipex-4e4k) required !\n\n![Image](https://github.com/user-attachments/assets/99795742-3c10-417d-ac68-1e17ea35a675)",
      "state": "open",
      "author": "redo33",
      "author_type": "User",
      "created_at": "2025-02-19T06:51:28Z",
      "updated_at": "2025-05-16T15:43:43Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12851/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12851",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12851",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:34.040243",
      "comments": [
        {
          "author": "arxdeus",
          "body": "@redo33 close this issue, as you solved it",
          "created_at": "2025-05-16T15:43:27Z"
        }
      ]
    },
    {
      "issue_number": 13046,
      "title": "ImportError: cannot import name 'VllmConfig' from 'vllm.config'",
      "body": "Bash Command: \npython -m ipex_llm.vllm.xpu.entrypoints.openai.api_server   --served-model-name $served_model_name   --port 8000   --model $model   --trust-remote-code   --gpu-memory-utilization 0.75   --device xpu   --dtype float16   --enforce-eager   --load-in-low-bit sym_int4   --max-model-len 4096   --max-num-batched-tokens 10240   --max-num-seqs 12   --tensor-parallel-size 1\n\nError:\nImportError: cannot import name 'VllmConfig' from 'vllm.config' (/home/am/codes/GPT/llama8/vllm/vllm/config.py)",
      "state": "closed",
      "author": "Vaccummer",
      "author_type": "User",
      "created_at": "2025-04-03T14:08:57Z",
      "updated_at": "2025-05-16T03:11:13Z",
      "closed_at": "2025-05-16T03:11:13Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13046/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "gc-fu"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13046",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13046",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:34.261403",
      "comments": [
        {
          "author": "Vaccummer",
          "body": "WSL2\nIntel Arc A770\n\nif just run with ipex_llm.transformers.AutoModelForCausalLM instead of vllm,  it can work.\nI passed the compile of vllm, but in ipex_llm.vllm.xpu.engine.engine.py, many packages are missing(the white items):\n\n![Image](https://github.com/user-attachments/assets/6b4946fa-3611-464b",
          "created_at": "2025-04-03T14:12:41Z"
        },
        {
          "author": "gc-fu",
          "body": "Hi, can you post your running environment?\n\nAre you running in docker container or ?",
          "created_at": "2025-04-07T01:50:34Z"
        }
      ]
    },
    {
      "issue_number": 13133,
      "title": "RAM usage increase upon termination of llama-server.exe",
      "body": "**Describe the bug**\nWhen llama-server.exe is terminated (e.g., using Ctrl + C), there is an unexpected increase in RAM usage during the clearing process. This behavior prevents the utilization of the full system RAM, as the RAM saturates upon task termination, leading to disk swapping. \nExpected behavior: Upon termination, RAM usage should decrease as resources are freed, without causing saturation or disk swapping. \nActual behavior: RAM usage increases during the clearing process, leading to saturation and disk swapping. \n\n**How to reproduce**\nSteps to reproduce the error:\n1. Launch llama-server.exe using the following command in PowerShell (in my case I use qwen2.5-coder-14b):\n`./llama-server.exe --host 172.24.32.1 --port 5001 -m \"qwen2.5-coder-14b-instruct-q4_k_m.gguf\" -n 512  -t 3 -e -ngl 99 -c 4096 --temp 0 --top-k 100 --top-p 0.92 --repeat-penalty 0.75 --mlock --verbose-prompt -v`\n2. Allow the model to load and complete the warmup process. \n3. Terminate the task using Ctrl + C in the terminal. \n\n**Screenshots**\n![Image](https://github.com/user-attachments/assets/543753f2-f34c-4755-8dab-95569430d60e)\n\n**Environment information**\nDue to sensitivity concerns, detailed environment information is not provided. \nSoftware environment: Windows 11 Version 23H2 (OS Build 22631.5189)\nHardware environment:\nCPU i5-1145G7\nRAM 32GB 3200 MT/s\nIntel Iris Xe Graphics is used for LLM inference. \n\n**Additional context**\nEnsure the warmup process is completed to consistently reproduce the issue. \n\nThanks so much for the great work on this project! It's really cool how you're helping people run LLMs on their own PCs. You're making advanced tech more accessible, and that's really appreciated. Keep it up!",
      "state": "closed",
      "author": "GuillaumeRattin",
      "author_type": "User",
      "created_at": "2025-05-06T13:34:48Z",
      "updated_at": "2025-05-14T13:27:54Z",
      "closed_at": "2025-05-14T13:27:53Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13133/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13133",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13133",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:34.484222",
      "comments": [
        {
          "author": "rnwang04",
          "body": "Hi @GuillaumeRattin , thanks for pointing out this issue.\nI have reproduced similar phenomenon on our Windows MTL machine.\nWe will conduct further investigation on this issue and update here to let you know if there is any progress.",
          "created_at": "2025-05-08T01:21:24Z"
        },
        {
          "author": "rnwang04",
          "body": "Hi @GuillaumeRattin , this issue should have been fixed with latest `ipex-llm[cpp]==2.3.0b20250513`.\nYou could try it again with `pip install --pre --upgrade ipex-llm[cpp]`.  😊",
          "created_at": "2025-05-13T13:35:42Z"
        },
        {
          "author": "GuillaumeRattin",
          "body": "Hi @rnwang04,\nI'm currently using the release package, so I'll keep an eye out for the upcoming 2.3.0 nightly release that includes your modifications. Thanks a lot for your support! For now, I think we can close this issue, and I'll open a new one if the bug reappears in the next nightly release.\nG",
          "created_at": "2025-05-14T13:27:53Z"
        }
      ]
    },
    {
      "issue_number": 13159,
      "title": "[BMG]: Please add a guidelines to perform fine-tuning and inferencing on multi-gpu BMG machines",
      "body": "**Describe the bug**\nA new guidelines is needed to perform fine-tuning and inferencing on multi-gpu BMG machines.\n\nIssue:\n1. Now, we are using following xpu libraries  -->[bmg-xpu]( pip install --pre --upgrade ipex-llm[xpu_2.6] --extra-index-url https://download.pytorch.org/whl/xpu) \n2. which doesn't requires additional one-api installation and other packages.\n3. most of the document related to gpu parallelism to perform fine-tune and inference are based on old xpu version\n4. such as -->[example]( https://github.com/intel/ipex-llm/tree/bd71739e64730597e64965617fca67f68ce0b966/python/llm/example/GPU/Deepspeed-AutoTP)\n5. [inf-mutli-gpu](https://github.com/intel/ipex-llm/tree/main/python/llm/example/GPU/Pipeline-Parallel-Inference)\n6. [finetuning-multi-gpu-bmg](https://github.com/intel/ipex-llm/tree/main/python/llm/example/GPU/LLM-Finetuning/QLoRA/alpaca-qlora)\n7. need an additional documentation for multi-gpu support for BMG machines. ",
      "state": "open",
      "author": "raj-ritu17",
      "author_type": "User",
      "created_at": "2025-05-14T12:29:04Z",
      "updated_at": "2025-05-14T12:29:04Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13159/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13159",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13159",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:34.707609",
      "comments": []
    },
    {
      "issue_number": 13149,
      "title": "whisper failed on ARC B580",
      "body": "**Describe the bug**\ncannot import name 'top_k_top_p_filtering' from 'trl.core'\n\n**How to reproduce**\nSteps to reproduce the error:\n1. git commit commit 45f7bf6688abbd0beb12aba36ad51b14 for ipex-llm\n2. start intelanalytics/multi-arc-serving:0.2.0-b1 docker image\n3. cd ipex-llm/python/llm/dev/benchmark/whisper\n4. python run_whisper.py /llm/model/whisper-medium --date_type other --device xpu\n**Screenshots**\n\n![Image](https://github.com/user-attachments/assets/3ce09a75-cfbb-4ff7-b71a-e0abd600368e)\n\n**Environment information**\nroot@b580:/llm/models/ipex-llm/python/llm/scripts# bash env-check.sh \n-----------------------------------------------------------------\nPYTHON_VERSION=3.11.12\n-----------------------------------------------------------------\n[W513 01:43:54.591534205 OperatorEntry.cpp:154] Warning: Warning only once for all operators,  other operators may also be overridden.\n  Overriding a previously registered kernel for the same operator and the same dispatch key\n  operator: aten::_validate_compressed_sparse_indices(bool is_crow, Tensor compressed_idx, Tensor plain_idx, int cdim, int dim, int nnz) -> ()\n    registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6\n  dispatch key: XPU\n  previous kernel: registered at /pytorch/build/aten/src/ATen/RegisterCPU.cpp:30477\n       new kernel: registered at /build/intel-pytorch-extension/build/Release/csrc/gpu/csrc/aten/generated/ATen/RegisterXPU.cpp:468 (function operator())\n[W513 01:43:55.264482308 OperatorEntry.cpp:154] Warning: Warning only once for all operators,  other operators may also be overridden.\n  Overriding a previously registered kernel for the same operator and the same dispatch key\n  operator: aten::_validate_compressed_sparse_indices(bool is_crow, Tensor compressed_idx, Tensor plain_idx, int cdim, int dim, int nnz) -> ()\n    registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6\n  dispatch key: XPU\n  previous kernel: registered at /pytorch/build/aten/src/ATen/RegisterCPU.cpp:30477\n       new kernel: registered at /build/intel-pytorch-extension/build/Release/csrc/gpu/csrc/aten/generated/ATen/RegisterXPU.cpp:468 (function operator())\ntransformers=4.51.3\n-----------------------------------------------------------------\n[W513 01:44:01.254327982 OperatorEntry.cpp:154] Warning: Warning only once for all operators,  other operators may also be overridden.\n  Overriding a previously registered kernel for the same operator and the same dispatch key\n  operator: aten::_validate_compressed_sparse_indices(bool is_crow, Tensor compressed_idx, Tensor plain_idx, int cdim, int dim, int nnz) -> ()\n    registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6\n  dispatch key: XPU\n  previous kernel: registered at /pytorch/build/aten/src/ATen/RegisterCPU.cpp:30477\n       new kernel: registered at /build/intel-pytorch-extension/build/Release/csrc/gpu/csrc/aten/generated/ATen/RegisterXPU.cpp:468 (function operator())\n[W513 01:44:04.560325613 OperatorEntry.cpp:154] Warning: Warning only once for all operators,  other operators may also be overridden.\n  Overriding a previously registered kernel for the same operator and the same dispatch key\n  operator: aten::_validate_compressed_sparse_indices(bool is_crow, Tensor compressed_idx, Tensor plain_idx, int cdim, int dim, int nnz) -> ()\n    registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6\n  dispatch key: XPU\n  previous kernel: registered at /pytorch/build/aten/src/ATen/RegisterCPU.cpp:30477\n       new kernel: registered at /build/intel-pytorch-extension/build/Release/csrc/gpu/csrc/aten/generated/ATen/RegisterXPU.cpp:468 (function operator())\ntorch=2.6.0+xpu\n-----------------------------------------------------------------\nipex-llm Version: 2.3.0b20250427\n-----------------------------------------------------------------\n[W513 01:44:10.333946657 OperatorEntry.cpp:154] Warning: Warning only once for all operators,  other operators may also be overridden.\n  Overriding a previously registered kernel for the same operator and the same dispatch key\n  operator: aten::_validate_compressed_sparse_indices(bool is_crow, Tensor compressed_idx, Tensor plain_idx, int cdim, int dim, int nnz) -> ()\n    registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6\n  dispatch key: XPU\n  previous kernel: registered at /pytorch/build/aten/src/ATen/RegisterCPU.cpp:30477\n       new kernel: registered at /build/intel-pytorch-extension/build/Release/csrc/gpu/csrc/aten/generated/ATen/RegisterXPU.cpp:468 (function operator())\n[W513 01:44:12.144697430 OperatorEntry.cpp:154] Warning: Warning only once for all operators,  other operators may also be overridden.\n  Overriding a previously registered kernel for the same operator and the same dispatch key\n  operator: aten::_validate_compressed_sparse_indices(bool is_crow, Tensor compressed_idx, Tensor plain_idx, int cdim, int dim, int nnz) -> ()\n    registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6\n  dispatch key: XPU\n  previous kernel: registered at /pytorch/build/aten/src/ATen/RegisterCPU.cpp:30477\n       new kernel: registered at /build/intel-pytorch-extension/build/Release/csrc/gpu/csrc/aten/generated/ATen/RegisterXPU.cpp:468 (function operator())\nipex=2.6.10+xpu\n-----------------------------------------------------------------\nCPU Information: \nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               20\nOn-line CPU(s) list:                  0-19\nVendor ID:                            GenuineIntel\nModel name:                           12th Gen Intel(R) Core(TM) i7-12700\nCPU family:                           6\nModel:                                151\nThread(s) per core:                   2\nCore(s) per socket:                   12\nSocket(s):                            1\nStepping:                             2\nCPU(s) scaling MHz:                   50%\nCPU max MHz:                          4900.0000\nCPU min MHz:                          800.0000\n-----------------------------------------------------------------\nTotal CPU Memory: 61.317 GB\n-----------------------------------------------------------------\nOperating System: \nUbuntu 24.04.1 LTS \\n \\l\n\n-----------------------------------------------------------------\nLinux b580 6.11.0-25-generic #25-Ubuntu SMP PREEMPT_DYNAMIC Fri Apr 11 23:29:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux\n-----------------------------------------------------------------\nenv-check.sh: line 148: xpu-smi: command not found\n-----------------------------------------------------------------\nenv-check.sh: line 154: clinfo: command not found\n-----------------------------------------------------------------\nDriver related package version:\nii  intel-level-zero-gpu                             1.6.32961.7                       amd64        Intel(R) Graphics Compute Runtime for oneAPI Level Zero.\nii  intel-level-zero-gpu-dbgsym                      1.6.32961.7                       amd64        debug symbols for intel-level-zero-gpu\n-----------------------------------------------------------------\nigpu detected\n[level_zero:gpu][level_zero:1] Intel(R) oneAPI Unified Runtime over Level-Zero, Intel(R) UHD Graphics 770 12.2.0 [1.6.32961.700000]\n[opencl:gpu][opencl:2] Intel(R) OpenCL Graphics, Intel(R) UHD Graphics 770 OpenCL 3.0 NEO  [25.09.32961.7]\n-----------------------------------------------------------------\nxpu-smi is not installed. Please install xpu-smi according to README.md\n\n**Additional context**\nAdd any other context about the problem here.\n",
      "state": "open",
      "author": "aitss2017",
      "author_type": "User",
      "created_at": "2025-05-12T01:50:30Z",
      "updated_at": "2025-05-12T07:54:36Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13149/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13149",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13149",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:36.486450",
      "comments": [
        {
          "author": "hkvision",
          "body": "Hi, this should be issue caused by the version of dependencies, you may try `trll==0.11.0`, which is recommended and tested by us.\n\nSimilar issue: https://github.com/intel/ipex-llm/issues/13087#issue-3001412276",
          "created_at": "2025-05-12T02:01:54Z"
        },
        {
          "author": "aitss2017",
          "body": "Another error after install trl==0.11.0\n\nroot@b580:/llm/models/ipex-llm/python/llm/dev/benchmark/whisper# python run_whisper.py --model_path /llm/models/whisper-medium --data_type other --device xpu\n[W513 02:47:24.597734290 OperatorEntry.cpp:154] Warning: Warning only once for all operators,  other ",
          "created_at": "2025-05-12T02:58:48Z"
        },
        {
          "author": "hkvision",
          "body": "Synced offline, the error is encountered when running torch operations on xpu. The user will try to run ipex on this machine to further check the environment.",
          "created_at": "2025-05-12T07:54:35Z"
        }
      ]
    },
    {
      "issue_number": 13099,
      "title": "[BUG] Run 8-bit and16-bit gemma3-4b",
      "body": "Hello!\n\nI am experimenting with gemma-3-4b and noticed that 4bit works smoothly:.\n```\nollama run modelscope.cn/lmstudio-community/gemma-3-4b-it-GGUF\n```\n\nIn the same model repository, there is 8bit version also, so when I execute the following commands. It gives error.\n```\nollama run modelscope.cn/lmstudio-community/gemma-3-4b-it-GGUF:Q8_0\n```\n\nI want to run the 16bit. Is it possible at the current state?\n\n**IpexLLM-Ollama Version** \nollama version is `0.5.4-ipexllm-20250318`.  I tried with the latest shared bare metal executable (tag-v2.2.0) also, it gives the same issue.\n\n**Device Details**\nDevice Name: LG Gram Pro Laptop (MFD 2024/05)\nOperating System: Windows 11\nProcessor: Intel(R) Core(TM) Ultra 7 155H @ 3.80 GHz\nRAM: 32.0 GB (31.5 GB usable)\nGraphics and NPU VRAM: 16.0 GB usable (Intel(R) Arc(™) Graphics and Intel(R) AI Boost)\nSystem Type: 64-bit operating system, x64-based processor\nGPU driver version: 32.0.101.6734\n\nLong error message with 8bit gemma3-4b is attached below:\n```\n\n18:18:00.894 > stderr: time=2025-04-21T18:18:00.893+09:00 level=INFO source=runner.go:967 msg=\"starting go runner\"\ntime=2025-04-21T18:18:00.893+09:00 level=INFO source=runner.go:968 msg=system info=\"CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | cgo(clang)\" threads=6\n\n18:18:00.896 > stderr: time=2025-04-21T18:18:00.894+09:00 level=INFO source=runner.go:1026 msg=\"Server listening on 127.0.0.1:57297\"\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\nllama_load_model_from_file: using device SYCL0 (Intel(R) Arc(TM) Graphics) - 16847 MiB free\n\n18:18:00.937 > stderr: llama_model_loader: loaded meta data with 40 key-value pairs and 444 tensors from C:\\Users\\bibek\\neoali\\models\\blobs\\sha256-283baeca5e0ffc2a7f6cd56b9b7b5ce1d4dda08ca11f11afa869127caf745e94 (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str         \n     = gemma3\nllama_model_loader: - kv   1:                               general.type str         \n     = model\nllama_model_loader: - kv   2:                               general.name str         \n     = Gemma 3 4b It\nllama_model_loader: - kv   3:                           general.finetune str         \n     = it\nllama_model_loader: - kv   4:                           general.basename str         \n     = gemma-3\nllama_model_loader: - kv   5:                         general.size_label str         \n     = 4B\nllama_model_loader: - kv   6:                            general.license str         \n     = gemma\nllama_model_loader: - kv   7:                   general.base_model.count u32         \n     = 1\nllama_model_loader: - kv   8:                  general.base_model.0.name str         \n     = Gemma 3 4b Pt\nllama_model_loader: - kv   9:          general.base_model.0.organization str         \n     = Google\nllama_model_loader: - kv  10:              general.base_model.0.repo_url str         \n     = https://huggingface.co/google/gemma-3...\nllama_model_loader: - kv  11:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\nllama_model_loader: - kv  12:                      gemma3.context_length u32         \n     = 131072\nllama_model_loader: - kv  13:                    gemma3.embedding_length u32         \n     = 2560\nllama_model_loader: - kv  14:                         gemma3.block_count u32         \n     = 34\nllama_model_loader: - kv  15:                 gemma3.feed_forward_length u32         \n     = 10240\nllama_model_loader: - kv  16:                gemma3.attention.head_count u32         \n     = 8\nllama_model_loader: - kv  17:    gemma3.attention.layer_norm_rms_epsilon f32         \n     = 0.000001\nllama_model_loader: - kv  18:                gemma3.attention.key_length u32         \n     = 256\nllama_model_loader: - kv  19:              gemma3.attention.value_length u32         \n     = 256\nllama_model_loader: - kv  20:                      gemma3.rope.freq_base f32         \n     = 1000000.000000\nllama_model_loader: - kv  21:            gemma3.attention.sliding_window u32         \n     = 1024\nllama_model_loader: - kv  22:             gemma3.attention.head_count_kv u32         \n     = 4\nllama_model_loader: - kv  23:                   gemma3.rope.scaling.type str         \n     = linear\nllama_model_loader: - kv  24:                 gemma3.rope.scaling.factor f32         \n     = 8.000000\nllama_model_loader: - kv  25:                       tokenizer.ggml.model str         \n     = llama\nllama_model_loader: - kv  26:                         tokenizer.ggml.pre str         \n     = default\n\n18:18:01.001 > stderr: llama_model_loader: - kv  27:                      tokenizer.ggml.tokens arr[str,262144]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n\n18:18:01.124 > stderr: llama_model_loader: - kv  28:                      tokenizer.ggml.scores arr[f32,262144]  = [-1000.000000, -1000.000000, -1000.00...\n\n18:18:01.127 > stderr: time=2025-04-21T18:18:01.126+09:00 level=INFO source=server.go:605 msg=\"waiting for server to become available\" status=\"llm server loading model\"  \n\n18:18:01.145 > stderr: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32         \n     = 2\nllama_model_loader: - kv  31:                tokenizer.ggml.eos_token_id u32         \n     = 1\nllama_model_loader: - kv  32:            tokenizer.ggml.unknown_token_id u32         \n     = 3\nllama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32         \n     = 0\nllama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  36:                    tokenizer.chat_template str         \n     = {{ bos_token }}\\n{%- if messages[0]['r...\nllama_model_loader: - kv  37:            tokenizer.ggml.add_space_prefix bool             = false\nllama_model_loader: - kv  38:               general.quantization_version u32         \n     = 2\nllama_model_loader: - kv  39:                          general.file_type u32         \n     = 7\nllama_model_loader: - type  f32:  205 tensors\nllama_model_loader: - type q8_0:  239 tensors\n\n18:18:01.294 > stderr: llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n\n18:18:01.296 > stderr: llm_load_vocab: special tokens cache size = 6414\n\n18:18:01.325 > stderr: llm_load_vocab: token to piece cache size = 1.9446 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = gemma3\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 262144\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 131072\nllm_load_print_meta: n_embd           = 2560\nllm_load_print_meta: n_layer          = 34\nllm_load_print_meta: n_head           = 8\nllm_load_print_meta: n_head_kv        = 4\nllm_load_print_meta: n_rot            = 256\nllm_load_print_meta: n_swa            = 1024\nllm_load_print_meta: n_embd_head_k    = 256\nllm_load_print_meta: n_embd_head_v    = 256\nllm_load_print_meta: n_gqa            = 2\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: f_attn_scale     = 6.2e-02\nllm_load_print_meta: n_ff             = 10240\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 2\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 1000000.0\nllm_load_print_meta: freq_scale_train = 0.125\nllm_load_print_meta: n_ctx_orig_yarn  = 131072\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 4B\nllm_load_print_meta: model ftype      = Q8_0\nllm_load_print_meta: model params     = 3.88 B\nllm_load_print_meta: model size       = 3.84 GiB (8.50 BPW)\nllm_load_print_meta: general.name     = Gemma 3 4b It\nllm_load_print_meta: BOS token        = 2 '<bos>'\nllm_load_print_meta: EOS token        = 1 '<eos>'\nllm_load_print_meta: EOT token        = 106 '<end_of_turn>'\nllm_load_print_meta: UNK token        = 3 '<unk>'\nllm_load_print_meta: PAD token        = 0 '<pad>'\nllm_load_print_meta: LF token         = 248 '<0x0A>'\nllm_load_print_meta: EOG token        = 1 '<eos>'\nllm_load_print_meta: EOG token        = 106 '<end_of_turn>'\nllm_load_print_meta: max token length = 48\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\n\n18:18:01.331 > stderr: get_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory     \n\n18:18:02.175 > stderr: get_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory     \n\n18:18:02.339 > stderr: llm_load_tensors: offloading 34 repeating layers to GPU\nllm_load_tensors: offloading output layer to GPU\nllm_load_tensors: offloaded 35/35 layers to GPU\nllm_load_tensors:        SYCL0 model buffer size =  3932.65 MiB\nllm_load_tensors:    SYCL_Host model buffer size =   680.00 MiB\n\n18:18:12.178 > stderr: llama_new_context_with_model: n_seq_max     = 1\nllama_new_context_with_model: n_ctx         = 2048\nllama_new_context_with_model: n_ctx_per_seq = 2048\nllama_new_context_with_model: n_batch       = 512\nllama_new_context_with_model: n_ubatch      = 512\nllama_new_context_with_model: flash_attn    = 0\nllama_new_context_with_model: freq_base     = 1000000.0\nllama_new_context_with_model: freq_scale    = 0.125\nllama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n[SYCL] call ggml_check_sycl\nggml_check_sycl: GGML_SYCL_DEBUG: 0\nggml_check_sycl: GGML_SYCL_F16: no\nFound 1 SYCL devices:\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\n| 0| [level_zero:gpu:0]|                     Intel Arc Graphics|  12.71|    128|    1024|   32| 17665M|            1.6.32960|\n\n18:18:12.278 > stderr: llama_kv_cache_init:      SYCL0 KV buffer size =   272.00 MiB\nllama_new_context_with_model: KV self size  =  272.00 MiB, K (f16):  136.00 MiB, V (f16):  136.00 MiB\n\n18:18:12.280 > stderr: llama_new_context_with_model:  SYCL_Host  output buffer size =     1.01 MiB\n\n18:18:12.661 > stderr: llama_new_context_with_model:      SYCL0 compute buffer size =   517.00 MiB\nllama_new_context_with_model:  SYCL_Host compute buffer size =    13.01 MiB\nllama_new_context_with_model: graph nodes  = 1401\nllama_new_context_with_model: graph splits = 2\n\n18:18:12.664 > stderr: key general.file_type not found in file\n\n18:18:12.668 > stderr: Exception 0xe06d7363 0x19930520 0x51d2ff770 0x7ffe0c5c933a    \nPC=0x7ffe0c5c933a\nsignal arrived during external code execution\n\nruntime.cgocall(0x7ff7c7a1d8b0, 0xc00047dc78)\n        runtime/cgocall.go:167 +0x3e fp=0xc00047dc50 sp=0xc00047dbe8 pc=0x7ff7c6e69c1e\nollama/llama/llamafile._Cfunc_clip_model_load(0x1d9000f15f0, 0x1)\n        _cgo_gotypes.go:307 +0x56 fp=0xc00047dc78 sp=0xc00047dc50 pc=0x7ff7c723f8d6  \nollama/llama/llamafile.NewClipContext(0xc000384bd0, {0xc000040230, 0x6a})\n        ollama/llama/llamafile/llama.go:488 +0x90 fp=0xc00047dd38 sp=0xc00047dc78 pc=0x7ff7c7246cd0\n\n18:18:12.671 > stderr: ollama/llama/runner.NewImageContext(0xc000384bd0, {0xc000040230, 0x6a})\n        ollama/llama/runner/image.go:37 +0xf8 fp=0xc00047ddb8 sp=0xc00047dd38 pc=0x7ff7c724be58\nollama/llama/runner.(*Server).loadModel(0xc0000fd560, {0x3e7, 0x0, 0x0, 0x0, {0x0, 0x0, 0x0}, 0xc000208820, 0x0}, ...)\n        ollama/llama/runner/runner.go:881 +0x24f fp=0xc00047df10 sp=0xc00047ddb8 pc=0x7ff7c72519cf\nollama/llama/runner.Execute.gowrap1()\n        ollama/llama/runner/runner.go:1001 +0xda fp=0xc00047dfe0 sp=0xc00047df10 pc=0x7ff7c72533da\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00047dfe8 sp=0xc00047dfe0 pc=0x7ff7c6e78901\ncreated by ollama/llama/runner.Execute in goroutine 1\n        ollama/llama/runner/runner.go:1001 +0xd0d\n\ngoroutine 1 gp=0xc00008e000 m=nil [IO wait]:\nruntime.gopark(0x7ff7c6e7a0c0?, 0x7ff7c8613ac0?, 0x20?, 0x4f?, 0xc0001f4fcc?)        \n        runtime/proc.go:424 +0xce fp=0xc000587418 sp=0xc0005873f8 pc=0x7ff7c6e703ce  \n\n18:18:12.672 > stderr: runtime.netpollblock(0x3c0?, 0xc6e08366?, 0xf7?)\n        runtime/netpoll.go:575 +0xf7 fp=0xc000587450 sp=0xc000587418 pc=0x7ff7c6e34f97\ninternal/poll.runtime_pollWait(0x1d97ee7ec90, 0x72)\n        runtime/netpoll.go:351 +0x85 fp=0xc000587470 sp=0xc000587450 pc=0x7ff7c6e6f645\ninternal/poll.(*pollDesc).wait(0x7ff7c6f02bd5?, 0x7ff7c6e6ae7d?, 0x0)\n        internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc000587498 sp=0xc000587470 pc=0x7ff7c6f04207\ninternal/poll.execIO(0xc0001f4f20, 0xc000587540)\n        internal/poll/fd_windows.go:177 +0x105 fp=0xc000587510 sp=0xc000587498 pc=0x7ff7c6f05645\ninternal/poll.(*FD).acceptOne(0xc0001f4f08, 0x3d4, {0xc0003860f0?, 0xc0005875a0?, 0x7ff7c6f0d3c5?}, 0xc0005875d4?)\n        internal/poll/fd_windows.go:946 +0x65 fp=0xc000587570 sp=0xc000587510 pc=0x7ff7c6f09c85\ninternal/poll.(*FD).Accept(0xc0001f4f08, 0xc000587720)\n        internal/poll/fd_windows.go:980 +0x1b6 fp=0xc000587628 sp=0xc000587570 pc=0x7ff7c6f09fb6\nnet.(*netFD).accept(0xc0001f4f08)\n        net/fd_windows.go:182 +0x4b fp=0xc000587740 sp=0xc000587628 pc=0x7ff7c6f7082b\nnet.(*TCPListener).accept(0xc0002ba7c0)\n        net/tcpsock_posix.go:159 +0x1e fp=0xc000587790 sp=0xc000587740 pc=0x7ff7c6f8699e\nnet.(*TCPListener).Accept(0xc0002ba7c0)\n        net/tcpsock.go:372 +0x30 fp=0xc0005877c0 sp=0xc000587790 pc=0x7ff7c6f85750   \nnet/http.(*onceCloseListener).Accept(0xc0000fd5f0?)\n        <autogenerated>:1 +0x24 fp=0xc0005877d8 sp=0xc0005877c0 pc=0x7ff7c7200044    \nnet/http.(*Server).Serve(0xc00047ef00, {0x7ff7c7e4c6f0, 0xc0002ba7c0})\n        net/http/server.go:3330 +0x30c fp=0xc000587908 sp=0xc0005877d8 pc=0x7ff7c71d7fcc\nollama/llama/runner.Execute({0xc0000ce010?, 0x0?, 0x0?})\n        ollama/llama/runner/runner.go:1027 +0x11a9 fp=0xc000587ca8 sp=0xc000587908 pc=0x7ff7c7252fa9\nollama/cmd.NewCLI.func2(0xc000498f00?, {0x7ff7c7c8e8ce?, 0x4?, 0x7ff7c7c8e8d2?})     \n        ollama/cmd/cmd.go:1430 +0x45 fp=0xc000587cd0 sp=0xc000587ca8 pc=0x7ff7c7a1d0c5\ngithub.com/spf13/cobra.(*Command).execute(0xc0002be908, {0xc0002b85a0, 0x11, 0x11})  \n        github.com/spf13/cobra@v1.8.1/command.go:985 +0xaaa fp=0xc000587e58 sp=0xc000587cd0 pc=0x7ff7c700a4ea\ngithub.com/spf13/cobra.(*Command).ExecuteC(0xc00027e308)\n        github.com/spf13/cobra@v1.8.1/command.go:1117 +0x3ff fp=0xc000587f30 sp=0xc000587e58 pc=0x7ff7c700adbf\ngithub.com/spf13/cobra.(*Command).Execute(...)\n        github.com/spf13/cobra@v1.8.1/command.go:1041\ngithub.com/spf13/cobra.(*Command).ExecuteContext(...)\n        github.com/spf13/cobra@v1.8.1/command.go:1034\nmain.main()\n        ollama/main.go:12 +0x4d fp=0xc000587f50 sp=0xc000587f30 pc=0x7ff7c7a1d72d    \nruntime.main()\n        runtime/proc.go:272 +0x27d fp=0xc000587fe0 sp=0xc000587f50 pc=0x7ff7c6e3df9d \nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000587fe8 sp=0xc000587fe0 pc=0x7ff7c6e78901\n\n18:18:12.675 > stderr:\ngoroutine 2 gp=0xc00008e700 m=nil [force gc (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc000091fa8 sp=0xc000091f88 pc=0x7ff7c6e703ce  \nruntime.goparkunlock(...)\n        runtime/proc.go:430\nruntime.forcegchelper()\n        runtime/proc.go:337 +0xb8 fp=0xc000091fe0 sp=0xc000091fa8 pc=0x7ff7c6e3e2b8  \nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000091fe8 sp=0xc000091fe0 pc=0x7ff7c6e78901\ncreated by runtime.init.7 in goroutine 1\n        runtime/proc.go:325 +0x1a\n\ngoroutine 3 gp=0xc00008ea80 m=nil [GC sweep wait]:\nruntime.gopark(0x1?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc000093f80 sp=0xc000093f60 pc=0x7ff7c6e703ce  \nruntime.goparkunlock(...)\n        runtime/proc.go:430\nruntime.bgsweep(0xc000034100)\n        runtime/mgcsweep.go:317 +0xdf fp=0xc000093fc8 sp=0xc000093f80 pc=0x7ff7c6e26f9f\nruntime.gcenable.gowrap1()\n        runtime/mgc.go:204 +0x25 fp=0xc000093fe0 sp=0xc000093fc8 pc=0x7ff7c6e1b5c5   \nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000093fe8 sp=0xc000093fe0 pc=0x7ff7c6e78901\ncreated by runtime.gcenable in goroutine 1\n        runtime/mgc.go:204 +0x66\n\ngoroutine 4 gp=0xc00008ec40 m=nil [GC scavenge wait]:\nruntime.gopark(0x10000?, 0x7ff7c7e3bb18?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc0000a3f78 sp=0xc0000a3f58 pc=0x7ff7c6e703ce  \nruntime.goparkunlock(...)\n        runtime/proc.go:430\nruntime.(*scavengerState).park(0x7ff7c86379c0)\n        runtime/mgcscavenge.go:425 +0x49 fp=0xc0000a3fa8 sp=0xc0000a3f78 pc=0x7ff7c6e24969\nruntime.bgscavenge(0xc000034100)\n        runtime/mgcscavenge.go:658 +0x59 fp=0xc0000a3fc8 sp=0xc0000a3fa8 pc=0x7ff7c6e24ef9\nruntime.gcenable.gowrap2()\n        runtime/mgc.go:205 +0x25 fp=0xc0000a3fe0 sp=0xc0000a3fc8 pc=0x7ff7c6e1b565   \nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc0000a3fe8 sp=0xc0000a3fe0 pc=0x7ff7c6e78901\ncreated by runtime.gcenable in goroutine 1\n        runtime/mgc.go:205 +0xa5\n\ngoroutine 5 gp=0xc00008f180 m=nil [finalizer wait]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc0000a5e20 sp=0xc0000a5e00 pc=0x7ff7c6e703ce  \nruntime.runfinq()\n        runtime/mfinal.go:193 +0x107 fp=0xc0000a5fe0 sp=0xc0000a5e20 pc=0x7ff7c6e1a687\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc0000a5fe8 sp=0xc0000a5fe0 pc=0x7ff7c6e78901\ncreated by runtime.createfing in goroutine 1\n        runtime/mfinal.go:163 +0x3d\n\ngoroutine 6 gp=0xc0001ea380 m=nil [chan receive]:\nruntime.gopark(0xc000095f60?, 0x7ff7c6f5a2e5?, 0x10?, 0xa8?, 0x7ff7c7e628a0?)        \n        runtime/proc.go:424 +0xce fp=0xc000095f18 sp=0xc000095ef8 pc=0x7ff7c6e703ce  \n\n18:18:12.677 > stderr: runtime.chanrecv(0xc0000404d0, 0x0, 0x1)\n        runtime/chan.go:639 +0x41e fp=0xc000095f90 sp=0xc000095f18 pc=0x7ff7c6e0ac9e \nruntime.chanrecv1(0x7ff7c6e3e100?, 0xc000095f76?)\n        runtime/chan.go:489 +0x12 fp=0xc000095fb8 sp=0xc000095f90 pc=0x7ff7c6e0a852  \nruntime.unique_runtime_registerUniqueMapCleanup.func1(...)\n        runtime/mgc.go:1781\nruntime.unique_runtime_registerUniqueMapCleanup.gowrap1()\n        runtime/mgc.go:1784 +0x2f fp=0xc000095fe0 sp=0xc000095fb8 pc=0x7ff7c6e1e6af  \nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000095fe8 sp=0xc000095fe0 pc=0x7ff7c6e78901\ncreated by unique.runtime_registerUniqueMapCleanup in goroutine 1\n        runtime/mgc.go:1779 +0x96\n\ngoroutine 7 gp=0xc0001eaa80 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc00009ff38 sp=0xc00009ff18 pc=0x7ff7c6e703ce  \nruntime.gcBgMarkWorker(0xc0000418f0)\n        runtime/mgc.go:1412 +0xe9 fp=0xc00009ffc8 sp=0xc00009ff38 pc=0x7ff7c6e1d9a9  \nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc00009ffe0 sp=0xc00009ffc8 pc=0x7ff7c6e1d885  \nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00009ffe8 sp=0xc00009ffe0 pc=0x7ff7c6e78901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 18 gp=0xc000484000 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc00048bf38 sp=0xc00048bf18 pc=0x7ff7c6e703ce  \nruntime.gcBgMarkWorker(0xc0000418f0)\n        runtime/mgc.go:1412 +0xe9 fp=0xc00048bfc8 sp=0xc00048bf38 pc=0x7ff7c6e1d9a9  \nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc00048bfe0 sp=0xc00048bfc8 pc=0x7ff7c6e1d885  \nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00048bfe8 sp=0xc00048bfe0 pc=0x7ff7c6e78901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 34 gp=0xc0001061c0 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc000487f38 sp=0xc000487f18 pc=0x7ff7c6e703ce  \nruntime.gcBgMarkWorker(0xc0000418f0)\n        runtime/mgc.go:1412 +0xe9 fp=0xc000487fc8 sp=0xc000487f38 pc=0x7ff7c6e1d9a9  \nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc000487fe0 sp=0xc000487fc8 pc=0x7ff7c6e1d885  \nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000487fe8 sp=0xc000487fe0 pc=0x7ff7c6e78901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 19 gp=0xc0004841c0 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc00048df38 sp=0xc00048df18 pc=0x7ff7c6e703ce  \nruntime.gcBgMarkWorker(0xc0000418f0)\n        runtime/mgc.go:1412 +0xe9 fp=0xc00048dfc8 sp=0xc00048df38 pc=0x7ff7c6e1d9a9  \nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc00048dfe0 sp=0xc00048dfc8 pc=0x7ff7c6e1d885  \nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00048dfe8 sp=0xc00048dfe0 pc=0x7ff7c6e78901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 8 gp=0xc0001eac40 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc0000a1f38 sp=0xc0000a1f18 pc=0x7ff7c6e703ce  \nruntime.gcBgMarkWorker(0xc0000418f0)\n        runtime/mgc.go:1412 +0xe9 fp=0xc0000a1fc8 sp=0xc0000a1f38 pc=0x7ff7c6e1d9a9  \nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc0000a1fe0 sp=0xc0000a1fc8 pc=0x7ff7c6e1d885  \nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc0000a1fe8 sp=0xc0000a1fe0 pc=0x7ff7c6e78901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 35 gp=0xc000106380 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc000489f38 sp=0xc000489f18 pc=0x7ff7c6e703ce  \nruntime.gcBgMarkWorker(0xc0000418f0)\n        runtime/mgc.go:1412 +0xe9 fp=0xc000489fc8 sp=0xc000489f38 pc=0x7ff7c6e1d9a9  \nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc000489fe0 sp=0xc000489fc8 pc=0x7ff7c6e1d885  \nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000489fe8 sp=0xc000489fe0 pc=0x7ff7c6e78901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 20 gp=0xc000484380 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc000495f38 sp=0xc000495f18 pc=0x7ff7c6e703ce  \nruntime.gcBgMarkWorker(0xc0000418f0)\n        runtime/mgc.go:1412 +0xe9 fp=0xc000495fc8 sp=0xc000495f38 pc=0x7ff7c6e1d9a9  \nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc000495fe0 sp=0xc000495fc8 pc=0x7ff7c6e1d885  \nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000495fe8 sp=0xc000495fe0 pc=0x7ff7c6e78901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 9 gp=0xc0001eae00 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc000491f38\n18:18:12.683 > stderr:  sp=0xc000491f18 pc=0x7ff7c6e703ce\nruntime.gcBgMarkWorker(0xc0000418f0)\n        runtime/mgc.go:1412 +0xe9 fp=0xc000491fc8 sp=0xc000491f38 pc=0x7ff7c6e1d9a9  \nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc000491fe0 sp=0xc000491fc8 pc=0x7ff7c6e1d885  \nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000491fe8 sp=0xc000491fe0 pc=0x7ff7c6e78901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 36 gp=0xc000106540 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc000113f38 sp=0xc000113f18 pc=0x7ff7c6e703ce  \nruntime.gcBgMarkWorker(0xc0000418f0)\n        runtime/mgc.go:1412 +0xe9 fp=0xc000113fc8 sp=0xc000113f38 pc=0x7ff7c6e1d9a9  \nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc000113fe0 sp=0xc000113fc8 pc=0x7ff7c6e1d885  \nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000113fe8 sp=0xc000113fe0 pc=0x7ff7c6e78901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 21 gp=0xc000484540 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc000497f38 sp=0xc000497f18 pc=0x7ff7c6e703ce  \nruntime.gcBgMarkWorker(0xc0000418f0)\n        runtime/mgc.go:1412 +0xe9 fp=0xc000497fc8 sp=0xc000497f38 pc=0x7ff7c6e1d9a9  \nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc000497fe0 sp=0xc000497fc8 pc=0x7ff7c6e1d885  \nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000497fe8 sp=0xc000497fe0 pc=0x7ff7c6e78901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 10 gp=0xc0001eafc0 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc000493f38 sp=0xc000493f18 pc=0x7ff7c6e703ce  \nruntime.gcBgMarkWorker(0xc0000418f0)\n        runtime/mgc.go:1412 +0xe9 fp=0xc000493fc8 sp=0xc000493f38 pc=0x7ff7c6e1d9a9  \nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc000493fe0 sp=0xc000493fc8 pc=0x7ff7c6e1d885  \nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000493fe8 sp=0xc000493fe0 pc=0x7ff7c6e78901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 37 gp=0xc000106700 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc000115f38 sp=0xc000115f18 pc=0x7ff7c6e703ce  \nruntime.gcBgMarkWorker(0xc0000418f0)\n        runtime/mgc.go:1412 +0xe9 fp=0xc000115fc8 sp=0xc000115f38 pc=0x7ff7c6e1d9a9  \nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc000115fe0 sp=0xc000115fc8 pc=0x7ff7c6e1d885  \nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000115fe8 sp=0xc000115fe0 pc=0x7ff7c6e78901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 22 gp=0xc000484700 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc00010ff38 sp=0xc00010ff18 pc=0x7ff7c6e703ce  \nruntime.gcBgMarkWorker(0xc0000418f0)\n        runtime/mgc.go:1412 +0xe9 fp=0xc00010ffc8 sp=0xc00010ff38 pc=0x7ff7c6e1d9a9  \nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc00010ffe0 sp=0xc00010ffc8 pc=0x7ff7c6e1d885  \nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00010ffe8 sp=0xc00010ffe0 pc=0x7ff7c6e78901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 23 gp=0xc0004848c0 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc000111f38 sp=0xc000111f18 pc=0x7ff7c6e703ce  \nruntime.gcBgMarkWorker(0xc0000418f0)\n        runtime/mgc.go:1412 +0xe9 fp=0xc000111fc8 sp=0xc000111f38 pc=0x7ff7c6e1d9a9  \nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc000111fe0 sp=0xc000111fc8 pc=0x7ff7c6e1d885  \nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000111fe8 sp=0xc000111fe0 pc=0x7ff7c6e78901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 11 gp=0xc0001eb180 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc000473f38 sp=0xc000473f18 pc=0x7ff7c6e703ce  \nruntime.gcBgMarkWorker(0xc0000418f0)\n        runtime/mgc.go:1412 +0xe9 fp=0xc000473fc8 sp=0xc000473f38 pc=0x7ff7c6e1d9a9  \nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc000473fe0 sp=0xc000473fc8 pc=0x7ff7c6e1d885  \nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000473fe8 sp=0xc000473fe0 pc=0x7ff7c6e78901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 38 gp=0xc0001068c0 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc00046ff38 sp=0xc00046ff18 pc=0x7ff7c6e703ce  \nruntime.gcBgMarkWorker(0xc0000418f0)\n        runtime/mgc.go:1412 +0xe9 fp=0xc00046ffc8 sp=0xc00046ff38 pc=0x7ff7c6e1d9a9  \nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc00046ffe0 sp=0xc00046ffc8 pc=0x7ff7c6e1d885  \nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00046ffe8 sp=0xc00046ffe0 pc=0x7ff7c6e78901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 24 gp=0xc000484a80 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc00049ff38 sp=0xc00049ff18 pc=0x7ff7c6e703ce  \nruntime.gcBgMarkWorker(0xc0000418f0)\n        runtime/mgc.go:1412 +0xe9 fp=0xc00049ffc8 sp=0xc00049ff38 pc=0x7ff7c6e1d9a9  \nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc00049ffe0 sp=0xc00049ffc8 pc=0x7ff7c6e1d885  \nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00049ffe8 sp=0xc00049ffe0 pc=0x7ff7c6e78901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 39 gp=0xc000106a80 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc000471f38 sp=0xc000471f18 pc=0x7ff7c6e703ce  \nruntime.gcBgMarkWorker(0xc0000418f0)\n        runtime/mgc.go:1412 +0xe9 fp=0xc000471fc8 sp=0xc000471f38 pc=0x7ff7c6e1d9a9  \nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc000471fe0 sp=0xc000471fc8 pc=0x7ff7c6e1d885  \nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000471fe8 sp=0xc000471fe0 pc=0x7ff7c6e78901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 12 gp=0xc0001eb340 m=nil [GC worker (idle)]:\nruntime.gopark(0x7ff7c86864a0?, 0x1?, 0x28?, 0x3e?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc000475f38 sp=0xc000475f18 pc=0x7ff7c6e703ce  \nruntime.gcBgMarkWorker(0xc0000418f0)\n        runtime/mgc.go:1412 +0xe9 fp=0xc000475fc8 sp=0xc000475f38 pc=0x7ff7c6e1d9a9  \nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc000475fe0 sp=0xc000475fc8 pc=0x7ff7c6e1d885  \nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000475fe8 sp=0xc000475fe0 pc=0x7ff7c6e78901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 40 gp=0xc000106c40 m=nil [GC worker (idle)]:\nruntime.gopark(0x4a86ff95cb0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc00049bf38 sp=0xc00049bf18 pc=0x7ff7c6e703ce  \nruntime.gcBgMarkWorker(0xc0000418f0)\n        runtime/mgc.go:1412 +0xe9 fp=0xc00049bfc8 sp=0xc00049bf38 pc=0x7ff7c6e1d9a9  \nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc00049bfe0 sp=0xc00049bfc8 pc=0x7ff7c6e1d885  \nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00049bfe8 sp=0xc00049bfe0 pc=0x7ff7c6e78901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 25 gp=0xc000484c40 m=nil [GC worker (idle)]:\nruntime.gopark(0x7ff7c86864a0?, 0x1?, 0x28?, 0x3e?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc0004a1f38 sp=0xc0004a1f18 pc=0x7ff7c6e703ce  \nruntime.gcBgMarkWorker(0xc0000418f0)\n        runtime/mgc.go:1412 +0xe9 fp=0xc0004a1fc8 sp=0xc0004a1f38 pc=0x7ff7c6e1d9a9  \nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc0004a1fe0 sp=0xc0004a1fc8 pc=0x7ff7c6e1d885  \nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc0004a1fe8 sp=0xc0004a1fe0 pc=0x7ff7c6e78901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 13 gp=0xc0001eb500 m=nil [GC worker (idle)]:\nruntime.gopark(0x4a86ff95cb0?, 0x1?, 0x8c?, 0x59?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc00047bf38 sp=0xc00047bf18 pc=0x7ff7c6e703ce  \nruntime.gcBgMarkWorker(0xc0000418f0)\n        runtime/mgc.go:1412 +0xe9 fp=0xc00047bfc8 sp=0xc00047bf38 pc=0x7ff7c6e1d9a9  \nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc00047bfe0 sp=0xc00047bfc8 pc=0x7ff7c6e1d885  \nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00047bfe8 sp=0xc00047bfe0 pc=0x7ff7c6e78901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 15 gp=0xc000107880 m=nil [semacquire]:\nruntime.gopark(0x0?, 0x0?, 0x60?, 0x3e?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc00049de18 sp=0xc00049ddf8 pc=0x7ff7c6e703ce  \nruntime.goparkunlock(...)\n        runtime/proc.go:430\nruntime.semacquire1(0xc0000fd568, 0x0, 0x1, 0x0, 0x12)\n        runtime/sema.go:178 +0x232 fp=0xc00049de80 sp=0xc00049de18 pc=0x7ff7c6e50092 \nsync.runtime_Semacquire(0x0?)\n        runtime/sema.go:71 +0x25 fp=0xc00049deb8 sp=0xc00049de80 pc=0x7ff7c6e718a5   \nsync.(*WaitGroup).Wait(0x0?)\n        sync/waitgroup.go:118 +0x48 fp=0xc00049dee0 sp=0xc00049deb8 pc=0x7ff7c6e896c8\nollama/llama/runner.(*Server).run(0xc0000fd560, {0x7ff7c7e4e9b0, 0xc0000f50e0})      \n        ollama/llama/runner/runner.go:315 +0x47 fp=0xc00049dfb8 sp=0xc00049dee0 pc=0x7ff7c724dec7\nollama/llama/runner.Execute.gowrap2()\n        ollama/llama/runner/runner.go:1006 +0x28 fp=0xc00049dfe0 sp=0xc00049dfb8 pc=0x7ff7c72532c8\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00049dfe8 sp=0xc00049dfe0 pc=0x7ff7c6e78901\ncreated by ollama/llama/runner.Execute in goroutine 1\n        ollama/llama/runner/runner.go:1006 +0xde5\n\ngoroutine 16 gp=0xc000107c00 m=nil [IO wait]:\nruntime.gopark(0x0?, 0xc0001f51a0?, 0x48?, 0x52?, 0xc0001f524c?)\n        runtime/proc.go:424 +0xce fp=0xc00004d890 sp=0xc00004d870 pc=0x7ff7c6e703ce  \nruntime.netpollblock(0x3cc?, 0xc6e08366?, 0xf7?)\n        runtime/netpoll.go:575 +0xf7 fp=0xc00004d8c8 sp=0xc00004d890 pc=0x7ff7c6e34f97\ninternal/poll.runtime_pollWait(0x1d97ee7eb78, 0x72)\n        runtime/netpoll.go:351 +0x85 fp=0xc00004d8e8 sp=0xc00004d8c8 pc=0x7ff7c6e6f645\ninternal/poll.(*pollDesc).wait(0x1d938b7d768?, 0xc0000a8600?, 0x0)\n        internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc00004d910 sp=0xc00004d8e8 pc=0x7ff7c6f04207\ninternal/poll.execIO(0xc0001f51a0, 0x7ff7c7d105e8)\n        internal/poll/fd_windows.go:177 +0x105 fp=0xc00004d988 sp=0xc00004d910 pc=0x7ff7c6f05645\ninternal/poll.(*FD).Read(0xc0001f5188, {0xc0001cf000, 0x1000, 0x1000})\n        internal/poll/fd_windows.go:438 +0x2a7 fp=0xc00004da30 sp=0xc00004d988 pc=0x7ff7c6f06347\nnet.(*netFD).Read(0xc0001f5188, {0xc0001cf000?, 0xc00004daa0?, 0x7ff7c6f046c5?})     \n        net/fd_posix.go:55 +0x25 fp=0xc00004da78 sp=0xc00004da30 pc=0x7ff7c6f6e945   \nnet.(*conn).Read(0xc0000989f8, {0xc0001cf000?, 0x0?, 0xc00020a0f8?})\n        net/net.go:189 +0x45 fp=0xc00004dac0 sp=0xc00004da78 pc=0x7ff7c6f7df25       \nnet.(*TCPConn).Read(0xc00020a0f0?, {0xc0001cf000?, 0xc0001f5188?, 0xc00004daf8?})    \n        <autogenerated>:1 +0x25 fp=0xc00004daf0 sp=0xc00004dac0 pc=0x7ff7c6f8f945    \nnet/http.(*connReader).Read(0xc00020a0f0, {0xc0001cf000, 0x1000, 0x1000})\n        net/http/server.go:798 +0x14b fp=0xc00004db40 sp=0xc00004daf0 pc=0x7ff7c71cdd8b\nbufio.(*Reader).fill(0xc0000a8120)\n        bufio/bufio.go:110 +0x103 fp=0xc00004db78 sp=0xc00004db40 pc=0x7ff7c6f94583  \nbufio.(*Reader).Peek(0xc0000a8120, 0x4)\n        bufio/bufio.go:148 +0x53 fp=0xc00004db98 sp=0xc00004db78 pc=0x7ff7c6f946b3   \nnet/http.(*conn).serve(0xc0000fd5f0, {0x7ff7c7e4e978, 0xc0001c1380})\n        net/http/server.go:2127 +0x738 fp=0xc00004dfb8 sp=0xc00004db98 pc=0x7ff7c71d30d8\nnet/http.(*Server).Serve.gowrap3()\n        net/http/server.go:3360 +0x28 fp=0xc00004dfe0 sp=0xc00004dfb8 pc=0x7ff7c71d83c8\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00004dfe8 sp=0xc00004dfe0 pc=0x7ff7c6e78901\ncreated by net/http.(*Server).Serve in goroutine 1\n        net/http/server.go:3360 +0x485\nrax     0x0\nrbx     0x51d2ff718\nrcx     0x26\nrdx     0x51d2fee60\nrdi     0xe06d7363\nrsi     0x1\nrbp     0x4\nrsp     0x51d2ff5f0\nr8      0xffff0000\nr9      0x51d2ff0ec\nr10     0x4\nr11     0x7ffe0c000000\nr12     0xc00047dcf8\nrflags  0x202\ncs      0x33\nfs      0x53\ngs      0x2b\n```\n\n\n",
      "state": "open",
      "author": "bibekyess",
      "author_type": "User",
      "created_at": "2025-04-21T09:25:54Z",
      "updated_at": "2025-05-12T03:34:11Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13099/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13099",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13099",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:36.699180",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @bibekyess, you may install our latest v0.6.2 ipex-llm ollama in https://github.com/ipex-llm/ipex-llm/releases/tag/v2.3.0-nightly, which could support `gemma3-fp16`. ",
          "created_at": "2025-04-22T02:25:17Z"
        },
        {
          "author": "chnxq",
          "body": "\n> Hi [@bibekyess](https://github.com/bibekyess), you may install our latest v0.6.2 ipex-llm ollama in https://github.com/ipex-llm/ipex-llm/releases/tag/v2.3.0-nightly, which could support .`gemma3-fp16`\n\nHi @sgwhat \n\nI made the Ollama's source code support OneApi by making changes. However, the inf",
          "created_at": "2025-04-22T16:22:11Z"
        },
        {
          "author": "bibekyess",
          "body": "Hi @sgwhat! \nThank you for your response. Unfortunately, nightly versions (both updated last week and yesterday) are not able to serve `gemma3:4b-it-fp16`.\nFor `ollama-ipex-llm-2.3.0b20250415-win`, the error logs is as follow:\n```\ntime=2025-04-30T17:25:34.370+09:00 level=INFO source=server.go:106 ms",
          "created_at": "2025-04-30T08:34:43Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @bibekyess , I have fixed the `ngl` issue, you may try it tmr via `pip install --pre --upgrade ipex-llm[cpp]` or download a zip from [link](https://www.modelscope.cn/models/Intel/ollama/files). Good luck for you :)",
          "created_at": "2025-04-30T08:43:24Z"
        },
        {
          "author": "bibekyess",
          "body": "Hi @sgwhat \nThank you for your prompt response. I am using zip version and it fixes the `ngl` issue but still I cannot run both of fp16 and Q8_0 models.\nLogs are attached:\n```\ntime=2025-04-30T17:47:57.573+09:00 level=INFO source=server.go:107 msg=\"system memory\" total=\"31.5 GiB\" free=\"15.6 GiB\" free",
          "created_at": "2025-04-30T08:50:45Z"
        }
      ]
    },
    {
      "issue_number": 13087,
      "title": "Running ipex-llm harness on A770 failed.",
      "body": "\"After installing the harness from https://github.com/intel/ipex-llm/tree/main/python/llm/dev/benchmark/harness, I encountered the following image error.\"\n\n2025-04-17 12:46:04,825 - INFO - Converting the current model to sym_int4 format......\nJob config of task=winogrande, precision=sym_int4 failed. Error Message: cannot import name 'top_k_top_p_filtering' from 'transformers' (/home/test/miniforge3/envs/harness_test/lib/python3.11/site-packages/transformers/__init__.py)\nHere are results of all successful tasks:\nTraceback (most recent call last):\n  File \"/home/test/ipex-llm/python/llm/dev/benchmark/harness/run_llb.py\", line 150, in <module>\n    main()\n  File \"/home/test/ipex-llm/python/llm/dev/benchmark/harness/run_llb.py\", line 146, in main\n    raise RuntimeError('\\n'.join(fail))\nRuntimeError: Job config of task=winogrande, precision=sym_int4 failed. Error Message: cannot import name 'top_k_top_p_filtering' from 'transformers' (/home/test/miniforge3/envs/harness_test/lib/python3.11/site-packages/transformers/__init__.py)\n\n![Image](https://github.com/user-attachments/assets/9870e8cf-219a-4e6d-a3d0-d6ea558728a7)\nMy environment setup is:\npip install accelerate==1.5.2 transformers==4.50.1 peft==0.15.0\n\nMy run command is:\npython run_llb.py --model ipex-llm --pretrained /home/test/models/LLM/baichuan2-7b/pytorch/ --precision sym_int4 --device xpu --tasks winogrande --batch 1 --no_cache --model_args dtype=float16\n\nPlease provide the correct dependency versions. Thank you!\n\n\n\n\n",
      "state": "open",
      "author": "tao-ov",
      "author_type": "User",
      "created_at": "2025-04-17T04:56:46Z",
      "updated_at": "2025-05-12T02:02:09Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13087/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hkvision"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13087",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13087",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:36.920982",
      "comments": [
        {
          "author": "hkvision",
          "body": "Hi, \n\nYou are recommended to use transformers 4.37.0 if possible since our harness script is mainly tested on this version.\nIf you must use 4.50.1, you may try check your trl version, trll==0.11.0 is recommended.",
          "created_at": "2025-04-18T02:46:30Z"
        },
        {
          "author": "tao-ov",
          "body": "when i fix the dependency ,the ipex-llm repo use the latest version.\naccelerate                  0.23.0\ntransformers                4.37.0\ntrl                         0.4.7\npeft                        0.10.0\n\nit occours the following error, would you please confirm the correct version, thank you~\n20",
          "created_at": "2025-04-18T03:43:13Z"
        },
        {
          "author": "hkvision",
          "body": "https://github.com/intel/ipex-llm/blob/main/python/llm/dev/benchmark/harness/run_llb.py#L135\nMaybe you can remove the try catch block and rerun to have more error messages. I suppose the error may due to failure to download or find the dataset.",
          "created_at": "2025-04-18T05:26:53Z"
        },
        {
          "author": "tao-ov",
          "body": "Do you wish to run the custom code? [y/N] y\n2025-04-18 13:43:30,942 - WARNING - Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\npip install xformers.\n/home/test/miniforge3/envs/harness_test/lib/py",
          "created_at": "2025-04-18T05:45:17Z"
        },
        {
          "author": "tao-ov",
          "body": "export IPEX_LLM_LAST_LM_HEAD=0\npip install datasets==2.14.6\npip install transformers == 4.37.0\n\nnow, it success! Thank you~",
          "created_at": "2025-04-22T07:39:52Z"
        }
      ]
    },
    {
      "issue_number": 13106,
      "title": "Can't set Ollama context size - seems to be fixed to 8k",
      "body": "**Describe the bug**\nI followed https://github.com/intel/ipex-llm/blob/main/docker/llm/inference-cpp/README.md and started Ollama via the provided script. I want ollama to process long context prompts.\n\nHowever I am seeing in the ollama container logs\n```\ntime=2025-04-24T20:37:57.881+08:00 level=WARN source=runner.go:131 msg=\"truncating input prompt\" limit=8192 prompt=18538 keep=4 new=8192\n```\n\nno matter what I do.\n\nI tried putting\n```\nOLLAMA_CONTEXT_LENGTH=\"32768\"\nIPEX_LLM_NUM_CTX=\"32768\"\n```\ninto the container env but that doesn't help. I tried setting `num_ctx` via Modelfile but nothing.\nOn my mac setting via Modelfile works as expected.\n\nSome other select log lines that seem relevant:\n```\n2025/04/24 20:35:06 routes.go:1230: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:32768 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:true OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE:q8_0 OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:localhost,127.0.0.1]\"\n\nllama_init_from_model: n_ctx_per_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n\ntime=2025-04-24T20:37:57.881+08:00 level=WARN source=runner.go:131 msg=\"truncating input prompt\" limit=8192 prompt=18538 keep=4 new=8192\n```\n\n**How to reproduce**\nFollow https://github.com/intel/ipex-llm/blob/main/docker/llm/inference-cpp/README.md and start OLLAMA\n\n```\necho $(seq 1 32000) > numbers.txt\nexport OLLAMA_HOST=ailab.lan # your host\nollama ls\nbase=\"qwen2.5\"\nquant=\"14b\"\nollama show $base:$quant\nctx=32768\n\ncat <<EOF > model.Modelfile\nFROM $base:$quant\nPARAMETER num_ctx $ctx\nEOF\n\ncat model.Modelfile\n\nollama create $base-$ctx:$quant -f model.Modelfile\nollama ls\nollama show $base-$ctx:$quant\n\nollama run $base-$ctx:$quant < numbers.txt\n```\n\n**Environment information**\n\nHost env\n```\ndaniel@ailab:~$ sudo bash env-check.sh\n-----------------------------------------------------------------\nPYTHON_VERSION=3.12.7\n-----------------------------------------------------------------\nTransformers is not installed.\n-----------------------------------------------------------------\nPyTorch is not installed.\n-----------------------------------------------------------------\nipex-llm WARNING: Package(s) not found: ipex-llm\n-----------------------------------------------------------------\nIPEX is not installed.\n-----------------------------------------------------------------\nCPU Information:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               12\nOn-line CPU(s) list:                  0-11\nVendor ID:                            GenuineIntel\nBIOS Vendor ID:                       Intel(R) Corporation\nModel name:                           12th Gen Intel(R) Core(TM) i5-12500\nBIOS Model name:                      12th Gen Intel(R) Core(TM) i5-12500 To Be Filled By O.E.M. CPU @ 4.0GHz\nBIOS CPU family:                      205\nCPU family:                           6\nModel:                                151\nThread(s) per core:                   2\nCore(s) per socket:                   6\nSocket(s):                            1\nStepping:                             5\n-----------------------------------------------------------------\nTotal CPU Memory: 46.7931 GB\nMemory Type: DDR5\n-----------------------------------------------------------------\nOperating System:\nUbuntu 24.10 \\n \\l\n\n-----------------------------------------------------------------\nLinux ailab 6.11.0-24-generic #24-Ubuntu SMP PREEMPT_DYNAMIC Fri Mar 14 18:13:56 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux\n-----------------------------------------------------------------\nCLI:\n    Version: 1.2.39.20241101\n    Build ID: 00000000\n\nService:\n    Version: 1.2.39.20241101\n    Build ID: 00000000\n    Level Zero Version: 1.20.2\n-----------------------------------------------------------------\n  Driver UUID                                     32352e30-352e-3332-3536-370000000000\n  Driver Version                                  25.05.32567\n  Driver UUID                                     32352e30-352e-3332-3536-370000000000\n  Driver Version                                  25.05.32567\n-----------------------------------------------------------------\nDriver related package version:\nrc  intel-fw-gpu                                   2024.24.5-337~22.04                      all          Firmware package for Intel integrated and discrete GPUs\nii  intel-level-zero-gpu-raytracing                1.0.0-0ubuntu1~24.10~ppa4                amd64        Level Zero Ray Tracing Support library\n-----------------------------------------------------------------\nenv-check.sh: line 167: sycl-ls: command not found\nigpu not detected\n-----------------------------------------------------------------\nxpu-smi is properly installed.\n-----------------------------------------------------------------\n+-----------+--------------------------------------------------------------------------------------+\n| Device ID | Device Information                                                                   |\n+-----------+--------------------------------------------------------------------------------------+\n| 0         | Device Name: Intel(R) Arc(TM) A770 Graphics                                          |\n|           | Vendor Name: Intel(R) Corporation                                                    |\n|           | SOC UUID: 00000000-0000-0003-0000-000856a08086                                       |\n|           | PCI BDF Address: 0000:03:00.0                                                        |\n|           | DRM Device: /dev/dri/card0                                                           |\n|           | Function Type: physical                                                              |\n+-----------+--------------------------------------------------------------------------------------+\n| 1         | Device Name: Intel(R) Arc(TM) A770 Graphics                                          |\n|           | Vendor Name: Intel(R) Corporation                                                    |\n|           | SOC UUID: 00000000-0000-0007-0000-000856a08086                                       |\n|           | PCI BDF Address: 0000:07:00.0                                                        |\n|           | DRM Device: /dev/dri/card1                                                           |\n|           | Function Type: physical                                                              |\n+-----------+--------------------------------------------------------------------------------------+\nGPU0 Memory size=16G\nGPU1 Memory size=16G\n-----------------------------------------------------------------\n03:00.0 VGA compatible controller: Intel Corporation DG2 [Arc A770] (rev 08) (prog-if 00 [VGA controller])\n\tSubsystem: Intel Corporation Device 1020\n\tFlags: bus master, fast devsel, latency 0, IRQ 187, IOMMU group 19\n\tMemory at 75000000 (64-bit, non-prefetchable) [size=16M]\n\tMemory at 4800000000 (64-bit, prefetchable) [size=16G]\n\tExpansion ROM at 76000000 [disabled] [size=2M]\n\tCapabilities: [40] Vendor Specific Information: Len=0c <?>\n\tCapabilities: [70] Express Endpoint, IntMsgNum 0\n\tCapabilities: [ac] MSI: Enable+ Count=1/1 Maskable+ 64bit+\n--\n07:00.0 VGA compatible controller: Intel Corporation DG2 [Arc A770] (rev 08) (prog-if 00 [VGA controller])\n\tSubsystem: Device 172f:3937\n\tFlags: bus master, fast devsel, latency 0, IRQ 190, IOMMU group 24\n\tMemory at 73000000 (64-bit, non-prefetchable) [size=16M]\n\tMemory at 4000000000 (64-bit, prefetchable) [size=16G]\n\tExpansion ROM at 74000000 [disabled] [size=2M]\n\tCapabilities: [40] Vendor Specific Information: Len=0c <?>\n\tCapabilities: [70] Express Endpoint, IntMsgNum 0\n\tCapabilities: [ac] MSI: Enable+ Count=1/1 Maskable+ 64bit+\n-----------------------------------------------------------------\n```\n\ncontainer env\n```\nroot@ailab:/llm# bash env-check.sh\n-----------------------------------------------------------------\nPYTHON_VERSION=3.11.12\n-----------------------------------------------------------------\n/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\ntransformers=4.36.2\n-----------------------------------------------------------------\ntorch=2.2.0+cu121\n-----------------------------------------------------------------\nipex-llm Version: 2.3.0b20250423\n-----------------------------------------------------------------\nIPEX is not installed.\n-----------------------------------------------------------------\nCPU Information:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               12\nOn-line CPU(s) list:                  0-11\nVendor ID:                            GenuineIntel\nModel name:                           12th Gen Intel(R) Core(TM) i5-12500\nCPU family:                           6\nModel:                                151\nThread(s) per core:                   2\nCore(s) per socket:                   6\nSocket(s):                            1\nStepping:                             5\nCPU max MHz:                          4600.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             5990.40\n-----------------------------------------------------------------\nTotal CPU Memory: 46.7931 GB\nMemory Type: sudo: dmidecode: command not found\n-----------------------------------------------------------------\nOperating System:\nUbuntu 22.04.5 LTS \\n \\l\n\n-----------------------------------------------------------------\nLinux ailab 6.11.0-24-generic #24-Ubuntu SMP PREEMPT_DYNAMIC Fri Mar 14 18:13:56 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux\n-----------------------------------------------------------------\nenv-check.sh: line 148: xpu-smi: command not found\n-----------------------------------------------------------------\nenv-check.sh: line 154: clinfo: command not found\n-----------------------------------------------------------------\nDriver related package version:\nii  intel-level-zero-gpu                             1.6.32224.5                             amd64        Intel(R) Graphics Compute Runtime for oneAPI Level Zero.\nii  intel-level-zero-gpu-legacy1                     1.3.30872.22                            amd64        Intel(R) Graphics Compute Runtime for oneAPI Level Zero.\nii  level-zero-devel                                 1.20.2                                  amd64        oneAPI Level Zero\n-----------------------------------------------------------------\nigpu not detected\n-----------------------------------------------------------------\nxpu-smi is not installed. Please install xpu-smi according to README.md\n```\n",
      "state": "closed",
      "author": "kirel",
      "author_type": "User",
      "created_at": "2025-04-24T13:10:50Z",
      "updated_at": "2025-05-11T22:19:17Z",
      "closed_at": "2025-05-11T22:19:16Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13106/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13106",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13106",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:37.148019",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @kirel, what's your ipex-llm ollama version? We have two quick solutions:\n\n1. Install our latest version of ollama via `pip install --pre --upgrade ipex-llm[cpp]`, then run `export IPEX_LLM_NUM_CTX=xxx`\n2. Use ollama `Modelfile` to modify the `num_ctx` parameter. See https://github.com/ollama/oll",
          "created_at": "2025-04-25T02:24:24Z"
        },
        {
          "author": "kirel",
          "body": "I'm building the container and as I built yesterday I got\n\nipex-llm Version: 2.3.0b20250423\n\nAnd ollama --version reports 0.0.0\n\n(See Container env summary I included).\n\nI am passing IPEX_LLM_NUM_CTX to the container as env var but it doesn't seem to have an effect. \n\n```\ndaniel@ailab:~$ sudo docker",
          "created_at": "2025-04-25T05:59:30Z"
        },
        {
          "author": "kirel",
          "body": "Some more demonstration:\n\nI sent \"ipex [lorem ipsum of varying length] what's the first word I sent\n\n1. Short context\n![image](https://github.com/user-attachments/assets/84b32288-5e0e-4295-861a-898217a96ae3)\nWorks\n\n2. Medium context about 500 tokens\n![image](https://github.com/user-attachments/asset",
          "created_at": "2025-04-25T06:52:23Z"
        },
        {
          "author": "kirel",
          "body": "I have tested some more and the problem seems to be `IPEX_LLM_NUM_CTX=\"32768\"`. When I vary that number I get an actual context size of 1/4 of that. Something is weird there. I've removed that env var and set the context through Ollama directly and get the expected behavior. I think there is a bug w",
          "created_at": "2025-05-05T15:00:03Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @kirel, sry for the later reply. We have switched it to `OLLAMA_NUM_CTX=32768`.",
          "created_at": "2025-05-07T01:08:29Z"
        }
      ]
    },
    {
      "issue_number": 12987,
      "title": "ipex-llm[xpu] is not compatible with ipex-llm[cpp]",
      "body": "ipex-llm[xpu] require torch==2.1.0a0 and intel-extension-for-pytorch==2.1.10+xpu, but ipex-llm[cpp] require different torch(2.2.0) which is not a intel special version. There are some issuess about this:\n\n1. ipex-llm[cpp] install will even cause cudnn to be downloaded automatically which is completely ridiculous: the reason of why I want to try something about ipex is I dont have NVIDIA device in my environment, but the ipex-llm[cpp] still download tons of cuda and cudnn libraries into my conda env, which are totally useless.\n2. ipex-llm[cpp] and ipex-llm[xpu] have many common dependencies, Why can't they be combined together? Why release the separate portable ollama and lamma.cpp package? Let me tell you This does not make using ipex easier, but more complicated and confusing, we want ONE pip package just like ipex-llm[xpu], and contains all features!\n3. intel-extension-for-pytorch now update to 2.6.10 but the latest ipex-llm still depends on ipex 2.1.10, so there is a dependency chain: if i want to use ipex-llm, i must use ipex 2.1.10, and then must use the pytorch 2.1.0a0, god please!! its too old !! look at the ipex please, it keep up the pace of offical pytorch, we hope the ipex-llm also do the same.",
      "state": "open",
      "author": "jiafeng5513",
      "author_type": "User",
      "created_at": "2025-03-21T03:19:13Z",
      "updated_at": "2025-05-09T02:45:25Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12987/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12987",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12987",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:37.327290",
      "comments": [
        {
          "author": "jiafeng5513",
          "body": "4. the ipex-llm[xpu] need different version of oneapi components with ipex-llm[cpp] and ollama, its hard to install ipex-llm[xpu] and ollama together. I'm really curious why completely incompatible dependencies are needed, as these packages(ipex-llm[xpu], Ollama portable and ipex-llm[cpp]) are publi",
          "created_at": "2025-03-21T08:04:19Z"
        },
        {
          "author": "qiuxin2012",
          "body": "So we have portable zip now, for llamacpp and ollama users. Most of they don't need pytorch. See https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/llamacpp_portable_zip_gpu_quickstart.md and https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_portable_zip_quick",
          "created_at": "2025-03-24T02:04:28Z"
        },
        {
          "author": "jiafeng5513",
          "body": "1. 首先,看到有人回复我的疑问我非常感动,你们一定非常忙.\n\n2. 是这样的,我的疑问主要是,这个仓库中发布的所有制品,为什么不能有相同版本的依赖?,这个问题不是portable zip 能回答的,甚至也不是提供封装好的docker image能回答的:那是一种绕过问题的方案,但是没有解决问题的根源.\n\n3. 换句话讲我提到的这些疑问或许并不影响大多数人使用,但是这些问题的存在让人觉得ipex,ipex-llm这些制品并不可靠:他们在我看来都是intel的产品,来自intel不同的team,他们甚至都不能协调好发布出来的产品的依赖版本,同一个仓库里面发布的两个pip package甚至不能同",
          "created_at": "2025-05-08T08:50:10Z"
        },
        {
          "author": "Oscilloscope98",
          "body": "Hi @jiafeng5513,\n\nThank you for the feedback. You may refer to [this guide](https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/install_pytorch26_gpu.md) regarding IPEX-LLM support  on Intel GPU with PyTorch 2.6 :) ",
          "created_at": "2025-05-09T02:43:51Z"
        }
      ]
    },
    {
      "issue_number": 13130,
      "title": "Llama.cpp portable fails to initialise with context sizes above 22528 (24 x 1024).",
      "body": "**Describe the bug**\nThe portable nightly build of Llama.cpp fails to initialise when setting context size above 22528. This is using -sm layer across 3 Arc GPUs. There is more than enough VRAM available.\n\n**How to reproduce**\nSteps to reproduce the error:\n1. Download a copy of Qwen3-30B-A3B-Q4_K_L.gguf.\n2. Download the latest nightly build of Llama.cpp from the releases section.\n3. Start the server with:\n`ONEAPI_DEVICE_SELECTOR=level_zero:0,1,2 ZES_ENABLE_SYSMAN=1 SYCL_CACHE_PERSISTENT=1 SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1 ./llama-server -c 22528 -ngl 999 -m /home/llm/models/Qwen_Qwen3-30B-A3B-Q4_K_L.gguf --host 0.0.0.0 --port 8001 -sm layer --jinja`\n5. If the context is set above 22628, the engine crashes with the following error:\n`llama_kv_cache_init: kv_size = 23552, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 48, can_shift = 1\nllama_kv_cache_init:      SYCL0 KV buffer size =   782.00 MiB\nllama_kv_cache_init:      SYCL1 KV buffer size =   736.00 MiB\nllama_kv_cache_init:      SYCL2 KV buffer size =   690.00 MiB\nllama_init_from_model: KV self size  = 2208.00 MiB, K (f16): 1104.00 MiB, V (f16): 1104.00 MiB\nllama_init_from_model:  SYCL_Host  output buffer size =     0.58 MiB\nllama_init_from_model: pipeline parallelism enabled (n_copies=4)\nggml_backend_sycl_buffer_type_alloc_buffer: can't allocate 4334944256 Bytes of memory on device\nggml_gallocr_reserve_n: failed to allocate SYCL2 buffer of size 4334944256\nggml_backend_sycl_buffer_type_alloc_buffer: can't allocate 6065967104 Bytes of memory on device\nggml_gallocr_reserve_n: failed to allocate SYCL0 buffer of size 6065967104\nggml_backend_sycl_buffer_type_alloc_buffer: can't allocate 5626382848 Bytes of memory on device\nggml_gallocr_reserve_n: failed to allocate SYCL1 buffer of size 5626382848\nllama_init_from_model: failed to allocate compute buffers\ncommon_init_from_params: failed to create context with model '/home/llm/models/Qwen_Qwen3-30B-A3B-Q4_K_L.gguf'\nterminate called without an active exception\n./llama-server: line 2: 142366 Aborted                 (core dumped) LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$(cd \"$(dirname \"$0\")\";pwd) $(cd \"$(dirname \"$0\")\";pwd)/llama-server-bin \"$@\"`\n\nBy comparison, setting the context to or below 22528, the following log for KV cache information is generated:\n`llama_kv_cache_init: kv_size = 22528, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 48, can_shift = 1\nllama_kv_cache_init:      SYCL0 KV buffer size =   748.00 MiB\nllama_kv_cache_init:      SYCL1 KV buffer size =   704.00 MiB\nllama_kv_cache_init:      SYCL2 KV buffer size =   660.00 MiB\nllama_init_from_model: KV self size  = 2112.00 MiB, K (f16): 1056.00 MiB, V (f16): 1056.00 MiB\nllama_init_from_model:  SYCL_Host  output buffer size =     0.58 MiB\nllama_init_from_model: pipeline parallelism enabled (n_copies=4)\nllama_init_from_model:      SYCL0 compute buffer size =  2016.06 MiB\nllama_init_from_model:      SYCL1 compute buffer size =  2016.06 MiB\nllama_init_from_model:      SYCL2 compute buffer size =  4070.12 MiB\nllama_init_from_model:  SYCL_Host compute buffer size =  1440.19 MiB\nllama_init_from_model: graph nodes  = 3270 (with bs=4096), 2646 (with bs=1)\nllama_init_from_model: graph splits = 4`\n\nAs you can see, the difference in compute buffer size is hardly an issue when comparing 22528 context to 24576, however it still fails to initialise.\n",
      "state": "open",
      "author": "HumerousGorgon",
      "author_type": "User",
      "created_at": "2025-05-03T02:33:28Z",
      "updated_at": "2025-05-07T05:02:33Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13130/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13130",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13130",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:37.565594",
      "comments": [
        {
          "author": "kirel",
          "body": "If I'm reading https://github.com/ggml-org/llama.cpp/pull/10026 correctly you could try `-sm row` - KV maybe isn't split by default and with `-sm layer`?",
          "created_at": "2025-05-05T15:17:06Z"
        },
        {
          "author": "HumerousGorgon",
          "body": "SYCL backend doesn't support `-sm row` unfortuantely :(\nMaybe in the future!",
          "created_at": "2025-05-05T15:23:34Z"
        },
        {
          "author": "qiuxin2012",
          "body": "sycl buffer size is limited to 4GB. kv cache is too large in your case.",
          "created_at": "2025-05-06T02:27:04Z"
        },
        {
          "author": "HumerousGorgon",
          "body": "Had a feeling you might say that. I remember this being a limitation.\nHere’s a question though: on mainline llama.cpp I can assign buffer sizes larger than 4GB, it just knows when to split it up correctly. Either that, or we use KV Cache quantisation to shrink the size. Are either of these a possibi",
          "created_at": "2025-05-06T02:29:50Z"
        },
        {
          "author": "HumerousGorgon",
          "body": "> sycl buffer size is limited to 4GB. kv cache is too large in your case.\n\nI've read that it's possible to compile oneAPI applications with the patch that enables above 4GB allocations.\nIs there a way we can download a docker image of the environment the devs are using for building the llama.cpp ins",
          "created_at": "2025-05-07T05:02:32Z"
        }
      ]
    },
    {
      "issue_number": 13126,
      "title": "Instructions for Ollama Installation with IPEX on Multi-GPU Fail to Work with Arc GPU",
      "body": "**Describe the bug**\n\nI am following the documentation from:\n1. https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_portable_zip_quickstart.md\n\n\n**How to reproduce**\nSteps to reproduce the error:\n1. Download the Ollama portable zip.\n2. Attempt running as in the instructions.\n\n**Screenshots**\nThe following instruction is missing what needs to be ran to find the supported SYCL devices:\n\n![Image](https://github.com/user-attachments/assets/5dc4ef05-b3d4-4919-9665-afa7a94873db)\n\n**Environment information**\n\nRelevant GPU information:\n```\nPython 3.13.0\n-----------------------------------------------------------------\nTransformers is not installed.\n-----------------------------------------------------------------\nPyTorch is not installed.\n-----------------------------------------------------------------\n'pip' is not recognized as an internal or external command,\noperable program or batch file.\nipex-llm is not installed\n-----------------------------------------------------------------\nIPEX is not installed properly.\n-----------------------------------------------------------------\nTotal Memory: 31.927 GB\n\nChip 0 Memory: 8 GB | Speed: 2133 MHz\nChip 1 Memory: 8 GB | Speed: 2133 MHz\nChip 2 Memory: 8 GB | Speed: 2133 MHz\nChip 3 Memory: 8 GB | Speed: 2133 MHz\n-----------------------------------------------------------------\nCPU Manufacturer: AuthenticAMD\nCPU MaxClockSpeed: 3801\nCPU Name: AMD Ryzen 9 3900X 12-Core Processor\nCPU NumberOfCores: 12\nCPU NumberOfLogicalProcessors: 24\n-----------------------------------------------------------------\nGPU 0: Intel(R) Arc(TM) A770 Graphics    Driver Version: 32.0.101.6737\n-----------------------------------------------------------------\n-----------------------------------------------------------------\n...\nOS Name:                       Microsoft Windows 11 Pro\n...\n'xpu-smi' is not recognized as an internal or external command,\noperable program or batch file.\nxpu-smi is not installed properly.\n```\n\n**Additional context**\n\nLogs from running the `.\\start-ollama.bat` (there is no option to select the Intel Arc GPU):\n\n```\ntime=2025-04-30T17:27:50.331+02:00 level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\ntime=2025-04-30T17:27:50.331+02:00 level=INFO source=gpu_windows.go:167 msg=packages count=1\ntime=2025-04-30T17:27:50.331+02:00 level=INFO source=gpu_windows.go:214 msg=\"\" package=0 cores=12 efficiency=0 threads=24\ntime=2025-04-30T17:27:50.497+02:00 level=INFO source=gpu.go:319 msg=\"detected OS VRAM overhead\" id=GPU-8876a550-ac1e-a380-98d9-a8ce5b9eadc8 library=cuda compute=7.5 driver=12.9 name=\"NVIDIA GeForce GTX 1660 SUPER\" overhead=\"831.5 MiB\"\ntime=2025-04-30T17:27:50.500+02:00 level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-8876a550-ac1e-a380-98d9-a8ce5b9eadc8 library=cuda variant=v12 compute=7.5 driver=12.9 name=\"NVIDIA GeForce GTX 1660 SUPER\" total=\"6.0 GiB\" available=\"5.0 GiB\"\n```\n",
      "state": "open",
      "author": "Kaszanas",
      "author_type": "User",
      "created_at": "2025-04-30T15:39:28Z",
      "updated_at": "2025-05-07T01:19:18Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13126/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13126",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13126",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:37.835750",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "The `Find x sycl devices` will show when you are running a model, you can try to run a model first.",
          "created_at": "2025-05-06T02:30:14Z"
        },
        {
          "author": "Kaszanas",
          "body": "Thank you very much for responding @qiuxin2012 \n\nWhen I run it, it shows that the selected GPU is CUDA 1660 Super so I suspect the model should work. I'll try that to make sure.\n\nMy guess was it wouldn't be using the Arc GPU.",
          "created_at": "2025-05-06T11:27:53Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @Kaszanas, you will not see any intel gpu info until you loading a model.\n\n",
          "created_at": "2025-05-07T01:19:18Z"
        }
      ]
    },
    {
      "issue_number": 13105,
      "title": "Intel iGPU is not used",
      "body": "I'm using Ollama running ollama3, deepseek-r1:1.5b/7b/14b, and every one of these only calls the CPU and doesn't use the GPU. i've watched the task manager and noticed that the GPU isn't being used at all.\nI followed this tutorial.\n[https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_portable_zip_quickstart.md](url)\n\n```\n2025/04/24 11:13:00 routes.go:1231: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY:localhost,127.0.0.1 OLLAMA_CONTEXT_LENGTH:2048 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:10m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\\\Users\\\\zhangdh17\\\\.ollama\\\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]\"\ntime=2025-04-24T11:13:00.704+08:00 level=INFO source=images.go:458 msg=\"total blobs: 9\"\ntime=2025-04-24T11:13:00.706+08:00 level=INFO source=images.go:465 msg=\"total unused blobs removed: 0\"\ntime=2025-04-24T11:13:00.708+08:00 level=INFO source=routes.go:1298 msg=\"Listening on 127.0.0.1:11434 (version 0.6.5)\"\ntime=2025-04-24T11:13:00.708+08:00 level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\ntime=2025-04-24T11:13:00.709+08:00 level=INFO source=gpu_windows.go:167 msg=packages count=1\ntime=2025-04-24T11:13:00.709+08:00 level=INFO source=gpu_windows.go:183 msg=\"efficiency cores detected\" maxEfficiencyClass=1\ntime=2025-04-24T11:13:00.709+08:00 level=INFO source=gpu_windows.go:214 msg=\"\" package=0 cores=16 efficiency=10 threads=22\ntime=2025-04-24T11:13:00.748+08:00 level=INFO source=gpu.go:377 msg=\"no compatible GPUs were discovered\"\ntime=2025-04-24T11:13:00.748+08:00 level=INFO source=types.go:130 msg=\"inference compute\" id=0 library=cpu variant=\"\" compute=\"\" driver=0.0 name=\"\" total=\"31.5 GiB\" available=\"17.1 GiB\"\n[GIN] 2025/04/24 - 11:13:20 | 200 |            0s |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/04/24 - 11:13:20 | 200 |     49.7927ms |       127.0.0.1 | POST     \"/api/show\"\ntime=2025-04-24T11:13:20.815+08:00 level=INFO source=server.go:105 msg=\"system memory\" total=\"31.5 GiB\" free=\"17.2 GiB\" free_swap=\"14.2 GiB\"\ntime=2025-04-24T11:13:20.815+08:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=qwen2.vision.block_count default=0\ntime=2025-04-24T11:13:20.816+08:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=qwen2.attention.key_length default=128\ntime=2025-04-24T11:13:20.817+08:00 level=WARN source=ggml.go:152 msg=\"key not found\" key=qwen2.attention.value_length default=128\ntime=2025-04-24T11:13:20.818+08:00 level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=29 layers.offload=0 layers.split=\"\" memory.available=\"[17.2 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"1.5 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"224.0 MiB\" memory.required.allocations=\"[1.5 GiB]\" memory.weights.total=\"934.7 MiB\" memory.weights.repeating=\"752.1 MiB\" memory.weights.nonrepeating=\"182.6 MiB\" memory.graph.full=\"299.8 MiB\" memory.graph.partial=\"482.3 MiB\"\nllama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from C:\\Users\\zhangdh17\\.ollama\\models\\blobs\\sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B\nllama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen\nllama_model_loader: - kv   4:                         general.size_label str              = 1.5B\nllama_model_loader: - kv   5:                          qwen2.block_count u32              = 28\nllama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072\nllama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536\nllama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960\nllama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12\nllama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2\nllama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  13:                          general.file_type u32              = 15\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646\nllama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\nllama_model_loader: - kv  25:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  141 tensors\nllama_model_loader: - type q4_K:  169 tensors\nllama_model_loader: - type q6_K:   29 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 1.04 GiB (5.00 BPW)\nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: special tokens cache size = 22\nload: token to piece cache size = 0.9310 MB\nprint_info: arch             = qwen2\nprint_info: vocab_only       = 1\nprint_info: model type       = ?B\nprint_info: model params     = 1.78 B\nprint_info: general.name     = DeepSeek R1 Distill Qwen 1.5B\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'\nprint_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'\nprint_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'\nprint_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nllama_model_load: vocab only - skipping tensors\ntime=2025-04-24T11:13:21.205+08:00 level=INFO source=server.go:405 msg=\"starting llama server\" cmd=\"C:\\\\Users\\\\zhangdh17\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\ollama.exe runner --model C:\\\\Users\\\\zhangdh17\\\\.ollama\\\\models\\\\blobs\\\\sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc --ctx-size 8192 --batch-size 512 --threads 6 --no-mmap --parallel 4 --port 58399\"\ntime=2025-04-24T11:13:21.237+08:00 level=INFO source=sched.go:451 msg=\"loaded runners\" count=1\ntime=2025-04-24T11:13:21.237+08:00 level=INFO source=server.go:580 msg=\"waiting for llama runner to start responding\"\ntime=2025-04-24T11:13:21.239+08:00 level=INFO source=server.go:614 msg=\"waiting for server to become available\" status=\"llm server error\"\ntime=2025-04-24T11:13:21.355+08:00 level=INFO source=runner.go:853 msg=\"starting go runner\"\nload_backend: loaded CPU backend from C:\\Users\\zhangdh17\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\ggml-cpu-alderlake.dll\ntime=2025-04-24T11:13:21.804+08:00 level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(clang)\ntime=2025-04-24T11:13:21.808+08:00 level=INFO source=runner.go:913 msg=\"Server listening on 127.0.0.1:58399\"\nllama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from C:\\Users\\zhangdh17\\.ollama\\models\\blobs\\sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B\nllama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen\nllama_model_loader: - kv   4:                         general.size_label str              = 1.5B\nllama_model_loader: - kv   5:                          qwen2.block_count u32              = 28\nllama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072\nllama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536\nllama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960\nllama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12\nllama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2\nllama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  13:                          general.file_type u32              = 15\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646\nllama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\nllama_model_loader: - kv  25:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  141 tensors\nllama_model_loader: - type q4_K:  169 tensors\nllama_model_loader: - type q6_K:   29 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 1.04 GiB (5.00 BPW)\ntime=2025-04-24T11:13:21.998+08:00 level=INFO source=server.go:614 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: special tokens cache size = 22\nload: token to piece cache size = 0.9310 MB\nprint_info: arch             = qwen2\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 1536\nprint_info: n_layer          = 28\nprint_info: n_head           = 12\nprint_info: n_head_kv        = 2\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 6\nprint_info: n_embd_k_gqa     = 256\nprint_info: n_embd_v_gqa     = 256\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: n_ff             = 8960\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 10000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 1.5B\nprint_info: model params     = 1.78 B\nprint_info: general.name     = DeepSeek R1 Distill Qwen 1.5B\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'\nprint_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'\nprint_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'\nprint_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = false)\nload_tensors:          CPU model buffer size =  1059.89 MiB\nllama_init_from_model: n_seq_max     = 4\nllama_init_from_model: n_ctx         = 8192\nllama_init_from_model: n_ctx_per_seq = 2048\nllama_init_from_model: n_batch       = 2048\nllama_init_from_model: n_ubatch      = 512\nllama_init_from_model: flash_attn    = 0\nllama_init_from_model: freq_base     = 10000.0\nllama_init_from_model: freq_scale    = 1\nllama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1\nllama_kv_cache_init:        CPU KV buffer size =   224.00 MiB\nllama_init_from_model: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB\nllama_init_from_model:        CPU  output buffer size =     2.34 MiB\nllama_init_from_model:        CPU compute buffer size =   302.75 MiB\nllama_init_from_model: graph nodes  = 986\nllama_init_from_model: graph splits = 1\ntime=2025-04-24T11:13:22.753+08:00 level=INFO source=server.go:619 msg=\"llama runner started in 1.52 seconds\"\n[GIN] 2025/04/24 - 11:13:22 | 200 |    2.0106736s |       127.0.0.1 | POST     \"/api/generate\"\n```\n\nOS:Windows 11\nCPU:Core ULTRA 9 185H\nGPU:Intel Arc Graphics(integrated)\n\n**Additional context**\nAt first I followed the tutorial [https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/install_windows_gpu.md](url) and configured ipex-llm[xpu] and tested it successfully. Then I followed the tutorial [https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/llamacpp_portable_zip_gpu_quickstart.md](url) in llama.cpp and created a new virtual environment to test llama3, where I found that only the cpu is used, but not the gpu. Then I found Ollama Portable Zip [https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_portable_zip_quickstart.md](url) and installed it, after installing and running it I found that the gpu was being called and the gpu information started to be displayed in the ollama serve. I went back and tested ollama.cpp and found that the gpu was also being called. When the gpu can be called, no matter if it is llama3,deepseek-r1:1.5b/7b/14b, the gpu is called 100%. But later on, the gpu starts not being used again, but only the gpu is being used, I have tried many ways to fix the problem and it really bothers me.\n\n",
      "state": "open",
      "author": "PharahAmari",
      "author_type": "User",
      "created_at": "2025-04-24T03:20:41Z",
      "updated_at": "2025-05-07T01:11:39Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13105/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13105",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13105",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:38.094025",
      "comments": [
        {
          "author": "sgwhat",
          "body": "See `time=2025-04-24T11:13:00.708+08:00 level=INFO source=routes.go:1298 msg=\"Listening on 127.0.0.1:11434 (version 0.6.5)\"`, I believe you are running the community edition of Ollama.",
          "created_at": "2025-04-25T02:31:01Z"
        },
        {
          "author": "PharahAmari",
          "body": "> See `time=2025-04-24T11:13:00.708+08:00 level=INFO source=routes.go:1298 msg=\"Listening on 127.0.0.1:11434 (version 0.6.5)\"`, I believe you are running the community edition of Ollama.\n\nExactly. But does it matter?",
          "created_at": "2025-04-25T02:37:15Z"
        },
        {
          "author": "FilipLaurentiu",
          "body": "Same issue, any updates ?\n` level=INFO source=gpu.go:377 msg=\"no compatible GPUs were discovered\"`",
          "created_at": "2025-05-01T10:17:55Z"
        },
        {
          "author": "sgwhat",
          "body": "> Same issue, any updates ? ` level=INFO source=gpu.go:377 msg=\"no compatible GPUs were discovered\"`\n\nHi it's a normal behaviour. You will see the gpu usage after loading a model.",
          "created_at": "2025-05-07T01:11:38Z"
        }
      ]
    },
    {
      "issue_number": 13124,
      "title": "ImportError: cannot import name 'LoadLoraAdapterRequest'",
      "body": "I just did a fresh vllm docker build (via https://github.com/intel/ipex-llm/blob/main/docker/llm/serving/xpu/docker/README.md) after 51b41faad76da6757195670bf9e5f946cac6460c and `start-vllm-service.sh` crashes with\n```\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/entrypoints/openai/api_server.py\", line 43, in <module>\n    from vllm.entrypoints.openai.protocol import (ChatCompletionRequest,\nImportError: cannot import name 'LoadLoraAdapterRequest' from 'vllm.entrypoints.openai.protocol' (/usr/local/lib/python3.11/dist-packages/vllm-0.8.3+ipexllm.xpu-py3.11-linux-x86_64.egg/vllm/entrypoints/openai/protocol.py)\n```",
      "state": "closed",
      "author": "kirel",
      "author_type": "User",
      "created_at": "2025-04-30T12:25:14Z",
      "updated_at": "2025-05-06T08:51:57Z",
      "closed_at": "2025-05-06T07:59:10Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13124/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "liu-shaojun"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13124",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13124",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:38.271382",
      "comments": [
        {
          "author": "kirel",
          "body": "Seems to be a capitalization error: https://github.com/vllm-project/vllm/blob/v0.8.3/vllm/entrypoints/openai/protocol.py#L1528\nShould be `LoadLoRAAdapterRequest` not `LoadLoraAdapterRequest`",
          "created_at": "2025-04-30T12:53:13Z"
        },
        {
          "author": "kirel",
          "body": "I'm a bit surprised - the change is clearly here: https://github.com/intel/ipex-llm/pull/13118/files#diff-cf900b93ab0ee51f6cf8a203409af65ae14c4f42ad7c6636926e8cffa6c6f7d4R60 for `xpu/.../api_server.py`\n\nBut it seems to be missing for `cpu`. I've thrown together https://github.com/intel/ipex-llm/pull",
          "created_at": "2025-04-30T13:18:43Z"
        },
        {
          "author": "kirel",
          "body": "Aw man. In the Dockerfile you clone the repo instead of using the local context in https://github.com/intel/ipex-llm/blob/main/docker/llm/serving/xpu/docker/Dockerfile#L131 - I can build from a branch as much as I want - I'm still getting the code without a fix built. Is that a good practice?",
          "created_at": "2025-04-30T16:00:21Z"
        },
        {
          "author": "liu-shaojun",
          "body": "Hi @kirel \n\nThanks for your findings — let me clarify a few things.\n\nFirst, regarding the `xpu.api_server` you mentioned: in our Dockerfile, the one we actually use is installed via `pip` from the **nightly ipex-llm PyPI package**, not built from a `git clone` of ipex-llm. We only clone the ipex-llm",
          "created_at": "2025-05-06T03:19:53Z"
        },
        {
          "author": "kirel",
          "body": "> First, regarding the xpu.api_server you mentioned: in our Dockerfile, the one we actually use is installed via pip from the nightly ipex-llm PyPI package, not built from a git clone of ipex-llm. We only clone the ipex-llm repo to copy certain necessary folders — not to build the package.\n\nWhy do t",
          "created_at": "2025-05-06T07:59:10Z"
        }
      ]
    },
    {
      "issue_number": 13107,
      "title": "Ollama failed to run deepseek-coder-v2,Error: unable to load model",
      "body": "Ollama failed to run deepseek-coder-v2,Error: unable to load model.\nIs there any way to solve it?\n\nollama run deepseek-coder-v2:16b-lite-instruct-q4_0\nError: unable to load model: C:\\Users\\Admin\\.ollama\\models\\blobs\\sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046\n\ntime=2025-04-24T22:20:29.220+08:00 level=INFO source=server.go:106 msg=\"system memory\" total=\"31.8 GiB\" free=\"21.6 GiB\" free_swap=\"25.1 GiB\"\ntime=2025-04-24T22:20:29.220+08:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=deepseek2.vision.block_count default=0\ntime=2025-04-24T22:20:29.220+08:00 level=INFO source=server.go:139 msg=offload library=cpu layers.requested=-1 layers.model=28 layers.offload=0 layers.split=\"\" memory.available=\"[21.6 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"10.7 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"2.1 GiB\" memory.required.allocations=\"[10.7 GiB]\" memory.weights.total=\"8.0 GiB\" memory.weights.repeating=\"8.0 GiB\" memory.weights.nonrepeating=\"164.1 MiB\" memory.graph.full=\"296.0 MiB\" memory.graph.partial=\"391.4 MiB\"\nllama_model_load_from_file_impl: using device SYCL0 (Intel(R) Arc(TM) B580 Graphics) - 10630 MiB free\nllama_model_load_from_file_impl: using device SYCL0 (Intel(R) Arc(TM) B580 Graphics) - 10630 MiB free\nllama_model_load_from_file_impl: using device SYCL0 (Intel(R) Arc(TM) B580 Graphics) - 10630 MiB free\nllama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from C:\\Users\\Admin\\.ollama\\models\\blobs\\sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = deepseek2\nllama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct\nllama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27\nllama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840\nllama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048\nllama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944\nllama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16\nllama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16\nllama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6\nllama_model_loader: - kv  11:                          general.file_type u32              = 2\nllama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1\nllama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400\nllama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512\nllama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192\nllama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128\nllama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408\nllama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64\nllama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2\nllama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000\nllama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64\nllama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn\nllama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000\nllama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096\nllama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700\nllama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm\nllama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [\"Ġ Ġ\", \"Ġ t\", \"Ġ a\", \"i n\", \"h e...\nllama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000\nllama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001\nllama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001\nllama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\nllama_model_loader: - kv  37:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  108 tensors\nllama_model_loader: - type q4_0:  268 tensors\nllama_model_loader: - type q6_K:    1 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_0\nprint_info: file size   = 8.29 GiB (4.53 BPW)\nFailed to process regex: '\\s?[A-Za-z碌脌-脰脴-枚酶-坪萍-瓶莿-蕮蕰-石桶-统投头突-徒涂螁螆-螉螌螏-巍危-系戏-襾見-辕员-諙醾?醿呩帬-釓滇徃-釓结矏-岵横步-岵酷磤-岽但-岬丰倒-岫氠竴-峒曖紭-峒濁紶-峤呩綀-峤嶀綈-峤椺綑峤涐綕峤?峤结線-峋瘁径-峋坚揪峥?峥勧繂-峥屷繍-峥撫繓-峥涐繝-峥坎-峥瘁慷-峥尖剛鈩団剨-鈩撯剷鈩?鈩濃劋鈩︹劏鈩?鈩劘-鈩粹劰鈩?鈩库厖-鈪夆厧鈫冣唲獍€-獗烩本-獬も倡-獬巢獬酬檧-隀殌-隁涥湤-隄澅-隇囮瀷-隇庩-戤匡瑎-铿嗭瑩-铿楋肌-锛猴絹-锝氿悙€-饜憦饜挵-饜摀饜摌-饜摶饜瞼-饜膊饜硛-饜巢饝-饝馂-馂]+'\nRegex error: regex_error(error_range): The expression contained an invalid character range, such as [b-a] in most encodings.\nllama_model_load: error loading model: error loading model vocabulary: Failed to process regex\nllama_model_load_from_file_impl: failed to load model\ntime=2025-04-24T22:20:29.344+08:00 level=INFO source=sched.go:429 msg=\"NewLlamaServer failed\" model=C:\\Users\\Admin\\.ollama\\models\\blobs\\sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 error=\"unable to load model: C:\\\\Users\\\\Admin\\\\.ollama\\\\models\\\\blobs\\\\sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046\"",
      "state": "open",
      "author": "weryswang",
      "author_type": "User",
      "created_at": "2025-04-24T14:28:26Z",
      "updated_at": "2025-05-06T04:37:46Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13107/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "qiuxin2012"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13107",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13107",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:38.512592",
      "comments": [
        {
          "author": "weryswang",
          "body": " The version used is ollama-ipex-llm-2.3.0b20250415-win.",
          "created_at": "2025-04-24T14:34:24Z"
        },
        {
          "author": "SiewwenL",
          "body": "Tried on iGPU and it is working. \n\n![Image](https://github.com/user-attachments/assets/5c8a96af-90b2-4c91-b3f5-f6a0f6b0bae2)\n",
          "created_at": "2025-05-06T04:37:45Z"
        }
      ]
    },
    {
      "issue_number": 13129,
      "title": "Gemma3 model doesn't work",
      "body": "Trying to use either `gemma3:4b` or `gemma3:12b` doesn't work, both in docker container, or the precompiled ollama\n\n**How to reproduce**\npull and run the models\n\n```\ntime=2025-05-02T20:18:08.678+08:00 level=INFO source=server.go:107 msg=\"system memory\" total=\"62.3 GiB\" free=\"52.9 GiB\" free_swap=\"1.6 GiB\"\ntime=2025-05-02T20:18:08.679+08:00 level=INFO source=server.go:154 msg=offload library=cpu layers.requested=-1 layers.model=35 layers.offload=0 layers.split=\"\" memory.available=\"[52.9 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"5.8 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"1.1 GiB\" memory.required.allocations=\"[5.8 GiB]\" memory.weights.total=\"1.8 GiB\" memory.weights.repeating=\"1.8 GiB\" memory.weights.nonrepeating=\"525.0 MiB\" memory.graph.full=\"517.0 MiB\" memory.graph.partial=\"1.0 GiB\" projector.weights=\"795.9 MiB\" projector.graph=\"1.0 GiB\"\ntime=2025-05-02T20:18:08.747+08:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=tokenizer.ggml.pretokenizer default=\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\\\r\\\\n\\\\p{L}\\\\p{N}]?\\\\p{L}+|\\\\p{N}{1,3}| ?[^\\\\s\\\\p{L}\\\\p{N}]+[\\\\r\\\\n]*|\\\\s*[\\\\r\\\\n]+|\\\\s+(?!\\\\S)|\\\\s+\"\ntime=2025-05-02T20:18:08.750+08:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=tokenizer.ggml.add_eot_token default=false\ntime=2025-05-02T20:18:08.752+08:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=tokenizer.ggml.pretokenizer default=\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\\\r\\\\n\\\\p{L}\\\\p{N}]?\\\\p{L}+|\\\\p{N}{1,3}| ?[^\\\\s\\\\p{L}\\\\p{N}]+[\\\\r\\\\n]*|\\\\s*[\\\\r\\\\n]+|\\\\s+(?!\\\\S)|\\\\s+\"\ntime=2025-05-02T20:18:08.757+08:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07\ntime=2025-05-02T20:18:08.757+08:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=gemma3.rope.local.freq_base default=10000\ntime=2025-05-02T20:18:08.757+08:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=gemma3.rope.global.freq_base default=1e+06\ntime=2025-05-02T20:18:08.757+08:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=gemma3.rope.freq_scale default=1\ntime=2025-05-02T20:18:08.757+08:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=gemma3.mm_tokens_per_image default=256\ntime=2025-05-02T20:18:08.757+08:00 level=INFO source=server.go:430 msg=\"starting llama server\" cmd=\"/usr/local/lib/python3.11/dist-packages/bigdl/cpp/libs/ollama/ollama-lib runner --ollama-engine --model /root/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 --ctx-size 8192 --batch-size 512 --n-gpu-layers 999 --threads 8 --no-mmap --parallel 4 --port 41573\"\ntime=2025-05-02T20:18:08.757+08:00 level=INFO source=sched.go:450 msg=\"loaded runners\" count=1\ntime=2025-05-02T20:18:08.757+08:00 level=INFO source=server.go:605 msg=\"waiting for llama runner to start responding\"\ntime=2025-05-02T20:18:08.757+08:00 level=INFO source=server.go:639 msg=\"waiting for server to become available\" status=\"llm server error\"\ntime=2025-05-02T20:18:08.800+08:00 level=INFO source=runner.go:757 msg=\"starting ollama engine\"\ntime=2025-05-02T20:18:08.800+08:00 level=INFO source=runner.go:817 msg=\"Server listening on 127.0.0.1:41573\"\ntime=2025-05-02T20:18:08.864+08:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=general.name default=\"\"\ntime=2025-05-02T20:18:08.864+08:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=general.description default=\"\"\ntime=2025-05-02T20:18:08.864+08:00 level=INFO source=ggml.go:68 msg=\"\" architecture=gemma3 file_type=Q4_K_M name=\"\" description=\"\" num_tensors=883 num_key_values=36\nload_backend: loaded SYCL backend from /usr/local/lib/python3.11/dist-packages/bigdl/cpp/libs/ollama/libggml-sycl.so\nload_backend: loaded CPU backend from /usr/local/lib/python3.11/dist-packages/bigdl/cpp/libs/ollama/libggml-cpu-skylakex.so\ntime=2025-05-02T20:18:08.944+08:00 level=INFO source=ggml.go:109 msg=system CPU.0.LLAMAFILE=1 CPU.0.OPENMP=1 CPU.0.AARCH64_REPACK=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\ntime=2025-05-02T20:18:08.974+08:00 level=INFO source=ggml.go:296 msg=\"Number of model weight buffers\" count=2\ntime=2025-05-02T20:18:08.974+08:00 level=INFO source=ggml.go:299 msg=\"model weights\" buffer=SYCL0 size=\"3.1 GiB\"\ntime=2025-05-02T20:18:08.974+08:00 level=INFO source=ggml.go:299 msg=\"model weights\" buffer=CPU size=\"525.0 MiB\"\ntime=2025-05-02T20:18:09.012+08:00 level=INFO source=server.go:639 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nRunning with Environment Variables:\n  GGML_SYCL_DEBUG: 0\n  GGML_SYCL_DISABLE_OPT: 1\nBuild with Macros:\n  GGML_SYCL_FORCE_MMQ: no\n  GGML_SYCL_F16: no\nFound 1 SYCL devices:\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\n| 0| [level_zero:gpu:0]|                Intel Graphics [0xe20b]|   20.1|    160|    1024|   32| 12168M|     1.6.32224.500000|\nSYCL Optimization Feature:\n|ID|        Device Type|Reorder|\n|--|-------------------|-------|\n| 0| [level_zero:gpu:0]|      Y|\ntime=2025-05-02T20:18:09.575+08:00 level=INFO source=ggml.go:369 msg=\"compute graph\" backend=SYCL0 buffer_type=SYCL0\ntime=2025-05-02T20:18:09.575+08:00 level=INFO source=ggml.go:369 msg=\"compute graph\" backend=CPU buffer_type=SYCL_Host\ntime=2025-05-02T20:18:09.575+08:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=tokenizer.ggml.pretokenizer default=\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\\\r\\\\n\\\\p{L}\\\\p{N}]?\\\\p{L}+|\\\\p{N}{1,3}| ?[^\\\\s\\\\p{L}\\\\p{N}]+[\\\\r\\\\n]*|\\\\s*[\\\\r\\\\n]+|\\\\s+(?!\\\\S)|\\\\s+\"\ntime=2025-05-02T20:18:09.578+08:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=tokenizer.ggml.add_eot_token default=false\ntime=2025-05-02T20:18:09.580+08:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=tokenizer.ggml.pretokenizer default=\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\\\r\\\\n\\\\p{L}\\\\p{N}]?\\\\p{L}+|\\\\p{N}{1,3}| ?[^\\\\s\\\\p{L}\\\\p{N}]+[\\\\r\\\\n]*|\\\\s*[\\\\r\\\\n]+|\\\\s+(?!\\\\S)|\\\\s+\"\ntime=2025-05-02T20:18:09.584+08:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07\ntime=2025-05-02T20:18:09.584+08:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=gemma3.rope.local.freq_base default=10000\ntime=2025-05-02T20:18:09.584+08:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=gemma3.rope.global.freq_base default=1e+06\ntime=2025-05-02T20:18:09.584+08:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=gemma3.rope.freq_scale default=1\ntime=2025-05-02T20:18:09.584+08:00 level=WARN source=ggml.go:149 msg=\"key not found\" key=gemma3.mm_tokens_per_image default=256\ntime=2025-05-02T20:18:09.765+08:00 level=INFO source=server.go:644 msg=\"llama runner started in 1.01 seconds\"\npanic: failed to sample token: no tokens to sample from\n\ngoroutine 11 [running]:\ngithub.com/ollama/ollama/runner/ollamarunner.(*Server).run(0xc0004da240, {0x1569620, 0xc00018f9a0})\n        /home/runner/_work/llm.cpp/llm.cpp/ollama-internal/runner/ollamarunner/runner.go:335 +0x65\ncreated by github.com/ollama/ollama/runner/ollamarunner.Execute in goroutine 1\n        /home/runner/_work/llm.cpp/llm.cpp/ollama-internal/runner/ollamarunner/runner.go:794 +0xa9c\n[GIN] 2025/05/02 - 20:18:10 | 500 |  2.072373405s |       10.89.1.2 | POST     \"/api/chat\"\n```\n\n**Environment information**\nIntel Arc B580\n\n",
      "state": "closed",
      "author": "FilipLaurentiu",
      "author_type": "User",
      "created_at": "2025-05-02T12:22:03Z",
      "updated_at": "2025-05-03T11:35:08Z",
      "closed_at": "2025-05-03T11:35:07Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13129/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13129",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13129",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:38.718150",
      "comments": [
        {
          "author": "jason-dai",
          "body": "For now, only the fp16 version works",
          "created_at": "2025-05-02T12:24:54Z"
        },
        {
          "author": "FilipLaurentiu",
          "body": "I can confirm that `gemma3:4b-it-fp16` works",
          "created_at": "2025-05-03T11:35:07Z"
        }
      ]
    },
    {
      "issue_number": 13115,
      "title": "Error: flag provided but not defined: -ngl",
      "body": "**Describe the bug**\nOllama runner fails with exit code 2 because -ngl is not a parameter.\n\n**How to reproduce**\nSteps to reproduce the error:\n```\ncurl http://localhost:11434/api/generate -d '\n{\n   \"model\": \"gemma3:4b\",\n   \"prompt\": \"What is AI?\",\n   \"stream\": false\n}'\n```\n\n**Screenshots**\n```\ntime=2025-04-27T17:34:25.745+08:00 level=INFO source=server.go:426 msg=\"starting llama server\" cmd=\"/usr/local/lib/python3.11/dist-packages/bigdl/cpp/libs/ollama/ollama-lib runner --ollama-engine --model /root/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 --ctx-size 2048 --batch-size 512 -ngl 999 --threads 18 --no-mmap --parallel 1 --port 34611\"\ntime=2025-04-27T17:34:25.746+08:00 level=INFO source=sched.go:450 msg=\"loaded runners\" count=1\ntime=2025-04-27T17:34:25.746+08:00 level=INFO source=server.go:601 msg=\"waiting for llama runner to start responding\"\ntime=2025-04-27T17:34:25.746+08:00 level=INFO source=server.go:635 msg=\"waiting for server to become available\" status=\"llm server error\"\nflag provided but not defined: -ngl\nRunner usage\n...\n   -n-gpu-layers int\n    \tNumber of layers to offload to GPU\n```\n\n**Environment information**\nIf possible, please attach the output of the environment check script, using:\n- https://github.com/intel/ipex-llm/blob/main/python/llm/scripts/env-check.bat, or\n- https://github.com/intel/ipex-llm/blob/main/python/llm/scripts/env-check.sh\n```\n-----------------------------------------------------------------\nPYTHON_VERSION=3.11.12\n-----------------------------------------------------------------\n/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\ntransformers=4.36.2\n-----------------------------------------------------------------\ntorch=2.2.0+cu121\n-----------------------------------------------------------------\nipex-llm Version: 2.3.0b20250426\n-----------------------------------------------------------------\nIPEX is not installed. \n-----------------------------------------------------------------\nCPU Information: \nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               18\nOn-line CPU(s) list:                  0-17\nVendor ID:                            GenuineIntel\nModel name:                           13th Gen Intel(R) Core(TM) i7-1370P\nCPU family:                           6\nModel:                                186\nThread(s) per core:                   1\nCore(s) per socket:                   18\nSocket(s):                            1\nStepping:                             2\nBogoMIPS:                             4377.60\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology cpuid tsc_known_freq pni pclmulqdq vmx ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves avx_vnni arat vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq rdpid movdiri movdir64b fsrm md_clear serialize flush_l1d arch_capabilities\nVirtualization:                       VT-x\n-----------------------------------------------------------------\nTotal CPU Memory: 17.529 GB\nMemory Type: sudo: dmidecode: command not found\n-----------------------------------------------------------------\nOperating System: \nUbuntu 22.04.5 LTS \\n \\l\n\n-----------------------------------------------------------------\nLinux ollama-c98c7b486-5j6w5 6.9.10+bpo-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.9.10-1~bpo12+1 (2024-07-26) x86_64 x86_64 x86_64 GNU/Linux\n-----------------------------------------------------------------\n./env-check.sh: line 148: xpu-smi: command not found\n-----------------------------------------------------------------\n  Driver Version                                  2024.18.12.0.05_160000\n  Driver UUID                                     32342e35-322e-3332-3232-342e35000000\n  Driver Version                                  24.52.32224.5\n-----------------------------------------------------------------\nDriver related package version:\nii  intel-level-zero-gpu                             1.6.32224.5                             amd64        Intel(R) Graphics Compute Runtime for oneAPI Level Zero.\nii  intel-level-zero-gpu-legacy1                     1.3.30872.22                            amd64        Intel(R) Graphics Compute Runtime for oneAPI Level Zero.\nii  level-zero-devel                                 1.20.2                                  amd64        oneAPI Level Zero\n-----------------------------------------------------------------\nigpu not detected\n[level_zero:gpu][level_zero:0] Intel(R) oneAPI Unified Runtime over Level-Zero, Intel(R) Iris(R) Xe Graphics 12.3.0 [1.6.32224.500000]\n[opencl:cpu][opencl:0] Intel(R) OpenCL, 13th Gen Intel(R) Core(TM) i7-1370P OpenCL 3.0 (Build 0) [2024.18.12.0.05_160000]\n[opencl:gpu][opencl:1] Intel(R) OpenCL Graphics, Intel(R) Iris(R) Xe Graphics OpenCL 3.0 NEO  [24.52.32224.5]\n-----------------------------------------------------------------\nxpu-smi is not installed. Please install xpu-smi according to README.md\n```\n\n**Additional context**\nNote that runner fails because `-ngl` is passed when the parameter allowed is `-n-gpu-layers`.\n",
      "state": "closed",
      "author": "aarononeal",
      "author_type": "User",
      "created_at": "2025-04-27T09:53:01Z",
      "updated_at": "2025-05-03T07:25:26Z",
      "closed_at": "2025-05-03T07:25:25Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 15,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13115/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13115",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13115",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:38.922523",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @aarononeal, which version of ipex-llm ollama are you running?\n",
          "created_at": "2025-04-28T02:10:43Z"
        },
        {
          "author": "kirel",
          "body": "I see the same when I build the latest xpu-ccp container according to the guide.",
          "created_at": "2025-04-28T12:33:54Z"
        },
        {
          "author": "aarononeal",
          "body": "@sgwhat I followed the [Docker quickstart](https://github.com/intel/ipex-llm/blob/main/docs/mddocs/DockerGuides/docker_cpp_xpu_quickstart.md) using image `intelanalytics/ipex-llm-inference-cpp-xpu:latest` which for some reason is reporting Ollama version `0.0.0`.",
          "created_at": "2025-04-28T19:09:45Z"
        },
        {
          "author": "kirel",
          "body": "Can also confirm that. ",
          "created_at": "2025-04-28T19:59:07Z"
        },
        {
          "author": "Apokalypzx",
          "body": "Confirmed \n```\n/llm/ollama/./ollama --version\nversion=0.0.0 \n```\n\nGemma3:4b (Q4_0 and QA along with Gemma3:1b) result in error about \"-ngl provided but not defined.\"\n\nAlso tested models Mistral:7b, llama3.1:8b. Both load and inference on the GPU fine without error.\n\nEnvironment: Docker container bui",
          "created_at": "2025-04-29T08:46:12Z"
        }
      ]
    },
    {
      "issue_number": 12172,
      "title": "Ollama fails to load model to a380 GPU",
      "body": "Hello,\r\n\r\ni have got a issue while runnning ollama on the A380 gpu.\r\n\r\nThis log snippet is from the ollama log while executing  a prompt from open-webui.\r\n\r\nThe system runs:\r\nFedora 39\r\nKernel 6.10.7\r\nThe GPU has resizable bar enabled\r\n\r\nollama and open-webui are running in two separate docker containers.\r\nI have tried multiple models, smaller ones, bigger ones, always the same error.\r\n\r\nAny recommodations on resolving this?\r\n\r\n```\r\ntime=2024-10-10T06:16:56.429+08:00 level=INFO source=memory.go:133 msg=\"offload to gpu\" layers.requested=-1 layers.real=25 memory.available=\"18.4 GiB\" memory.required.full=\"706.6 MiB\" memory.required.partial=\"706.6 MiB\" memory.required.kv=\"24.0 MiB\" memory.weights.total=\"373.7 MiB\" memory.weights.repeating=\"235.8 MiB\" memory.weights.nonrepeating=\"137.9 MiB\" memory.graph.full=\"298.5 MiB\" memory.graph.partial=\"405.0 MiB\"\r\ntime=2024-10-10T06:16:56.429+08:00 level=INFO source=server.go:342 msg=\"starting llama server\" cmd=\"/tmp/ollama2656666813/runners/cpu_avx2/ollama_llama_server --model /root/.ollama/models/blobs/sha256-c5396e06af294bd101b30dce59131a76d2b773e76950acc870eda801d3ab0515 --ctx-size 2048 --batch-size 512 --embedding --log-disable --n-gpu-layers 999 --parallel 1 --port 34029\"\r\ntime=2024-10-10T06:16:56.429+08:00 level=INFO source=sched.go:338 msg=\"loaded runners\" count=1\r\ntime=2024-10-10T06:16:56.429+08:00 level=INFO source=server.go:529 msg=\"waiting for llama runner to start responding\"\r\ntime=2024-10-10T06:16:56.429+08:00 level=INFO source=server.go:566 msg=\"waiting for server to become available\" status=\"llm server error\"\r\nINFO [main] build info | build=1 commit=\"f6b084d\" tid=\"140384432765952\" timestamp=1728512216\r\nINFO [main] system info | n_threads=12 n_threads_batch=-1 system_info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \" tid=\"140384432765952\" timestamp=1728512216 total_threads=24\r\nINFO [main] HTTP server listening | hostname=\"127.0.0.1\" n_threads_http=\"23\" port=\"34029\" tid=\"140384432765952\" timestamp=1728512216\r\nllama_model_loader: loaded meta data with 34 key-value pairs and 290 tensors from /root/.ollama/models/blobs/sha256-c5396e06af294bd101b30dce59131a76d2b773e76950acc870eda801d3ab0515 (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = Qwen2.5 0.5B Instruct\r\nllama_model_loader: - kv   3:                           general.finetune str              = Instruct\r\nllama_model_loader: - kv   4:                           general.basename str              = Qwen2.5\r\nllama_model_loader: - kv   5:                         general.size_label str              = 0.5B\r\nllama_model_loader: - kv   6:                            general.license str              = apache-2.0\r\nllama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-0...\r\nllama_model_loader: - kv   8:                   general.base_model.count u32              = 1\r\nllama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 0.5B\r\nllama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen\r\nllama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-0.5B\r\nllama_model_loader: - kv  12:                               general.tags arr[str,2]       = [\"chat\", \"text-generation\"]\r\nllama_model_loader: - kv  13:                          general.languages arr[str,1]       = [\"en\"]\r\nllama_model_loader: - kv  14:                          qwen2.block_count u32              = 24\r\nllama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768\r\nllama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 896\r\nllama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 4864\r\nllama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 14\r\nllama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 2\r\nllama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\r\nllama_model_loader: - kv  22:                          general.file_type u32              = 15\r\nllama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2\r\nllama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\r\nllama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645\r\nllama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643\r\nllama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643\r\nllama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false\r\nllama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\r\nllama_model_loader: - kv  33:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:  121 tensors\r\nllama_model_loader: - type q5_0:  132 tensors\r\nllama_model_loader: - type q8_0:   13 tensors\r\nllama_model_loader: - type q4_K:   12 tensors\r\nllama_model_loader: - type q6_K:   12 tensors\r\nllm_load_vocab: special tokens definition check successful ( 293/151936 ).\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = qwen2\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 151936\r\nllm_load_print_meta: n_merges         = 151387\r\nllm_load_print_meta: n_ctx_train      = 32768\r\nllm_load_print_meta: n_embd           = 896\r\nllm_load_print_meta: n_head           = 14\r\nllm_load_print_meta: n_head_kv        = 2\r\nllm_load_print_meta: n_layer          = 24\r\nllm_load_print_meta: n_rot            = 64\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 64\r\nllm_load_print_meta: n_embd_head_v    = 64\r\nllm_load_print_meta: n_gqa            = 7\r\nllm_load_print_meta: n_embd_k_gqa     = 128\r\nllm_load_print_meta: n_embd_v_gqa     = 128\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 4864\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 2\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 32768\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 1B\r\nllm_load_print_meta: model ftype      = Q4_K - Medium\r\nllm_load_print_meta: model params     = 494.03 M\r\nllm_load_print_meta: model size       = 373.71 MiB (6.35 BPW) \r\nllm_load_print_meta: general.name     = Qwen2.5 0.5B Instruct\r\nllm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\r\nllm_load_print_meta: EOS token        = 151645 '<|im_end|>'\r\nllm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\r\nllm_load_print_meta: LF token         = 148848 'ÄĬ'\r\nllm_load_print_meta: EOT token        = 151645 '<|im_end|>'\r\n[SYCL] call ggml_init_sycl\r\nggml_init_sycl: GGML_SYCL_DEBUG: 1\r\nggml_init_sycl: GGML_SYCL_F16: no\r\ntime=2024-10-10T06:16:56.680+08:00 level=INFO source=server.go:566 msg=\"waiting for server to become available\" status=\"llm server loading model\"\r\n[SYCL] call ggml_backend_sycl_print_sycl_devices\r\nfound 1 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|                Intel Arc A380 Graphics|    1.3|    128|    1024|   32|  6064M|            1.3.30049|\r\n[SYCL] call ggml_backend_sycl_set_mul_device_mode\r\nggml_backend_sycl_set_mul_device_mode: true\r\ndetect 1 SYCL GPUs: [0] with top Max compute units:128\r\n[SYCL] call ggml_backend_sycl_host_buffer_type\r\n[SYCL] call ggml_backend_sycl_get_device_count\r\n[SYCL] call ggml_backend_sycl_get_device_memory\r\n[SYCL] call ggml_backend_sycl_buffer_type\r\n[SYCL] call ggml_backend_sycl_buffer_type\r\n[SYCL] call ggml_backend_sycl_buffer_type\r\n[SYCL] call ggml_backend_sycl_buffer_type\r\n[SYCL] call ggml_backend_sycl_buffer_type\r\n[SYCL] call ggml_backend_sycl_buffer_type\r\n[SYCL] call ggml_backend_sycl_buffer_type\r\n[SYCL] call ggml_backend_sycl_buffer_type\r\n[SYCL] call ggml_backend_sycl_buffer_type\r\n[SYCL] call ggml_backend_sycl_buffer_type\r\n[SYCL] call ggml_backend_sycl_buffer_type\r\n[SYCL] call ggml_backend_sycl_buffer_type\r\n[SYCL] call ggml_backend_sycl_buffer_type\r\n[SYCL] call ggml_backend_sycl_buffer_type\r\n[SYCL] call ggml_backend_sycl_buffer_type\r\n[SYCL] call ggml_backend_sycl_buffer_type\r\n[SYCL] call ggml_backend_sycl_buffer_type\r\n[SYCL] call ggml_backend_sycl_buffer_type\r\n[SYCL] call ggml_backend_sycl_buffer_type\r\n[SYCL] call ggml_backend_sycl_buffer_type\r\n[SYCL] call ggml_backend_sycl_buffer_type\r\n[SYCL] call ggml_backend_sycl_buffer_type\r\n[SYCL] call ggml_backend_sycl_buffer_type\r\n[SYCL] call ggml_backend_sycl_buffer_type\r\n[SYCL] call ggml_backend_sycl_buffer_type\r\nllm_load_tensors: ggml ctx size =    0.28 MiB\r\n[SYCL] call ggml_backend_sycl_host_buffer_type\r\n[SYCL] call ggml_backend_sycl_host_buffer_type\r\nllm_load_tensors: offloading 24 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 25/25 layers to GPU\r\nllm_load_tensors:      SYCL0 buffer size =   373.73 MiB\r\nllm_load_tensors:        CPU buffer size =   137.94 MiB\r\nllama_new_context_with_model: n_ctx      = 2048\r\nllama_new_context_with_model: n_batch    = 512\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 1000000.0\r\nllama_new_context_with_model: freq_scale = 1\r\n[SYCL] call ggml_backend_sycl_get_device_count\r\n[SYCL] call ggml_backend_sycl_init\r\nUsing device 0 (Intel(R) Arc(TM) A380 Graphics) as main device\r\n[SYCL] call ggml_backend_sycl_get_device_count\r\n[SYCL] call ggml_backend_sycl_get_device_id\r\nllama_kv_cache_init:      SYCL0 KV buffer size =    24.00 MiB\r\nllama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB\r\n[SYCL] call ggml_backend_sycl_host_buffer_type\r\nllama_new_context_with_model:  SYCL_Host  output buffer size =     0.58 MiB\r\n[SYCL] call ggml_backend_sycl_buffer_type\r\n[SYCL] call ggml_backend_sycl_host_buffer_type\r\n[SYCL] call ggml_backend_sycl_get_device_count\r\n[1728512217] warming up the model with an empty run\r\nllama_new_context_with_model:      SYCL0 compute buffer size =   298.50 MiB\r\nllama_new_context_with_model:  SYCL_Host compute buffer size =     5.76 MiB\r\nllama_new_context_with_model: graph nodes  = 870\r\nllama_new_context_with_model: graph splits = 2\r\n[SYCL] call ggml_backend_sycl_buffer_type\r\ncall ggml_sycl_rms_norm\r\nThe program was built for 1 devices\r\nBuild program log for 'Intel(R) Arc(TM) A380 Graphics':\r\n -999 (Unknown PI error)Exception caught at file:/home/runner/_work/llm.cpp/llm.cpp/ollama-internal/llm/llama.cpp/ggml-sycl.cpp, line:14714\r\ntime=2024-10-10T06:16:57.683+08:00 level=ERROR source=sched.go:344 msg=\"error loading llama server\" error=\"llama runner process has terminated: exit status 1 \"\r\n\r\n```",
      "state": "open",
      "author": "GamerSocke",
      "author_type": "User",
      "created_at": "2024-10-09T22:34:26Z",
      "updated_at": "2025-04-29T12:03:17Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12172/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12172",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12172",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:39.118709",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @GamerSocke , we are reproducing your issue, we will inform you once we get a solution.",
          "created_at": "2024-10-11T01:23:18Z"
        },
        {
          "author": "qiuxin2012",
          "body": "@GamerSocke We notice you are using `Fedora 39 and Kernel 6.10.7`, we only support ubuntu 22.04 and linux kernel 6.2 and 6.5.",
          "created_at": "2024-10-11T02:08:19Z"
        },
        {
          "author": "NikosDi",
          "body": "@GamerSocke \r\nUse `OLLAMA_NUM_PARALLEL=1` in your script just before `ollama serve`\r\n\r\nI have also an Intel ARC A380 and it's the only way to load a model in 6GB VRAM\r\n\r\nI use Ubuntu 24.04 with Linux Kernel 6.8",
          "created_at": "2024-11-30T05:25:55Z"
        },
        {
          "author": "NooNameR",
          "body": "having similar issue on unraid\n```\nuname -r\n6.6.78-Unraid\n```\n\ndeepseek-r1:7b:\n```\nllama_init_from_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nRunning with Environment Variables:\n  GGML_SYCL_DEBUG: 0\n  GGML_SYCL_DISABLE_OPT: 1\nBuild with",
          "created_at": "2025-04-29T12:02:45Z"
        }
      ]
    },
    {
      "issue_number": 13056,
      "title": "repository does not exist intelanalytics/ipex-llm-inference-cpp-xpu",
      "body": "**Describe the bug**\nThe repo intelanalytics/ipex-llm-inference-cpp-xpu is no longer on dockerhub\n\n**How to reproduce**\nSteps to reproduce the error:\n1. run docker pull intelanalytics/ipex-llm-inference-cpp-xpu:latest\n\n**Screenshots**\n\n![Image](https://github.com/user-attachments/assets/c71097d0-4197-47a1-9065-d9cce1373349)\n\n![Image](https://github.com/user-attachments/assets/0db39309-b1c9-4ed0-bc42-5d15cb6ded2a)\n\n",
      "state": "open",
      "author": "TacitTactics",
      "author_type": "User",
      "created_at": "2025-04-09T02:35:44Z",
      "updated_at": "2025-04-29T01:37:14Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13056/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13056",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13056",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:39.357604",
      "comments": [
        {
          "author": "fallenleavesgocrunch",
          "body": "Same here.",
          "created_at": "2025-04-09T07:06:58Z"
        },
        {
          "author": "huichuno",
          "body": "The image is missing from https://hub.docker.com/u/intelanalytics",
          "created_at": "2025-04-09T08:00:02Z"
        },
        {
          "author": "zouy414",
          "body": "FYI: https://github.com/intel/ipex-llm/pull/13057",
          "created_at": "2025-04-09T10:07:54Z"
        },
        {
          "author": "bermudahonk",
          "body": "Hello,\nto anyone who has been using that image in their own compose-files, you can/have to substitute the original image with a build rule like this to make things work as \"usual\" again:\n```\n    # before (simply pull the image)\n    image: docker.io/intelanalytics/ipex-llm-inference-cpp-xpu:latest\n  ",
          "created_at": "2025-04-10T05:19:51Z"
        },
        {
          "author": "TacitTactics",
          "body": "Thank you this did work. Do we know when the ollama version will be updated for the image? I am not able to pull the new gemma 3. Thank you again\n",
          "created_at": "2025-04-11T22:05:31Z"
        }
      ]
    },
    {
      "issue_number": 13112,
      "title": "IPv6 needs to be disabled before PPA install",
      "body": "**Describe the bug**\n\nIPv6 needs to be disabled before PPA install.\n\nFor some reason a clean Ubuntu install will try to push IPv6 to pull the Kobuk Team PPA. Disabling it within the common settings does not help as it is still reinforced and gives you \"connection time out\" errors. \n\nCan be fixed by using the below: \n\n```\nsudo sysctl -w net.ipv6.conf.all.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.default.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.lo.disable_ipv6=1\n```\nDisconnect the network adapter and connect it back again.\n\nWould be useful to have this stated somewhere in the instructions.\n\n**How to reproduce**\nSteps to reproduce the error:\n1. Clean Ubuntu install\n2. Follow the Linux B580 installation steps\n3. Install the Kobuk-team PPA\n4. Get connection time out error [Errno 110]\n\n",
      "state": "open",
      "author": "dennis-george0",
      "author_type": "User",
      "created_at": "2025-04-26T18:21:28Z",
      "updated_at": "2025-04-29T01:26:24Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13112/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Oscilloscope98"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13112",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13112",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:39.581294",
      "comments": [
        {
          "author": "Oscilloscope98",
          "body": "Hi @dennis-george0,\n\nThank you for pointing it out. We have added the [related troubleshooting guide](https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/bmg_quickstart.md#42-connection-timeout-error-when-installing-the-intel-graphics-ppa) in IPEX-LLM Intel Arc B-Series GPU QuickStart",
          "created_at": "2025-04-29T01:26:22Z"
        }
      ]
    },
    {
      "issue_number": 13108,
      "title": "[XPU] library mismatch and version issue while performing fine-tuning on B580",
      "body": "**Describe the bug**\nperforming, fine-tuning of llm-model,  on BattleImage, facing conflicting with library issue, specially between transformer and supported bitsandbyte.\n\n```\nTraceback (most recent call last):\n  File \"./qlora_finetuning.py\", line 22, in <module>\n    from peft import LoraConfig\n  File \"/envs/ft-test/lib/python3.11/site-packages/peft/__init__.py\", line 22, in <module>\n    from .auto import (\n  File \"/envs/ft-test/lib/python3.11/site-packages/peft/auto.py\", line 31, in <module>\n    from .config import PeftConfig\n  File \"/envs/ft-test/lib/python3.11/site-packages/peft/config.py\", line 23, in <module>\n    from .utils import CONFIG_NAME, PeftType, TaskType\n  File \"/envs/ft-test/lib/python3.11/site-packages/peft/utils/__init__.py\", line 21, in <module>\n    from .loftq_utils import replace_lora_weights_loftq\n  File \"/envs/ft-test/lib/python3.11/site-packages/peft/utils/loftq_utils.py\", line 35, in <module>\n    import bitsandbytes as bnb\n  File \"/envs/ft-test/lib/python3.11/site-packages/bitsandbytes/__init__.py\", line 15, in <module>\n    from .nn import modules\n  File \"/envs/ft-test/lib/python3.11/site-packages/bitsandbytes/nn/__init__.py\", line 21, in <module>\n    from .triton_based_modules import (\n  File \"/envs/ft-test/lib/python3.11/site-packages/bitsandbytes/nn/triton_based_modules.py\", line 6, in <module>\n    from bitsandbytes.triton.dequantize_rowwise import dequantize_rowwise\n  File \"/envs/ft-test/lib/python3.11/site-packages/bitsandbytes/triton/dequantize_rowwise.py\", line 12, in <module>\n    import triton\n  File \"/envs/ft-test/lib/python3.11/site-packages/triton/__init__.py\", line 8, in <module>\n    from .runtime import (\n  File \"/envs/ft-test/lib/python3.11/site-packages/triton/runtime/__init__.py\", line 1, in <module>\n    from .autotuner import (Autotuner, Config, Heuristics, autotune, heuristics)\n  File \"/envs/ft-test/lib/python3.11/site-packages/triton/runtime/autotuner.py\", line 9, in <module>\n    from .jit import KernelInterface\n  File \"/envs/ft-test/lib/python3.11/site-packages/triton/runtime/jit.py\", line 12, in <module>\n    from ..runtime.driver import driver\n  File \"/envs/ft-test/lib/python3.11/site-packages/triton/runtime/driver.py\", line 1, in <module>\n    from ..backends import backends\n  File \"/envs/ft-test/lib/python3.11/site-packages/triton/backends/__init__.py\", line 50, in <module>\n    backends = _discover_backends()\n               ^^^^^^^^^^^^^^^^^^^^\n  File \"/envs/ft-test/lib/python3.11/site-packages/triton/backends/__init__.py\", line 43, in _discover_backends\n    compiler = _load_module(name, os.path.join(root, name, 'compiler.py'))\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/envs/ft-test/lib/python3.11/site-packages/triton/backends/__init__.py\", line 12, in _load_module\n    spec.loader.exec_module(module)\n  File \"/envs/ft-test/lib/python3.11/site-packages/triton/backends/intel/compiler.py\", line 2, in <module>\n    from triton._C.libtriton import ir, passes, llvm, intel\nImportError: cannot import name 'intel' from 'triton._C.libtriton' (/envs/ft-test/lib/python3.11/site-packages/triton/_C/libtriton.so)\n```\n\n\n**How to reproduce**\nSteps to reproduce the error:\n1. following BMG guide --> https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/bmg_quickstart.md\n2. pytorch test done , and its works fine--> \n```\n >>> import torch\n>>> from ipex_llm.transformers import AutoModelForCausalLM\n>>>\n>>> tensor_1 = torch.randn(1, 1, 40, 128).to('xpu')\n>>> tensor_2 = torch.randn(1, 1, 128, 40).to('xpu')\n>>> print(torch.matmul(tensor_1, tensor_2).size())\ntorch.Size([1, 1, 40, 40])\n>>>\n```\n3. followed the fine-tuning doc --> https://github.com/intel/ipex-llm/tree/main/python/llm/example/GPU/LLM-Finetuning/QLoRA/trl-example (tried with both pytroch 4.37 and 4.45)\n4. got the issue there -->` ImportError: cannot import name 'intel' from 'triton._C.libtriton' (/envs/ft-test/lib/python3.11/site-packages/triton/_C/libtriton.so`\n5. Also, believe so. we don't need opeAPI separately, , have tested with and without oneAPI installed:with oneAPI installed and xpu_2.3 (which supposed to be pytorch issue as its compiled for particular xpu version+pytorch)\n```\n  File \"/envs/ft-test/lib/python3.11/site-packages/torch/__init__.py\", line 405, in <module>\n    from torch._C import *  # noqa: F403\n    ^^^^^^^^^^^^^^^^^^^^^^\nImportError: /envs/ft-test/lib/python3.11/site-packages/torch/lib/../../../../libsycl.so.8: undefined symbol: urBindlessImagesImportExternalMemoryExp, version LIBUR_LOADER_0.10\n\n```\n**Environment information**\n\n**### w/o oneAPI (xpu_2.6)**\n```\n  PYTHON_VERSION=3.11.12\n-----------------------------------------------------------------\ntransformers=4.45.0\n-----------------------------------------------------------------\ntorch=2.6.0+xpu\n-----------------------------------------------------------------\nipex-llm Version: 2.3.0b20250423\n-----------------------------------------------------------------\nIPEX is not installed.\n-----------------------------------------------------------------\nCPU Information:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               32\nOn-line CPU(s) list:                  0-31\nVendor ID:                            GenuineIntel\nModel name:                           13th Gen Intel(R) Core(TM) i9-13900K\nCPU family:                           6\nModel:                                183\nThread(s) per core:                   2\nCore(s) per socket:                   24\nSocket(s):                            1\nStepping:                             1\nCPU(s) scaling MHz:                   24%\nCPU max MHz:                          5800.0000\nCPU min MHz:                          800.0000\n-----------------------------------------------------------------\nTotal CPU Memory: 61.5439 GB\nMemory Type: DDR5\n-----------------------------------------------------------------\nOperating System:\nUbuntu 24.10 \\n \\l\n\n-----------------------------------------------------------------\nLinux IMU-LAB1-BMG3-SUT 6.14.0-rc1-custom-rt #9 SMP PREEMPT_RT Mon Mar 31 15:51:25 CEST 2025 x86_64 x86_64 x86_64 GNU/Linux\n-----------------------------------------------------------------\nCLI:\n    Version: 1.2.39.20241101\n    Build ID: 00000000\n\nService:\n    Version: 1.2.39.20241101\n    Build ID: 00000000\n    Level Zero Version: 1.20.2\n-----------------------------------------------------------------\n  Driver UUID                                     32352e30-392e-3332-3936-310000000000\n  Driver Version                                  25.09.32961\n  Driver Version                                  2023.16.12.0.12_195853.xmain-hotfix\n  Driver Version                                  2023.16.12.0.12_195853.xmain-hotfix\n-----------------------------------------------------------------\nDriver related package version:\nii  intel-level-zero-gpu-raytracing                1.0.0-0ubuntu1~24.10~ppa4                amd64        Level Zero Ray Tracing Support library\n-----------------------------------------------------------------\nenv-check.sh: line 167: sycl-ls: command not found\nigpu not detected\n-----------------------------------------------------------------\nxpu-smi is properly installed.\n-----------------------------------------------------------------\nNo device discovered\nGPU0 Memory size=16G\n-----------------------------------------------------------------\n03:00.0 VGA compatible controller: Intel Corporation Battlemage G21 [Intel Graphics] (prog-if 00 [VGA controller])\n        Subsystem: Intel Corporation Device 1100\n        Flags: bus master, fast devsel, latency 0, IRQ 190, IOMMU group 20\n        Memory at 84000000 (64-bit, non-prefetchable) [size=16M]\n        Memory at 4000000000 (64-bit, prefetchable) [size=16G]\n        Expansion ROM at 85000000 [disabled] [size=2M]\n        Capabilities: <access denied>\n        Kernel driver in use: xe\n        Kernel modules: xe\n------------------------------\n\n```\n\n**### with oneAPI (xpu_2.3)**\n```\n-----------------------------------------------------------------\nPYTHON_VERSION=3.11.12\n-----------------------------------------------------------------\nTransformers is not installed.\n-----------------------------------------------------------------\nPyTorch is not installed.\n-----------------------------------------------------------------\nipex-llm Version: 2.3.0b20250423\n-----------------------------------------------------------------\nIPEX is not installed.\n-----------------------------------------------------------------\nCPU Information:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               32\nOn-line CPU(s) list:                  0-31\nVendor ID:                            GenuineIntel\nModel name:                           13th Gen Intel(R) Core(TM) i9-13900K\nCPU family:                           6\nModel:                                183\nThread(s) per core:                   2\nCore(s) per socket:                   24\nSocket(s):                            1\nStepping:                             1\nCPU(s) scaling MHz:                   23%\nCPU max MHz:                          5800.0000\nCPU min MHz:                          800.0000\n-----------------------------------------------------------------\nTotal CPU Memory: 61.5439 GB\nMemory Type: DDR5\n-----------------------------------------------------------------\nOperating System:\nUbuntu 24.10 \\n \\l\n\n-----------------------------------------------------------------\nLinux IMU-LAB1-BMG3-SUT 6.14.0-rc1-custom-rt #9 SMP PREEMPT_RT Mon Mar 31 15:51:25 CEST 2025 x86_64 x86_64 x86_64 GNU/Linux\n-----------------------------------------------------------------\nCLI:\n    Version: 1.2.39.20241101\n    Build ID: 00000000\n\nService:\n    Version: 1.2.39.20241101\n    Build ID: 00000000\n    Level Zero Version: 1.20.2\n-----------------------------------------------------------------\n  Driver Version                                  2023.16.12.0.12_195853.xmain-hotfix\n  Driver Version                                  2023.16.12.0.12_195853.xmain-hotfix\n  Driver UUID                                     32352e30-392e-3332-3936-310000000000\n  Driver Version                                  25.09.32961\n-----------------------------------------------------------------\nDriver related package version:\nii  intel-level-zero-gpu-raytracing                1.0.0-0ubuntu1~24.10~ppa4                amd64        Level Zero Ray Tracing Support library\n-----------------------------------------------------------------\nigpu not detected\n-----------------------------------------------------------------\nxpu-smi is properly installed.\n-----------------------------------------------------------------\nNo device discovered\nGPU0 Memory size=16G\n-----------------------------------------------------------------\n03:00.0 VGA compatible controller: Intel Corporation Battlemage G21 [Intel Graphics] (prog-if 00 [VGA controller])\n        Subsystem: Intel Corporation Device 1100\n        Flags: bus master, fast devsel, latency 0, IRQ 190, IOMMU group 20\n        Memory at 84000000 (64-bit, non-prefetchable) [size=16M]\n        Memory at 4000000000 (64-bit, prefetchable) [size=16G]\n        Expansion ROM at 85000000 [disabled] [size=2M]\n        Capabilities: <access denied>\n        Kernel driver in use: xe\n        Kernel modules: xe\n\n```",
      "state": "open",
      "author": "raj-ritu17",
      "author_type": "User",
      "created_at": "2025-04-24T16:09:01Z",
      "updated_at": "2025-04-29T01:23:43Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13108/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Uxito-Ada"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13108",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13108",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:39.764414",
      "comments": [
        {
          "author": "Uxito-Ada",
          "body": "Hi @raj-ritu17 ,\n\nAfter validating torch tensor multiplication on BMG as [here](https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/bmg_quickstart.md#31-pytorch), I run the following to install trl dependencies (since ipex-llm has already been installed in BMG setup before):\n```python",
          "created_at": "2025-04-27T07:35:25Z"
        },
        {
          "author": "raj-ritu17",
          "body": "@Uxito-Ada Thanks for the update :)\n\nin my opinion issue is not there 'from peft import LoraConfig'; its actually coming from calling the **\"DistributedType\"** from the wrong file, I have mentioned those in the last lines (for fine tuning on BMG with xpu_2.6)\n\n---------------------------------------",
          "created_at": "2025-04-28T17:12:46Z"
        },
        {
          "author": "Uxito-Ada",
          "body": "Hi @raj-ritu17 ,\n\nThanks for your analysis. I have reproduced the DistributedType error, which is different from BMG machine issue and we are going to fix it.",
          "created_at": "2025-04-29T01:23:43Z"
        }
      ]
    },
    {
      "issue_number": 13111,
      "title": "Unable to run or serve phi4-mini",
      "body": "**Describe the bug**\n1. Version: ollama-ipex-llm-2.3.0b20250415-ubuntu (nighly)\n\n2. ./ollama serve or bash start-ollama.sh is not working with phi4-mini .\n\n{\n  \"message\": \"500 llama runner process has terminated: error loading model: missing tensor 'output.weight'\",\n  \"status\": 500,\n  \"error\": {\n    \"message\": \"llama runner process has terminated: error loading model: missing tensor 'output.weight'\",\n    \"type\": \"api_error\",\n    \"param\": null,\n    \"code\": null\n  },\n  \"code\": null,\n  \"param\": null,\n  \"type\": \"api_error\"\n}\n\n\n\n\n\n**Additional context**\nAdd any other context about the problem here.\n",
      "state": "closed",
      "author": "WayneJin88888",
      "author_type": "User",
      "created_at": "2025-04-26T03:23:41Z",
      "updated_at": "2025-04-28T05:29:48Z",
      "closed_at": "2025-04-28T05:29:48Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13111/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13111",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13111",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:39.970577",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @WayneJin88888 , we currently do not support phi4-mini.",
          "created_at": "2025-04-28T02:27:31Z"
        },
        {
          "author": "WayneJin88888",
          "body": "> Hi [@WayneJin88888](https://github.com/WayneJin88888) , we currently do not support phi4-mini.\n\nok, understood. thx for reply.",
          "created_at": "2025-04-28T05:29:44Z"
        }
      ]
    },
    {
      "issue_number": 12983,
      "title": "Fail to run gemma3 in the IPEX Ollama released in 3.19",
      "body": "**Describe the bug**\nFail to run gemma3 in the IPEX Ollama released in 3.19, the latest documentation is strickly followed, updated to the latest version, and configured the environment variables 'set IPEX_LLM_MODEL_SOURCE=modelscope' that gemma3 needed.\n\nWhile I tried phi4 on it, and it was running well, and much faster than by llama.cpp\n\n**How to reproduce**\nSteps to reproduce the error:\n\nHere the steps on terminal\n\nPS C:\\Soft\\AI\\ollama-ipex> .\\start-ollama.bat\nPS C:\\Soft\\AI\\ollama-ipex> set IPEX_LLM_MODEL_SOURCE=modelscope\nPS C:\\Soft\\AI\\ollama-ipex> .\\ollama.exe run gemma3:27b\nggml_sycl_init: found 1 SYCL devices:\npulling manifest\npulling 6a2cf0085006... 100% ▕████████████████████████████████████████████████████████▏  16 GB\npulling e0a42594d802... 100% ▕████████████████████████████████████████████████████████▏  358 B\npulling d80b22736c51... 100% ▕████████████████████████████████████████████████████████▏   47 B\npulling dd084c7d92a3... 100% ▕████████████████████████████████████████████████████████▏ 8.4 KB\npulling 54cb61c842fe... 100% ▕████████████████████████████████████████████████████████▏ 857 MB\npulling 556c7538d4d4... 100% ▕████████████████████████████████████████████████████████▏  195 B\nverifying sha256 digest\nwriting manifest\nsuccess\nError: llama runner process has terminated: error:CHECK_TRY_ERROR(ctx->stream->memset( (char *)tensor->data + original_size, 0, padded_size - original_size).wait()): Meet error in this line code!\n  in function ggml_backend_sycl_buffer_init_tensor at D:\\actions-runner\\release-cpp-oneapi_2024_2\\_work\\llm.cpp\\llm.cpp\\ollama-llama-cpp\\ggml\\src\\ggml-sycl\\ggml-sycl.cpp:320\n\n**Environment information**\nwindows 11, ultra7 258v, only has a C drive and no partitions（maybe the reason?）.\n",
      "state": "open",
      "author": "larria",
      "author_type": "User",
      "created_at": "2025-03-20T04:06:00Z",
      "updated_at": "2025-04-25T02:12:19Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 18,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12983/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12983",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12983",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:40.189208",
      "comments": [
        {
          "author": "chbacher",
          "body": "I Have the same problem on Linux with Phi4 and Gemma3 - I fear the ollama version needs to be updated!\n\nThanks.",
          "created_at": "2025-03-20T14:25:11Z"
        },
        {
          "author": "sgwhat",
          "body": "Could you try running `gemma3:4b` to confirm whether it's related to VRAM?",
          "created_at": "2025-03-21T02:22:38Z"
        },
        {
          "author": "chbacher",
          "body": "Loading the phi4-mini results in\n\n`Error: llama runner process has terminated: error loading model: missing tensor 'output.weight'\nllama_load_model_from_file: failed to load model`\n\n`llama_model_load: error loading model: missing tensor 'output.weight'\nllama_load_model_from_file: failed to load mode",
          "created_at": "2025-03-21T15:34:54Z"
        },
        {
          "author": "tsobczynski",
          "body": "I have seen a similar error several times while running `qwen2.5-coder:32b-instruct-q5_K_M`.\n\n`ollama version is 0.5.4-ipexllm-20250228`\n\n```\nError: an error was encountered while running the model: error:CHECK_TRY_ERROR((*stream).memcpy((char *)tensor->data + offset, host_buf, size) .wait()): Meet ",
          "created_at": "2025-03-22T14:30:00Z"
        },
        {
          "author": "larria",
          "body": "> Could you try running `gemma3:4b` to confirm whether it's related to VRAM?\n\nTried gemma3:4b, also failed, but the error information is different:\n\nPS C:\\Soft\\AI\\ollama-ipex> .\\start-ollama.bat\nPS C:\\Soft\\AI\\ollama-ipex> set IPEX_LLM_MODEL_SOURCE=modelscope\nPS C:\\Soft\\AI\\ollama-ipex> .\\ollama.exe r",
          "created_at": "2025-03-23T12:43:17Z"
        }
      ]
    },
    {
      "issue_number": 13100,
      "title": "GLM-4-9B-0414 meet ValueError: The checkpoint you are trying to load has model type `glm4` but Transformers does not recognize this architecture.",
      "body": "**Describe the bug**\ndownload code from https://github.com/intel/ipex-llm/blob/main/python/llm/example/GPU/HuggingFace/LLM/glm4/generate.py\nrename generate.py to generate-glm4-9b.py.\n\n```\n C:\\Users\\1\\Documents\\kai>python generate-glm4-9b.py --repo-id-or-model-path C:\\Users\\1\\Documents\\GLM-4-9B-0414\nTraceback (most recent call last):\n  File \"C:\\Users\\1\\miniforge3\\envs\\kai-xpu\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\", line 1023, in from_pretrained\n    config_class = CONFIG_MAPPING[config_dict[\"model_type\"]]\n                   ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\1\\miniforge3\\envs\\kai-xpu\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\", line 725, in __getitem__\n    raise KeyError(key)\nKeyError: 'glm4'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\1\\Documents\\kai\\generate-glm4-9b.py\", line 56, in <module>\n    model = AutoModel.from_pretrained(model_path,\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\1\\miniforge3\\envs\\kai-xpu\\Lib\\unittest\\mock.py\", line 1378, in patched\n    return func(*newargs, **newkeywargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\1\\miniforge3\\envs\\kai-xpu\\Lib\\site-packages\\ipex_llm\\transformers\\model.py\", line 349, in from_pretrained\n    model = cls.load_convert(q_k, optimize_model, *args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\1\\miniforge3\\envs\\kai-xpu\\Lib\\site-packages\\ipex_llm\\transformers\\model.py\", line 493, in load_convert\n    model = cls.HF_Model.from_pretrained(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\1\\miniforge3\\envs\\kai-xpu\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 526, in from_pretrained\n    config, kwargs = AutoConfig.from_pretrained(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\1\\miniforge3\\envs\\kai-xpu\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\", line 1025, in from_pretrained\n    raise ValueError(\nValueError: The checkpoint you are trying to load has model type `glm4` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n```\nThen update transformers\n```\n>pip install --upgrade transformers\nSuccessfully installed tokenizers-0.21.1 transformers-4.51.3\n\n(kai-xpu) C:\\Users\\1\\Documents\\kai>python generate-glm4-9b.py --repo-id-or-model-path C:\\Users\\1\\Documents\\GLM-4-9B-0414\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 47.81it/s]\n2025-04-21 18:32:44,166 - INFO - Converting the current model to sym_int4 format......\n2025-04-21 18:32:44,230 - INFO - PyTorch version 2.6.0.post0+xpu available.\nTraceback (most recent call last):\n  File \"C:\\Users\\1\\Documents\\kai\\generate-glm4-9b.py\", line 65, in <module>\n    tokenizer = AutoTokenizer.from_pretrained(model_path,\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\1\\miniforge3\\envs\\kai-xpu\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\", line 1009, in from_pretrained\n    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\1\\miniforge3\\envs\\kai-xpu\\Lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2062, in from_pretrained\n    return cls._from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\1\\miniforge3\\envs\\kai-xpu\\Lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2136, in _from_pretrained\n    init_kwargs[\"chat_template\"] = chat_template_handle.read()  # Clobbers any template in the config\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nUnicodeDecodeError: 'gbk' codec can't decode byte 0xaf in position 45: illegal multibyte sequence\n```\n\n\n**How to reproduce**\nSteps to reproduce the error:\n1. install env\n\npip install ipex-llm[xpu_2.6_arl]==2.2.0 --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/arl/cn/\npip install transformers==4.45.0 accelerate==0.26.0 trl==0.11.0 fastapi uvicorn\n\n2. download GLM-4-9B-0414\n\n**Screenshots**\nIf applicable, add screenshots to help explain the problem\nNeed to support new model GLM-4-9B-0414\n\n**Environment information**\n```\nPackage                                  Version\n---------------------------------------- ----------------\naccelerate                               0.26.0\naiocache                                 0.12.3\naiofiles                                 24.1.0\naiohappyeyeballs                         2.6.1\naiohttp                                  3.11.11\naiosignal                                1.3.2\nalembic                                  1.14.0\nannotated-types                          0.7.0\nanthropic                                0.49.0\nanyio                                    4.9.0\nappdirs                                  1.4.4\nAPScheduler                              3.10.4\nargon2-cffi                              23.1.0\nargon2-cffi-bindings                     21.2.0\nasgiref                                  3.8.1\nasync-timeout                            5.0.1\nattrs                                    25.3.0\nAuthlib                                  1.4.1\nav                                       14.2.0\nazure-ai-documentintelligence            1.0.0\nazure-core                               1.32.0\nazure-identity                           1.20.0\nazure-storage-blob                       12.24.1\nbackoff                                  2.2.1\nbcrypt                                   4.3.0\nbeautifulsoup4                           4.13.3\nbidict                                   0.23.1\nbigdl-core-xe-all                        2.6.0\nbitarray                                 3.2.0\nblack                                    25.1.0\nblinker                                  1.9.0\nboto3                                    1.35.53\nbotocore                                 1.35.99\nbuild                                    1.2.2.post1\ncachetools                               5.5.2\ncertifi                                  2025.1.31\ncffi                                     1.17.1\nchardet                                  5.2.0\ncharset-normalizer                       3.4.1\nchroma-hnswlib                           0.7.6\nchromadb                                 0.6.2\nclick                                    8.1.8\ncolbert-ai                               0.2.21\ncolorama                                 0.4.6\ncolorclass                               2.2.2\ncoloredlogs                              15.0.1\ncompressed_rtf                           1.0.6\ncryptography                             44.0.2\nctranslate2                              4.5.0\ndataclasses-json                         0.6.7\ndatasets                                 3.4.1\ndefusedxml                               0.7.1\nDeprecated                               1.2.18\ndill                                     0.3.8\ndistro                                   1.9.0\ndnspython                                2.7.0\ndocker                                   7.1.0\ndocstring_parser                         0.16\ndocx2txt                                 0.8\ndpcpp-cpp-rt                             2025.0.2\nduckduckgo_search                        7.3.2\ndurationpy                               0.9\neasygui                                  0.98.3\nebcdic                                   1.1.1\necdsa                                    0.19.1\neinops                                   0.8.0\nelastic-transport                        8.17.1\nelasticsearch                            8.17.1\nemoji                                    2.14.1\net_xmlfile                               2.0.0\neval_type_backport                       0.2.2\nEvents                                   0.5\nextract-msg                              0.54.0\nfake-useragent                           1.5.1\nfastapi                                  0.115.7\nfaster-whisper                           1.1.1\nfilelock                                 3.18.0\nfiletype                                 1.2.0\nfirecrawl-py                             1.12.0\nFlask                                    3.1.0\nflatbuffers                              25.2.10\nfonttools                                4.56.0\nfpdf2                                    2.8.2\nfrozenlist                               1.5.0\nfs                                       2.4.16\nfsspec                                   2024.12.0\nftfy                                     6.2.3\ngcp-storage-emulator                     2024.8.3\ngit-python                               1.0.3\ngitdb                                    4.0.12\nGitPython                                3.1.44\ngoogle-ai-generativelanguage             0.6.6\ngoogle-api-core                          2.24.2\ngoogle-api-python-client                 2.165.0\ngoogle-auth                              2.38.0\ngoogle-auth-httplib2                     0.2.0\ngoogle-auth-oauthlib                     1.2.1\ngoogle-cloud-core                        2.4.3\ngoogle-cloud-storage                     2.19.0\ngoogle-crc32c                            1.7.0\ngoogle-generativeai                      0.7.2\ngoogle-resumable-media                   2.7.2\ngoogleapis-common-protos                 1.63.2\ngreenlet                                 3.1.1\ngrpcio                                   1.67.1\ngrpcio-status                            1.63.0rc1\ngrpcio-tools                             1.62.3\nh11                                      0.14.0\nh2                                       4.2.0\nhpack                                    4.1.0\nhtml5lib                                 1.1\nhttpcore                                 1.0.7\nhttplib2                                 0.22.0\nhttptools                                0.6.4\nhttpx                                    0.28.1\nhttpx-sse                                0.4.0\nhuggingface-hub                          0.30.2\nhumanfriendly                            10.0\nhyperframe                               6.1.0\nidna                                     3.10\nimportlib_metadata                       8.4.0\nimportlib_resources                      6.5.2\niniconfig                                2.1.0\nintel-cmplr-lib-rt                       2025.0.2\nintel-cmplr-lib-ur                       2025.0.2\nintel-cmplr-lic-rt                       2025.0.2\nintel-opencl-rt                          2025.0.2\nintel-openmp                             2025.0.2\nintel-sycl-rt                            2025.0.2\nipex-llm                                 2.2.0\nisodate                                  0.7.2\nitsdangerous                             2.2.0\nJinja2                                   3.1.6\njiter                                    0.9.0\njmespath                                 1.0.1\njoblib                                   1.4.2\njsonpatch                                1.33\njsonpointer                              3.0.0\nkubernetes                               32.0.1\nlangchain                                0.3.19\nlangchain-community                      0.3.18\nlangchain-core                           0.3.47\nlangchain-text-splitters                 0.3.7\nlangdetect                               1.0.9\nlangfuse                                 2.44.0\nlangsmith                                0.3.18\nlark                                     1.1.9\nldap3                                    2.9.1\nloguru                                   0.7.2\nlxml                                     5.3.1\nMako                                     1.3.9\nMarkdown                                 3.7\nmarkdown-it-py                           3.0.0\nMarkupSafe                               3.0.2\nmarshmallow                              3.26.1\nmdurl                                    0.1.2\nmmh3                                     5.1.0\nmonotonic                                1.6\nmoto                                     5.1.1\nmpmath                                   1.3.0\nmsal                                     1.32.0\nmsal-extensions                          1.3.1\nmsoffcrypto-tool                         5.4.2\nmultidict                                6.2.0\nmultiprocess                             0.70.16\nmypy-extensions                          1.0.0\nnest-asyncio                             1.6.0\nnetworkx                                 3.4.2\nninja                                    1.11.1.4\nnltk                                     3.9.1\nnumpy                                    1.26.4\noauthlib                                 3.2.2\nolefile                                  0.47\noletools                                 0.60.2\nonednn                                   2025.0.1\nonednn-devel                             2025.0.1\nonnxruntime                              1.21.0\nopen-webui                               0.5.20\nopenai                                   1.68.2\nopencv-python                            4.11.0.86\nopencv-python-headless                   4.11.0.86\nopenpyxl                                 3.1.5\nopensearch-py                            2.8.0\nopentelemetry-api                        1.27.0\nopentelemetry-exporter-otlp-proto-common 1.27.0\nopentelemetry-exporter-otlp-proto-grpc   1.27.0\nopentelemetry-instrumentation            0.48b0\nopentelemetry-instrumentation-asgi       0.48b0\nopentelemetry-instrumentation-fastapi    0.48b0\nopentelemetry-proto                      1.27.0\nopentelemetry-sdk                        1.27.0\nopentelemetry-semantic-conventions       0.48b0\nopentelemetry-util-http                  0.48b0\norjson                                   3.10.15\noverrides                                7.7.0\npackaging                                23.2\npandas                                   2.2.3\npasslib                                  1.7.4\npathspec                                 0.12.1\npcodedmp                                 1.2.6\npeewee                                   3.17.9\npeewee-migrate                           1.12.2\npgvector                                 0.3.5\npillow                                   11.1.0\npip                                      25.0.1\nplatformdirs                             4.3.7\nplaywright                               1.49.1\npluggy                                   1.5.0\nportalocker                              2.10.1\nposthog                                  3.21.0\nprimp                                    0.14.0\npropcache                                0.3.0\nproto-plus                               1.26.1\nprotobuf                                 4.25.6\npsutil                                   7.0.0\npsycopg2-binary                          2.9.9\npy-cpuinfo                               9.0.0\npy-partiql-parser                        0.6.1\npyarrow                                  19.0.1\npyasn1                                   0.4.8\npyasn1_modules                           0.4.1\npyclipper                                1.3.0.post6\npycparser                                2.22\npydantic                                 2.10.6\npydantic_core                            2.27.2\npydantic-settings                        2.8.1\npydub                                    0.25.1\npyee                                     12.0.0\nPygments                                 2.19.1\nPyJWT                                    2.10.1\npymdown-extensions                       10.14.2\npymilvus                                 2.5.0\npymongo                                  4.11.3\nPyMySQL                                  1.1.1\npypandoc                                 1.13\npyparsing                                3.2.2\npypdf                                    4.3.1\nPyPika                                   0.48.9\npyproject_hooks                          1.2.0\npyreadline3                              3.5.4\npytest                                   8.3.5\npytest-docker                            3.1.2\npython-dateutil                          2.9.0.post0\npython-dotenv                            1.0.1\npython-engineio                          4.11.2\npython-iso639                            2025.2.18\npython-jose                              3.4.0\npython-magic                             0.4.27\npython-multipart                         0.0.18\npython-oxmsg                             0.0.2\npython-pptx                              1.0.0\npython-socketio                          5.11.3\npytube                                   15.0.0\npytz                                     2025.1\npywin32                                  310\npyxlsb                                   1.0.10\nPyYAML                                   6.0.2\nqdrant-client                            1.12.2\nrank-bm25                                0.2.2\nRapidFuzz                                3.12.2\nrapidocr-onnxruntime                     1.3.24\nred-black-tree-mod                       1.22\nredis                                    5.2.1\nregex                                    2024.11.6\nrequests                                 2.32.3\nrequests-oauthlib                        2.0.0\nrequests-toolbelt                        1.0.0\nresponses                                0.25.7\nRestrictedPython                         8.0\nrich                                     13.9.4\nrsa                                      4.9\nRTFDE                                    0.1.2\ns3transfer                               0.10.4\nsafetensors                              0.5.3\nscikit-learn                             1.6.1\nscipy                                    1.15.2\nsentence-transformers                    3.3.1\nsentencepiece                            0.2.0\nsetuptools                               75.8.2\nshapely                                  2.0.7\nshellingham                              1.5.4\nshtab                                    1.7.1\nsimple-websocket                         1.1.0\nsix                                      1.17.0\nsmmap                                    5.0.2\nsniffio                                  1.3.1\nsoundfile                                0.13.1\nsoupsieve                                2.6\nSQLAlchemy                               2.0.38\nstarlette                                0.45.3\nsympy                                    1.13.1\ntabulate                                 0.9.0\ntbb                                      2022.0.0\ntcmlib                                   1.2.0\ntenacity                                 9.0.0\nthreadpoolctl                            3.6.0\ntiktoken                                 0.9.0\ntokenizers                               0.20.3\ntorch                                    2.6.0.post0+xpu\ntorchaudio                               2.6.0.post0+xpu\ntorchvision                              0.21.0.post0+xpu\ntqdm                                     4.67.1\ntransformers                             4.45.0\ntrl                                      0.11.0\ntypeguard                                4.4.2\ntyper                                    0.15.2\ntyping_extensions                        4.13.0rc1\ntyping-inspect                           0.9.0\ntyping-inspection                        0.4.0\ntyro                                     0.9.17\ntzdata                                   2025.1\ntzlocal                                  5.3.1\nujson                                    5.10.0\numf                                      0.9.1\nunstructured                             0.16.17\nunstructured-client                      0.31.3\nuritemplate                              4.1.1\nurllib3                                  2.3.0\nuvicorn                                  0.34.0\nvalidators                               0.34.0\nwatchfiles                               1.0.4\nwcwidth                                  0.2.13\nwebencodings                             0.5.1\nwebsocket-client                         1.8.0\nwebsockets                               15.0.1\nWerkzeug                                 3.1.3\nwheel                                    0.45.1\nwin_unicode_console                      0.5\nwin32_setctime                           1.2.0\nwrapt                                    1.17.2\nwsproto                                  1.2.0\nxlrd                                     2.0.1\nXlsxWriter                               3.2.2\nxmltodict                                0.14.2\nxxhash                                   3.5.0\nyarl                                     1.18.3\nyoutube-transcript-api                   0.6.3\nzipp                                     3.21.0\nzstandard                                0.23.0\n\n\n```",
      "state": "open",
      "author": "KiwiHana",
      "author_type": "User",
      "created_at": "2025-04-21T10:34:00Z",
      "updated_at": "2025-04-23T06:11:36Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13100/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13100",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13100",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:40.406637",
      "comments": [
        {
          "author": "qiyuangong",
          "body": "Hi @KiwiHana \n\nAre you using GLM-4, which is downloaded from this link: https://huggingface.co/THUDM/glm-4-9b-chat?\n\nSeems they made some changes recently, e.g., https://huggingface.co/THUDM/glm-4-9b-chat/commit/bd8234fe5e0c09c48637a92abb0c797cb5fa0e73 .\n",
          "created_at": "2025-04-22T13:37:46Z"
        },
        {
          "author": "KiwiHana",
          "body": "Hi Qiyuan, @qiyuangong \nI download model from https://www.modelscope.cn/models/ZhipuAI/GLM-4-9B-0414",
          "created_at": "2025-04-23T01:22:56Z"
        },
        {
          "author": "qiyuangong",
          "body": "> Hi Qiyuan, [@qiyuangong](https://github.com/qiyuangong) I download model from https://www.modelscope.cn/models/ZhipuAI/GLM-4-9B-0414\n\nThank you @KiwiHana \nThis model is also a new version. We will try to reproduce this issue in our local env.",
          "created_at": "2025-04-23T06:11:34Z"
        }
      ]
    },
    {
      "issue_number": 13084,
      "title": "IPEX-llm B16 release long Mean TPOT (ms) with QwQ 32B (1k input, 4k output)",
      "body": "**Describe the bug**\nWith IPEX-LLM B16 release on 4 ARC 770, benchmark test QwQ 32B (1k input, 4k output).\nthe result show high Mean TPOT (ms)\n\n\n**How to reproduce**\nSteps to reproduce the error:\n1. download the B16 docker image.\n2. start the vllm serving with the following command.\n```\nMODEL_PATH=${MODEL_PATH:-\"default_model_path\"}\nSERVED_MODEL_NAME=${SERVED_MODEL_NAME:-\"default_model_name\"}\nTENSOR_PARALLEL_SIZE=${TENSOR_PARALLEL_SIZE:-1}  # Default to 1 if not set\n\necho \"Starting service with model: $MODEL_PATH\"\necho \"Served model name: $SERVED_MODEL_NAME\"\necho \"Tensor parallel size: $TENSOR_PARALLEL_SIZE\"\n\nexport CCL_WORKER_COUNT=2\nexport SYCL_CACHE_PERSISTENT=1\nexport FI_PROVIDER=shm\nexport CCL_ATL_TRANSPORT=ofi\nexport CCL_ZE_IPC_EXCHANGE=sockets\nexport CCL_ATL_SHM=1\n\nexport USE_XETLA=OFF\nexport SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=2\nexport TORCH_LLM_ALLREDUCE=0\n\nexport CCL_SAME_STREAM=1\nexport CCL_BLOCKING_WAIT=0\n\nsource /opt/intel/1ccl-wks/setvars.sh\n\npython -m ipex_llm.vllm.xpu.entrypoints.openai.api_server \\\n  --served-model-name $SERVED_MODEL_NAME \\\n  --port 8000 \\\n  --model $MODEL_PATH \\\n  --trust-remote-code \\\n  --block-size 8 \\\n  --gpu-memory-utilization 0.95 \\\n  --device xpu \\\n  --dtype float16 \\\n  --enforce-eager \\\n  --load-in-low-bit fp8 \\\n  --max-model-len 5500 \\\n  --max-num-batched-tokens 6000 \\\n  --max-num-seqs 256 \\\n  --tensor-parallel-size $TENSOR_PARALLEL_SIZE \\\n  --disable-async-output-proc \\\n  --distributed-executor-backend ray\n```\n\n3. ...\nrun the following benchmark\n`python /llm/vllm/benchmarks/benchmark_serving.py --model /llm/models/$models  --dataset-name random --trust_remote_code --num_prompt $i --random-input-len=$inputsize --random-output-len=$outputsize >> $FILE 2>&1`\n4. check the result\n\n```\n============ Serving Benchmark Result ============\nSuccessful requests:                     8         \nBenchmark duration (s):                  248.63    \nTotal input tokens:                      8192      \nTotal generated tokens:                  28652     \nRequest throughput (req/s):              0.03      \nOutput token throughput (tok/s):         115.24    \nTotal Token throughput (tok/s):          148.19    \n---------------Time to First Token----------------\nMean TTFT (ms):                          3056.11   \nMedian TTFT (ms):                        2644.66   \nP99 TTFT (ms):                           3779.86   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          30657.55  \nMedian TPOT (ms):                        60.07     \nP99 TPOT (ms):                           227705.54 \n---------------Inter-token Latency----------------\nMean ITL (ms):                           60.05     \nMedian ITL (ms):                         59.62     \nP99 ITL (ms):                            68.36     \n==================================================\n```",
      "state": "closed",
      "author": "oldmikeyang",
      "author_type": "User",
      "created_at": "2025-04-16T07:58:44Z",
      "updated_at": "2025-04-22T23:34:15Z",
      "closed_at": "2025-04-22T23:34:14Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13084/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hzjane"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13084",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13084",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:40.641214",
      "comments": [
        {
          "author": "flekol",
          "body": "Hi @oldmikeyang,\n\ncan you write what configuration do you have?\nI have a 4xArc 770 system with absymal performance comparing to you... (output token throughput is at 15 tok/s)\nwith EPYC 7282 \n\nI am also about to raise a ticket\n",
          "created_at": "2025-04-16T13:45:41Z"
        },
        {
          "author": "hzjane",
          "body": "@oldmikeyang This is a problem with the QwQ model. Some requests may generate empty outputs(on vllm serving service, it generate many token=29333, but all the generated 29333 tokens are empty to return). The calculation formula [here](https://github.com/analytics-zoo/vllm/blob/0.6.6/benchmarks/bench",
          "created_at": "2025-04-17T07:02:11Z"
        },
        {
          "author": "oldmikeyang",
          "body": "Yes. The issue was due to the random input prompt in the test script.\nFix this issue with the benchmark_serving_input.py test script.",
          "created_at": "2025-04-22T23:34:14Z"
        }
      ]
    },
    {
      "issue_number": 12513,
      "title": "GPU Runner crash in Ollama when offloading multiple layers",
      "body": "Hi,\r\n\r\nI experience crashes of the gpu runner when offloading multiple layers to the gpu. \r\n\r\n```\r\ntime=2024-12-09T00:58:03.646+08:00 level=INFO source=server.go:629 msg=\"waiting for server to become available\" status=\"llm server not responding\"\r\ntime=2024-12-09T00:58:04.348+08:00 level=ERROR source=sched.go:456 msg=\"error loading llama server\" error=\"llama runner process has terminated: signal: bus error (core dumped)\"\r\n[GIN] 2024/12/09 - 00:58:04 | 500 |  1.520528721s |      172.16.6.3 | POST     \"/api/chat\"\r\ntime=2024-12-09T00:58:04.348+08:00 level=DEBUG source=sched.go:459 msg=\"triggering expiration for failed load\" model=/root/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff\r\n``` \r\n\r\nIt seems to work for one layer. The error message is not really helpful. The GPU is small (4gb A310) but so is the model (llama 3.2@3.2 params., 1.87 GiB model size). VRAM shouldn't be the problem. \r\n\r\nI use docker on Debian on kernel 6.6.44 with the following docker compose:\r\n\r\n```\r\n  ipex-llm:\r\n    image: intelanalytics/ipex-llm-inference-cpp-xpu:latest\r\n    container_name: ollama\r\n    restart: unless-stopped\r\n    networks:\r\n       - backend\r\n    command: >\r\n      /bin/bash -c \"\r\n        sycl-ls &&\r\n        source ipex-llm-init --gpu --device Arc &&\r\n\r\n        bash ./scripts/start-ollama.sh && # run the scripts\r\n        kill $(pgrep -f ollama) && # kill background ollama\r\n        /llm/ollama/ollama serve # run foreground ollama\r\n      \"\r\n    devices:\r\n      - /dev/dri\r\n    volumes:\r\n      - /dev/dri:/dev/dri\r\n      - /mnt/fast_storage/docker/ollama:/root/.ollama\r\n    environment:\r\n      DEVICE: Arc\r\n      NEOReadDebugKeys: 1\r\n      OverrideGpuAddressSpace: 48\r\n      ZES_ENABLE_SYSMAN: 1\r\n      OLLAMA_DEBUG: 1\r\n      #OLLAMA_INTEL_GPU: 1\r\n      OLLAMA_NUM_PARALLEL: 1\r\n      OLLAMA_HOST: 0.0.0.0\r\n      OLLAMA_NUM_GPU: 999 # layers to offload -> this is the problem \r\n      SYCL_CACHE_PERSISTENT: 1\r\n      SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS: 1\r\n      ONEAPI_DEVICE_SELECTOR: level_zero=gpu:0 \r\n``` \r\n\r\nAny ideas for further debugging? Full logs below.\r\n\r\n```\r\nWarning: ONEAPI_DEVICE_SELECTOR environment variable is set to level_zero=gpu:0.\r\nTo see the correct device id, please unset ONEAPI_DEVICE_SELECTOR.\r\n[ext_oneapi_level_zero:gpu:0] Intel(R) Level-Zero, Intel(R) Arc(TM) A310 LP Graphics 1.6 [1.3.31294]\r\nfound oneapi in /opt/intel/oneapi/setvars.sh\r\n \r\n:: initializing oneAPI environment ...\r\n   bash: BASH_VERSION = 5.1.16(1)-release\r\n   args: Using \"$@\" for setvars.sh arguments: --force\r\n:: advisor -- latest\r\n:: ccl -- latest\r\n:: compiler -- latest\r\n:: dal -- latest\r\n:: debugger -- latest\r\n:: dev-utilities -- latest\r\n:: dnnl -- latest\r\n:: dpcpp-ct -- latest\r\n:: dpl -- latest\r\n:: ipp -- latest\r\n:: ippcp -- latest\r\n:: mkl -- latest\r\n:: mpi -- latest\r\n:: tbb -- latest\r\n:: vtune -- latest\r\n:: oneAPI environment initialized ::\r\n \r\n/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\r\n  _torch_pytree._register_pytree_node(\r\n/usr/local/lib/python3.11/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\r\n  warnings.warn(\r\n/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\r\n  _torch_pytree._register_pytree_node(\r\n/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\r\n  _torch_pytree._register_pytree_node(\r\n+++++ Env Variables +++++\r\n\u0001\u0000\u0000\u0000\u0000\u0000\u0000\r\n:\r\n    ENABLE_IOMP     = 1\r\n    ENABLE_GPU      = 1\r\n    ENABLE_JEMALLOC = 0\r\n    ENABLE_TCMALLOC = 0\r\n    LIB_DIR    = /usr/local/lib\r\n    BIN_DIR    = bin64\r\n    LLM_DIR    = /usr/local/lib/python3.11/dist-packages/ipex_llm\r\n\u0001\u0000\u0000\u0000\u0000\u0000\u0000\r\n:\r\n    LD_PRELOAD             = \r\n    OMP_NUM_THREADS        = \r\n    MALLOC_CONF            = \r\n    USE_XETLA              = OFF\r\n    ENABLE_SDP_FUSION      = \r\n    SYCL_CACHE_PERSISTENT  = 1\r\n    BIGDL_LLM_XMX_DISABLED = \r\n    SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS = 1\r\n+++++++++++++++++++++++++\r\n\u0001\u0000\u0000\u0000\u0000\u0000\u0000\r\n.\r\n2024/12/09 00:57:46 routes.go:1125: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:true OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]\"\r\ntime=2024-12-09T00:57:46.256+08:00 level=INFO source=images.go:753 msg=\"total blobs: 42\"\r\ntime=2024-12-09T00:57:46.257+08:00 level=INFO source=images.go:760 msg=\"total unused blobs removed: 0\"\r\n[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.\r\n[GIN-debug] [WARNING] Running in \"debug\" mode. Switch to \"release\" mode in production.\r\n - using env:\texport GIN_MODE=release\r\n - using code:\tgin.SetMode(gin.ReleaseMode)\r\n[GIN-debug] POST   /api/pull                 --> github.com/ollama/ollama/server.(*Server).PullModelHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/generate             --> github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/chat                 --> github.com/ollama/ollama/server.(*Server).ChatHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/embed                --> github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/embeddings           --> github.com/ollama/ollama/server.(*Server).EmbeddingsHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/create               --> github.com/ollama/ollama/server.(*Server).CreateModelHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/push                 --> github.com/ollama/ollama/server.(*Server).PushModelHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/copy                 --> github.com/ollama/ollama/server.(*Server).CopyModelHandler-fm (5 handlers)\r\n[GIN-debug] DELETE /api/delete               --> github.com/ollama/ollama/server.(*Server).DeleteModelHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/show                 --> github.com/ollama/ollama/server.(*Server).ShowModelHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/blobs/:digest        --> github.com/ollama/ollama/server.(*Server).CreateBlobHandler-fm (5 handlers)\r\n[GIN-debug] HEAD   /api/blobs/:digest        --> github.com/ollama/ollama/server.(*Server).HeadBlobHandler-fm (5 handlers)\r\n[GIN-debug] GET    /api/ps                   --> github.com/ollama/ollama/server.(*Server).ProcessHandler-fm (5 handlers)\r\n[GIN-debug] POST   /v1/chat/completions      --> github.com/ollama/ollama/server.(*Server).ChatHandler-fm (6 handlers)\r\n[GIN-debug] POST   /v1/completions           --> github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (6 handlers)\r\n[GIN-debug] POST   /v1/embeddings            --> github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (6 handlers)\r\n[GIN-debug] GET    /v1/models                --> github.com/ollama/ollama/server.(*Server).ListModelsHandler-fm (6 handlers)\r\ntime=2024-12-09T00:57:46.257+08:00 level=INFO source=routes.go:1172 msg=\"Listening on [::]:11434 (version 0.3.6-ipexllm-20241204)\"\r\n[GIN-debug] GET    /v1/models/:model         --> github.com/ollama/ollama/server.(*Server).ShowModelHandler-fm (6 handlers)\r\n[GIN-debug] GET    /                         --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\r\n[GIN-debug] GET    /api/tags                 --> github.com/ollama/ollama/server.(*Server).ListModelsHandler-fm (5 handlers)\r\n[GIN-debug] GET    /api/version              --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\r\n[GIN-debug] HEAD   /                         --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\r\n[GIN-debug] HEAD   /api/tags                 --> github.com/ollama/ollama/server.(*Server).ListModelsHandler-fm (5 handlers)\r\n[GIN-debug] HEAD   /api/version              --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\r\ntime=2024-12-09T00:57:46.257+08:00 level=INFO source=payload.go:30 msg=\"extracting embedded files\" dir=/tmp/ollama2506652849/runners\r\ntime=2024-12-09T00:57:46.258+08:00 level=DEBUG source=payload.go:182 msg=extracting variant=cpu file=build/linux/x86_64/cpu/bin/libggml.so.gz\r\ntime=2024-12-09T00:57:46.258+08:00 level=DEBUG source=payload.go:182 msg=extracting variant=cpu file=build/linux/x86_64/cpu/bin/libllama.so.gz\r\ntime=2024-12-09T00:57:46.258+08:00 level=DEBUG source=payload.go:182 msg=extracting variant=cpu file=build/linux/x86_64/cpu/bin/ollama_llama_server.gz\r\ntime=2024-12-09T00:57:46.258+08:00 level=DEBUG source=payload.go:182 msg=extracting variant=cpu_avx file=build/linux/x86_64/cpu_avx/bin/libggml.so.gz\r\ntime=2024-12-09T00:57:46.258+08:00 level=DEBUG source=payload.go:182 msg=extracting variant=cpu_avx file=build/linux/x86_64/cpu_avx/bin/libllama.so.gz\r\ntime=2024-12-09T00:57:46.258+08:00 level=DEBUG source=payload.go:182 msg=extracting variant=cpu_avx file=build/linux/x86_64/cpu_avx/bin/ollama_llama_server.gz\r\ntime=2024-12-09T00:57:46.258+08:00 level=DEBUG source=payload.go:182 msg=extracting variant=cpu_avx2 file=build/linux/x86_64/cpu_avx2/bin/libggml.so.gz\r\ntime=2024-12-09T00:57:46.258+08:00 level=DEBUG source=payload.go:182 msg=extracting variant=cpu_avx2 file=build/linux/x86_64/cpu_avx2/bin/libllama.so.gz\r\ntime=2024-12-09T00:57:46.258+08:00 level=DEBUG source=payload.go:182 msg=extracting variant=cpu_avx2 file=build/linux/x86_64/cpu_avx2/bin/ollama_llama_server.gz\r\ntime=2024-12-09T00:57:46.395+08:00 level=DEBUG source=payload.go:71 msg=\"availableServers : found\" file=/tmp/ollama2506652849/runners/cpu/ollama_llama_server\r\ntime=2024-12-09T00:57:46.395+08:00 level=DEBUG source=payload.go:71 msg=\"availableServers : found\" file=/tmp/ollama2506652849/runners/cpu_avx/ollama_llama_server\r\ntime=2024-12-09T00:57:46.395+08:00 level=DEBUG source=payload.go:71 msg=\"availableServers : found\" file=/tmp/ollama2506652849/runners/cpu_avx2/ollama_llama_server\r\ntime=2024-12-09T00:57:46.395+08:00 level=INFO source=payload.go:44 msg=\"Dynamic LLM libraries [cpu cpu_avx cpu_avx2]\"\r\ntime=2024-12-09T00:57:46.395+08:00 level=DEBUG source=payload.go:45 msg=\"Override detection logic by setting OLLAMA_LLM_LIBRARY\"\r\ntime=2024-12-09T00:57:46.395+08:00 level=DEBUG source=sched.go:105 msg=\"starting llm scheduler\"\r\n[GIN] 2024/12/09 - 00:57:58 | 200 |    1.402222ms |      172.16.6.3 | GET      \"/api/tags\"\r\n[GIN] 2024/12/09 - 00:57:58 | 200 |      64.402µs |      172.16.6.3 | GET      \"/api/version\"\r\n[GIN] 2024/12/09 - 00:58:02 | 200 |    1.948256ms |      172.16.6.3 | GET      \"/api/tags\"\r\ntime=2024-12-09T00:58:02.875+08:00 level=INFO source=gpu.go:168 msg=\"looking for compatible GPUs\"\r\ntime=2024-12-09T00:58:02.875+08:00 level=WARN source=gpu.go:560 msg=\"unable to locate gpu dependency libraries\"\r\ntime=2024-12-09T00:58:02.875+08:00 level=DEBUG source=gpu.go:79 msg=\"searching for GPU discovery libraries for NVIDIA\"\r\ntime=2024-12-09T00:58:02.875+08:00 level=DEBUG source=gpu.go:382 msg=\"Searching for GPU library\" name=libcuda.so*\r\ntime=2024-12-09T00:58:02.876+08:00 level=WARN source=gpu.go:560 msg=\"unable to locate gpu dependency libraries\"\r\ntime=2024-12-09T00:58:02.876+08:00 level=DEBUG source=gpu.go:405 msg=\"gpu library search\" globs=\"[libcuda.so* /opt/intel/oneapi/tbb/2021.11/lib/intel64/gcc4.8/libcuda.so* /opt/intel/oneapi/mpi/2021.11/opt/mpi/libfabric/lib/libcuda.so* /opt/intel/oneapi/mpi/2021.11/lib/libcuda.so* /opt/intel/oneapi/mkl/2024.0/lib/libcuda.so* /opt/intel/oneapi/ippcp/2021.9/lib/libcuda.so* /opt/intel/oneapi/ipp/2021.10/lib/libcuda.so* /opt/intel/oneapi/dpl/2022.3/lib/libcuda.so* /opt/intel/oneapi/dnnl/2024.0/lib/libcuda.so* /opt/intel/oneapi/debugger/2024.0/opt/debugger/lib/libcuda.so* /opt/intel/oneapi/dal/2024.0/lib/libcuda.so* /opt/intel/oneapi/compiler/2024.0/opt/oclfpga/host/linux64/lib/libcuda.so* /opt/intel/oneapi/compiler/2024.0/opt/compiler/lib/libcuda.so* /opt/intel/oneapi/compiler/2024.0/lib/libcuda.so* /opt/intel/oneapi/ccl/2021.11/lib/libcuda.so* /opt/intel/oneapi/tbb/2021.11/lib/intel64/gcc4.8/libcuda.so* /opt/intel/oneapi/mpi/2021.11/opt/mpi/libfabric/lib/libcuda.so* /opt/intel/oneapi/mpi/2021.11/lib/libcuda.so* /opt/intel/oneapi/mkl/2024.0/lib/libcuda.so* /opt/intel/oneapi/ippcp/2021.9/lib/libcuda.so* /opt/intel/oneapi/ipp/2021.10/lib/libcuda.so* /opt/intel/oneapi/dpl/2022.3/lib/libcuda.so* /opt/intel/oneapi/dnnl/2024.0/lib/libcuda.so* /opt/intel/oneapi/debugger/2024.0/opt/debugger/lib/libcuda.so* /opt/intel/oneapi/dal/2024.0/lib/libcuda.so* /opt/intel/oneapi/compiler/2024.0/opt/oclfpga/host/linux64/lib/libcuda.so* /opt/intel/oneapi/compiler/2024.0/opt/compiler/lib/libcuda.so* /opt/intel/oneapi/compiler/2024.0/lib/libcuda.so* /opt/intel/oneapi/ccl/2021.11/lib/libcuda.so* /usr/local/cuda*/targets/*/lib/libcuda.so* /usr/lib/*-linux-gnu/nvidia/current/libcuda.so* /usr/lib/*-linux-gnu/libcuda.so* /usr/lib/wsl/lib/libcuda.so* /usr/lib/wsl/drivers/*/libcuda.so* /opt/cuda/lib*/libcuda.so* /usr/local/cuda/lib*/libcuda.so* /usr/lib*/libcuda.so* /usr/local/lib*/libcuda.so*]\"\r\ntime=2024-12-09T00:58:02.880+08:00 level=DEBUG source=gpu.go:439 msg=\"discovered GPU libraries\" paths=[]\r\ntime=2024-12-09T00:58:02.880+08:00 level=DEBUG source=gpu.go:382 msg=\"Searching for GPU library\" name=libcudart.so*\r\ntime=2024-12-09T00:58:02.880+08:00 level=WARN source=gpu.go:560 msg=\"unable to locate gpu dependency libraries\"\r\ntime=2024-12-09T00:58:02.880+08:00 level=DEBUG source=gpu.go:405 msg=\"gpu library search\" globs=\"[libcudart.so* /opt/intel/oneapi/tbb/2021.11/lib/intel64/gcc4.8/libcudart.so* /opt/intel/oneapi/mpi/2021.11/opt/mpi/libfabric/lib/libcudart.so* /opt/intel/oneapi/mpi/2021.11/lib/libcudart.so* /opt/intel/oneapi/mkl/2024.0/lib/libcudart.so* /opt/intel/oneapi/ippcp/2021.9/lib/libcudart.so* /opt/intel/oneapi/ipp/2021.10/lib/libcudart.so* /opt/intel/oneapi/dpl/2022.3/lib/libcudart.so* /opt/intel/oneapi/dnnl/2024.0/lib/libcudart.so* /opt/intel/oneapi/debugger/2024.0/opt/debugger/lib/libcudart.so* /opt/intel/oneapi/dal/2024.0/lib/libcudart.so* /opt/intel/oneapi/compiler/2024.0/opt/oclfpga/host/linux64/lib/libcudart.so* /opt/intel/oneapi/compiler/2024.0/opt/compiler/lib/libcudart.so* /opt/intel/oneapi/compiler/2024.0/lib/libcudart.so* /opt/intel/oneapi/ccl/2021.11/lib/libcudart.so* /opt/intel/oneapi/tbb/2021.11/lib/intel64/gcc4.8/libcudart.so* /opt/intel/oneapi/mpi/2021.11/opt/mpi/libfabric/lib/libcudart.so* /opt/intel/oneapi/mpi/2021.11/lib/libcudart.so* /opt/intel/oneapi/mkl/2024.0/lib/libcudart.so* /opt/intel/oneapi/ippcp/2021.9/lib/libcudart.so* /opt/intel/oneapi/ipp/2021.10/lib/libcudart.so* /opt/intel/oneapi/dpl/2022.3/lib/libcudart.so* /opt/intel/oneapi/dnnl/2024.0/lib/libcudart.so* /opt/intel/oneapi/debugger/2024.0/opt/debugger/lib/libcudart.so* /opt/intel/oneapi/dal/2024.0/lib/libcudart.so* /opt/intel/oneapi/compiler/2024.0/opt/oclfpga/host/linux64/lib/libcudart.so* /opt/intel/oneapi/compiler/2024.0/opt/compiler/lib/libcudart.so* /opt/intel/oneapi/compiler/2024.0/lib/libcudart.so* /opt/intel/oneapi/ccl/2021.11/lib/libcudart.so* /tmp/ollama2506652849/runners/cuda*/libcudart.so* /usr/local/cuda/lib64/libcudart.so* /usr/lib/x86_64-linux-gnu/nvidia/current/libcudart.so* /usr/lib/x86_64-linux-gnu/libcudart.so* /usr/lib/wsl/lib/libcudart.so* /usr/lib/wsl/drivers/*/libcudart.so* /opt/cuda/lib64/libcudart.so* /usr/local/cuda*/targets/aarch64-linux/lib/libcudart.so* /usr/lib/aarch64-linux-gnu/nvidia/current/libcudart.so* /usr/lib/aarch64-linux-gnu/libcudart.so* /usr/local/cuda/lib*/libcudart.so* /usr/lib*/libcudart.so* /usr/local/lib*/libcudart.so*]\"\r\ntime=2024-12-09T00:58:02.882+08:00 level=DEBUG source=gpu.go:439 msg=\"discovered GPU libraries\" paths=[]\r\ntime=2024-12-09T00:58:02.882+08:00 level=DEBUG source=amd_linux.go:371 msg=\"amdgpu driver not detected /sys/module/amdgpu\"\r\ntime=2024-12-09T00:58:02.882+08:00 level=INFO source=gpu.go:280 msg=\"no compatible GPUs were discovered\"\r\ntime=2024-12-09T00:58:02.882+08:00 level=DEBUG source=sched.go:181 msg=\"updating default concurrency\" OLLAMA_MAX_LOADED_MODELS=0x83a520 gpu_count=1\r\ntime=2024-12-09T00:58:02.941+08:00 level=DEBUG source=sched.go:211 msg=\"cpu mode with first model, loading\"\r\ntime=2024-12-09T00:58:02.941+08:00 level=DEBUG source=server.go:101 msg=\"system memory\" total=\"62.7 GiB\" free=\"23.5 GiB\" free_swap=\"0 B\"\r\ntime=2024-12-09T00:58:02.941+08:00 level=DEBUG source=payload.go:71 msg=\"availableServers : found\" file=/tmp/ollama2506652849/runners/cpu/ollama_llama_server\r\ntime=2024-12-09T00:58:02.941+08:00 level=DEBUG source=payload.go:71 msg=\"availableServers : found\" file=/tmp/ollama2506652849/runners/cpu_avx/ollama_llama_server\r\ntime=2024-12-09T00:58:02.941+08:00 level=DEBUG source=payload.go:71 msg=\"availableServers : found\" file=/tmp/ollama2506652849/runners/cpu_avx2/ollama_llama_server\r\ntime=2024-12-09T00:58:02.941+08:00 level=DEBUG source=memory.go:101 msg=evaluating library=cpu gpu_count=1 available=\"[23.5 GiB]\"\r\ntime=2024-12-09T00:58:02.941+08:00 level=INFO source=memory.go:309 msg=\"offload to cpu\" layers.requested=-1 layers.model=29 layers.offload=0 layers.split=\"\" memory.available=\"[23.5 GiB]\" memory.required.full=\"2.3 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"224.0 MiB\" memory.required.allocations=\"[2.3 GiB]\" memory.weights.total=\"1.8 GiB\" memory.weights.repeating=\"1.5 GiB\" memory.weights.nonrepeating=\"308.2 MiB\" memory.graph.full=\"124.0 MiB\" memory.graph.partial=\"570.7 MiB\"\r\ntime=2024-12-09T00:58:02.942+08:00 level=DEBUG source=payload.go:71 msg=\"availableServers : found\" file=/tmp/ollama2506652849/runners/cpu/ollama_llama_server\r\ntime=2024-12-09T00:58:02.942+08:00 level=DEBUG source=payload.go:71 msg=\"availableServers : found\" file=/tmp/ollama2506652849/runners/cpu_avx/ollama_llama_server\r\ntime=2024-12-09T00:58:02.942+08:00 level=DEBUG source=payload.go:71 msg=\"availableServers : found\" file=/tmp/ollama2506652849/runners/cpu_avx2/ollama_llama_server\r\ntime=2024-12-09T00:58:02.943+08:00 level=DEBUG source=gpu.go:531 msg=\"no filter required for library cpu\"\r\ntime=2024-12-09T00:58:02.943+08:00 level=INFO source=server.go:395 msg=\"starting llama server\" cmd=\"/tmp/ollama2506652849/runners/cpu_avx2/ollama_llama_server --model /root/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 2048 --batch-size 512 --embedding --log-disable --n-gpu-layers 999 --verbose --no-mmap --parallel 1 --port 41009\"\r\ntime=2024-12-09T00:58:02.943+08:00 level=DEBUG source=server.go:412 msg=subprocess environment=\"[LD_LIBRARY_PATH=/tmp/ollama2506652849/runners/cpu_avx2:/opt/intel/oneapi/tbb/2021.11/env/../lib/intel64/gcc4.8:/opt/intel/oneapi/mpi/2021.11/opt/mpi/libfabric/lib:/opt/intel/oneapi/mpi/2021.11/lib:/opt/intel/oneapi/mkl/2024.0/lib:/opt/intel/oneapi/ippcp/2021.9/lib/:/opt/intel/oneapi/ipp/2021.10/lib:/opt/intel/oneapi/dpl/2022.3/lib:/opt/intel/oneapi/dnnl/2024.0/lib:/opt/intel/oneapi/debugger/2024.0/opt/debugger/lib:/opt/intel/oneapi/dal/2024.0/lib:/opt/intel/oneapi/compiler/2024.0/opt/oclfpga/host/linux64/lib:/opt/intel/oneapi/compiler/2024.0/opt/compiler/lib:/opt/intel/oneapi/compiler/2024.0/lib:/opt/intel/oneapi/ccl/2021.11/lib/:/opt/intel/oneapi/tbb/2021.11/env/../lib/intel64/gcc4.8:/opt/intel/oneapi/mpi/2021.11/opt/mpi/libfabric/lib:/opt/intel/oneapi/mpi/2021.11/lib:/opt/intel/oneapi/mkl/2024.0/lib:/opt/intel/oneapi/ippcp/2021.9/lib/:/opt/intel/oneapi/ipp/2021.10/lib:/opt/intel/oneapi/dpl/2022.3/lib:/opt/intel/oneapi/dnnl/2024.0/lib:/opt/intel/oneapi/debugger/2024.0/opt/debugger/lib:/opt/intel/oneapi/dal/2024.0/lib:/opt/intel/oneapi/compiler/2024.0/opt/oclfpga/host/linux64/lib:/opt/intel/oneapi/compiler/2024.0/opt/compiler/lib:/opt/intel/oneapi/compiler/2024.0/lib:/opt/intel/oneapi/ccl/2021.11/lib/ PATH=/opt/intel/oneapi/vtune/2024.0/bin64:/opt/intel/oneapi/mpi/2021.11/opt/mpi/libfabric/bin:/opt/intel/oneapi/mpi/2021.11/bin:/opt/intel/oneapi/mkl/2024.0/bin/:/opt/intel/oneapi/dpcpp-ct/2024.0/bin:/opt/intel/oneapi/dev-utilities/2024.0/bin:/opt/intel/oneapi/debugger/2024.0/opt/debugger/bin:/opt/intel/oneapi/compiler/2024.0/opt/oclfpga/bin:/opt/intel/oneapi/compiler/2024.0/bin:/opt/intel/oneapi/advisor/2024.0/bin64:/opt/intel/oneapi/vtune/2024.0/bin64:/opt/intel/oneapi/mpi/2021.11/opt/mpi/libfabric/bin:/opt/intel/oneapi/mpi/2021.11/bin:/opt/intel/oneapi/mkl/2024.0/bin/:/opt/intel/oneapi/dpcpp-ct/2024.0/bin:/opt/intel/oneapi/dev-utilities/2024.0/bin:/opt/intel/oneapi/debugger/2024.0/opt/debugger/bin:/opt/intel/oneapi/compiler/2024.0/opt/oclfpga/bin:/opt/intel/oneapi/compiler/2024.0/bin:/opt/intel/oneapi/advisor/2024.0/bin64:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin]\"\r\ntime=2024-12-09T00:58:02.944+08:00 level=INFO source=sched.go:450 msg=\"loaded runners\" count=1\r\ntime=2024-12-09T00:58:02.944+08:00 level=INFO source=server.go:595 msg=\"waiting for llama runner to start responding\"\r\ntime=2024-12-09T00:58:02.944+08:00 level=INFO source=server.go:629 msg=\"waiting for server to become available\" status=\"llm server error\"\r\nINFO [main] build info | build=1 commit=\"f711d1d\" tid=\"140603310369792\" timestamp=1733677082\r\nINFO [main] system info | n_threads=6 n_threads_batch=-1 system_info=\"AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \" tid=\"140603310369792\" timestamp=1733677082 total_threads=12\r\nINFO [main] HTTP server listening | hostname=\"127.0.0.1\" n_threads_http=\"11\" port=\"41009\" tid=\"140603310369792\" timestamp=1733677082\r\nllama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /root/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\r\nllama_model_loader: - kv   3:                           general.finetune str              = Instruct\r\nllama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\r\nllama_model_loader: - kv   5:                         general.size_label str              = 3B\r\nllama_model_loader: - kv   6:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\r\nllama_model_loader: - kv   7:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\r\nllama_model_loader: - kv   8:                          llama.block_count u32              = 28\r\nllama_model_loader: - kv   9:                       llama.context_length u32              = 131072\r\nllama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072\r\nllama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192\r\nllama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24\r\nllama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000\r\nllama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\r\nllama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\r\nllama_model_loader: - kv  18:                          general.file_type u32              = 15\r\nllama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\r\nllama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\r\nllama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\r\nllama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\r\nllama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009\r\nllama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\r\nllama_model_loader: - kv  29:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   58 tensors\r\nllama_model_loader: - type q4_K:  168 tensors\r\nllama_model_loader: - type q6_K:   29 tensors\r\ntime=2024-12-09T00:58:03.195+08:00 level=INFO source=server.go:629 msg=\"waiting for server to become available\" status=\"llm server loading model\"\r\nllm_load_vocab: special tokens cache size = 256\r\nllm_load_vocab: token to piece cache size = 0.7999 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 128256\r\nllm_load_print_meta: n_merges         = 280147\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 131072\r\nllm_load_print_meta: n_embd           = 3072\r\nllm_load_print_meta: n_layer          = 28\r\nllm_load_print_meta: n_head           = 24\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 3\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 8192\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 500000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 131072\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = ?B\r\nllm_load_print_meta: model ftype      = Q4_K - Medium\r\nllm_load_print_meta: model params     = 3.21 B\r\nllm_load_print_meta: model size       = 1.87 GiB (5.01 BPW) \r\nllm_load_print_meta: general.name     = Llama 3.2 3B Instruct\r\nllm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\r\nllm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: LF token         = 128 'Ä'\r\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\r\nllm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\r\nllm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: max token length = 256\r\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nggml_sycl_init: SYCL_USE_XMX: yes\r\nggml_sycl_init: found 1 SYCL devices:\r\nllm_load_tensors: ggml ctx size =    0.24 MiB\r\nllm_load_tensors: offloading 28 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 29/29 layers to GPU\r\nllm_load_tensors:      SYCL0 buffer size =  1918.36 MiB\r\nllm_load_tensors:  SYCL_Host buffer size =   308.23 MiB\r\ntime=2024-12-09T00:58:03.646+08:00 level=INFO source=server.go:629 msg=\"waiting for server to become available\" status=\"llm server not responding\"\r\ntime=2024-12-09T00:58:04.348+08:00 level=ERROR source=sched.go:456 msg=\"error loading llama server\" error=\"llama runner process has terminated: signal: bus error (core dumped)\"\r\n[GIN] 2024/12/09 - 00:58:04 | 500 |  1.520528721s |      172.16.6.3 | POST     \"/api/chat\"\r\ntime=2024-12-09T00:58:04.348+08:00 level=DEBUG source=sched.go:459 msg=\"triggering expiration for failed load\" model=/root/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff\r\ntime=2024-12-09T00:58:04.348+08:00 level=DEBUG source=sched.go:360 msg=\"runner expired event received\" modelPath=/root/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff\r\ntime=2024-12-09T00:58:04.348+08:00 level=DEBUG source=sched.go:376 msg=\"got lock to unload\" modelPath=/root/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff\r\ntime=2024-12-09T00:58:04.348+08:00 level=DEBUG source=server.go:1052 msg=\"stopping llama server\"\r\ntime=2024-12-09T00:58:04.348+08:00 level=DEBUG source=sched.go:381 msg=\"runner released\" modelPath=/root/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff\r\ntime=2024-12-09T00:58:04.348+08:00 level=DEBUG source=sched.go:385 msg=\"sending an unloaded event\" modelPath=/root/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff\r\ntime=2024-12-09T00:58:04.348+08:00 level=DEBUG source=sched.go:308 msg=\"ignoring unload event with no pending requests\"\r\n\r\n``` ",
      "state": "open",
      "author": "pauleseifert",
      "author_type": "User",
      "created_at": "2024-12-08T17:14:00Z",
      "updated_at": "2025-04-22T21:07:35Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12513/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12513",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12513",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:40.838426",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @pauleseifert. I think this should be an OOM issue, you may try to set `OLLAMA_PARALLEL=1` before you start `ollama serve` to reduce memory usage.",
          "created_at": "2024-12-09T02:18:51Z"
        },
        {
          "author": "pauleseifert",
          "body": "Hi @sgwhat. I agree, that's what it looks like. ENV ```OLLAMA_NUM_PARALLEL=1``` is, however, already set in my docker compose file. Any other ideas? ",
          "created_at": "2024-12-09T09:39:14Z"
        },
        {
          "author": "sgwhat",
          "body": "1. Sorry for the typo error, it should be `OLLAMA_PARALLEL=1` instead of `OLLAMA_NUM_PARALLEL`.\r\n2. Could you please check and provide your GPU memory usage when running Ollama?\r\n",
          "created_at": "2024-12-10T02:09:36Z"
        },
        {
          "author": "pauleseifert",
          "body": "This doesn't help. The runner still crashes. intel_gpu_top showed normal behavior for the short moment the runner was visible. There are no other processes running so all memory should be available. ",
          "created_at": "2024-12-16T19:51:30Z"
        },
        {
          "author": "sgwhat",
          "body": "Can you provide the memory usage before and after running `ollama run <model>`? This can help us resolve the issue.",
          "created_at": "2024-12-17T02:09:40Z"
        }
      ]
    },
    {
      "issue_number": 12290,
      "title": "run harness on A770 error",
      "body": "when i run harness as the following link on A770\r\n\r\nhttps://github.com/intel-analytics/ipex-llm/blob/main/python/llm/dev/benchmark/harness/run_llb.py\r\n\r\nthe cmd is：python run_llb.py --model ipex-llm --pretrained /home/test/models/LLM/baichuan2-7b/pytorch/ --precision sym_int4 --device xpu --tasks hellaswag --batch 1 --no_cache\r\n\r\nit occurs this error：\r\nRuntimeError: Job config of task=hellaswag, precision=sym_int4 failed. Error Message: 'utf-8' codec can't decode byte 0xb5 in position 1: invalid start byte\r\n![image](https://github.com/user-attachments/assets/2d0909a4-8837-42a5-ac48-3a3ff529456b)\r\n",
      "state": "closed",
      "author": "tao-ov",
      "author_type": "User",
      "created_at": "2024-10-29T08:08:37Z",
      "updated_at": "2025-04-22T07:39:25Z",
      "closed_at": "2024-10-31T06:50:33Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12290/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "lalalapotter"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12290",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12290",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:41.059585",
      "comments": [
        {
          "author": "lalalapotter",
          "body": "Could you please remove the try-except clause [here](https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/dev/benchmark/harness/run_llb.py#L101-L135) and provide more error log?",
          "created_at": "2024-10-30T02:32:06Z"
        },
        {
          "author": "tao-ov",
          "body": "(llm) test@test-Z590-VISION-D:~/ipexllm_whowhat/ipex-llm/python/llm/dev/benchmark/harness$ python run_llb.py --model ipex-llm --pretrained /home/test/models/LLM/baichuan2-7b/pytorch/ --precision sym_int4 --device xpu --tasks hellaswag --batch 1 --no_cache\r\n/home/test/miniforge3/envs/llm/lib/python3.",
          "created_at": "2024-10-30T03:13:33Z"
        },
        {
          "author": "lalalapotter",
          "body": "Given this issue may caused by `datasets` lib version. Could you please provide some information about python libs version, so that we can reproduce the issue.",
          "created_at": "2024-10-31T00:41:57Z"
        },
        {
          "author": "tao-ov",
          "body": "\r\n(llm) test@test-Z590-VISION-D:~/ipexllm_whowhat/ipex-llm/python/llm/dev/benchmark/harness$ pip show datasets\r\nDEPRECATION: Loading egg at /home/test/miniforge3/envs/llm/lib/python3.11/site-packages/whowhatbench-1.0.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible",
          "created_at": "2024-10-31T03:16:23Z"
        },
        {
          "author": "lalalapotter",
          "body": "Our verified datasets lib version is 2.14.6, could you please try it in your env. At the same time, we will reproduce the issue with the datasets version your provided.",
          "created_at": "2024-10-31T03:21:04Z"
        }
      ]
    },
    {
      "issue_number": 13096,
      "title": "http://localhost:11434 cannot be accessed",
      "body": "export DOCKER_IMAGE=intelanalytics/ipex-llm-inference-cpp-xpu:latest\nexport CONTAINER_NAME=intel-llm\ndocker run -itd \\\n            --net=host \\\n            --device=/dev/dri \\\n            --privileged \\\n            -v /ollama/models:/models \\\n            -v /usr/lib/wsl:/usr/lib/wsl \\\n            -e no_proxy=localhost,127.0.0.1 \\\n            -e OLLAMA_HOST=0.0.0.0 \\\n            --memory=\"16G\" \\\n            --name=$CONTAINER_NAME \\\n            -e DEVICE=Arc \\\n            --shm-size=\"16g\" \\\n            $DOCKER_IMAGE\n\n\nroot@docker-desktop:/llm/scripts# curl http://localhost:11434\nOllama is running\n\nroot@docker:~#  curl http://localhost:11434\ncurl: (7) Failed to connect to localhost port 11434 after 0 ms: Couldn't connect to server\n\n\nThe inside of the container can be accessed, but it cannot be accessed on the host.",
      "state": "open",
      "author": "RICHES-2020",
      "author_type": "User",
      "created_at": "2025-04-20T08:07:50Z",
      "updated_at": "2025-04-22T02:08:52Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13096/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13096",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13096",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:41.279481",
      "comments": [
        {
          "author": "RICHES-2020",
          "body": "root@docker-desktop:/llm/scripts# bash start-ollama.sh\nroot@docker-desktop:/llm/scripts# 2025/04/20 15:36:47 routes.go:1230: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:2",
          "created_at": "2025-04-20T09:24:03Z"
        },
        {
          "author": "RICHES-2020",
          "body": "root@docker-desktop:/llm/scripts# source ipex-llm-init --gpu --device $DEVICE\nfound oneapi in /opt/intel/oneapi/setvars.sh\n\n:: initializing oneAPI environment ...\n   bash: BASH_VERSION = 5.1.16(1)-release\n   args: Using \"$@\" for setvars.sh arguments: --force\n:: advisor -- latest\n:: ccl -- latest\n:: ",
          "created_at": "2025-04-20T09:26:07Z"
        },
        {
          "author": "RICHES-2020",
          "body": "no compatible GPUs were discovered, What's the problem?",
          "created_at": "2025-04-20T09:45:04Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @RICHES-2020, GPU devices will only be discovered after you loading a model. Try it via `ollama run xxx`.",
          "created_at": "2025-04-21T01:27:17Z"
        },
        {
          "author": "bermudahonk",
          "body": "Hello there,\ntry bridge mode and expose/define ports in docker to be able to connect from outside (i.e. host).\nPersonally I never use host or privileged mode, if I don't really have to. It's a bad habit.\n\nI believe the issue lies within your localhost/127.0.0.1 definition. It's not the same what mos",
          "created_at": "2025-04-21T19:47:16Z"
        }
      ]
    },
    {
      "issue_number": 13067,
      "title": "Memory leak? Windows, llama-ipex",
      "body": "**Describe the bug**  \nUsing the prebuilt `llama.cpp` Portable Zip on Windows 11 with an Intel Arc A770 GPU (driver version 32.0.101.6734), there appears to be a memory leak. When running the model `DeepSeek-R1-Distill-Qwen-14B-Q4_0`, the `llama-server.exe` process gradually consumes increasing amounts of RAM over time as the model continues generating output. This eventually leads to system instability, including `Explorer.exe` becoming unresponsive or freezing.\n\n**How to reproduce**  \nSteps to reproduce the error:  \n1. Download and extract the prebuilt `llama.cpp` Portable Zip.  \n2. Launch `llama-server.exe` using the following command:  \n   ```\n   llama-server.exe -m \"%dir_path%\\!file[%selection%]!\" -c 9000 -ngl 99 --port 8000\n   ```  \n3. Load the model `DeepSeek-R1-Distill-Qwen-14B-Q4_0`.  \n4. Make a generation request to the model.  \n5. Monitor RAM usage of `llama-server.exe` — it will keep increasing gradually.  \n6. Eventually, system performance degrades and `Explorer.exe` may freeze.  \n7. Lowering the `-c` parameter (e.g. `-c 2048`, `-c 4096`) does not affect the behavior — memory usage still grows continuously.\n \n**Environment information**  \n- Windows 11  \n- Intel Arc A770 GPU  \n- GPU Driver: 32.0.101.6734  \n- Model: `DeepSeek-R1-Distill-Qwen-14B-Q4_0`  \n- llama.cpp version: IPEX-LLM release 2.2.0 porable llana.cpp\n",
      "state": "open",
      "author": "characharm",
      "author_type": "User",
      "created_at": "2025-04-10T19:44:55Z",
      "updated_at": "2025-04-19T15:19:55Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13067/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13067",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13067",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:41.507642",
      "comments": [
        {
          "author": "Sketchfellow",
          "body": "Have you tried adjusting the `-b` prompt processing batch size? I believe IPEX-LLM Llama.cpp defaults it to 4096 which is rather memory intensive. This allows for faster prompt processing but takes up a lot more memory. Try setting it `-b 1024` which is what I usually do. Doing it with Llama 3.1 8B ",
          "created_at": "2025-04-11T06:30:41Z"
        },
        {
          "author": "characharm",
          "body": "@Sketchfellow I tried it, but the llama-server process still gradually eats up memory. The GPU constantly offloads something into system memory as tokens are generated, and since reasoning models generate tons of tokens, the memory fills up very quickly.\n",
          "created_at": "2025-04-11T15:23:01Z"
        },
        {
          "author": "truncle",
          "body": "> Have you tried adjusting the `-b` prompt processing batch size? I believe IPEX-LLM Llama.cpp defaults it to 4096 which is rather memory intensive. This allows for faster prompt processing but takes up a lot more memory. Try setting it `-b 1024` which is what I usually do. Doing it with Llama 3.1 8",
          "created_at": "2025-04-14T05:39:14Z"
        },
        {
          "author": "tyzbit",
          "body": "Ollama just pre-released v0.6.6 which includes a fix for a memory leak https://github.com/ollama/ollama/issues/10040",
          "created_at": "2025-04-19T15:19:54Z"
        }
      ]
    },
    {
      "issue_number": 12799,
      "title": "Support for Hybrid Acceleration on CPU, GPU, and NPU",
      "body": "Hello,\n\nI am interested in whether ipex-llm currently supports hybrid acceleration across multiple hardware accelerators. Specifically, I would like to know if it is possible to assign different tasks to different compute units, for example:\n\nImage processing assigned to GPU\nEmbedding assigned to NPU\nOther tasks assigned to CPU\n\nWhile trying to install libraries for both NPU and GPU, I encountered dependency conflicts. This makes it difficult to configure an environment where both accelerators can be used simultaneously. \n\nLooking forward to your response. Thank you!",
      "state": "open",
      "author": "Ursue",
      "author_type": "User",
      "created_at": "2025-02-10T06:08:41Z",
      "updated_at": "2025-04-18T14:06:10Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12799/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12799",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12799",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:41.683176",
      "comments": [
        {
          "author": "qiyuangong",
          "body": "> Hello,\n> \n> I am interested in whether ipex-llm currently supports hybrid acceleration across multiple hardware accelerators. Specifically, I would like to know if it is possible to assign different tasks to different compute units, for example:\n> \n> Image processing assigned to GPU Embedding assi",
          "created_at": "2025-02-19T03:42:19Z"
        },
        {
          "author": "bbai123",
          "body": "That's the problem! Both Intel and PC manufacturers are promoting AI ability (such as 99 TOPS, 115 TOPS ), however this number  is actually the total of \"CPU+NPU+GPU\"! When you are using it, you can only choose one, so embarrassing...",
          "created_at": "2025-04-17T00:48:03Z"
        },
        {
          "author": "qiyuangong",
          "body": "> That's the problem! Both Intel and PC manufacturers are promoting AI ability (such as 99 TOPS, 115 TOPS ), however this number is actually the total of \"CPU+NPU+GPU\"! When you are using it, you can only choose one, so embarrassing...\n\nThat's not a problem for most AI applications. :)\n\nThese AI PC ",
          "created_at": "2025-04-18T14:05:10Z"
        }
      ]
    },
    {
      "issue_number": 13083,
      "title": "Using the Speech_Paraformer-Large model reports an error OSError: [WinError -529697949] Windows Error 0xe06d7363 when running on NPU.",
      "body": "**Describe the bug**\nAccording to the ipex-llm reference documentation, although the Speech_Paraformer-Large model can be run on the NPU and the example can be run through, when I run a longer audio file (about 25 seconds), an error is reported: OSError: [WinError -529697949] Windows Error 0xe06d7363.At the same time, it also warns and prompts[W socket.cpp:663] [c10d] The client socket has failed to connect to [WIN-U8UU51A4PON]:54791 (system error: 10049 - 在其上下文中，该请求的地址无效。)。The following is the complete output error content：\n\nC:\\Users\\Admin\\anaconda3\\envs\\FunASR_NPU\\Lib\\site-packages\\transformers\\deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n  warnings.warn(\nstart init process group, rank:  1 world_size:  3\n[W socket.cpp:663] [c10d] The client socket has failed to connect to [WIN-U8UU51A4PON]:54791 (system error: 10049 - 在其上下文中，该请求的地址无效。).\nC:\\Users\\Admin\\anaconda3\\envs\\FunASR_NPU\\Lib\\site-packages\\transformers\\deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n  warnings.warn(\n[W socket.cpp:663] [c10d] The client socket has failed to connect to [WIN-U8UU51A4PON]:54791 (system error: 10049 - 在其上下文中，该请求的地址无效。).\nstart init process group, rank:  2 world_size:  3\n[W socket.cpp:663] [c10d] The client socket has failed to connect to [WIN-U8UU51A4PON]:54791 (system error: 10049 - 在其上下文中，该请求的地址无效。).\n2025-04-16 10:04:11,898 - INFO - rank: 0, size: 3\n2025-04-16 10:04:11,898 - INFO - rank: 1, size: 3\n2025-04-16 10:04:11,898 - INFO - rank: 2, size: 3\n  0%|                                                                                | 0/1 [00:00<?, ?it/s]Process Process-1:\nTraceback (most recent call last):\n  File \"C:\\Users\\Admin\\anaconda3\\envs\\FunASR_NPU\\Lib\\multiprocessing\\process.py\", line 314, in _bootstrap   \n    self.run()\n  File \"C:\\Users\\Admin\\anaconda3\\envs\\FunASR_NPU\\Lib\\multiprocessing\\process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"C:\\Users\\Admin\\anaconda3\\envs\\FunASR_NPU\\Lib\\site-packages\\ipex_llm\\transformers\\npu_models\\paraformer_mp.py\", line 388, in run_prefill\n    encoder_outs = encoder_layer(\n                   ^^^^^^^^^^^^^^\n  File \"C:\\Users\\Admin\\anaconda3\\envs\\FunASR_NPU\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Admin\\anaconda3\\envs\\FunASR_NPU\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Admin\\anaconda3\\envs\\FunASR_NPU\\Lib\\site-packages\\ipex_llm\\transformers\\npu_models\\paraformer_mp.py\", line 310, in forward\n    outputs = run_model(\n              ^^^^^^^^^^\n  File \"C:\\Users\\Admin\\anaconda3\\envs\\FunASR_NPU\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Admin\\anaconda3\\envs\\FunASR_NPU\\Lib\\site-packages\\ipex_llm\\transformers\\npu_models\\mp_models_base.py\", line 92, in run_model\n    _model_cache[key] = deque([backend_cls(*input_shapes) for i in range(replica)])\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Admin\\anaconda3\\envs\\FunASR_NPU\\Lib\\site-packages\\ipex_llm\\transformers\\npu_models\\mp_models_base.py\", line 92, in <listcomp>\n    _model_cache[key] = deque([backend_cls(*input_shapes) for i in range(replica)])\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Admin\\anaconda3\\envs\\FunASR_NPU\\Lib\\site-packages\\ipex_llm\\transformers\\npu_models\\paraformer_mp.py\", line 158, in __init__\n    x, mask = self.build_encoder(\n              ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Admin\\anaconda3\\envs\\FunASR_NPU\\Lib\\site-packages\\ipex_llm\\transformers\\npu_models\\paraformer_mp.py\", line 205, in build_encoder\n    x = self.self_attn_sanm(x, mask, in_feat, n_feat, n_head, fsmn_weight, qkv_bias, out_bias)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Admin\\anaconda3\\envs\\FunASR_NPU\\Lib\\site-packages\\ipex_llm\\transformers\\npu_models\\mp_models_base.py\", line 324, in self_attn_sanm\n    q_h = self.reshape(q, [b, t, h, d_k])\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Admin\\anaconda3\\envs\\FunASR_NPU\\Lib\\site-packages\\intel_npu_acceleration_library\\backend\\factory.py\", line 88, in wrapper\n    node = fn(self, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Admin\\anaconda3\\envs\\FunASR_NPU\\Lib\\site-packages\\intel_npu_acceleration_library\\backend\\factory.py\", line 461, in reshape\n    return backend_lib.reshape(self._mm, input_node, shape_node,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nOSError: [WinError -529697949] Windows Error 0xe06d7363\n\n\nLooking forward to receiving your solution reply. Thank you very much.\n\n\n**Environment information**\nUsing the latest lunar lake notebook.\n\n![Image](https://github.com/user-attachments/assets/69be37ec-e89f-4b27-bbe2-cb75b8e49e75)\n\n\n",
      "state": "open",
      "author": "Bofuser",
      "author_type": "User",
      "created_at": "2025-04-16T06:08:18Z",
      "updated_at": "2025-04-18T13:40:52Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13083/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hkvision"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13083",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13083",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:41.919526",
      "comments": []
    },
    {
      "issue_number": 13090,
      "title": "--verbose-prompt does not print any additional information",
      "body": "Since I found that using llama-server from llama-cpp is faster than using ollama, I chose to use llama-server. \n\nHowever, I want to confirm how the template is being applied, so I’d like to use --verbose-prompt to print out the full prompt with the applied template before it is passed to the model for generation.\n\nI also tried using --verbose, but it just prints other stuff I'm not interested in.\n\nIf someone could tell me whether llama-server will secretly quantize the model by default, I would be very grateful,also.\n\nThank you all.\n\n\nThis is my directive:\n`./llama-cpp/llama-server -m f16.gguf -c 512 -ngl 999 --port 8002 --temp 0 --verbose-prompt\n`\n\nThis is the log on the server side when I make a request after the server is built:\n```\nmain: server is listening on 127.0.0.1:8002 - starting the main loop\nsrv  update_slots: all slots are idle\nslot launch_slot_: id  0 | task 0 | processing task\nslot update_slots: id  0 | task 0 | tokenizing prompt, len = 1\nslot update_slots: id  0 | task 0 | prompt tokenized, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 20\nslot update_slots: id  0 | task 0 | kv cache rm [0, end)\nslot update_slots: id  0 | task 0 | prompt processing progress, n_past = 20, n_tokens = 20, progress = 1.000000\nslot update_slots: id  0 | task 0 | prompt done, n_past = 20, n_tokens = 20\nslot      release: id  0 | task 0 | stop processing: n_past = 106, truncated = 0\nslot print_timing: id  0 | task 0 | \nprompt eval time =     143.28 ms /    20 tokens (    7.16 ms per token,   139.59 tokens per second)\n       eval time =    4435.10 ms /    87 tokens (   50.98 ms per token,    19.62 tokens per second)\n      total time =    4578.38 ms /   107 tokens\nsrv  update_slots: all slots are idle\nrequest: POST /completion 127.0.0.1 200\n```\n",
      "state": "open",
      "author": "HanShengGoodWay",
      "author_type": "User",
      "created_at": "2025-04-17T08:54:49Z",
      "updated_at": "2025-04-18T07:05:24Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13090/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13090",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13090",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:41.919548",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "@HanShengGoodWay  I just check the code of llama-server, verbose_prompt is not used in llama-server. You can ask [llama.cpp upstream](https://github.com/ggml-org/llama.cpp) for help, then we can follow their updates.",
          "created_at": "2025-04-18T02:38:20Z"
        },
        {
          "author": "HanShengGoodWay",
          "body": "> [@HanShengGoodWay](https://github.com/HanShengGoodWay) I just check the code of llama-server, verbose_prompt is not used in llama-server. You can ask [llama.cpp upstream](https://github.com/ggml-org/llama.cpp) for help, then we can follow their updates.\n\nThank you for your help.\n\nIs this llama-ser",
          "created_at": "2025-04-18T07:05:18Z"
        }
      ]
    },
    {
      "issue_number": 12864,
      "title": "unsupported SPIR-V version number 'unknown (66560)'",
      "body": "I'm using 'Ollama Portable Zip' ( v2.2.0-nightly). \nAfter 'start-ollama.bat' run successfully, I try to load a model and chat, then I got error: \n![Image](https://github.com/user-attachments/assets/ab34f5bd-fa81-443d-a059-79496e511689)\nError message on server side:\n![Image](https://github.com/user-attachments/assets/4bc060aa-162e-434d-8b8f-6ac6ca9ec752)\n\nMy iGPU model:  Intel(R) Iris(R) Plus Graphics 655\ndriver version: 31.0.101.2134\n\nI know this driver version '31.0.101.2134'  is low than current ipex-llm expected '32.0.101.6078'.\nBut it's already the newest version I can download from intel website for this iGPU model.\n\nAny suggestion or workaround for this? \n",
      "state": "open",
      "author": "lightgao",
      "author_type": "User",
      "created_at": "2025-02-20T09:02:02Z",
      "updated_at": "2025-04-17T13:50:45Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12864/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12864",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12864",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:42.096609",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "Sorry, your Intel(R) Iris(R) Plus Graphics 655 is not supported, it's not in the [driver](https://www.intel.com/content/www/us/en/download/785597/intel-arc-iris-xe-graphics-windows.html)'s valid list. ",
          "created_at": "2025-02-21T03:00:50Z"
        },
        {
          "author": "DocMAX",
          "body": "Please add support for older iGPUs. Thanks.",
          "created_at": "2025-04-17T13:50:44Z"
        }
      ]
    },
    {
      "issue_number": 13080,
      "title": "IPEX-LLM Slow Token Generation on Gemma 3 12B on Arc A770M",
      "body": "**Describe the bug**\n\nWhen running the `google_gemma-3-12b-it-Q4_0.gguf` model on an Intel Arc A770M 16GB using the IPEX-LLM backend for llama.cpp, token generation speed (`eval time`) becomes slow (around 6.25 tokens/second) when processing a large context (~16,000 tokens). This slowdown occurs even when dedicated VRAM is able to fit both the model and the context KV cache, ruling out simple VRAM capacity limits and memory overflow as the primary cause. This performance is significantly worse than expected and notably slower than the standard llama.cpp SYCL backend under the same conditions. Prompt processing (`prompt eval time`) with IPEX-LLM remains fast, as expected. Additionally the KV cache usage for IPEX-LLM llama.cpp is much larger than llama.cpp SYCL for Gemma 3 12B.\n\n**How to reproduce**\n\nSteps to reproduce the error:\n1. Use the `google_gemma-3-12b-it-Q4_0.gguf` model.\n2. Set the proper environmental variables `set SYCL_CACHE_PERSISTENT=1`\n3. Run `llama-server` using the command `llama-server.exe -m path\\to\\google_gemma-3-12b-it-Q4_0.gguf --ctx-size 16384 -ngl 99 -b 512`\n4. Submit a prompt containing approximately 16,000 tokens.\n5. Observe the token generation speed reported in the `llama-server` logs after the prompt is processed.\n6. Monitor GPU memory usage using Task Manager (or `intel_gpu_top`) to confirm high dedicated VRAM usage but minimal shared GPU memory usage during token generation.\n\n**Screenshots**\n\n![Image](https://github.com/user-attachments/assets/eacc922e-ad3f-41d4-8c85-b9f9b66b2965)\n\nThe attached screenshot shows GPU memory usage during runs with llama.cpp SYCL and IPEX-LLM using a large context:\n\n* **SYCL:** Shows ~14.1/16.0 GB dedicated VRAM usage and ~0.1 GB shared GPU memory usage. Token generation speed was ~9.62 toks/sec.\n* **IPEX-LLM:** Shows ~15.7/16.0 GB dedicated VRAM usage (nearly full) but still only ~0.1 GB shared GPU memory usage. Token generation speed dropped to ~6.25 toks/sec.\n\n**Environment information**\n\n* GPU: Intel Arc A770M (16GB)\n* GPU Driver Version: 32.0.101.6732 \n* OS: Windows 11\n* IPEX-LLM llama.cpp Version: Downloaded through Miniforge with `bigdl-core-cpp-2.7.0b20250415` and `ipex-llm-2.3.0b20250415`\n* llama.cpp SYCL Version: b5125\n\n**Additional context**\nThe primary issue is the unexpected and severe drop in token generation performance with IPEX-LLM for this specific model and hardware combination, even when memory overflow to system RAM is avoided. \n\n* **Model:** `google_gemma-3-12b-it-Q4_0.gguf`\n* **Prompt Size:** ~16,000 tokens\n* **Observed Performance (IPEX-LLM):**\n    * `prompt eval time = 22407.43 ms / 16000 tokens ( 1.40 ms/tok, 714.05 toks/sec)` - *Fast*\n    * `eval time = 39853.67 ms / 249 tokens ( 160.05 ms/tok, 6.25 toks/sec)` - *Very Slow*\n* **Observed Performance (Llama.cpp SYCL - for comparison):**\n    * `prompt eval time = 47131.78 ms / 16000 tokens ( 2.95 ms/tok, 339.47 toks/sec)` - *Slower than IPEX*\n    * `eval time = 26411.60 ms / 254 tokens ( 103.98 ms/tok, 9.62 toks/sec)` - *Significantly Faster than IPEX TG*\n\nThe key finding is that despite dedicated VRAM being fully utilized with no shared memory overflow, token generation speeds are much slower on IPEX-LLM than llama.cpp SYCL at the same settings. I also made sure to restart my PC before running the model for both IPEX-LLM and SYCL. Additionally, the KV cache memory usage is much larger than it is on regular SYCL (15.7 vs 14.1). This suggests that the KV context implementation optimizations on IPEX may not apply to Gemma 3 architecture and could result in performance degradation relative to llama.cpp SYCL. Typically, IPEX-LLM is expected to provide faster token generation speeds compared to the standard SYCL backend.\n",
      "state": "open",
      "author": "Sketchfellow",
      "author_type": "User",
      "created_at": "2025-04-15T19:52:49Z",
      "updated_at": "2025-04-17T05:24:56Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13080/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13080",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13080",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:42.287629",
      "comments": [
        {
          "author": "rnwang04",
          "body": "Hi @Sketchfellow , We don't have A770M so I tested with A770, and I can not reproduce the same performance with you.\n\nBelow is my test config and performance:\n\n**Environment information**\n\n* GPU: Intel Arc A770 (16GB)\n* GPU Driver Version: 32.0.101.6647\n* OS: Windows 11\n* IPEX-LLM llama.cpp Version:",
          "created_at": "2025-04-17T05:24:55Z"
        }
      ]
    },
    {
      "issue_number": 13055,
      "title": "Intel UHD Graphics 620 supported?",
      "body": "Is this GPU supported? Ipex-llm is working if i use ollama run directly. But as soon i feed it with context it's crashing...\n\n```\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\n| 0| [level_zero:gpu:0]|                 Intel UHD Graphics 620|    9.1|     24|     256|   32| 15030M|     1.5.30872.220000|\nllama_kv_cache_init:      SYCL0 KV buffer size =  1012.00 MiB\nllama_new_context_with_model: KV self size  = 1012.00 MiB, K (f16):  506.00 MiB, V (f16):  506.00 MiB\nllama_new_context_with_model:  SYCL_Host  output buffer size =     0.50 MiB\nllama_new_context_with_model:      SYCL0 compute buffer size =   258.50 MiB\nllama_new_context_with_model:  SYCL_Host compute buffer size =    23.82 MiB\nllama_new_context_with_model: graph nodes  = 902\nllama_new_context_with_model: graph splits = 2\ntime=2025-04-08T05:04:51.465+08:00 level=WARN source=runner.go:892 msg=\"%s: warming up the model with an empty run - please wait ... \" !BADKEY=loadModel\ntime=2025-04-08T05:04:51.596+08:00 level=INFO source=server.go:610 msg=\"llama runner started in 15.06 seconds\"\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\nllama_load_model_from_file: using device SYCL0 (Intel(R) UHD Graphics 620) - 14334 MiB free\nllama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\nllama_model_loader: - kv   3:                           general.finetune str              = Instruct\nllama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\nllama_model_loader: - kv   5:                         general.size_label str              = 8B\nllama_model_loader: - kv   6:                            general.license str              = llama3.1\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\nllama_model_loader: - kv   9:                          llama.block_count u32              = 32\nllama_model_loader: - kv  10:                       llama.context_length u32              = 131072\nllama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 15\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\nllama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\nllama_model_loader: - kv  28:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   66 tensors\nllama_model_loader: - type q4_K:  193 tensors\nllama_model_loader: - type q6_K:   33 tensors\nllm_load_vocab: special tokens cache size = 256\nllm_load_vocab: token to piece cache size = 0.7999 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 128256\nllm_load_print_meta: n_merges         = 280147\nllm_load_print_meta: vocab_only       = 1\nllm_load_print_meta: model type       = ?B\nllm_load_print_meta: model ftype      = all F32\nllm_load_print_meta: model params     = 8.03 B\nllm_load_print_meta: model size       = 4.58 GiB (4.89 BPW)\nllm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct\nllm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\nllm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\nllm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\nllm_load_print_meta: LF token         = 128 'Ä'\nllm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\nllm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\nllm_load_print_meta: max token length = 256\nllama_model_load: vocab only - skipping tensors\nollama-lib: /home/runner/_work/llm.cpp/llm.cpp/llm.cpp/bigdl-core-xe/llama_backend/sdp_xmx_kernel.cpp:439: auto ggml_sycl_op_sdp_xmx_casual(fp16 *, fp16 *, fp16 *, fp16 *, fp16 *, float *, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, float *, float, sycl::queue &)::(anonymous class)::operator()() const: Assertion `false' failed.\nSIGABRT: abort\nPC=0x7f0f340419fc m=10 sigcode=18446744073709551610\nsignal arrived during cgo execution\n\ngoroutine 36 gp=0xc000230a80 m=10 mp=0xc000604708 [syscall]:\nruntime.cgocall(0x55b174a5a260, 0xc000099b90)\n\truntime/cgocall.go:167 +0x4b fp=0xc000099b68 sp=0xc000099b30 pc=0x55b173eb7feb\nollama/llama/llamafile._Cfunc_llama_decode(0x7f0ecbc94bc0, {0x200, 0x7f0ec8094590, 0x0, 0x0, 0x7f0ec8094da0, 0x7f0ec8059e90, 0x7f0ec80484b0, 0x7f0ec81c46a0})\n\t_cgo_gotypes.go:557 +0x4f fp=0xc000099b90 sp=0xc000099b68 pc=0x55b17427a40f\nollama/llama/llamafile.(*Context).Decode.func1(0x55b17428918b?, 0x7f0ecbc94bc0?)\n\tollama/llama/llamafile/llama.go:143 +0xf5 fp=0xc000099c80 sp=0xc000099b90 pc=0x55b17427d035\nollama/llama/llamafile.(*Context).Decode(0xc000099d70?, 0x0?)\n\tollama/llama/llamafile/llama.go:143 +0x13 fp=0xc000099cc8 sp=0xc000099c80 pc=0x55b17427ceb3\nollama/llama/runner.(*Server).processBatch(0xc00051a120, 0xc0003d04e0, 0xc000099f20)\n\tollama/llama/runner/runner.go:434 +0x23f fp=0xc000099ee0 sp=0xc000099cc8 pc=0x55b174287e5f\nollama/llama/runner.(*Server).run(0xc00051a120, {0x55b17500b1d0, 0xc000708690})\n\tollama/llama/runner/runner.go:342 +0x1d5 fp=0xc000099fb8 sp=0xc000099ee0 pc=0x55b174287895\nollama/llama/runner.Execute.gowrap2()\n\tollama/llama/runner/runner.go:1006 +0x28 fp=0xc000099fe0 sp=0xc000099fb8 pc=0x55b17428cb08\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc000099fe8 sp=0xc000099fe0 pc=0x55b173ec6ac1\ncreated by ollama/llama/runner.Execute in goroutine 1\n\tollama/llama/runner/runner.go:1006 +0xde5\n\ngoroutine 1 gp=0xc0000061c0 m=nil [IO wait]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc0003f5560 sp=0xc0003f5540 pc=0x55b173ebe6ee\nruntime.netpollblock(0xc0005155b0?, 0x73e55506?, 0xb1?)\n\truntime/netpoll.go:575 +0xf7 fp=0xc0003f5598 sp=0xc0003f5560 pc=0x55b173e82357\ninternal/poll.runtime_pollWait(0x7f0f347c6680, 0x72)\n\truntime/netpoll.go:351 +0x85 fp=0xc0003f55b8 sp=0xc0003f5598 pc=0x55b173ebd9e5\ninternal/poll.(*pollDesc).wait(0xc000476280?, 0x900000036?, 0x0)\n\tinternal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc0003f55e0 sp=0xc0003f55b8 pc=0x55b173f45007\ninternal/poll.(*pollDesc).waitRead(...)\n\tinternal/poll/fd_poll_runtime.go:89\ninternal/poll.(*FD).Accept(0xc000476280)\n\tinternal/poll/fd_unix.go:620 +0x295 fp=0xc0003f5688 sp=0xc0003f55e0 pc=0x55b173f4a3d5\nnet.(*netFD).accept(0xc000476280)\n\tnet/fd_unix.go:172 +0x29 fp=0xc0003f5740 sp=0xc0003f5688 pc=0x55b173fb2aa9\nnet.(*TCPListener).accept(0xc00071ca00)\n\tnet/tcpsock_posix.go:159 +0x1e fp=0xc0003f5790 sp=0xc0003f5740 pc=0x55b173fc871e\nnet.(*TCPListener).Accept(0xc00071ca00)\n\tnet/tcpsock.go:372 +0x30 fp=0xc0003f57c0 sp=0xc0003f5790 pc=0x55b173fc75d0\nnet/http.(*onceCloseListener).Accept(0xc00051ae10?)\n\t<autogenerated>:1 +0x24 fp=0xc0003f57d8 sp=0xc0003f57c0 pc=0x55b174240d24\nnet/http.(*Server).Serve(0xc000722870, {0x55b175008ee0, 0xc00071ca00})\n\tnet/http/server.go:3330 +0x30c fp=0xc0003f5908 sp=0xc0003f57d8 pc=0x55b174218cac\nollama/llama/runner.Execute({0xc000036130?, 0x0?, 0x0?})\n\tollama/llama/runner/runner.go:1027 +0x11a9 fp=0xc0003f5ca8 sp=0xc0003f5908 pc=0x55b17428c7e9\nollama/cmd.NewCLI.func2(0xc00051c400?, {0x55b174a5ed1d?, 0x4?, 0x55b174a5ed21?})\n\tollama/cmd/cmd.go:1430 +0x45 fp=0xc0003f5cd0 sp=0xc0003f5ca8 pc=0x55b174a594e5\ngithub.com/spf13/cobra.(*Command).execute(0xc000518008, {0xc0007221e0, 0xf, 0xf})\n\tgithub.com/spf13/cobra@v1.8.1/command.go:985 +0xaaa fp=0xc0003f5e58 sp=0xc0003f5cd0 pc=0x55b17404be8a\ngithub.com/spf13/cobra.(*Command).ExecuteC(0xc000683208)\n\tgithub.com/spf13/cobra@v1.8.1/command.go:1117 +0x3ff fp=0xc0003f5f30 sp=0xc0003f5e58 pc=0x55b17404c75f\ngithub.com/spf13/cobra.(*Command).Execute(...)\n\tgithub.com/spf13/cobra@v1.8.1/command.go:1041\ngithub.com/spf13/cobra.(*Command).ExecuteContext(...)\n\tgithub.com/spf13/cobra@v1.8.1/command.go:1034\nmain.main()\n\tollama/main.go:12 +0x4d fp=0xc0003f5f50 sp=0xc0003f5f30 pc=0x55b174a59b4d\nruntime.main()\n\truntime/proc.go:272 +0x29d fp=0xc0003f5fe0 sp=0xc0003f5f50 pc=0x55b173e899fd\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc0003f5fe8 sp=0xc0003f5fe0 pc=0x55b173ec6ac1\n\ngoroutine 2 gp=0xc000007340 m=nil [force gc (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc000084fa8 sp=0xc000084f88 pc=0x55b173ebe6ee\nruntime.goparkunlock(...)\n\truntime/proc.go:430\nruntime.forcegchelper()\n\truntime/proc.go:337 +0xb8 fp=0xc000084fe0 sp=0xc000084fa8 pc=0x55b173e89d38\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc000084fe8 sp=0xc000084fe0 pc=0x55b173ec6ac1\ncreated by runtime.init.7 in goroutine 1\n\truntime/proc.go:325 +0x1a\n\ngoroutine 3 gp=0xc000007500 m=nil [GC sweep wait]:\nruntime.gopark(0x1?, 0x0?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc000085780 sp=0xc000085760 pc=0x55b173ebe6ee\nruntime.goparkunlock(...)\n\truntime/proc.go:430\nruntime.bgsweep(0xc0000ba000)\n\truntime/mgcsweep.go:317 +0xdf fp=0xc0000857c8 sp=0xc000085780 pc=0x55b173e743df\nruntime.gcenable.gowrap1()\n\truntime/mgc.go:204 +0x25 fp=0xc0000857e0 sp=0xc0000857c8 pc=0x55b173e68a25\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc0000857e8 sp=0xc0000857e0 pc=0x55b173ec6ac1\ncreated by runtime.gcenable in goroutine 1\n\truntime/mgc.go:204 +0x66\n\ngoroutine 4 gp=0xc0000076c0 m=nil [GC scavenge wait]:\nruntime.gopark(0x10000?, 0x55b174c04ed8?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc000085f78 sp=0xc000085f58 pc=0x55b173ebe6ee\nruntime.goparkunlock(...)\n\truntime/proc.go:430\nruntime.(*scavengerState).park(0x55b1757a2da0)\n\truntime/mgcscavenge.go:425 +0x49 fp=0xc000085fa8 sp=0xc000085f78 pc=0x55b173e71da9\nruntime.bgscavenge(0xc0000ba000)\n\truntime/mgcscavenge.go:658 +0x59 fp=0xc000085fc8 sp=0xc000085fa8 pc=0x55b173e72339\nruntime.gcenable.gowrap2()\n\truntime/mgc.go:205 +0x25 fp=0xc000085fe0 sp=0xc000085fc8 pc=0x55b173e689c5\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc000085fe8 sp=0xc000085fe0 pc=0x55b173ec6ac1\ncreated by runtime.gcenable in goroutine 1\n\truntime/mgc.go:205 +0xa5\n\ngoroutine 5 gp=0xc000007c00 m=nil [finalizer wait]:\nruntime.gopark(0xc000084648?, 0x55b173e5ef25?, 0xb0?, 0x1?, 0xc0000061c0?)\n\truntime/proc.go:424 +0xce fp=0xc000084620 sp=0xc000084600 pc=0x55b173ebe6ee\nruntime.runfinq()\n\truntime/mfinal.go:193 +0x107 fp=0xc0000847e0 sp=0xc000084620 pc=0x55b173e67aa7\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc0000847e8 sp=0xc0000847e0 pc=0x55b173ec6ac1\ncreated by runtime.createfing in goroutine 1\n\truntime/mfinal.go:163 +0x3d\n\ngoroutine 6 gp=0xc000180e00 m=nil [chan receive]:\nruntime.gopark(0xc000086760?, 0x55b173f9a125?, 0x40?, 0xe8?, 0x55b17501c400?)\n\truntime/proc.go:424 +0xce fp=0xc000086718 sp=0xc0000866f8 pc=0x55b173ebe6ee\nruntime.chanrecv(0xc00004a460, 0x0, 0x1)\n\truntime/chan.go:639 +0x41c fp=0xc000086790 sp=0xc000086718 pc=0x55b173e5811c\nruntime.chanrecv1(0x0?, 0x0?)\n\truntime/chan.go:489 +0x12 fp=0xc0000867b8 sp=0xc000086790 pc=0x55b173e57cd2\nruntime.unique_runtime_registerUniqueMapCleanup.func1(...)\n\truntime/mgc.go:1781\nruntime.unique_runtime_registerUniqueMapCleanup.gowrap1()\n\truntime/mgc.go:1784 +0x2f fp=0xc0000867e0 sp=0xc0000867b8 pc=0x55b173e6ba8f\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc0000867e8 sp=0xc0000867e0 pc=0x55b173ec6ac1\ncreated by unique.runtime_registerUniqueMapCleanup in goroutine 1\n\truntime/mgc.go:1779 +0x96\n\ngoroutine 7 gp=0xc000181880 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc000086f38 sp=0xc000086f18 pc=0x55b173ebe6ee\nruntime.gcBgMarkWorker(0xc00004ba40)\n\truntime/mgc.go:1412 +0xe9 fp=0xc000086fc8 sp=0xc000086f38 pc=0x55b173e6ad89\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1328 +0x25 fp=0xc000086fe0 sp=0xc000086fc8 pc=0x55b173e6ac65\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc000086fe8 sp=0xc000086fe0 pc=0x55b173ec6ac1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1328 +0x105\n\ngoroutine 18 gp=0xc000230380 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc000080738 sp=0xc000080718 pc=0x55b173ebe6ee\nruntime.gcBgMarkWorker(0xc00004ba40)\n\truntime/mgc.go:1412 +0xe9 fp=0xc0000807c8 sp=0xc000080738 pc=0x55b173e6ad89\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1328 +0x25 fp=0xc0000807e0 sp=0xc0000807c8 pc=0x55b173e6ac65\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc0000807e8 sp=0xc0000807e0 pc=0x55b173ec6ac1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1328 +0x105\n\ngoroutine 34 gp=0xc000504000 m=nil [GC worker (idle)]:\nruntime.gopark(0x55b1757cc920?, 0x1?, 0x5?, 0x2a?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc00050a738 sp=0xc00050a718 pc=0x55b173ebe6ee\nruntime.gcBgMarkWorker(0xc00004ba40)\n\truntime/mgc.go:1412 +0xe9 fp=0xc00050a7c8 sp=0xc00050a738 pc=0x55b173e6ad89\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1328 +0x25 fp=0xc00050a7e0 sp=0xc00050a7c8 pc=0x55b173e6ac65\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc00050a7e8 sp=0xc00050a7e0 pc=0x55b173ec6ac1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1328 +0x105\n\ngoroutine 19 gp=0xc000230540 m=nil [GC worker (idle)]:\nruntime.gopark(0xdea45b9ea56?, 0x3?, 0x3b?, 0xde?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc000080f38 sp=0xc000080f18 pc=0x55b173ebe6ee\nruntime.gcBgMarkWorker(0xc00004ba40)\n\truntime/mgc.go:1412 +0xe9 fp=0xc000080fc8 sp=0xc000080f38 pc=0x55b173e6ad89\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1328 +0x25 fp=0xc000080fe0 sp=0xc000080fc8 pc=0x55b173e6ac65\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc000080fe8 sp=0xc000080fe0 pc=0x55b173ec6ac1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1328 +0x105\n\ngoroutine 20 gp=0xc000230700 m=nil [GC worker (idle)]:\nruntime.gopark(0xdea45b9c523?, 0x3?, 0x61?, 0xc2?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc000081738 sp=0xc000081718 pc=0x55b173ebe6ee\nruntime.gcBgMarkWorker(0xc00004ba40)\n\truntime/mgc.go:1412 +0xe9 fp=0xc0000817c8 sp=0xc000081738 pc=0x55b173e6ad89\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1328 +0x25 fp=0xc0000817e0 sp=0xc0000817c8 pc=0x55b173e6ac65\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc0000817e8 sp=0xc0000817e0 pc=0x55b173ec6ac1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1328 +0x105\n\ngoroutine 8 gp=0xc000181a40 m=nil [GC worker (idle)]:\nruntime.gopark(0xdea45b2cac6?, 0x1?, 0xed?, 0x89?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc000087738 sp=0xc000087718 pc=0x55b173ebe6ee\nruntime.gcBgMarkWorker(0xc00004ba40)\n\truntime/mgc.go:1412 +0xe9 fp=0xc0000877c8 sp=0xc000087738 pc=0x55b173e6ad89\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1328 +0x25 fp=0xc0000877e0 sp=0xc0000877c8 pc=0x55b173e6ac65\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc0000877e8 sp=0xc0000877e0 pc=0x55b173ec6ac1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1328 +0x105\n\ngoroutine 9 gp=0xc000181c00 m=nil [GC worker (idle)]:\nruntime.gopark(0xdea45b1d34c?, 0x1?, 0xb1?, 0x40?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc000087f38 sp=0xc000087f18 pc=0x55b173ebe6ee\nruntime.gcBgMarkWorker(0xc00004ba40)\n\truntime/mgc.go:1412 +0xe9 fp=0xc000087fc8 sp=0xc000087f38 pc=0x55b173e6ad89\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1328 +0x25 fp=0xc000087fe0 sp=0xc000087fc8 pc=0x55b173e6ac65\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc000087fe8 sp=0xc000087fe0 pc=0x55b173ec6ac1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1328 +0x105\n\ngoroutine 10 gp=0xc000181dc0 m=nil [GC worker (idle)]:\nruntime.gopark(0xdea45b1cff0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc000506738 sp=0xc000506718 pc=0x55b173ebe6ee\nruntime.gcBgMarkWorker(0xc00004ba40)\n\truntime/mgc.go:1412 +0xe9 fp=0xc0005067c8 sp=0xc000506738 pc=0x55b173e6ad89\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1328 +0x25 fp=0xc0005067e0 sp=0xc0005067c8 pc=0x55b173e6ac65\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc0005067e8 sp=0xc0005067e0 pc=0x55b173ec6ac1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1328 +0x105\n\ngoroutine 91 gp=0xc000504540 m=nil [select]:\nruntime.gopark(0xc00005da68?, 0x2?, 0x6e?, 0x41?, 0xc00005d834?)\n\truntime/proc.go:424 +0xce fp=0xc00005d650 sp=0xc00005d630 pc=0x55b173ebe6ee\nruntime.selectgo(0xc00005da68, 0xc00005d830, 0xa6e?, 0x0, 0x1?, 0x1)\n\truntime/select.go:335 +0x7a5 fp=0xc00005d778 sp=0xc00005d650 pc=0x55b173e9b9e5\nollama/llama/runner.(*Server).completion(0xc00051a120, {0x55b1750090f0, 0xc00012b500}, 0xc000329cc0)\n\tollama/llama/runner/runner.go:696 +0xab6 fp=0xc00005dac0 sp=0xc00005d778 pc=0x55b174289cd6\nollama/llama/runner.(*Server).completion-fm({0x55b1750090f0?, 0xc00012b500?}, 0x55b174222a87?)\n\t<autogenerated>:1 +0x36 fp=0xc00005daf0 sp=0xc00005dac0 pc=0x55b17428d3b6\nnet/http.HandlerFunc.ServeHTTP(0xc000720700?, {0x55b1750090f0?, 0xc00012b500?}, 0x0?)\n\tnet/http/server.go:2220 +0x29 fp=0xc00005db18 sp=0xc00005daf0 pc=0x55b1742152a9\nnet/http.(*ServeMux).ServeHTTP(0x55b173e5ef25?, {0x55b1750090f0, 0xc00012b500}, 0xc000329cc0)\n\tnet/http/server.go:2747 +0x1ca fp=0xc00005db68 sp=0xc00005db18 pc=0x55b1742171aa\nnet/http.serverHandler.ServeHTTP({0x55b175005cb0?}, {0x55b1750090f0?, 0xc00012b500?}, 0x6?)\n\tnet/http/server.go:3210 +0x8e fp=0xc00005db98 sp=0xc00005db68 pc=0x55b17423470e\nnet/http.(*conn).serve(0xc00051ae10, {0x55b17500b198, 0xc000706ff0})\n\tnet/http/server.go:2092 +0x5d0 fp=0xc00005dfb8 sp=0xc00005db98 pc=0x55b174213c50\nnet/http.(*Server).Serve.gowrap3()\n\tnet/http/server.go:3360 +0x28 fp=0xc00005dfe0 sp=0xc00005dfb8 pc=0x55b1742190a8\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc00005dfe8 sp=0xc00005dfe0 pc=0x55b173ec6ac1\ncreated by net/http.(*Server).Serve in goroutine 1\n\tnet/http/server.go:3360 +0x485\n\ngoroutine 71 gp=0xc000230fc0 m=nil [IO wait]:\nruntime.gopark(0x55b173e63405?, 0x0?, 0x0?, 0x0?, 0xb?)\n\truntime/proc.go:424 +0xce fp=0xc000081da8 sp=0xc000081d88 pc=0x55b173ebe6ee\nruntime.netpollblock(0x55b173ee1918?, 0x73e55506?, 0xb1?)\n\truntime/netpoll.go:575 +0xf7 fp=0xc000081de0 sp=0xc000081da8 pc=0x55b173e82357\ninternal/poll.runtime_pollWait(0x7f0f347c6568, 0x72)\n\truntime/netpoll.go:351 +0x85 fp=0xc000081e00 sp=0xc000081de0 pc=0x55b173ebd9e5\ninternal/poll.(*pollDesc).wait(0xc000476380?, 0xc0002de041?, 0x0)\n\tinternal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc000081e28 sp=0xc000081e00 pc=0x55b173f45007\ninternal/poll.(*pollDesc).waitRead(...)\n\tinternal/poll/fd_poll_runtime.go:89\ninternal/poll.(*FD).Read(0xc000476380, {0xc0002de041, 0x1, 0x1})\n\tinternal/poll/fd_unix.go:165 +0x27a fp=0xc000081ec0 sp=0xc000081e28 pc=0x55b173f462fa\nnet.(*netFD).Read(0xc000476380, {0xc0002de041?, 0xc000081f48?, 0x55b173ec0370?})\n\tnet/fd_posix.go:55 +0x25 fp=0xc000081f08 sp=0xc000081ec0 pc=0x55b173fb0ae5\nnet.(*conn).Read(0xc000088568, {0xc0002de041?, 0x0?, 0x55b1757ca6a0?})\n\tnet/net.go:189 +0x45 fp=0xc000081f50 sp=0xc000081f08 pc=0x55b173fbf0e5\nnet.(*TCPConn).Read(0x55b1757070c0?, {0xc0002de041?, 0x0?, 0x0?})\n\t<autogenerated>:1 +0x25 fp=0xc000081f80 sp=0xc000081f50 pc=0x55b173fd22e5\nnet/http.(*connReader).backgroundRead(0xc0002de030)\n\tnet/http/server.go:690 +0x37 fp=0xc000081fc8 sp=0xc000081f80 pc=0x55b17420e5d7\nnet/http.(*connReader).startBackgroundRead.gowrap2()\n\tnet/http/server.go:686 +0x25 fp=0xc000081fe0 sp=0xc000081fc8 pc=0x55b17420e505\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc000081fe8 sp=0xc000081fe0 pc=0x55b173ec6ac1\ncreated by net/http.(*connReader).startBackgroundRead in goroutine 91\n\tnet/http/server.go:686 +0xb6\n\nrax    0x0\nrbx    0x7f0ead9fe640\nrcx    0x7f0f340419fc\nrdx    0x6\nrdi    0x230\nrsi    0x23b\nrbp    0x23b\nrsp    0x7f0ead9fc5e0\nr8     0x7f0ead9fc6b0\nr9     0xa2e64656c6961\nr10    0x8\nr11    0x246\nr12    0x6\nr13    0x16\nr14    0x7f0f3200fdf5\nr15    0xffffaaaeb8400000\nrip    0x7f0f340419fc\nrflags 0x246\ncs     0x33\nfs     0x0\ngs     0x0\n\n```",
      "state": "open",
      "author": "DocMAX",
      "author_type": "User",
      "created_at": "2025-04-07T21:05:55Z",
      "updated_at": "2025-04-16T09:15:01Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13055/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13055",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13055",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:42.498533",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "`Intel UHD Graphics 620` is not supported, supported list: https://dgpu-docs.intel.com/devices/hardware-table.html#gpus-with-supported-drivers",
          "created_at": "2025-04-08T00:13:30Z"
        },
        {
          "author": "DocMAX",
          "body": "OK for my GPU it says\n\"GPUs with unsupported drivers\nDriver support for the following devices remains available but is not under active development.\"\nMaybe someone can have a look anyway... when it's working there is a significant boost...",
          "created_at": "2025-04-08T07:18:29Z"
        },
        {
          "author": "DocMAX",
          "body": "But what is really rally rally strange is, the crash happens on some programs only, that use the Ollama API. \"Ollama run\" doesn't crash. Also some conversations with Open-WebUI won't crash.",
          "created_at": "2025-04-08T15:31:43Z"
        },
        {
          "author": "qiuxin2012",
          "body": "The devices outside the supported list are not tested, and we don't plan to support it. They are too slow for LLM use cases. You can use cpu to run ollama.",
          "created_at": "2025-04-09T02:22:28Z"
        },
        {
          "author": "DocMAX",
          "body": "I did a CPU/GPU comparison and yeah, GPU is even worse... so no need to look at this anymore...\nCPU (llama3.1): 3.74 tokes/s\nGPU (llama3.1): 2.90 tokes/s",
          "created_at": "2025-04-09T10:29:37Z"
        }
      ]
    },
    {
      "issue_number": 13065,
      "title": "llama-cpp-ipex-llm-2.2.0-ubuntu-xeon NOT Support 4XARC770",
      "body": "**Describe the bug**\nI'm using the llama-cpp-ipex-llm-2.2.0-ubuntu-xeon on Ubuntu 24.04.02. The llama-cpp can't run and give me the error.\n> The program was built for 1 devices\n\n> Build program log for 'Intel(R) Arc(TM) A770 Graphics':\n\n> Exception caught at file:/home/intel/qiyuan/llama-cpp-bigdl/ggml/src/ggml-sycl/common.cpp, line:99\n\n**How to reproduce**\nSteps to reproduce the error:\n1. Install the new Ubuntu system.\n2. Install the INTEL DRV according to the ipex-llm manual.\n3. Using the deepseek r1 70B model.\n\n**Screenshots**\n\n![Image](https://github.com/user-attachments/assets/eb3d4c2b-522a-44fe-b5be-f6f972036dff)\n\n**Environment information**\n1.Mainboard: X12DPG-QT6\n2.CPU:Xeon-6338*2\n3.RAM:1TB DDR-4 3200MHZ\n4.GPU:INTEL ARC 770 LIMITED EDITION\n\n**Additional context**\nIronically,  the llama-cpp-ipex-llm-2.2.0b20250313-ubuntu-xeon run smoothly.\n",
      "state": "open",
      "author": "macafeeee",
      "author_type": "User",
      "created_at": "2025-04-10T07:26:37Z",
      "updated_at": "2025-04-16T01:44:46Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13065/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13065",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13065",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:42.691014",
      "comments": [
        {
          "author": "cyita",
          "body": "Hi macafeeee,\n\nWe can’t reproduce this issue. Since `llama-cpp-ipex-llm-2.2.0b20250313-ubuntu-xeon` runs smoothly, we recommend using this version.",
          "created_at": "2025-04-11T04:03:27Z"
        },
        {
          "author": "macafeeee",
          "body": "> Hi macafeeee,\n> \n> We can’t reproduce this issue. Since `llama-cpp-ipex-llm-2.2.0b20250313-ubuntu-xeon` runs smoothly, we recommend using this version.\n\nTry llama-server",
          "created_at": "2025-04-11T06:17:07Z"
        },
        {
          "author": "rnwang04",
          "body": "Hi @macafeeee , we can only reproduce this error in very rare cases. Most of the time, we don't encounter this error, so we are still not sure what the root cause of this error. \nBut in our latest nightly release (`pip install --pre --upgrade ipex-llm[cpp]`), we have some optimizations for server. M",
          "created_at": "2025-04-16T01:44:45Z"
        }
      ]
    },
    {
      "issue_number": 12925,
      "title": "Instalntly crashing when trying to run with Ollama",
      "body": "I am running IPEX in a docker container and for about a month, as soon as I input a prompt ollama gives an internal service error right away. I have looked through them and I am not able to find any useful information that would point to a cause. Here are the logs: [https://pastebin.com/pjuizqYN](https://pastebin.com/pjuizqYN)\n\nAnd here are the environment variables I have set: \n\nONEAPI_DEVICE_SELECTOR=level_zero:0;level_zero:1\nIPEX_LLM_NUM_CTX=16384\nOLLAMA_PARALLEL=1\nOLLAMA_MAX_LOADED_MODELS=0\nOLLAMA_DEBUG=1\n\nI am running it on two Arc a770s, 9900k, and 32gb ram. \n\nAnd here is the versions I'm running on Ubuntu 24.04:\n\nollama-0.5.4-ipex-llm-2.2.0b20250220-ubuntu.tgz\n https://github.com/oneapi-src/level-zero/releases/download/v1.19.2/level-zero_1.19.2+u24.04_amd64.deb \n https://github.com/intel/intel-graphics-compiler/releases/download/v2.5.6/intel-igc-core-2_2.5.6+18417_amd64.deb \n https://github.com/intel/intel-graphics-compiler/releases/download/v2.5.6/intel-igc-opencl-2_2.5.6+18417_amd64.deb \n https://github.com/intel/compute-runtime/releases/download/24.52.32224.5/intel-level-zero-gpu_1.6.32224.5_amd64.deb \n https://github.com/intel/compute-runtime/releases/download/24.52.32224.5/intel-opencl-icd_24.52.32224.5_amd64.deb \n https://github.com/intel/compute-runtime/releases/download/24.52.32224.5/libigdgmm12_22.5.5_amd64.deb\n\nAny help is greatly appreciated.",
      "state": "closed",
      "author": "westcoastdevv",
      "author_type": "User",
      "created_at": "2025-03-04T04:39:44Z",
      "updated_at": "2025-04-11T06:38:11Z",
      "closed_at": "2025-04-11T06:38:10Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12925/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "qiuxin2012"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12925",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12925",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:42.939560",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "The error message is like https://github.com/intel/ipex-llm/issues/12654, but it's unresolved.\nAre you running on the host or the container?",
          "created_at": "2025-03-05T03:50:55Z"
        },
        {
          "author": "westcoastdevv",
          "body": "This is in this docker container [mattcurf/ollama-intel-gpu](https://github.com/mattcurf/ollama-intel-gpu). I have also gotten the same result running the portable zip version directly on the host.",
          "created_at": "2025-03-05T07:56:54Z"
        },
        {
          "author": "westcoastdevv",
          "body": "Are there only certain kernel versions that are supported for Ubuntu 24.04.2? I'm currently using 6.11.0-19-generic, and I have also tried 6.8.0-55.",
          "created_at": "2025-03-18T03:16:34Z"
        },
        {
          "author": "qiuxin2012",
          "body": "@westcoastdevv  can you help to try if llama cpp works? https://github.com/intel/ipex-llm/blob/dd026db50b0766940d2d4638ece39d08cacc839a/docs/mddocs/Quickstart/llamacpp_portable_zip_gpu_quickstart.md\nIt may be caused by `illegal instruction` due to your 9900k.",
          "created_at": "2025-03-19T02:25:13Z"
        },
        {
          "author": "westcoastdevv",
          "body": "@qiuxin2012 Here is the output after running a model with the exact parameters from the wiki you linked: <details><summary>Prompt 1</summary>\n<p>\n./llama-cli -m models/qwq-32b-q6_k.gguf -p \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistan",
          "created_at": "2025-03-21T01:36:34Z"
        }
      ]
    },
    {
      "issue_number": 13058,
      "title": "Consultation on memory utilization during flash-moe runtime.",
      "body": "Under my configuration with an AMD 5600X, A770 GPU, and 128GB RAM,\n I successfully ran the DeepSeek-R1-UD-IQ1_S-00001-of-00003 model (130GB size) in flash-moe mode with new llama.cpp Portable Zip\nbut the speed is very slow at only 1.5 tokens/s. I want to confirm:\n\n1.Is it normal that the memory usage during model execution is not high?\n2.Are there parameters not enabled that prevent the RAM from participating in the computation process?\n3.Since flash-moe’s recommended configuration requires 380GB RAM, I assumed memory usage would be much higher during runtime. Why is this not the case?",
      "state": "closed",
      "author": "Snaker1988",
      "author_type": "User",
      "created_at": "2025-04-09T17:58:27Z",
      "updated_at": "2025-04-10T12:03:48Z",
      "closed_at": "2025-04-10T12:03:48Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13058/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13058",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13058",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:43.223178",
      "comments": [
        {
          "author": "rnwang04",
          "body": "Hi @Snaker1988 , llama.cpp will default use memory-map model, so you will observe not obvious memory increasement.\nIf you add `--no-mmap` in your cli argument, you will observe much more memory used.",
          "created_at": "2025-04-10T02:22:16Z"
        },
        {
          "author": "Snaker1988",
          "body": "HI @rnwang04 Thank you for your help. With the --no-mmap parameter you suggested, I have successfully achieved full memory utilization during operation, and the speed has increased from the original ‌1.5 tokens/s‌ to ‌2.4 tokens/s‌.\n\n![Image](https://github.com/user-attachments/assets/a618b617-6b57-",
          "created_at": "2025-04-10T12:03:26Z"
        }
      ]
    },
    {
      "issue_number": 13062,
      "title": "Currently, there is no support for gguf-based ollama-ipex-llm inference in jina-embeddings-v3 or jina-reranker-v2-base-multilingual. Do we have any plans to add this support in the future?",
      "body": "Currently, there is no support for gguf-based ollama-ipex-llm inference in jina-embeddings-v3 or jina-reranker-v2-base-multilingual. Do we have any plans to add this support in the future?\n\nollama-ipex-llm [ollama-ipex-llm-2.2.0-ubuntu.tgz](https://github.com/intel/ipex-llm/releases/download/v2.2.0/ollama-ipex-llm-2.2.0-ubuntu.tgz)\n\nmodel url: \nembedding model: https://huggingface.co/jinaai/jina-embeddings-v3 \nreranker model: https://huggingface.co/jinaai/jina-reranker-v2-base-multilingual\n\ngguf model url:\nembedding model: https://huggingface.co/AgainstEntropy/jina-embeddings-v3-gguf\nreranker model: https://huggingface.co/gpustack/jina-reranker-v2-base-multilingual-GGUF\n",
      "state": "open",
      "author": "szzzh",
      "author_type": "User",
      "created_at": "2025-04-10T03:09:45Z",
      "updated_at": "2025-04-10T03:09:45Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13062/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13062",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13062",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:45.391055",
      "comments": []
    },
    {
      "issue_number": 12504,
      "title": "How can I use NPU to run Ollama on Intel Ultra7 155H chip in a laptop?",
      "body": null,
      "state": "open",
      "author": "Muzixin",
      "author_type": "User",
      "created_at": "2024-12-05T03:22:02Z",
      "updated_at": "2025-04-09T02:45:53Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12504/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12504",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12504",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:45.391075",
      "comments": [
        {
          "author": "Muzixin",
          "body": "Or how can I simultaneously use NPU and Arc GPU to run ollama？",
          "created_at": "2024-12-05T03:24:59Z"
        },
        {
          "author": "mordonez",
          "body": "+1",
          "created_at": "2024-12-05T14:40:30Z"
        },
        {
          "author": "Oscilloscope98",
          "body": "We currently do not support Ollama on Intel NPU.\r\n\r\nPlease refer to our [QuickStart](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/npu_quickstart.md) for more information and any updates :)",
          "created_at": "2024-12-06T02:16:01Z"
        },
        {
          "author": "Muzixin",
          "body": "@Oscilloscope98   Is there a support plan？",
          "created_at": "2024-12-06T02:20:20Z"
        },
        {
          "author": "Oscilloscope98",
          "body": "> @Oscilloscope98 Is there a support plan？\r\n\r\nWe will look into it :)",
          "created_at": "2024-12-06T03:10:23Z"
        }
      ]
    },
    {
      "issue_number": 13054,
      "title": "Performance reduction on integrated ARC GPU (140T) and Ollama portable (Linux)",
      "body": "IPEX Version: ollama-ipex-llm-2.2.0-ubuntu.tgz\n\nLLM  Used\nLlama 3.1 8B (4bit quantization)\n\nThis is a quick comparision between the same model (Llama 3.1 8b) running under the docker container intelanalytics/ipex-llm-inference-cpp-xpu:latest and a docker container that we have created using the ollama portable release.\n\nWhen running under the intelanalytics/ipex-llm-inference-cpp-xpu:latest we get around 17 token/sec (Intel ARC 140T, core Ultra 9 285H). When running with our docker container that includes the 2.2.0 version (released today) of the ollama portable version we get around 13- 14 tokens/sec. This is running in the exact same machine\n\nThis is the content of the docker container file that we have used to create our container with the ollama portable tar.gz:\n\n```\nFROM ubuntu:latest\n\nENV no_proxy=localhost,127.0.0.1\nENV OLLAMA_HOST='0.0.0.0'\n\nRUN apt update && apt install -y wget gpg\n&& wget -qO - https://repositories.intel.com/gpu/intel-graphics.key | gpg --yes --dearmor --output /usr/share/keyrings/intel-graphics.gpg\n&& echo \"deb [arch=amd64,i386 signed-by=/usr/share/keyrings/intel-graphics.gpg] https://repositories.intel.com/gpu/ubuntu noble unified\" | tee /etc/apt/sources.list.d/intel-gpu-noble.list\n&& apt update && apt install -y libze-intel-gpu1 libze1 intel-opencl-icd clinfo intel-gsc\n\nRUN cd ~/ && wget https://github.com/intel/ipex-llm/releases/download/v2.2.0/ollama-ipex-llm-2.2.0-ubuntu.tgz\n&& mkdir -p ollama-ipex\n&& tar -zxvf ollama-ipex-llm-2.2.0-ubuntu.tgz --strip-components=1 -C ollama-ipex\n&& rm ollama-ipex-llm-2.2.0-ubuntu.tgz\n\nWORKDIR /root/ollama-ipex\n\nEXPOSE 11434\n\nCMD [\"./start-ollama.sh\"]\n```\n\nIs there anything that we can do to reach performance parity with the intelanalytics/ipex-llm-inference-cpp-xpu:latest docker image? This is critical for us since we were able to reach a size reduction for the docker image from the original 20Gb to around 1GB,\n\nDetailed Steps to Reproduce\n\n1. Use the provided Dockerfile. Build the image using the following command: sudo docker build -t ollama-ipex .\n2. Run the local container with: sudo docker run -itd --net=host --device=/dev/dri -v ollamamodels:/root/.ollama/models --memory=\"32G\" --name=ollamaintel --shm-size=\"16g\" ollama-ipex:latest\n3. Enter the container with docker exec -it ollamaintel /bin/bash\n4. Run ollama with: ./ollama run llama3.1 --verbose\n",
      "state": "open",
      "author": "helderpe",
      "author_type": "User",
      "created_at": "2025-04-07T12:41:02Z",
      "updated_at": "2025-04-09T01:33:04Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13054/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "qiuxin2012"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13054",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13054",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:45.627070",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "Please share the ipex-llm version in your `intelanalytics/ipex-llm-inference-cpp-xpu:latest`?",
          "created_at": "2025-04-08T01:20:56Z"
        },
        {
          "author": "helderpe",
          "body": "Hello.\n\nI'm guessing you are asking for the Ollama build inside the ipex-llm container? ollama version is 0.5.4-ipexllm-20250406\n\nI'm also posting here the log of the model initialization: https://gist.github.com/helderpe/c6aa915084ec614bdfc606be892917d0",
          "created_at": "2025-04-08T08:05:33Z"
        },
        {
          "author": "qiuxin2012",
          "body": "https://github.com/intel/ipex-llm/blob/main/docker/llm/inference-cpp/Dockerfile is `intelanalytics/ipex-llm-inference-cpp-xpu:latest`'s Dockerfile. You can change it to minimize it's dependencies.",
          "created_at": "2025-04-09T01:27:45Z"
        }
      ]
    },
    {
      "issue_number": 12865,
      "title": "Alpaca QLoRA training doesn't support Gemma model",
      "body": "I have a customer that requires training/fine-tuning to be done on Gemma model, but it is not currently one of the supported models in python/llm/example/GPU/LLM-Finetuning/QLoRA/alpaca-qlora. Supported models are: \n\nLLaMA2-7B \nLLaMA2-13B \nLLaMA2-70B \nLLaMA3-8B \nChatGLM3-6B \nQwen-1.5-7B \nBaichuan2-7B \n\n\nIs there any way to fine tune a Gemma model instead?",
      "state": "closed",
      "author": "vishakh15nair",
      "author_type": "User",
      "created_at": "2025-02-20T09:29:14Z",
      "updated_at": "2025-04-08T08:26:00Z",
      "closed_at": "2025-04-08T08:26:00Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12865/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Uxito-Ada"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12865",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12865",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:45.819366",
      "comments": [
        {
          "author": "Uxito-Ada",
          "body": "Hi @vishakh15nair ,\n\nWe'd like to support Gemma to apply IPEX-LLM QLoRA.\n\nAs the the example codes depend on the specific model and dataset, we'd like to know more about your requirement.\n\nWhich Gemma model and dataset do you need? Since the official github has not come up with a QLoRA fine-tuning, ",
          "created_at": "2025-02-21T02:57:03Z"
        },
        {
          "author": "Uxito-Ada",
          "body": "Supported in #12969 ",
          "created_at": "2025-03-14T02:23:00Z"
        }
      ]
    },
    {
      "issue_number": 12420,
      "title": "Error: llama runner process has terminated: error loading model: No device of requested type available",
      "body": "Hello,\r\n\r\nI exactly followed the instruction below to try to run Ollama on my iGPU of i7-13700K, but got error when I run the ./ollama run llama3.1:8b.\r\n\r\nhttps://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.md\r\n\r\nhere are the output from Ollama serve before and after ollama run.\r\n\r\ndoes anyone can help?\r\n\r\n(ollama) llm@fanlessLinux:~$ hwinfo --display\r\n27: PCI 02.0: 0300 VGA compatible controller (VGA)              \r\n  [Created at pci.386]\r\n  Unique ID: _Znp.iTfrQPXqtxB\r\n  SysFS ID: /devices/pci0000:00/0000:00:02.0\r\n  SysFS BusID: 0000:00:02.0\r\n  Hardware Class: graphics card\r\n  Device Name: \"Onboard - Video\"\r\n  Model: \"Intel VGA compatible controller\"\r\n  Vendor: pci 0x8086 \"Intel Corporation\"\r\n  Device: pci 0xa780 \r\n  SubVendor: pci 0x1458 \"Gigabyte Technology Co., Ltd\"\r\n  SubDevice: pci 0xd000 \r\n  Revision: 0x04\r\n  Driver: \"i915\"\r\n  Driver Modules: \"i915\"\r\n  Memory Range: 0x6002000000-0x6002ffffff (rw,non-prefetchable)\r\n  Memory Range: 0x4000000000-0x403fffffff (ro,non-prefetchable)\r\n  I/O Ports: 0x5000-0x503f (rw)\r\n  Memory Range: 0x000c0000-0x000dffff (rw,non-prefetchable,disabled)\r\n  IRQ: 220 (11520 events)\r\n  Module Alias: \"pci:v00008086d0000A780sv00001458sd0000D000bc03sc00i00\"\r\n  Driver Info #0:\r\n    Driver Status: i915 is active\r\n    Driver Activation Cmd: \"modprobe i915\"\r\n  Config Status: cfg=new, avail=yes, need=no, active=unknown\r\n\r\nPrimary display adapter: #27\r\n\r\nsudo dmesg | grep i915\r\n[    8.042895] i915 0000:00:02.0: Using 24 cores (0-23) for kthreads\r\n[    8.043501] i915 0000:00:02.0: vgaarb: deactivate vga console\r\n[    8.043512] i915 0000:00:02.0: Using Transparent Hugepages\r\n[    8.053594] i915 0000:00:02.0: [drm] Finished loading DMC firmware i915/adls_dmc_ver2_01.bin (v2.1)\r\n[    8.085845] i915 0000:00:02.0: [drm] Protected Xe Path (PXP) protected content support initialized\r\n[    8.085848] i915 0000:00:02.0: GT0: GuC firmware i915/tgl_guc_70.26.4.bin version 70.26.4\r\n[    8.085851] i915 0000:00:02.0: GT0: HuC firmware i915/tgl_huc_7.9.3.bin version 7.9.3\r\n[    8.118088] [drm] Initialized i915 1.6.0 20201103 for 0000:00:02.0 on minor 0\r\n[    8.152002] snd_hda_intel 0000:00:1f.3: bound 0000:00:02.0 (ops i915_audio_component_bind_ops [i915])\r\n\r\n\r\n\r\n==========ollama serve output========\r\ntime=2024-11-20T15:05:56.394-05:00 level=INFO source=payload.go:44 msg=\"Dynamic LLM libraries [cpu cpu_avx cpu_avx2]\". <== does this mean gpu was not recognized?\r\n[GIN] 2024/11/20 - 15:06:09 | 200 |      82.264µs |       127.0.0.1 | HEAD     \"/\"\r\n[GIN] 2024/11/20 - 15:06:09 | 200 |   19.279993ms |       127.0.0.1 | POST     \"/api/show\"\r\n\r\n#########\r\nollama run from here\r\n#########\r\n\r\ntime=2024-11-20T15:06:09.925-05:00 level=INFO source=gpu.go:168 msg=\"looking for compatible GPUs\"\r\ntime=2024-11-20T15:06:09.925-05:00 level=WARN source=gpu.go:560 msg=\"unable to locate gpu dependency libraries\" <== this might be the real reason.\r\ntime=2024-11-20T15:06:09.925-05:00 level=WARN source=gpu.go:560 msg=\"unable to locate gpu dependency libraries\"\r\ntime=2024-11-20T15:06:09.928-05:00 level=WARN source=gpu.go:560 msg=\"unable to locate gpu dependency libraries\"\r\ntime=2024-11-20T15:06:09.929-05:00 level=INFO source=gpu.go:280 msg=\"no compatible GPUs were discovered\"\r\ntime=2024-11-20T15:06:09.952-05:00 level=INFO source=memory.go:309 msg=\"offload to cpu\" layers.requested=-1 layers.model=33 layers.offload=0 layers.split=\"\" memory.available=\"[91.5 GiB]\" memory.required.full=\"5.8 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"1.0 GiB\" memory.required.allocations=\"[5.8 GiB]\" memory.weights.total=\"4.7 GiB\" memory.weights.repeating=\"4.3 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"560.0 MiB\" memory.graph.partial=\"677.5 MiB\"\r\ntime=2024-11-20T15:06:09.953-05:00 level=INFO source=server.go:395 msg=\"starting llama server\" cmd=\"/tmp/ollama829815146/runners/cpu_avx2/ollama_llama_server --model /home/llm/.ollama/models/blobs/sha256-8eeb52dfb3bb9aefdf9d1ef24b3bdbcfbe82238798c4b918278320b6fcef18fe --ctx-size 8192 --batch-size 512 --embedding --log-disable --n-gpu-layers 999 --no-mmap --parallel 4 --port 41231\"",
      "state": "closed",
      "author": "fanlessfan",
      "author_type": "User",
      "created_at": "2024-11-20T20:22:41Z",
      "updated_at": "2025-04-07T15:13:29Z",
      "closed_at": "2025-04-07T15:13:28Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12420/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12420",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12420",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:45.992208",
      "comments": [
        {
          "author": "fanlessfan",
          "body": "how do I know if my iGPU driver installed properly?\r\n",
          "created_at": "2024-11-20T20:29:51Z"
        },
        {
          "author": "fanlessfan",
          "body": "for some reason I make it work once, at that time I want to compare the performance or CPU and GPU, so I installed the regular ollama and broke the GPU version. after that I re-installed the Ubuntu, but never make it work again.",
          "created_at": "2024-11-20T20:32:11Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @fanlessfan, `msg=\"Dynamic LLM libraries [cpu cpu_avx cpu_avx2]\"` does not mean that the iGPU was not recognized. If your iGPU is installed correctly, you will see the following log when ollama serve is running:\r\n\r\n```\r\nfound 1 SYCL devices:\r\n|  |                   |                              ",
          "created_at": "2024-11-21T05:48:26Z"
        },
        {
          "author": "fanlessfan",
          "body": "Thank you @sgwhat Here is my output from ollama serve. there is no found 1 SYCL devices. Could you help me to check what's wrong?\r\n\r\nthx again.\r\n\r\n(ollama) llm@fanlessLinux:~$ ./oserve \r\n \r\n:: initializing oneAPI environment ...\r\n   oserve: BASH_VERSION = 5.1.16(1)-release\r\n   args: Using \"$@\" for s",
          "created_at": "2024-11-21T19:19:49Z"
        },
        {
          "author": "fanlessfan",
          "body": "here is my hwinfo output\r\n\r\nhwinfo --display\r\n27: PCI 02.0: 0300 VGA compatible controller (VGA)              \r\n  [Created at pci.386]\r\n  Unique ID: _Znp.iTfrQPXqtxB\r\n  SysFS ID: /devices/pci0000:00/0000:00:02.0\r\n  SysFS BusID: 0000:00:02.0\r\n  Hardware Class: graphics card\r\n  Device Name: \"Onboard -",
          "created_at": "2024-11-21T19:20:57Z"
        }
      ]
    },
    {
      "issue_number": 12861,
      "title": "failed to load llama-3.2-vision using ollama-0.5.4-ipex-llm-2.2.0b20250218-win.zip",
      "body": "Hello,\n\nI got error below when I try to load llama3.2-vision:11b using ollama-0.5.4-ipex-llm-2.2.0b20250218-win. it's ok to load non vision models.\n\nmllama_model_load: model name:   Llama-3.2-11B-Vision-Instruct\nmllama_model_load: description:  vision encoder for Mllama\nmllama_model_load: GGUF version: 3\nmllama_model_load: alignment:    32\nmllama_model_load: n_tensors:    512\nmllama_model_load: n_kv:         17\nmllama_model_load: ftype:        f16\nmllama_model_load:\nmllama_model_load: vision using CPU backend\nD:\\actions-runner\\release-cpp-oneapi_2024_2\\_work\\llm.cpp\\llm.cpp\\ollama-llama-cpp\\ggml\\src\\ggml.c:3094: GGML_ASSERT(ggml_nelements(a) == ne0*ne1*ne2) failed\ntime=2025-02-19T22:24:40.082-05:00 level=INFO source=server.go:605 msg=\"waiting for server to become available\" status=\"llm server not responding\"\ntime=2025-02-19T22:24:40.332-05:00 level=ERROR source=sched.go:455 msg=\"error loading llama server\" error=\"llama runner process has terminated: GGML_ASSERT(ggml_nelements(a) == ne0*ne1*ne2) failed\"\n[GIN] 2025/02/19 - 22:24:40 | 500 |    5.2468821s |       127.0.0.1 | POST     \"/api/generate\"",
      "state": "closed",
      "author": "fanlessfan",
      "author_type": "User",
      "created_at": "2025-02-20T03:30:29Z",
      "updated_at": "2025-04-07T15:12:56Z",
      "closed_at": "2025-04-07T15:12:54Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 14,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12861/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "plusbang"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12861",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12861",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:46.185931",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @fanlessfan, I cannot reproduce this issue, could you provide detailed logs? ",
          "created_at": "2025-02-20T08:27:55Z"
        },
        {
          "author": "fanlessfan",
          "body": "here are the detail output from start-ollama.bat and ollama run\n\n.\\start-ollama.bat\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\nggml_sycl_init: SYCL_USE_XMX: yes\nggml_sycl_init: found 1 SYCL devices:\n2025/02/20 08:32:11 routes.go:1259: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDIN",
          "created_at": "2025-02-20T13:36:39Z"
        },
        {
          "author": "plusbang",
          "body": "Hi, @fanlessfan , according to your log, we also tried on Intel UHD Graphics 770, but still failed to reproduce it.\n\nPlease verify your driver version according to [doc](https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_portablze_zip_quickstart.md#prerequisites).",
          "created_at": "2025-02-21T07:47:36Z"
        },
        {
          "author": "fanlessfan",
          "body": "Hello @plusbang , I upgraded the driver to 32.0.101.6078 according to the document, but still got the exactly same error. btw, i can successfully run the linux portable on ubuntu without error. Is there any logs do you need to analyze it?",
          "created_at": "2025-02-21T14:00:27Z"
        },
        {
          "author": "plusbang",
          "body": "\n> Hello [@plusbang](https://github.com/plusbang) , I upgraded the driver to 32.0.101.6078 according to the document, but still got the exactly same error. btw, i can successfully run the linux portable on ubuntu without error. Is there any logs do you need to analyze it?\n\nCould you please provide i",
          "created_at": "2025-02-24T02:22:34Z"
        }
      ]
    },
    {
      "issue_number": 13047,
      "title": "Native API failed. Native API returns: 20 (UR_RESULT_ERROR_DEVICE_LOST)",
      "body": "**Describe the bug**\nWe have customer running intelanalytics/ipex-llm-inference-cpp-xpu:latest as part of there chatbot application as docker-compose.yaml on Intel Core Ultra 9 288V and 2/10 times they see belwow error after calling llama3.2 model running ipex-llm-inference-cpp-xpu:latest container running llama3.2 mode on iGPU.\n\nCustomer also mentioned : As ipex-llm is running as container along with whole Enterprise RAG conatiner (OPEA -RAG) and sometime in Task Manager when Graphics Utilizzation reached 100% this issue and black screen they see. But this Core Ultra 9 288H, has only ipex-llm conatiner running llama3.2 model using GPU , nothing else , except my Winodws 11 VGA ?? so this should not use GPU 100% llama3.2-3B model and small 10 pages pdf file as RAG ? anyhow this happens 2/10 times not always.\n\n**Host OS**: Windows 11\n**WSL2** : Ubuntu 24.04 \n**kernel**: 5.15.167.4-microsoft-standard-WSL2\n**Docker-desktop**\nWindows iGPU driver: Intel Arc 140V GPU (**Driver: 32.0.101.6653**) \n**Error:**\n```\nFound 1 SYCL devices:\n2025-04-02 15:23:58 ollama                       | |  |                   |                                       |       |Max    |        |Max  |Global |                     |\n2025-04-02 15:23:58 ollama                       | |  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\n2025-04-02 15:23:58 ollama                       | |ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\n2025-04-02 15:23:58 ollama                       | |--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\n2025-04-02 15:23:58 ollama                       | | 0| [level_zero:gpu:0]|                Intel Graphics [0x64a0]|   20.4|     64|    1024|   32| 17641M|     1.6.32224.500000|\n2025-04-02 15:23:58 ollama                       | llama_kv_cache_init:      SYCL0 KV buffer size =   224.00 MiB\n2025-04-02 15:23:58 ollama                       | llama_new_context_with_model: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB\n2025-04-02 15:23:58 ollama                       | llama_new_context_with_model:  SYCL_Host  output buffer size =     0.50 MiB\n2025-04-02 15:23:58 ollama                       | llama_new_context_with_model:      SYCL0 compute buffer size =   256.50 MiB\n2025-04-02 15:23:58 ollama                       | llama_new_context_with_model:  SYCL_Host compute buffer size =    10.01 MiB\n2025-04-02 15:23:58 ollama                       | llama_new_context_with_model: graph nodes  = 790\n2025-04-02 15:23:58 ollama                       | llama_new_context_with_model: graph splits = 2\n2025-04-02 15:23:58 ollama                       | time=2025-04-02T17:53:58.245+08:00 level=WARN source=runner.go:892 msg=\"%s: warming up the model with an empty run - please wait ... \" !BADKEY=loadModel\n2025-04-02 15:23:58 ollama                       | time=2025-04-02T17:53:58.384+08:00 level=INFO source=server.go:610 msg=\"llama runner started in 15.32 seconds\"\n2025-04-02 15:24:39 ollama                       | Native API failed. Native API returns: 20 (UR_RESULT_ERROR_DEVICE_LOST)\n2025-04-02 15:24:39 ollama                       | Exception caught at file:/home/runner/_work/llm.cpp/llm.cpp/ollama-llama-cpp/ggml/src/ggml-sycl/ggml-sycl.cpp, line:4893, func:operator()\n2025-04-02 15:24:39 ollama                       | SYCL error: CHECK_TRY_ERROR((stream)->memcpy( data, (const char *)tensor->data + offset, size).wait()): Meet error in this line code!\n2025-04-02 15:24:39 ollama                       |   in function ggml_backend_sycl_get_tensor_async at /home/runner/_work/llm.cpp/llm.cpp/ollama-llama-cpp/ggml/src/ggml-sycl/ggml-sycl.cpp:4893\n2025-04-02 15:24:39 ollama                       | /home/runner/_work/llm.cpp/llm.cpp/ollama-llama-cpp/ggml/src/ggml-sycl/../ggml-sycl/common.hpp:107: SYCL error\n2025-04-02 15:24:39 ollama                       | /usr/local/lib/python3.11/dist-packages/bigdl/cpp/libs/libollama-ggml-base.so(+0xce17)[0x7f30df20ce17]\n2025-04-02 15:24:39 ollama                       | /usr/local/lib/python3.11/dist-packages/bigdl/cpp/libs/libollama-ggml-base.so(ggml_abort+0xd0)[0x7f30df20cda0]\n2025-04-02 15:24:39 ollama                       | libollama-ggml-sycl.so(+0x76a28)[0x7f30dc476a28]\n2025-04-02 15:24:39 ollama                       | libollama-ggml-sycl.so(+0xa764f)[0x7f30dc4a764f]\n2025-04-02 15:24:39 ollama                       | /usr/local/lib/python3.11/dist-packages/bigdl/cpp/libs/libollama_llama.so(llama_decode+0xe30)[0x7f30dfa5dcc0]\n2025-04-02 15:24:39 ollama                       | /usr/local/lib/python3.11/dist-packages/bigdl/cpp/libs/ollama(_cgo_c1fbea34dcd6_Cfunc_llama_decode+0x42)[0x563b70c58522]\n2025-04-02 15:24:39 ollama                       | /usr/local/lib/python3.11/dist-packages/bigdl/cpp/libs/ollama(+0x2c5ca1)[0x563b700c5ca1]\n2025-04-02 15:24:39 ollama                       | SIGABRT: abort\n```\n![Image](https://github.com/user-attachments/assets/8cb6ac20-aa27-4ede-9752-7a3a9ae8cf5d)\n\n**How to reproduce**\nSteps to reproduce the error:\n1. Docker or docker-compose running llama3.2 model under Core Ultra 9 288V on iGPU using below ollama docker from ipex-llm:\n````\nollama:\n    image: intelanalytics/ipex-llm-inference-cpp-xpu:latest\n    # network_mode:  \"host\"\n    container_name: ollama\n    restart: unless-stopped\n    privileged: true\n    devices:\n      - /dev/dri:/dev/dri \n    ports:\n      # localhost only, protected from outside\n      #- 172.17.0.1:11434:11434\n      # expose port to outside, useful for developer and remote connections\n      - 11434:11434\n    environment:\n      - no_proxy=localhost,127.0.0.1\n      - DEVICE=iGPU\n      - OLLAMA_HOST=0.0.0.0\n      - OLLAMA_MODEL=llama3.2\n    mem_limit: 32G\n    shm_size: 16G\n    working_dir: /llm/scripts\n    volumes:\n      - /usr/lib/wsl:/usr/lib/wsl\n      - ollama:/root/.ollama\n      - ./startup.sh:/startup.sh\n      #pull_policy: always\n    tty: true\n    entrypoint: [\"/bin/bash\", \"-c\"]\n    command: /startup.sh\n````\nstartup.sh:\n```\n#!/bin/bash\n\ncd /llm/scripts/\nsource ipex-llm-init --gpu --device iGPU\nbash start-ollama.sh\n\n# Wait for initialization\n# sleep 5\n# cd /llm/ollama\n# ./ollama run llama3.2 --verbose\n# Keep container running\ntail -f /dev/null\n```\n\n**Screenshots**\n\n![Image](https://github.com/user-attachments/assets/bdec209e-fac0-4f18-a658-0238e3beab09)\n\n**Environment information**\nHost OS: Windows 11\nWSL2 : Ubuntu 24.04 \nkernel: 5.15.167.4-microsoft-standard-WSL2\nDocker-desktop\n\nWindows iGPU driver: Intel Arc 140V GPU (Driver: 32.0.101.6653) \n\n![Image](https://github.com/user-attachments/assets/8cb6ac20-aa27-4ede-9752-7a3a9ae8cf5d)\n\n**Additional context**\nThis happens intermitently : asking bigger question 2 lines -5 lines prompt.\n\n[Ollama-error.txt](https://github.com/user-attachments/files/19595089/Ollama-error.txt)\n",
      "state": "open",
      "author": "shailesh837",
      "author_type": "User",
      "created_at": "2025-04-03T21:38:21Z",
      "updated_at": "2025-04-07T07:25:54Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13047/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "qiuxin2012"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13047",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13047",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:46.415684",
      "comments": [
        {
          "author": "shailesh837",
          "body": "Please this is customer issue , using ipex-llm in product, can we have some reponse here.",
          "created_at": "2025-04-04T21:26:45Z"
        },
        {
          "author": "qiuxin2012",
          "body": "Does portable zip work? https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_portable_zip_quickstart.md",
          "created_at": "2025-04-07T03:01:10Z"
        },
        {
          "author": "shailesh837",
          "body": "@qiuxin2012 : Please see the docker-compose.yaml section, As we need to run as docker conatiner ? how can we do it with portable ollama zip ?\n\n```\nollama:\n    image: intelanalytics/ipex-llm-inference-cpp-xpu:latest\n    # network_mode:  \"host\"\n    container_name: ollama\n    restart: unless-stopped\n  ",
          "created_at": "2025-04-07T07:25:53Z"
        }
      ]
    },
    {
      "issue_number": 13048,
      "title": "OSError: [WinError 127]  AFTER RUN \"from ipex_llm.transformers import AutoModel,AutoModelForCausalLM\"",
      "body": "**Describe the bug**\nOSError: [WinError 127]  AFTER RUN \"from ipex_llm.transformers import AutoModel,AutoModelForCausalLM\"\n\n**How to reproduce**\nSteps to reproduce the error:\n1. Install the ipex\n2. Install the ipywidgets\n3. Install the Pandas\n4. Run \"from ipex_llm.transformers import AutoModel,AutoModelForCausalLM\"\n\n**Screenshots**\nIf applicable, add screenshots to help explain the problem\n\n![Image](https://github.com/user-attachments/assets/a7f0f86c-de89-4d28-9fab-250a4375961d)\n\n**Additional context**\n\nOSError: [WinError 127] 找不到指定的程序。 Error loading \"C:\\Users\\xxx\\anaconda3\\envs\\llm\\Lib\\site-packages\\intel_extension_for_pytorch\\bin\\intel-ext-pt-gpu.dll\" or one of its dependencies.\n\n",
      "state": "open",
      "author": "mygame182",
      "author_type": "User",
      "created_at": "2025-04-06T01:42:20Z",
      "updated_at": "2025-04-07T05:07:55Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13048/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13048",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13048",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:46.583607",
      "comments": [
        {
          "author": "hkvision",
          "body": "Hi, seems there's something wrong for your ipex installation. Could you please provide more information:\n- The command you install `ipex-llm`. You don't need to install ipex manually, it will be installed automatically as a dependency of ipex-llm. The reference documentation is here: https://github.",
          "created_at": "2025-04-07T02:20:05Z"
        },
        {
          "author": "mygame182",
          "body": "Thanks for hkvision.\n\nI think I found the problem and solved it :\n\nWhen I finish the install of ipex,  it can run the test code normally. \nThen I try it in a CUDA code program, instead by IPEX. When I run the model training code in the first time, the program just hang up in a few minutes.  But I IN",
          "created_at": "2025-04-07T03:26:58Z"
        }
      ]
    },
    {
      "issue_number": 13042,
      "title": "No Qwen2.5-VL-3B-Instruct gguf support. Do we have plan to support?",
      "body": "No Qwen2.5-VL-3B-Instruct gguf support. Do we have plan to support?\n\nhttps://github.com/intel/ipex-llm/releases/tag/v2.2.0-nightly\n[llama-cpp-ipex-llm-2.2.0b20250313-ubuntu-core.tgz](https://github.com/intel/ipex-llm/releases/download/v2.2.0-nightly/llama-cpp-ipex-llm-2.2.0b20250313-ubuntu-core.tgz)\n\nmodel url: https://hf-mirror.com/RzZ/Qwen2.5-VL-3B-GGUF\n",
      "state": "open",
      "author": "szzzh",
      "author_type": "User",
      "created_at": "2025-04-03T08:05:03Z",
      "updated_at": "2025-04-07T02:31:44Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13042/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "qiuxin2012"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13042",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13042",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:46.786052",
      "comments": []
    },
    {
      "issue_number": 13001,
      "title": "Met segment fault while running Whisper on Arc",
      "body": "Configuration:\n```\nOS: Ubuntu 24.04 \nCPU: 12th Gen Intel(R) Core(TM) i9-12900K\nMemory: 16G\nGPU:  04:00.0 VGA compatible controller: Intel Corporation DG2 [Arc A770] (rev 08)\nsoftware:\n    torch                                    2.1.0a0+cxx11.abi\n    intel-extension-for-pytorch           2.1.10+xpu\n    ipex-llm                                  2.2.0b20250322\n    bigdl-core-xe-21                    2.6.0b20250322\n```\n\nIssue met:\nrun whisper with command `python ./recognize.py` and get segment fault error\n\nLogs:\n```\n$ python recognize.py\n/home/cloud/ruoyu/miniforge3/envs/llm/lib/python3.11/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n  warnings.warn(\n/home/cloud/ruoyu/miniforge3/envs/llm/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n  warn(\n2025-03-25 09:18:43,572 - INFO - intel_extension_for_pytorch auto imported\n2025-03-25 09:18:43,855 - INFO - PyTorch version 2.1.0a0+cxx11.abi available.\nstep1:\n/home/cloud/ruoyu/miniforge3/envs/llm/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n2025-03-25 09:18:46,419 - INFO - Converting the current model to sym_int4 format......\n\nLIBXSMM_VERSION: main_stable-1.17-3651 (25693763)\nLIBXSMM_TARGET: adl [12th Gen Intel(R) Core(TM) i9-12900K]\nRegistry and code: 13 MB\nCommand: python recognize.py\nUptime: 3.432546 s\nSegmentation fault\n\n```\n",
      "state": "closed",
      "author": "Ruoyu-y",
      "author_type": "User",
      "created_at": "2025-03-25T01:30:58Z",
      "updated_at": "2025-04-07T01:37:15Z",
      "closed_at": "2025-04-04T01:07:16Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 16,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13001/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13001",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13001",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:46.786073",
      "comments": [
        {
          "author": "Ruoyu-y",
          "body": "Any hint for this issue? Or recommended configuration?",
          "created_at": "2025-03-25T13:09:26Z"
        },
        {
          "author": "hkvision",
          "body": "Hi,\n\nMay I ask if this segment fault only exists for whisper or it also exists in running other models https://github.com/intel/ipex-llm/tree/main/python/llm/example/GPU/HuggingFace/LLM ?\n\nAlso, you may use our script to check the environment so that we can better help to detect the issue: https://g",
          "created_at": "2025-03-26T01:25:02Z"
        },
        {
          "author": "Ruoyu-y",
          "body": "> Hi,\n> \n> May I ask if this segment fault only exists for whisper or it also exists in running other models https://github.com/intel/ipex-llm/tree/main/python/llm/example/GPU/HuggingFace/LLM ?\n> \n> Also, you may use our script to check the environment so that we can better help to detect the issue:",
          "created_at": "2025-03-26T03:10:53Z"
        },
        {
          "author": "Ruoyu-y",
          "body": "To provide more details, on the same machine, i could run the inference service in docker according to the guide `https://github.com/intel/ipex-llm/blob/main/docs/mddocs/DockerGuides/vllm_docker_quickstart.md`. But i cannot run the whisper or other LLMs under `python/llm/example/GPU/HuggingFace/LLM`",
          "created_at": "2025-03-26T07:14:07Z"
        },
        {
          "author": "hkvision",
          "body": "Hi, we checked your env, the following part might have issues.\n```\n-----------------------------------------------------------------\nNo device discovered\nGPU0 Memory ize=256M\n```\nCould you use `sycl-ls` and `xpu-smi discovery` to confirm if the Arc device is properly detected? Thanks!\n",
          "created_at": "2025-03-26T08:11:50Z"
        }
      ]
    },
    {
      "issue_number": 13019,
      "title": "怎么限制 gpu使用率",
      "body": "我的电脑间歇性闪屏",
      "state": "open",
      "author": "gaoconggit",
      "author_type": "User",
      "created_at": "2025-03-27T22:04:53Z",
      "updated_at": "2025-04-06T21:22:01Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13019/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13019",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13019",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:46.998295",
      "comments": [
        {
          "author": "qiyuangong",
          "body": "如果使用的是windows，请升级到最新的驱动\nhttps://www.intel.cn/content/www/cn/zh/support/articles/000091441/graphics.html\n\n如果使用的是Linux、可以通过xpu-smi限制GPU的频率\n```bash\n sudo xpu-smi config -d 0 -t 0 --frequencyrange 2000,2000\n```",
          "created_at": "2025-03-28T02:45:32Z"
        },
        {
          "author": "gaoconggit",
          "body": "![Image](https://github.com/user-attachments/assets/ece0d76c-6ae6-42aa-8e5c-3660b573296b)\n\n我这个你们的显卡自带的工具，提示已经是最新的了",
          "created_at": "2025-03-30T21:57:07Z"
        },
        {
          "author": "qiyuangong",
          "body": "> ![Image](https://github.com/user-attachments/assets/ece0d76c-6ae6-42aa-8e5c-3660b573296b)\n> \n> 我这个你们的显卡自带的工具，提示已经是最新的了\n\n如果已经安装了这个软件，可以在“性能调优”选项内，调节GPU的频率和供电等。\n\n更多相关文档，请查阅以下链接\nhttp://intel.cn/content/www/cn/zh/products/docs/discrete-gpus/arc/software/graphics-software.html",
          "created_at": "2025-04-01T10:02:50Z"
        },
        {
          "author": "gaoconggit",
          "body": "![Image](https://github.com/user-attachments/assets/37e0e11d-186d-4f80-8f15-fa5c3b98a7a3)\n---\n我没有这个选项 ",
          "created_at": "2025-04-06T21:22:00Z"
        }
      ]
    },
    {
      "issue_number": 12992,
      "title": "Running `intelanalytics/ipex-llm-inference-cpp-xpu` image with A770 GPU and AMD EPYC CPU",
      "body": "**Describe the bug**\nI'm getting a bus error (core dumped) when running benchmark in the `intelanalytics/ipex-llm-inference-cpp-xpu:latest` image on my A770 GPU and AMD EPYC CPU.\n\n**How to reproduce**\nSteps to reproduce the error:\n1. Start the image using the following shell script:\n```bash\n#/bin/bash\nexport DOCKER_IMAGE=intelanalytics/ipex-llm-inference-cpp-xpu:latest\n\ndocker run -it --rm \\\n           --net=host \\\n           --device=/dev/dri \\\n           -v /mnt/speedtank/llm/models:/models \\\n           -e no_proxy=localhost,127.0.0.1 \\\n           --memory=\"32G\" \\\n           -e bench_model=\"mistral-7b-v0.1.Q4_K_M.gguf\" \\\n           -e DEVICE=Arc \\\n           --shm-size=\"16g\" \\\n           $DOCKER_IMAGE /bin/bash\n```\n2. Run `bash /llm/scripts/benchmark_llama-cpp.sh` in the image\n3. Crash\n\n**Screenshots**\n```\nroot@brownie:/llm# bash /llm/scripts/benchmark_llama-cpp.sh\nfound oneapi in /opt/intel/oneapi/setvars.sh\n\n:: initializing oneAPI environment ...\n   benchmark_llama-cpp.sh: BASH_VERSION = 5.1.16(1)-release\n   args: Using \"$@\" for setvars.sh arguments: --force\n:: advisor -- latest\n:: ccl -- latest\n:: compiler -- latest\n:: dal -- latest\n:: debugger -- latest\n:: dev-utilities -- latest\n:: dnnl -- latest\n:: dpcpp-ct -- latest\n:: dpl -- latest\n:: ipp -- latest\n:: ippcp -- latest\n:: mkl -- latest\n:: mpi -- latest\n:: pti -- latest\n:: tbb -- latest\n:: umf -- latest\n:: vtune -- latest\n:: oneAPI environment initialized ::\n\n/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\n/usr/local/lib/python3.11/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\n/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\n+++++ Env Variables +++++\nInternal:\n    ENABLE_IOMP     = 1\n    ENABLE_GPU      = 1\n    ENABLE_JEMALLOC = 0\n    ENABLE_TCMALLOC = 0\n    LIB_DIR    = /usr/local/lib\n    BIN_DIR    = bin64\n    LLM_DIR    = /usr/local/lib/python3.11/dist-packages/ipex_llm\n\nExported:\n    LD_PRELOAD             =\n    OMP_NUM_THREADS        =\n    MALLOC_CONF            =\n    USE_XETLA              = OFF\n    ENABLE_SDP_FUSION      =\n    SYCL_CACHE_PERSISTENT  = 1\n    BIGDL_LLM_XMX_DISABLED =\n    SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS = 1\n+++++++++++++++++++++++++\nComplete.\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\nggml_sycl_init: SYCL_USE_XMX: yes\nggml_sycl_init: found 1 SYCL devices:\nbuild: 1 (aef9006) with Intel(R) oneAPI DPC++/C++ Compiler 2025.0.4 (2025.0.4.20241205) for x86_64-unknown-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device SYCL0 (Intel(R) Arc(TM) A770 Graphics) - 15473 MiB free\nllama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /models/mistral-7b-v0.1.Q4_K_M.gguf (version GGUF V2)\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-v0.1\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  11:                          general.file_type u32              = 15\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  19:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q4_K:  193 tensors\nllama_model_loader: - type q6_K:   33 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1637 MB\nllm_load_print_meta: format           = GGUF V2\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 32768\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: f_attn_scale     = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 32768\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = Q4_K - Medium\nllm_load_print_meta: model params     = 7.24 B\nllm_load_print_meta: model size       = 4.07 GiB (4.83 BPW)\nllm_load_print_meta: general.name     = mistralai_mistral-7b-v0.1\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: offloading 32 repeating layers to GPU\nllm_load_tensors: offloading output layer to GPU\nllm_load_tensors: offloaded 33/33 layers to GPU\nllm_load_tensors:          CPU model buffer size =    70.31 MiB\nllm_load_tensors:        SYCL0 model buffer size =  4095.05 MiB\n./llm/scripts/benchmark_llama-cpp.sh: line 21:   540 Bus error               (core dumped) ./llama-cli -m $model -n 128 --prompt \"${promt_1024_128}\" -t 8 -e -ngl 999 --color --ctx-size 1024 --no-mmap --temp 0\n```\n\nIf I run `llama-cli` with the -v flag I get the following output:\n```\nroot@brownie:/llm/llama-cpp# ./llama-cli -m $model -n 128 --prompt \"${promt_1024_128}\"  -t 8 -e -ngl 999 --color --ctx-size 1024 --no-mmap --temp 0 -v\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\nggml_sycl_init: SYCL_USE_XMX: yes\nggml_sycl_init: found 1 SYCL devices:\nbuild: 1 (aef9006) with Intel(R) oneAPI DPC++/C++ Compiler 2025.0.4 (2025.0.4.20241205) for x86_64-unknown-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device SYCL0 (Intel(R) Arc(TM) A770 Graphics) - 15473 MiB free\nllama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /models/mistral-7b-v0.1.Q4_K_M.gguf (version GGUF V2)\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-v0.1\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  11:                          general.file_type u32              = 15\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  19:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q4_K:  193 tensors\nllama_model_loader: - type q6_K:   33 tensors\nllm_load_vocab: control token:      2 '</s>' is not marked as EOG\nllm_load_vocab: control token:      1 '<s>' is not marked as EOG\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1637 MB\nllm_load_print_meta: format           = GGUF V2\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 32768\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: f_attn_scale     = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 32768\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = Q4_K - Medium\nllm_load_print_meta: model params     = 7.24 B\nllm_load_print_meta: model size       = 4.07 GiB (4.83 BPW)\nllm_load_print_meta: general.name     = mistralai_mistral-7b-v0.1\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nload_tensors: layer   0 assigned to device SYCL0, is_swa = 0\nload_tensors: layer   1 assigned to device SYCL0, is_swa = 0\nload_tensors: layer   2 assigned to device SYCL0, is_swa = 0\nload_tensors: layer   3 assigned to device SYCL0, is_swa = 0\nload_tensors: layer   4 assigned to device SYCL0, is_swa = 0\nload_tensors: layer   5 assigned to device SYCL0, is_swa = 0\nload_tensors: layer   6 assigned to device SYCL0, is_swa = 0\nload_tensors: layer   7 assigned to device SYCL0, is_swa = 0\nload_tensors: layer   8 assigned to device SYCL0, is_swa = 0\nload_tensors: layer   9 assigned to device SYCL0, is_swa = 0\nload_tensors: layer  10 assigned to device SYCL0, is_swa = 0\nload_tensors: layer  11 assigned to device SYCL0, is_swa = 0\nload_tensors: layer  12 assigned to device SYCL0, is_swa = 0\nload_tensors: layer  13 assigned to device SYCL0, is_swa = 0\nload_tensors: layer  14 assigned to device SYCL0, is_swa = 0\nload_tensors: layer  15 assigned to device SYCL0, is_swa = 0\nload_tensors: layer  16 assigned to device SYCL0, is_swa = 0\nload_tensors: layer  17 assigned to device SYCL0, is_swa = 0\nload_tensors: layer  18 assigned to device SYCL0, is_swa = 0\nload_tensors: layer  19 assigned to device SYCL0, is_swa = 0\nload_tensors: layer  20 assigned to device SYCL0, is_swa = 0\nload_tensors: layer  21 assigned to device SYCL0, is_swa = 0\nload_tensors: layer  22 assigned to device SYCL0, is_swa = 0\nload_tensors: layer  23 assigned to device SYCL0, is_swa = 0\nload_tensors: layer  24 assigned to device SYCL0, is_swa = 0\nload_tensors: layer  25 assigned to device SYCL0, is_swa = 0\nload_tensors: layer  26 assigned to device SYCL0, is_swa = 0\nload_tensors: layer  27 assigned to device SYCL0, is_swa = 0\nload_tensors: layer  28 assigned to device SYCL0, is_swa = 0\nload_tensors: layer  29 assigned to device SYCL0, is_swa = 0\nload_tensors: layer  30 assigned to device SYCL0, is_swa = 0\nload_tensors: layer  31 assigned to device SYCL0, is_swa = 0\nload_tensors: layer  32 assigned to device SYCL0, is_swa = 0\nllm_load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\nllm_load_tensors: offloading 32 repeating layers to GPU\nllm_load_tensors: offloading output layer to GPU\nllm_load_tensors: offloaded 33/33 layers to GPU\nllm_load_tensors:          CPU model buffer size =    70.31 MiB\nllm_load_tensors:        SYCL0 model buffer size =  4095.05 MiB\nload_all_data: no device found for buffer type CPU for async uploads\n.Bus error (core dumped)\n```\n\nAny ideas? ",
      "state": "closed",
      "author": "abjugard",
      "author_type": "User",
      "created_at": "2025-03-22T10:31:03Z",
      "updated_at": "2025-04-05T11:03:51Z",
      "closed_at": "2025-04-05T11:03:50Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12992/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12992",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12992",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:47.260440",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "Could you share your result of `lscpu`? I want to know the CPU flags.",
          "created_at": "2025-03-24T01:52:54Z"
        },
        {
          "author": "abjugard",
          "body": "Sure, output from `lscpu` run from inside the `ipex-llm` Docker image:\n```\nArchitecture:             x86_64\n  CPU op-mode(s):         32-bit, 64-bit\n  Address sizes:          45 bits physical, 48 bits virtual\n  Byte Order:             Little Endian\nCPU(s):                   10\n  On-line CPU(s) list:",
          "created_at": "2025-03-24T08:41:03Z"
        },
        {
          "author": "qiuxin2012",
          "body": "I guess the failure is caused by lacking of `avx_vnni`, we have removed this requirement in latest docker image, you can update your image and try again.",
          "created_at": "2025-03-25T02:29:02Z"
        },
        {
          "author": "abjugard",
          "body": "Still getting:\n```\nroot@brownie:/llm/llama-cpp# ./llama-cli -m $model -n 128 --prompt \"${promt_1024_128}\"  -t 8 -e -ngl 999 --color --ctx-size 1024 --no-mmap --temp 0 -v\nbuild: 1 (3240917) with Intel(R) oneAPI DPC++/C++ Compiler 2025.0.4 (2025.0.4.20241205) for x86_64-unknown-linux-gnu\nmain: llama b",
          "created_at": "2025-03-25T09:02:57Z"
        },
        {
          "author": "Ksdb104",
          "body": "also epyc cpu，same issue，any progress？",
          "created_at": "2025-03-30T17:38:33Z"
        }
      ]
    },
    {
      "issue_number": 12981,
      "title": "Is Ollama Portable Zip has open source code?",
      "body": "Or is [Ollama Portable Zip has plan to support new version of ollama ?",
      "state": "open",
      "author": "chnxq",
      "author_type": "User",
      "created_at": "2025-03-19T09:23:47Z",
      "updated_at": "2025-04-04T18:10:11Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12981/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12981",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12981",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:47.492093",
      "comments": [
        {
          "author": "hkvision",
          "body": "The support of ollama new version is work in progress, we will keep updated when it is ready :)",
          "created_at": "2025-03-20T02:11:07Z"
        },
        {
          "author": "chnxq",
          "body": "Great! I'm looking forward to its arrival.",
          "created_at": "2025-03-21T14:34:16Z"
        },
        {
          "author": "chnxq",
          "body": "Have  made it concave now? Even if there is an unstable branch?",
          "created_at": "2025-04-04T18:10:09Z"
        }
      ]
    },
    {
      "issue_number": 13008,
      "title": "MTL NPU Error Output",
      "body": "## **Describe the bug**\nPortable llama.cpp cli for NPU cannot use\n\n## **Screenshots**\n\nllama-cpp-ipex-llm-2.2.0b20250313-win-npu.zip has extracted to C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\n\n```bash\nPS C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu> conda activate llm-npu\n(llm-npu) PS C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu> set IPEX_LLM_NPU_MTL=1\n(llm-npu) PS C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu> init-llama-cpp.bat\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\cache.json <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\intel_npu_acceleration_library\\lib\\Release\\cache.json 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\intel_npu_acceleration_library.dll <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\intel_npu_acceleration_library\\lib\\Release\\intel_npu_acceleration_library.dll 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\openvino.dll <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\intel_npu_acceleration_library\\lib\\Release\\openvino.dll 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\openvino_auto_batch_plugin.dll <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\intel_npu_acceleration_library\\lib\\Release\\openvino_auto_batch_plugin.dll 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\openvino_auto_plugin.dll <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\intel_npu_acceleration_library\\lib\\Release\\openvino_auto_plugin.dll 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\openvino_c.dll <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\intel_npu_acceleration_library\\lib\\Release\\openvino_c.dll 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\openvino_hetero_plugin.dll <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\intel_npu_acceleration_library\\lib\\Release\\openvino_hetero_plugin.dll 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\openvino_intel_cpu_plugin.dll <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\intel_npu_acceleration_library\\lib\\Release\\openvino_intel_cpu_plugin.dll 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\openvino_intel_gpu_plugin.dll <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\intel_npu_acceleration_library\\lib\\Release\\openvino_intel_gpu_plugin.dll 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\openvino_intel_npu_plugin.dll <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\intel_npu_acceleration_library\\lib\\Release\\openvino_intel_npu_plugin.dll 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\openvino_ir_frontend.dll <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\intel_npu_acceleration_library\\lib\\Release\\openvino_ir_frontend.dll 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\openvino_onnx_frontend.dll <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\intel_npu_acceleration_library\\lib\\Release\\openvino_onnx_frontend.dll 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\openvino_paddle_frontend.dll <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\intel_npu_acceleration_library\\lib\\Release\\openvino_paddle_frontend.dll 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\openvino_pytorch_frontend.dll <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\intel_npu_acceleration_library\\lib\\Release\\openvino_pytorch_frontend.dll 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\openvino_tensorflow_frontend.dll <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\intel_npu_acceleration_library\\lib\\Release\\openvino_tensorflow_frontend.dll 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\openvino_tensorflow_lite_frontend.dll <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\intel_npu_acceleration_library\\lib\\Release\\openvino_tensorflow_lite_frontend.dll 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\tbb12.dll <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\intel_npu_acceleration_library\\lib\\Release\\tbb12.dll 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\tbb12_debug.dll <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\intel_npu_acceleration_library\\lib\\Release\\tbb12_debug.dll 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\tbbbind_2_5.dll <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\intel_npu_acceleration_library\\lib\\Release\\tbbbind_2_5.dll 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\tbbbind_2_5_debug.dll <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\intel_npu_acceleration_library\\lib\\Release\\tbbbind_2_5_debug.dll 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\tbbmalloc.dll <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\intel_npu_acceleration_library\\lib\\Release\\tbbmalloc.dll 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\tbbmalloc_debug.dll <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\intel_npu_acceleration_library\\lib\\Release\\tbbmalloc_debug.dll 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\tbbmalloc_proxy.dll <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\intel_npu_acceleration_library\\lib\\Release\\tbbmalloc_proxy.dll 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\tbbmalloc_proxy_debug.dll <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\intel_npu_acceleration_library\\lib\\Release\\tbbmalloc_proxy_debug.dll 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\common.lib <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\bigdl-core-npu\\common.lib 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\ggml.dll <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\bigdl-core-npu\\ggml.dll 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\ggml.lib <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\bigdl-core-npu\\ggml.lib 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\llama.dll <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\bigdl-core-npu\\llama.dll 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\llama.lib <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\bigdl-core-npu\\llama.lib 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\llm-cli.exe <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\bigdl-core-npu\\llm-cli.exe 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\npu_llm.dll <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\bigdl-core-npu\\npu_llm.dll 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\npu_llm.lib <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\bigdl-core-npu\\npu_llm.lib 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\zlib1.dll <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\bigdl-core-npu\\zlib1.dll 创建的符号链接\n为 C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\__init__.py <<===>> C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\\bigdl-core-npu\\__init__.py 创建的符号链接\n已复制         1 个文件。\n(llm-npu) PS C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu> ./llama-cli-npu -m C:\\Data\\AI\\LLM\\Models-GGUF\\DeepSeek-R1-Distill-Qwen-7B-Q6_K.gguf -n 32 --prompt \"What is AI?\"\nbuild: 1 (3ac676a) with MSVC 19.39.33519.0 for x64\nllama_model_loader: loaded meta data with 27 key-value pairs and 339 tensors from C:\\Data\\AI\\LLM\\Models-GGUF\\DeepSeek-R1-Distill-Qwen-7B-Q6_K.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 7B\nllama_model_loader: - kv   3:                       general.organization str              = Deepseek Ai\nllama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Qwen\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                          qwen2.block_count u32              = 28\nllama_model_loader: - kv   7:                       qwen2.context_length u32              = 131072\nllama_model_loader: - kv   8:                     qwen2.embedding_length u32              = 3584\nllama_model_loader: - kv   9:                  qwen2.feed_forward_length u32              = 18944\nllama_model_loader: - kv  10:                 qwen2.attention.head_count u32              = 28\nllama_model_loader: - kv  11:              qwen2.attention.head_count_kv u32              = 4\nllama_model_loader: - kv  12:                       qwen2.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  13:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen\nllama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = [\"臓 臓\", \"臓臓 臓臓\", \"i n\", \"臓 t\",...\nllama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646\nllama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151654\nllama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\nllama_model_loader: - kv  25:               general.quantization_version u32              = 2\nllama_model_loader: - kv  26:                          general.file_type u32              = 18\nllama_model_loader: - type  f32:  141 tensors\nllama_model_loader: - type q6_K:  198 tensors\nllm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 22\nllm_load_vocab: token to piece cache size = 0.9310 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = qwen2\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 152064\nllm_load_print_meta: n_merges         = 151387\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 131072\nllm_load_print_meta: n_embd           = 3584\nllm_load_print_meta: n_layer          = 28\nllm_load_print_meta: n_head           = 28\nllm_load_print_meta: n_head_kv        = 4\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 7\nllm_load_print_meta: n_embd_k_gqa     = 512\nllm_load_print_meta: n_embd_v_gqa     = 512\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 18944\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 2\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 131072\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = ?B\nllm_load_print_meta: model ftype      = Q6_K\nllm_load_print_meta: model params     = 7.62 B\nllm_load_print_meta: model size       = 5.82 GiB (6.56 BPW)\nllm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 7B\nllm_load_print_meta: BOS token        = 151646 '<锝渂egin鈻乷f鈻乻entence锝?'\nllm_load_print_meta: EOS token        = 151643 '<锝渆nd鈻乷f鈻乻entence锝?'\nllm_load_print_meta: PAD token        = 151654 '<|vision_pad|>'\nllm_load_print_meta: LF token         = 148848 '脛默'\nllm_load_print_meta: EOG token        = 151643 '<锝渆nd鈻乷f鈻乻entence锝?'\nllm_load_print_meta: max token length = 256\nllm_load_tensors: ggml ctx size =    0.15 MiB\nllm_load_tensors:        CPU buffer size =  5958.79 MiB\n........................................................................................\nDirectory created: \"C:\\\\Project\\\\CPP\\\\AI\\\\llama.cpp\\\\bin-npu\\\\NPU_models\\\\qwen2-28-3584-152064-Q4_0\"\nDirectory created: \"C:\\\\Project\\\\CPP\\\\AI\\\\llama.cpp\\\\bin-npu\\\\NPU_models\\\\qwen2-28-3584-152064-Q4_0\\\\model_weights\"\nConverting GGUF model to Q4_0 NPU model...\nModel weights saved to C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\NPU_models\\qwen2-28-3584-152064-Q4_0\\model_weights\nllama_model_loader: loaded meta data with 27 key-value pairs and 339 tensors from C:\\Data\\AI\\LLM\\Models-GGUF\\DeepSeek-R1-Distill-Qwen-7B-Q6_K.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 7B\nllama_model_loader: - kv   3:                       general.organization str              = Deepseek Ai\nllama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Qwen\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                          qwen2.block_count u32              = 28\nllama_model_loader: - kv   7:                       qwen2.context_length u32              = 131072\nllama_model_loader: - kv   8:                     qwen2.embedding_length u32              = 3584\nllama_model_loader: - kv   9:                  qwen2.feed_forward_length u32              = 18944\nllama_model_loader: - kv  10:                 qwen2.attention.head_count u32              = 28\nllama_model_loader: - kv  11:              qwen2.attention.head_count_kv u32              = 4\nllama_model_loader: - kv  12:                       qwen2.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  13:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen\nllama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = [\"臓 臓\", \"臓臓 臓臓\", \"i n\", \"臓 t\",...\nllama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646\nllama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151654\nllama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\nllama_model_loader: - kv  25:               general.quantization_version u32              = 2\nllama_model_loader: - kv  26:                          general.file_type u32              = 18\nllama_model_loader: - type  f32:  141 tensors\nllama_model_loader: - type q6_K:  198 tensors\nllm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 22\nllm_load_vocab: token to piece cache size = 0.9310 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = qwen2\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 152064\nllm_load_print_meta: n_merges         = 151387\nllm_load_print_meta: vocab_only       = 1\nllm_load_print_meta: model type       = ?B\nllm_load_print_meta: model ftype      = all F32\nllm_load_print_meta: model params     = 7.62 B\nllm_load_print_meta: model size       = 5.82 GiB (6.56 BPW)\nllm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 7B\nllm_load_print_meta: BOS token        = 151646 '<锝渂egin鈻乷f鈻乻entence锝?'\nllm_load_print_meta: EOS token        = 151643 '<锝渆nd鈻乷f鈻乻entence锝?'\nllm_load_print_meta: PAD token        = 151654 '<|vision_pad|>'\nllm_load_print_meta: LF token         = 148848 '脛默'\nllm_load_print_meta: EOG token        = 151643 '<锝渆nd鈻乷f鈻乻entence锝?'\nllm_load_print_meta: max token length = 256\nllama_model_load: vocab only - skipping tensors\nModel saved to C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\NPU_models\\qwen2-28-3584-152064-Q4_0//decoder_layer_0.blob\nModel saved to C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu\\NPU_models\\qwen2-28-3584-152064-Q4_0//decoder_layer_1.blob\nllama_new_context_with_model: n_ctx      = 1024\nllama_new_context_with_model: n_batch    = 1024\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 0.0\nllama_new_context_with_model: freq_scale = 1\n锘縰sing     1  2  3 喙傕笡喔｀箒  2喙傕笡喔｀箒<think>氐賳丿  1 2喙傕笡喔｀箒 \"crypto \"sync\n\nllm_perf_print:        load time =   45635.00 ms\nllm_perf_print: prompt eval time =    6463.00 ms /     7 tokens (  923.29 ms per token,     1.08 tokens per second)\nllm_perf_print:        eval time =    3049.00 ms /    31 runs   (   98.35 ms per token,    10.17 tokens per second)\nllm_perf_print:       total time =   55190.00 ms /    38 tokens\n(llm-npu) PS C:\\Project\\CPP\\AI\\llama.cpp\\bin-npu>\n```\n\n## **Environment information**\n\nPython 3.11.11\n-----------------------------------------------------------------\ntransformers=4.45.0\n-----------------------------------------------------------------\ntorch=2.1.2+cpu\n-----------------------------------------------------------------\nName: ipex-llm\nVersion: 2.2.0b20250321\nSummary: Large Language Model Develop Toolkit\nHome-page: https://github.com/intel-analytics/ipex-llm\nAuthor: BigDL Authors\nAuthor-email: bigdl-user-group@googlegroups.com\nLicense: Apache License, Version 2.0\nLocation: C:\\Users\\TabYe\\.conda\\envs\\llm-npu\\Lib\\site-packages\nRequires:\nRequired-by:\n-----------------------------------------------------------------\nIPEX is not installed properly.\n-----------------------------------------------------------------\nTotal Memory: 31.466 GB\n\nChip 0 Memory: 4 GB | Speed: 8533 MHz\nChip 1 Memory: 4 GB | Speed: 8533 MHz\nChip 2 Memory: 4 GB | Speed: 8533 MHz\nChip 3 Memory: 4 GB | Speed: 8533 MHz\nChip 4 Memory: 4 GB | Speed: 8533 MHz\nChip 5 Memory: 4 GB | Speed: 8533 MHz\nChip 6 Memory: 4 GB | Speed: 8533 MHz\nChip 7 Memory: 4 GB | Speed: 8533 MHz\n-----------------------------------------------------------------\nCPU Manufacturer: GenuineIntel\nCPU MaxClockSpeed: 1200\nCPU Name: Intel(R) Core(TM) Ultra 5 125H\nCPU NumberOfCores: 14\nCPU NumberOfLogicalProcessors: 18\n-----------------------------------------------------------------\nGPU 0: Intel(R) Arc(TM) Graphics         Driver Version:  32.0.101.6647\n-----------------------------------------------------------------\n-----------------------------------------------------------------\nSystem Information\n\n主机名:           TABREDMI\nOS 名称:          Microsoft Windows 11 家庭中文版\nOS 版本:          10.0.22631 暂缺 Build 22631\nOS 制造商:        Microsoft Corporation\nOS 配置:          独立工作站\nOS 构建类型:      Multiprocessor Free\n注册的所有人:     TabYe320@outlook.com\n注册的组织:       暂缺\n产品 ID:          00342-31531-05585-AAOEM\n初始安装日期:     25/2/2024, 下午 12:41:28\n系统启动时间:     24/3/2025, 上午 9:10:32\n系统制造商:       XIAOMI\n系统型号:         Redmi Book Pro 14 2024\n系统类型:         x64-based PC\n处理器:           安装了 1 个处理器。\n                  [01]: Intel64 Family 6 Model 170 Stepping 4 GenuineIntel ~1200 Mhz\nBIOS 版本:        XIAOMI RMAMT4B0P0A0A, 4/6/2024\nWindows 目录:     C:\\Windows\n系统目录:         C:\\Windows\\system32\n启动设备:         \\Device\\HarddiskVolume1\n系统区域设置:     zh-cn;中文(中国)\n输入法区域设置:   zh-cn;中文(中国)\n时区:             (UTC+08:00) 北京，重庆，香港特别行政区，乌鲁木齐\n物理内存总量:     32,221 MB\n可用的物理内存:   13,206 MB\n虚拟内存: 最大值: 35,720 MB\n虚拟内存: 可用:   13,087 MB\n虚拟内存: 使用中: 22,633 MB\n页面文件位置:     C:\\pagefile.sys\n域:               WORKGROUP\n登录服务器:       \\\\TABREDMI\n修补程序:         安装了 5 个修补程序。\n                  [01]: KB5049624\n                  [02]: KB5027397\n                  [03]: KB5033055\n                  [04]: KB5053602\n                  [05]: KB5052107\n网卡:             安装了 3 个 NIC。\n                  [01]: Remote NDIS Compatible Device\n                      连接名:      以太网 2\n                      启用 DHCP:   是\n                      DHCP 服务器: 192.168.215.116\n                      IP 地址\n                        [01]: 192.168.215.107\n                        [02]: fe80::d606:248d:8f84:4f17\n                        [03]: 240e:430:2a41:daa2:75eb:cb95:cb46:422c\n                        [04]: 240e:430:2a41:daa2:bdbd:bbe9:cdb3:a76\n                  [02]: Intel(R) Wi-Fi 6E AX211 160MHz\n                      连接名:      WLAN\n                      状态:        媒体连接已中断\n                  [03]: Bluetooth Device (Personal Area Network)\n                      连接名:      蓝牙网络连接\n                      状态:        媒体连接已中断\nHyper-V 要求:     已检测到虚拟机监控程序。将不显示 Hyper-V 所需的功能。\n-----------------------------------------------------------------\n'xpu-smi' 不是内部或外部命令，也不是可运行的程序\n或批处理文件。\nxpu-smi is not installed properly.\n\n",
      "state": "closed",
      "author": "TabNahida",
      "author_type": "User",
      "created_at": "2025-03-26T02:32:10Z",
      "updated_at": "2025-04-03T15:12:02Z",
      "closed_at": "2025-04-03T15:12:02Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13008/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13008",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13008",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:47.727405",
      "comments": []
    },
    {
      "issue_number": 13028,
      "title": "support >= 4GB SYCL compute buffer size for longer context length ",
      "body": "**Describe the bug**\nThe SYCL Unified Shared Memory (USM) type of device memory has maximum constraint of 4 GB. Ipex-llm will report error if the calculated kv cache size is more than 4GB.\n\n\n**How to reproduce**\ncomputer setup with igpu only inference and >= 32GB ram, thus expecting no allocation issue with larger context size.\nencounter this issue with Gemma-3 model\n\nSteps to reproduce the error:\n1. configure the -c argument to smaller count\n2. observe the buffer size reported used for SYCL buffer, safe if less than 4GB\n3. increase the -c argument till expectation is larger than 4GB. Will getting the reported error on memory allocation issue.\n\n\n**Additional context**\nAm running gemma 3 model with llama server, thus expecting similar issue for other moe models\n\n\ndeclaring multiple SYCL USM device instances might overcome this constraint, to have more than 4GB buffer size for longer context length (few k and above, and case with parallel enabled)\n",
      "state": "open",
      "author": "ytliew82",
      "author_type": "User",
      "created_at": "2025-03-31T10:05:51Z",
      "updated_at": "2025-04-03T09:11:45Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13028/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13028",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13028",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:47.727426",
      "comments": [
        {
          "author": "cyita",
          "body": "Hi ytliew82,\n\nWe previously encountered the same error with Gemma-3 4B on ARC, while Gemma-3 12B seemed to work fine. Are you using the 4B model in your test?",
          "created_at": "2025-04-01T08:38:33Z"
        },
        {
          "author": "ytliew82",
          "body": "tested with Gemma-3 4B, 12B, having same error on not fit into device buffer.\ncurrently run with cpu only inference as workaround, and limiting the -ngl argument to fit into 4GB device buffer.\n\nanyway, based on my understanding, the USM type of host/device/shared mostly apply for dGPU.\nhttps://www.i",
          "created_at": "2025-04-01T16:20:16Z"
        },
        {
          "author": "cyita",
          "body": "Hi ytliew82,\n\nThank you for the information! We'll provide updates once it's supported.",
          "created_at": "2025-04-02T02:00:48Z"
        },
        {
          "author": "toncao",
          "body": "+1\n",
          "created_at": "2025-04-03T09:11:45Z"
        }
      ]
    },
    {
      "issue_number": 13039,
      "title": "Unable to use ollama create to load custom gguf model",
      "body": "## **Issue: Error When Using `ollama create` to Load a Model**\n\n### **Description**\nWhen attempting to use the `ollama create` API to load a model, the following error is returned:\n\n```plaintext\n*** Error:\n{\"error\":\"path or Modelfile are required\"}\n***\n```\n\n### **Environment**\n- **Ollama Version**: 0.5.4\n- **Docker Compose Service**: ollama running on `localhost:8015`\n\n---\n\n### **Steps to Reproduce**\n1. **Generate SHA-256 Checksum**:\n   ```bash\n   sha256sum {model path}\n   ```\n   Example:\n   ```bash\n   sha256sum Llama-3.2-3B-Instruct-Q4_K_M.gguf\n   ```\n   Output:\n   ```plaintext\n   6c1a2b41161032677be168d354123594c0e6e67d2b9227c84f296ad037c728ff  Llama-3.2-3B-Instruct-Q4_K_M.gguf\n   ```\n\n2. **Push Blob to Ollama**:\n   ```bash\n   curl -T Llama-3.2-3B-Instruct-Q4_K_M.gguf -X POST http://localhost:8015/api/blobs/sha256:6c1a2b41161032677be168d354123594c0e6e67d2b9227c84f296ad037c728ff\n   ```\n\n3. **Verify Blob Exists**:\n   ```bash\n   curl -I http://localhost:8015/api/blobs/sha256:6c1a2b41161032677be168d354123594c0e6e67d2b9227c84f296ad037c728ff\n   ```\n   Output:\n   ```plaintext\n   HTTP/1.1 200 OK\n   Date: Thu, 03 Apr 2025 06:43:44 GMT\n   ```\n\n4. **Use `ollama create` to Load the Model**:\n   ```bash\n   curl http://localhost:8015/api/create -d '{\n     \"model\": \"onepiece\",\n     \"files\": {\n       \"Llama-3.2-3B-Instruct-Q4_K_M.gguf\": \"sha256:6c1a2b41161032677be168d354123594c0e6e67d2b9227c84f296ad037c728ff\"\n     },\n     \"template\": \"{{- if .System }}\\n<|system|>\\n{{ .System }}\\n</s>\\n{{- end }}\\n<|user|>\\n{{ .Prompt }}\\n</s>\\n<|assistant|>\",\n     \"parameters\": {\n         \"temperature\": 0.2,\n         \"num_ctx\": 8192,\n         \"stop\": [\"<|system|>\", \"<|user|>\", \"<|assistant|>\", \"</s>\"]\n     },\n     \"system\": \"You are Luffy from One Piece, acting as an assistant.\"\n   }'\n   ```\n\n5. **Error Output**:\n   ```plaintext\n   {\"error\":\"path or Modelfile are required\"}\n   ```\n\n---\n\n### **Expected Behavior**\nThe model should be successfully created and loaded into the Ollama server.\n\n---\n\n### **Actual Behavior**\nThe server returns the following error:\n```plaintext\n{\"error\":\"path or Modelfile are required\"}\n```\n\n---\n\n### **Additional Information**\n- **Blob Verification Screenshot**:\n  ![Blob Verification](https://github.com/user-attachments/assets/03d1fe7f-6c75-4f03-9ab7-ea889b80b7c2)\n\n- **Command Used**:\n   ```bash\n   curl http://localhost:8015/api/create -d '{\n     \"model\": \"onepiece\",\n     \"files\": {\n       \"Llama-3.2-3B-Instruct-Q4_K_M.gguf\": \"sha256:6c1a2b41161032677be168d354123594c0e6e67d2b9227c84f296ad037c728ff\"\n     },\n     \"template\": \"{{- if .System }}\\n<|system|>\\n{{ .System }}\\n</s>\\n{{- end }}\\n<|user|>\\n{{ .Prompt }}\\n</s>\\n<|assistant|>\",\n     \"parameters\": {\n         \"temperature\": 0.2,\n         \"num_ctx\": 8192,\n         \"stop\": [\"<|system|>\", \"<|user|>\", \"<|assistant|>\", \"</s>\"]\n     },\n     \"system\": \"You are Luffy from One Piece, acting as an assistant.\"\n   }'\n   ```\n\n- **Model Reference**:\n   The model used for this process is available at [Llama-3.2-3B-Instruct-Q4_K_M.gguf](https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/blob/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf).\n\n---\n\n### **Questions**\n1. Is the `path` or `modelfile` field required in the `/api/create` payload? If so, how should it be structured?\n2. Are there additional steps or configurations needed to successfully create a model from a GGUF file?\n\n\n---\n\n### **Request for Assistance**\nAny guidance on how to resolve this issue and successfully create a model using the `/api/create` endpoint is greatly appreciated!!!\n\n\n\n### OS\n\nDocker\n\n### GPU\n\nIntel\n\n### CPU\n\nIntel\n\n### Ollama version\n\n0.5.4",
      "state": "open",
      "author": "ngwarrencinyen",
      "author_type": "User",
      "created_at": "2025-04-02T07:46:49Z",
      "updated_at": "2025-04-03T05:59:17Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13039/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13039",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13039",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:47.928617",
      "comments": [
        {
          "author": "hkvision",
          "body": "Hi, the request format you are using is for recent versions of ollama, but for ollama 0.5.4 it is not supported: https://github.com/ollama/ollama/blob/v0.5.4/api/types.go#L297\nSeems for 0.5.4 you can only create using a Modelfile following the sample command here: https://github.com/ollama/ollama/bl",
          "created_at": "2025-04-03T04:16:38Z"
        },
        {
          "author": "ngwarrencinyen",
          "body": "Alright thanks for the information. Cheers!",
          "created_at": "2025-04-03T05:59:16Z"
        }
      ]
    },
    {
      "issue_number": 13030,
      "title": "Ollama stopped working after Intel driver updates",
      "body": "**Describe the bug**\n\nMy existing, working ipex-llm installation broke. The only change I am aware of that preceded the breakage is that Windows Update installed a couple of Intel driver updates. Correlation is not causation, but it's all I've got.\n\n**How to reproduce**\nSteps to reproduce the error:\n1. Open PowerShell with existing conda environment for ipex-llm\n2. run \"ollama serve\": observe no output\n3. run \"ollama --help\": observe no output\n4. \"$?\": True\n\n**Screenshots**\n![Image](https://github.com/user-attachments/assets/ee51b40f-24f7-44e0-b4a0-fa1efd03b817)\n\n**Environment information**\nIf possible, please attach the output of the environment check script, using:\n```\nPython 3.11.11\npython: can't open file 'C:\\\\Users\\\\toms\\\\ollama\\\\check.py': [Errno 2] No such file or directory\n-----------------------------------------------------------------\nSystem Information\n\nHost Name:                 (redacted)\nOS Name:                   Microsoft Windows 11 Pro\nOS Version:                10.0.22631 N/A Build 22631\nOS Manufacturer:           Microsoft Corporation\nOS Configuration:          Member Workstation\nOS Build Type:             Multiprocessor Free\nRegistered Owner:          (redacted)\nRegistered Organization:   N/A\nProduct ID:                00355-61607-80370-AAOEM\nOriginal Install Date:     2024-06-04, 1:50:27 AM\nSystem Boot Time:          2025-03-31, 12:12:22 PM\nSystem Manufacturer:       LENOVO\nSystem Model:              21KCS0EG00\nSystem Type:               x64-based PC\nProcessor(s):              1 Processor(s) Installed.\n                           [01]: Intel64 Family 6 Model 170 Stepping 4 GenuineIntel ~1700 Mhz\nBIOS Version:              LENOVO N3YET74W (1.39 ), 2024-12-18\nWindows Directory:         C:\\Windows\nSystem Directory:          C:\\Windows\\system32\nBoot Device:               \\Device\\HarddiskVolume1\nSystem Locale:             en-us;English (United States)\nInput Locale:              en-us;English (United States)\nTime Zone:                 (UTC-05:00) Eastern Time (US & Canada)\nTotal Physical Memory:     64,974 MB\nAvailable Physical Memory: 46,536 MB\nVirtual Memory: Max Size:  76,750 MB\nVirtual Memory: Available: 56,400 MB\nVirtual Memory: In Use:    20,350 MB\nPage File Location(s):     C:\\pagefile.sys\nDomain:                    (redacted)\nLogon Server:             (redacted)\nHotfix(s):                 5 Hotfix(s) Installed.\n                           [01]: KB5049624\n                           [02]: KB5027397\n                           [03]: KB5036212\n                           [04]: KB5053602\n                           [05]: KB5052107\nNetwork Card(s):           (redacted)\nHyper-V Requirements:      A hypervisor has been detected. Features required for Hyper-V will not be displayed.\n-----------------------------------------------------------------\n'xpu-smi' is not recognized as an internal or external command,\noperable program or batch file.\nxpu-smi is not installed properly.\n```\n\n**Additional context**\nAdd any other context about the problem here.\n",
      "state": "closed",
      "author": "tsobczynski",
      "author_type": "User",
      "created_at": "2025-03-31T16:50:53Z",
      "updated_at": "2025-04-02T16:52:10Z",
      "closed_at": "2025-04-02T16:52:09Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13030/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13030",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13030",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:48.137558",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "You can try `conda prompt` or `miniforge prompt` instead of `PowerShell`.",
          "created_at": "2025-04-01T02:16:21Z"
        },
        {
          "author": "tsobczynski",
          "body": "I deleted the conda environment and retraced my steps to set everything up exactly as I had before, and now it's working again. So, whatever broke, it was apparently a transient failure. In which case, I'm sure you have bigger fish to fry, so I'm going to close my own report.",
          "created_at": "2025-04-02T16:52:09Z"
        }
      ]
    },
    {
      "issue_number": 13000,
      "title": "UHD Graphics 730 run DeepSeek-R1-14B error",
      "body": "./start-ollama.bat\n\n\n .\\ollama run deepseek-r1:14b\n\nerror log:\n\n```\ntime=2025-03-24T21:28:12.267+08:00 level=INFO source=gpu_windows.go:167 msg=packages count=1\ntime=2025-03-24T21:28:12.267+08:00 level=INFO source=gpu_windows.go:214 msg=\"\" package=0 cores=6 efficiency=0 threads=12\ntime=2025-03-24T21:28:12.326+08:00 level=INFO source=server.go:104 msg=\"system memory\" total=\"15.8 GiB\" free=\"8.2 GiB\" free_swap=\"7.2 GiB\"\ntime=2025-03-24T21:28:12.328+08:00 level=INFO source=memory.go:356 msg=\"offload to device\" layers.requested=-1 layers.model=49 layers.offload=0 layers.split=\"\" memory.available=\"[8.2 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"9.4 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"384.0 MiB\" memory.required.allocations=\"[8.1 GiB]\" memory.weights.total=\"7.7 GiB\" memory.weights.repeating=\"7.1 GiB\" memory.weights.nonrepeating=\"609.1 MiB\" memory.graph.full=\"307.0 MiB\" memory.graph.partial=\"916.1 MiB\"\ntime=2025-03-24T21:28:12.338+08:00 level=INFO source=server.go:392 msg=\"starting llama server\" cmd=\"F:\\\\Soft\\\\ollama-ipex-llm-2.2.0\\\\ollama-lib.exe runner --model F:\\\\Soft\\\\Ollama\\\\Models\\\\blobs\\\\sha256-6e9f90f02bb3b39b59e81916e8cfce9deb45aeaeb9a54a5be4414486b907dc1e --ctx-size 2048 --batch-size 512 --n-gpu-layers 999 --threads 6 --no-mmap --parallel 1 --port 58205\"\ntime=2025-03-24T21:28:12.389+08:00 level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\ntime=2025-03-24T21:28:12.389+08:00 level=INFO source=server.go:571 msg=\"waiting for llama runner to start responding\"\ntime=2025-03-24T21:28:12.389+08:00 level=INFO source=server.go:605 msg=\"waiting for server to become available\" status=\"llm server error\"\nggml_sycl_init: found 1 SYCL devices:\ntime=2025-03-24T21:28:12.657+08:00 level=INFO source=runner.go:967 msg=\"starting go runner\"\ntime=2025-03-24T21:28:12.663+08:00 level=INFO source=runner.go:968 msg=system info=\"CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | cgo(clang)\" threads=6\ntime=2025-03-24T21:28:12.664+08:00 level=INFO source=runner.go:1026 msg=\"Server listening on 127.0.0.1:58205\"\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\nllama_load_model_from_file: using device SYCL0 (Intel(R) UHD Graphics 730) - 7144 MiB free\nllama_model_loader: loaded meta data with 26 key-value pairs and 579 tensors from F:\\Soft\\Ollama\\Models\\blobs\\sha256-6e9f90f02bb3b39b59e81916e8cfce9deb45aeaeb9a54a5be4414486b907dc1e (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 14B\nllama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen\nllama_model_loader: - kv   4:                         general.size_label str              = 14B\nllama_model_loader: - kv   5:                          qwen2.block_count u32              = 48\nllama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072\nllama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120\nllama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 13824\nllama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40\nllama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  13:                          general.file_type u32              = 15\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646\nllama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\nllama_model_loader: - kv  25:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  241 tensors\nllama_model_loader: - type q4_K:  289 tensors\nllama_model_loader: - type q6_K:   49 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 22\ntime=2025-03-24T21:28:12.892+08:00 level=INFO source=server.go:605 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nllm_load_vocab: token to piece cache size = 0.9310 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = qwen2\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 152064\nllm_load_print_meta: n_merges         = 151387\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 131072\nllm_load_print_meta: n_embd           = 5120\nllm_load_print_meta: n_layer          = 48\nllm_load_print_meta: n_head           = 40\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 5\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: f_attn_scale     = 0.0e+00\nllm_load_print_meta: n_ff             = 13824\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 2\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 1000000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 131072\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 14B\nllm_load_print_meta: model ftype      = Q4_K - Medium\nllm_load_print_meta: model params     = 14.77 B\nllm_load_print_meta: model size       = 8.37 GiB (4.87 BPW)\nllm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 14B\nllm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'\nllm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'\nllm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'\nllm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'\nllm_load_print_meta: LF token         = 148848 'ÄĬ'\nllm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'\nllm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'\nllm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'\nllm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'\nllm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'\nllm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'\nllm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'\nllm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'\nllm_load_print_meta: EOG token        = 151663 '<|repo_name|>'\nllm_load_print_meta: EOG token        = 151664 '<|file_sep|>'\nllm_load_print_meta: max token length = 256\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\nllm_load_tensors: offloading 48 repeating layers to GPU\nllm_load_tensors: offloading output layer to GPU\nllm_load_tensors: offloaded 49/49 layers to GPU\nllm_load_tensors:        SYCL0 model buffer size =  8148.38 MiB\nllm_load_tensors:          CPU model buffer size =   417.66 MiB\nNative API failed. Native API returns: 39 (UR_RESULT_ERROR_OUT_OF_DEVICE_MEMORY)\nException caught at file:D:\\actions-runner\\release-cpp-oneapi_2024_2\\_work\\llm.cpp\\llm.cpp\\ollama-llama-cpp\\ggml\\src\\ggml-sycl\\ggml-sycl.cpp, line:345, func:operator()\nSYCL error: CHECK_TRY_ERROR((*stream).memcpy((char *)tensor->data + offset, host_buf, size) .wait()): Meet error in this line code!\n  in function ggml_backend_sycl_buffer_set_tensor at D:\\actions-runner\\release-cpp-oneapi_2024_2\\_work\\llm.cpp\\llm.cpp\\ollama-llama-cpp\\ggml\\src\\ggml-sycl\\ggml-sycl.cpp:345\nD:\\actions-runner\\release-cpp-oneapi_2024_2\\_work\\llm.cpp\\llm.cpp\\ollama-llama-cpp\\ggml\\src\\ggml-sycl\\..\\ggml-sycl\\common.hpp:107: SYCL error\ntime=2025-03-24T21:28:14.094+08:00 level=INFO source=server.go:605 msg=\"waiting for server to become available\" status=\"llm server not responding\"\ntime=2025-03-24T21:28:14.535+08:00 level=INFO source=server.go:605 msg=\"waiting for server to become available\" status=\"llm server error\"\ntime=2025-03-24T21:28:14.785+08:00 level=ERROR source=sched.go:455 msg=\"error loading llama server\" error=\"llama runner process has terminated: error:CHECK_TRY_ERROR((*stream).memcpy((char *)tensor->data + offset, host_buf, size) .wait()): Meet error in this line code!\\r\\n  in function ggml_backend_sycl_buffer_set_tensor at D:\\\\actions-runner\\\\release-cpp-oneapi_2024_2\\\\_work\\\\llm.cpp\\\\llm.cpp\\\\ollama-llama-cpp\\\\ggml\\\\src\\\\ggml-sycl\\\\ggml-sycl.cpp:345\\r\\nD:\\\\actions-runner\\\\release-cpp-oneapi_2024_2\\\\_work\\\\llm.cpp\\\\llm.cpp\\\\ollama-llama-cpp\\\\ggml\\\\src\\\\ggml-sycl\\\\..\\\\ggml-sycl\\\\common.hpp:107: SYCL error\"\n\n```\n\nEnvironmental variables(OLLAMA_GPU_MEMORY):\n\n```\nOLLAMA_GPU_MEMORY=6144\nOLLAMA_NUM_GPU=1\nZES_ENABLE_SYSMAN=1\n```\n\nWithout using ipex-llm, you can use Olama to run DeepSeek-R1- 14B normally, but without using a GPU.\n\nHow can I configure to use a GPU for Olama inference (DeepSeek-R1: 14b)? Thank you.\n",
      "state": "closed",
      "author": "pruidong",
      "author_type": "User",
      "created_at": "2025-03-24T13:55:08Z",
      "updated_at": "2025-04-02T13:43:39Z",
      "closed_at": "2025-04-02T13:43:39Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13000/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13000",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13000",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:48.358528",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "`(UR_RESULT_ERROR_OUT_OF_DEVICE_MEMORY)` in your log means out of memory. Pure CPU can use all your memory, but iGPU can only share half of total memory. ",
          "created_at": "2025-03-25T00:37:19Z"
        },
        {
          "author": "pruidong",
          "body": "> `(UR_RESULT_ERROR_OUT_OF_DEVICE_MEMORY)` in your log means out of memory. Pure CPU can use all your memory, but iGPU can only share half of total memory.\n\nThank you, I have identified this issue.\n\nI can switch to DeepSeeker R1:7B and it will run normally.\n\nIf using pure CPU inference, DeepSeeker R",
          "created_at": "2025-03-25T03:04:41Z"
        },
        {
          "author": "tombii",
          "body": "Yes, you can run larger models with a dedicated GPU but there are no consumer options with 48GB memory. ",
          "created_at": "2025-03-27T09:33:32Z"
        }
      ]
    },
    {
      "issue_number": 13036,
      "title": "MiniCPM-O cannot use TTS",
      "body": "**Describe the bug**\nAfter enabling TTS parameter, the program will crash with error\n![Image](https://github.com/user-attachments/assets/d63fa444-5825-4959-8384-ed1f245dd9e2)\n\n**How to reproduce**\nSteps to reproduce the error:\n1. intelanalytics/ipex-llm-inference-cpp-xpu:latest\n2. Follow setup guidance on IPEX-LLM\n3. Input voice is random input audio.\n4. There is a script try.py:\n5.import torch\n```\nimport librosa\nfrom transformers import AutoModel\nfrom ipex_llm.transformers import AutoModel\n\nmodel = AutoModel.from_pretrained('./minicpm-o', trust_remote_code=True,\n    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager\nmodel = model.half().to('xpu')\ntokenizer = AutoTokenizer.from_pretrained('./minicpm-o', trust_remote_code=True)\n\nmodel.init_tts()\nmodel.tts.float()\n\nmimick_prompt = \"Please repeat each user's speech, including voice style and speech content.\"\naudio_input, _ = librosa.load('./question.wav', sr=16000, mono=True)\nmsgs = [{'role': 'user', 'content': [mimick_prompt,audio_input]}]\nres = model.chat(\n    msgs=msgs,\n    tokenizer=tokenizer,\n    sampling=True,\n    max_new_tokens=128,\n    use_tts_template=True,\n    temperature=0.3,\n    generate_audio=True,\n    output_audio_path='./output.wav', # save the tts result to output_audio_path\n)\n\"\"\"\n\n\n\n\n\n\n> ERROR Message:\n\n\n\"\"\"model = AutoModel.from_pretrained(\n\n    model_path,\n\n    trust_remote_code=True,\n\n    attn_implementation='sdpa', \n\n    load_in_low_bit=\"sym_int4\",\n\n    optimize_model=True,\n\n    init_vision=True,\n\n    init_audio=True,\n\n    init_tts=True\n\n) # sdpa or flash_attention_2, no eager\n\nmodel = model.half().to('xpu')\n \nTraceback (most recent call last):\n\n  File \"/data/jimin/dev/ipex-llm/python/llm/example/GPU/HuggingFace/Multimodal/MiniCPM-o-2_6/tts.py\", line 15, in <module>\n\n    model = AutoModel.from_pretrained(\n\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/root/miniforge3/envs/llm/lib/python3.11/unittest/mock.py\", line 1378, in patched\n\n    return func(*newargs, **newkeywargs)\n\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/root/miniforge3/envs/llm/lib/python3.11/site-packages/ipex_llm/transformers/model.py\", line 349, in from_pretrained\n\n    model = cls.load_convert(q_k, optimize_model, *args, **kwargs)\n\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/root/miniforge3/envs/llm/lib/python3.11/site-packages/ipex_llm/transformers/model.py\", line 502, in load_convert\n\n    model = ggml_convert_low_bit(model, qtype, optimize_model,\n\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/root/miniforge3/envs/llm/lib/python3.11/site-packages/ipex_llm/transformers/convert.py\", line 1123, in ggml_convert_low_bit\n\n    model, has_been_replaced = _replace_with_low_bit_linear(\n\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/root/miniforge3/envs/llm/lib/python3.11/site-packages/ipex_llm/transformers/convert.py\", line 732, in _replace_with_low_bit_linear\n\n    _, _flag = _replace_with_low_bit_linear(\n\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/root/miniforge3/envs/llm/lib/python3.11/site-packages/ipex_llm/transformers/convert.py\", line 732, in _replace_with_low_bit_linear\n\n    _, _flag = _replace_with_low_bit_linear(\n\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/root/miniforge3/envs/llm/lib/python3.11/site-packages/ipex_llm/transformers/convert.py\", line 722, in _replace_with_low_bit_linear\n\n    module.weight = None\n\n    ^^^^^^^^^^^^^\n\n  File \"/root/miniforge3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1768, in __setattr__\n\n    super().__setattr__(name, value)\n\n  File \"/root/miniforge3/envs/llm/lib/python3.11/site-packages/torch/nn/utils/parametrize.py\", line 373, in set_original\n\n    self.parametrizations[tensor_name].right_inverse(value)\n\n  File \"/root/miniforge3/envs/llm/lib/python3.11/site-packages/torch/nn/utils/parametrize.py\", line 217, in right_inverse\n\n    value = module.right_inverse(value)\n\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/root/miniforge3/envs/llm/lib/python3.11/site-packages/torch/nn/utils/parametrizations.py\", line 303, in right_inverse\n\n    weight_g = torch.norm_except_dim(weight, 2, self.dim)\n\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nTypeError: norm_except_dim(): argument 'v' (position 1) must be Tensor, not NoneType\n\n \n",
      "state": "open",
      "author": "RobinJing",
      "author_type": "User",
      "created_at": "2025-04-01T08:40:37Z",
      "updated_at": "2025-04-02T05:38:39Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13036/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13036",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13036",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:48.546999",
      "comments": [
        {
          "author": "MeouSker77",
          "body": "you can try setting `modules_to_not_convert=[\"apm\", \"vpm\", \"resampler\", \"tts\"]` in `AutoModel.from_pretrained`",
          "created_at": "2025-04-02T02:32:21Z"
        },
        {
          "author": "RobinJing",
          "body": "![Image](https://github.com/user-attachments/assets/d4e5dd78-4a89-4cb3-a1e5-6eb856e93770)\n\n![Image](https://github.com/user-attachments/assets/8bf76cae-a4ec-4148-9dab-07d7a8cfaa8e)",
          "created_at": "2025-04-02T03:27:00Z"
        },
        {
          "author": "MeouSker77",
          "body": "`modules_to_not_convert` is a parameter of `ipex_llm`'s `AutoModel.from_pretrained`,\ntry\n\n```python\nfrom ipex_llm.transformers import AutoModel\nmodel = AutoModel.from_pretrained(\n    model_path,\n    trust_remote_code=True,\n    attn_implementation='sdpa',\n    torch_dtype=torch.half,\n    load_in_low_b",
          "created_at": "2025-04-02T03:51:45Z"
        },
        {
          "author": "RobinJing",
          "body": "![Image](https://github.com/user-attachments/assets/daa44c60-fa1a-484b-9fc5-bbda3ca58cf1)\n\n![Image](https://github.com/user-attachments/assets/5547ae4f-49d8-40c7-8816-2b55a90231f6)",
          "created_at": "2025-04-02T05:22:47Z"
        },
        {
          "author": "MeouSker77",
          "body": "hi, you need to update downloaded modeling file\n\nold modeling file will move inputs to `CUDA` device, you need to update to [latest modeling file](https://hf-mirror.com/openbmb/MiniCPM-o-2_6/blob/main/modeling_minicpmo.py), or update your downloaded modeling file according to [this PR](https://hf-mi",
          "created_at": "2025-04-02T05:38:38Z"
        }
      ]
    },
    {
      "issue_number": 13035,
      "title": "unable to run llama.cpp on 2 A770 cards with x99 platform",
      "body": "when i run llama.cpp portable version on my ubuntu desktop, there is the wrong message \"./llama-cli: the 2 line： 33168 illegal instrutions（Segmentation fault ） LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$(cd \"$(dirname \"$0\")\";pwd) $(cd \"$(dirname \"$0\")\";pwd)/llama-cli-bin \"$@\"\n\nmy computer config: X99 platform, two way E5-2170 v3 CPUs, 2 A770 video cards, Ubuntu 22.04.05 desktop.\n\n**How to reproduce**\nSteps to reproduce the error:\n1. download llama.cpp portable version, and unzip to the current directory.\n\n2. open the terminal at the directory and run the following command:\n./llama-cli -m /llama-cpp-ipex-llm-2.2.0b20250313-ubuntu-xeon/QwQ-32B-Q3_K_L.gguf -p \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: Question:The product of the ages of three teenagers is 4590. How old is the oldest? a. 18 b. 19 c. 15 d. 17 Assistant: <think>\" -n 2048  -t 8 -e -ngl 99 --color -c 2500 --temp 0 -no-cnv\n\n3.after running, the screen displays:\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\nggml_sycl_init: SYCL_USE_XMX: yes\nggml_sycl_init: found 2 SYCL devices:\n/llama-cli: the 2 line： 33168 illegal instrutions（Segmentation fault ） LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$(cd \"$(dirname \"$0\")\";pwd) $(cd \"$(dirname \"$0\")\";pwd)/llama-cli-bin \"$@\"\n\n4. the command stopped and return.\n\nthanks for the help!",
      "state": "open",
      "author": "luningxie",
      "author_type": "User",
      "created_at": "2025-04-01T03:20:28Z",
      "updated_at": "2025-04-02T02:20:11Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13035/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13035",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13035",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:48.772932",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "llama-cpp-ipex-llm-2.2.0b20250313-ubuntu-xeon is for SPR，E5 v3 is too old. You can try our nightly llama.cpp https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/llama_cpp_quickstart.md or our next portable zip(on the way).",
          "created_at": "2025-04-02T02:20:10Z"
        }
      ]
    },
    {
      "issue_number": 12938,
      "title": "RuntimeError: \"qlinear_forward_xpu\" not implemented for 'Byte'",
      "body": "```python\nfrom transformers import Text2TextGenerationPipeline, AutoTokenizer\nfrom ipex_llm.transformers import AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/madlad400-3b-mt\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"google/madlad400-3b-mt\", load_in_4bit=True, trust_remote_code=True).to('xpu')\n\npipe = Text2TextGenerationPipeline(model=model, tokenizer=tokenizer, device='xpu')\n    \nout = pipe(\"<2zh> Hello world\")\nprint(out)\n```\nResults in a RuntimeError.\n\n```\nTraceback (most recent call last):\n  File \"/home/kev/stheno/legacy_of_the_fallen/demo.py\", line 9, in <module>\n    out = pipe(\"<2zh> Hello world\")\n          ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kev/anaconda3/envs/ipex_llm/lib/python3.11/site-packages/transformers/pipelines/text2text_generation.py\", line 167, in __call__\n    result = super().__call__(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kev/anaconda3/envs/ipex_llm/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 1162, in __call__\n    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kev/anaconda3/envs/ipex_llm/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 1169, in run_single\n    model_outputs = self.forward(model_inputs, **forward_params)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kev/anaconda3/envs/ipex_llm/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 1068, in forward\n    model_outputs = self._forward(model_inputs, **forward_params)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kev/anaconda3/envs/ipex_llm/lib/python3.11/site-packages/transformers/pipelines/text2text_generation.py\", line 191, in _forward\n    output_ids = self.model.generate(**model_inputs, **generate_kwargs)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kev/anaconda3/envs/ipex_llm/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kev/anaconda3/envs/ipex_llm/lib/python3.11/site-packages/ipex_llm/transformers/lookup.py\", line 125, in generate\n    return original_generate(self,\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kev/anaconda3/envs/ipex_llm/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kev/anaconda3/envs/ipex_llm/lib/python3.11/site-packages/ipex_llm/transformers/speculative.py\", line 127, in generate\n    return original_generate(self,\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kev/anaconda3/envs/ipex_llm/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kev/anaconda3/envs/ipex_llm/lib/python3.11/site-packages/ipex_llm/transformers/pipeline_parallel.py\", line 283, in generate\n    return original_generate(self,\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kev/anaconda3/envs/ipex_llm/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kev/anaconda3/envs/ipex_llm/lib/python3.11/site-packages/transformers/generation/utils.py\", line 1365, in generate\n    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kev/anaconda3/envs/ipex_llm/lib/python3.11/site-packages/transformers/generation/utils.py\", line 486, in _prepare_encoder_decoder_kwargs_for_generation\n    model_kwargs[\"encoder_outputs\"]: ModelOutput = encoder(**encoder_kwargs)\n                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kev/anaconda3/envs/ipex_llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kev/anaconda3/envs/ipex_llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kev/anaconda3/envs/ipex_llm/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py\", line 1110, in forward\n    layer_outputs = layer_module(\n                    ^^^^^^^^^^^^^\n  File \"/home/kev/anaconda3/envs/ipex_llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kev/anaconda3/envs/ipex_llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kev/anaconda3/envs/ipex_llm/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py\", line 754, in forward\n    hidden_states = self.layer[-1](hidden_states)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kev/anaconda3/envs/ipex_llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kev/anaconda3/envs/ipex_llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kev/anaconda3/envs/ipex_llm/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py\", line 343, in forward\n    forwarded_states = self.DenseReluDense(forwarded_states)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kev/anaconda3/envs/ipex_llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kev/anaconda3/envs/ipex_llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kev/anaconda3/envs/ipex_llm/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py\", line 326, in forward\n    hidden_states = self.wo(hidden_states)\n                    ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kev/anaconda3/envs/ipex_llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kev/anaconda3/envs/ipex_llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kev/anaconda3/envs/ipex_llm/lib/python3.11/site-packages/ipex_llm/transformers/low_bit_linear.py\", line 668, in forward\n    result = xe_linear.forward_new(x_2d, w, self.qtype, self.out_len)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: \"qlinear_forward_xpu\" not implemented for 'Byte'\n```",
      "state": "open",
      "author": "tripzero",
      "author_type": "User",
      "created_at": "2025-03-05T23:07:38Z",
      "updated_at": "2025-04-02T00:56:00Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12938/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12938",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12938",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:48.963867",
      "comments": [
        {
          "author": "ATMxsp01",
          "body": "We have reproduced this issue and will provide further updates here :)",
          "created_at": "2025-03-06T09:51:46Z"
        },
        {
          "author": "tripzero",
          "body": "Using ipex-llm xpu_2.6 there's an additional issue in transformers with how they check for is_torch_xpu_available(), but that's a transformers issue.  After patching, this issue still exists.",
          "created_at": "2025-03-06T19:12:19Z"
        },
        {
          "author": "tripzero",
          "body": "Any update?",
          "created_at": "2025-03-22T06:44:45Z"
        },
        {
          "author": "tripzero",
          "body": "I attempted to simply cast the x_2d object to float, but the translation ended up garbage:\n\n```python\n               else:\n                    import xe_linear\n                    if x_2d.dtype == torch.uint8:\n                        x_2d = x_2d.float()\n                    result = xe_linear.forward",
          "created_at": "2025-03-28T15:52:22Z"
        },
        {
          "author": "tripzero",
          "body": "Using \"bf16\" I can get good output, so perhaps my change didn't cause a regression. Seems like all the low bit conversions (fp4, int8, etc) produce garbage output.",
          "created_at": "2025-04-02T00:55:37Z"
        }
      ]
    },
    {
      "issue_number": 13029,
      "title": "\"llama runner process has terminated: exit status 2\" on Ryzen 5600/Arc A770",
      "body": "**Describe the bug**\nWhen following the steps at  ipex-llm/docs/mddocs/Quickstart/ollama_portable_zip_quickstart.md, I get \"Error: llama runner process has terminated: exit status 2\" when running the model in step 3.\n\n**How to reproduce**\nSteps to reproduce the error:\nFollow the steps at  ipex-llm/docs/mddocs/Quickstart/ollama_portable_zip_quickstart.md. At step 3 I get:\n```$ ./ollama run deepseek-r1:7b\nggml_sycl_init: found 1 SYCL devices:\npulling manifest \npulling 96c415656d37... 100% ▕███████████████████████████████████████████████████████████▏ 4.7 GB                         \npulling 369ca498f347... 100% ▕███████████████████████████████████████████████████████████▏  387 B                         \npulling 6e4c38e1172f... 100% ▕███████████████████████████████████████████████████████████▏ 1.1 KB                         \npulling f4d24e9138dd... 100% ▕███████████████████████████████████████████████████████████▏  148 B                         \npulling 40fb844194b2... 100% ▕███████████████████████████████████████████████████████████▏  487 B                         \nverifying sha256 digest \nwriting manifest \nsuccess \nError: llama runner process has terminated: exit status 2\n```\n\n**Screenshots**\nOutput from back-end:\n```2025/03/31 13:44:51 routes.go:1259: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:10m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/berend/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:localhost,127.0.0.1]\"\ntime=2025-03-31T13:44:51.807+02:00 level=INFO source=images.go:757 msg=\"total blobs: 0\"\ntime=2025-03-31T13:44:51.807+02:00 level=INFO source=images.go:764 msg=\"total unused blobs removed: 0\"\n[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.\n\n[GIN-debug] [WARNING] Running in \"debug\" mode. Switch to \"release\" mode in production.\n - using env:\texport GIN_MODE=release\n - using code:\tgin.SetMode(gin.ReleaseMode)\n\n[GIN-debug] POST   /api/pull                 --> ollama/server.(*Server).PullHandler-fm (5 handlers)\n[GIN-debug] POST   /api/generate             --> ollama/server.(*Server).GenerateHandler-fm (5 handlers)\n[GIN-debug] POST   /api/chat                 --> ollama/server.(*Server).ChatHandler-fm (5 handlers)\n[GIN-debug] POST   /api/embed                --> ollama/server.(*Server).EmbedHandler-fm (5 handlers)\n[GIN-debug] POST   /api/embeddings           --> ollama/server.(*Server).EmbeddingsHandler-fm (5 handlers)\n[GIN-debug] POST   /api/create               --> ollama/server.(*Server).CreateHandler-fm (5 handlers)\n[GIN-debug] POST   /api/push                 --> ollama/server.(*Server).PushHandler-fm (5 handlers)\n[GIN-debug] POST   /api/copy                 --> ollama/server.(*Server).CopyHandler-fm (5 handlers)\n[GIN-debug] DELETE /api/delete               --> ollama/server.(*Server).DeleteHandler-fm (5 handlers)\n[GIN-debug] POST   /api/show                 --> ollama/server.(*Server).ShowHandler-fm (5 handlers)\n[GIN-debug] POST   /api/blobs/:digest        --> ollama/server.(*Server).CreateBlobHandler-fm (5 handlers)\n[GIN-debug] HEAD   /api/blobs/:digest        --> ollama/server.(*Server).HeadBlobHandler-fm (5 handlers)\n[GIN-debug] GET    /api/ps                   --> ollama/server.(*Server).PsHandler-fm (5 handlers)\n[GIN-debug] POST   /v1/chat/completions      --> ollama/server.(*Server).ChatHandler-fm (6 handlers)\n[GIN-debug] POST   /v1/completions           --> ollama/server.(*Server).GenerateHandler-fm (6 handlers)\n[GIN-debug] POST   /v1/embeddings            --> ollama/server.(*Server).EmbedHandler-fm (6 handlers)\n[GIN-debug] GET    /v1/models                --> ollama/server.(*Server).ListHandler-fm (6 handlers)\n[GIN-debug] GET    /v1/models/:model         --> ollama/server.(*Server).ShowHandler-fm (6 handlers)\n[GIN-debug] GET    /                         --> ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\n[GIN-debug] GET    /api/tags                 --> ollama/server.(*Server).ListHandler-fm (5 handlers)\n[GIN-debug] GET    /api/version              --> ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\n[GIN-debug] HEAD   /                         --> ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\n[GIN-debug] HEAD   /api/tags                 --> ollama/server.(*Server).ListHandler-fm (5 handlers)\n[GIN-debug] HEAD   /api/version              --> ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\ntime=2025-03-31T13:44:51.807+02:00 level=INFO source=routes.go:1310 msg=\"Listening on 127.0.0.1:11434 (version 0.5.4-ipexllm-20250318)\"\ntime=2025-03-31T13:44:51.807+02:00 level=INFO source=routes.go:1339 msg=\"Dynamic LLM libraries\" runners=[ipex_llm]\n```\n\nThen on running './ollama run deepseek-r1:7b' I get:\n```\n[GIN] 2025/03/31 - 13:46:18 | 200 |      42.481µs |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/03/31 - 13:46:18 | 404 |     211.595µs |       127.0.0.1 | POST     \"/api/show\"\n[GIN] 2025/03/31 - 13:46:19 | 200 |  396.736393ms |       127.0.0.1 | POST     \"/api/pull\"\n[GIN] 2025/03/31 - 13:46:47 | 200 |       20.87µs |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/03/31 - 13:46:47 | 404 |      95.762µs |       127.0.0.1 | POST     \"/api/show\"\n[GIN] 2025/03/31 - 13:46:47 | 200 |   635.81324ms |       127.0.0.1 | POST     \"/api/pull\"\n[GIN] 2025/03/31 - 13:46:56 | 200 |       20.93µs |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/03/31 - 13:46:56 | 404 |      87.742µs |       127.0.0.1 | POST     \"/api/show\"\ntime=2025-03-31T13:46:57.565+02:00 level=INFO source=download.go:175 msg=\"downloading 96c415656d37 in 16 292 MB part(s)\"\ntime=2025-03-31T13:47:39.903+02:00 level=INFO source=download.go:175 msg=\"downloading 369ca498f347 in 1 387 B part(s)\"\ntime=2025-03-31T13:47:41.288+02:00 level=INFO source=download.go:175 msg=\"downloading 6e4c38e1172f in 1 1.1 KB part(s)\"\ntime=2025-03-31T13:47:42.629+02:00 level=INFO source=download.go:175 msg=\"downloading f4d24e9138dd in 1 148 B part(s)\"\ntime=2025-03-31T13:47:43.959+02:00 level=INFO source=download.go:175 msg=\"downloading 40fb844194b2 in 1 487 B part(s)\"\n[GIN] 2025/03/31 - 13:47:47 | 200 | 50.813429899s |       127.0.0.1 | POST     \"/api/pull\"\n[GIN] 2025/03/31 - 13:47:47 | 200 |   13.468933ms |       127.0.0.1 | POST     \"/api/show\"\ntime=2025-03-31T13:47:47.620+02:00 level=INFO source=gpu.go:226 msg=\"looking for compatible GPUs\"\ntime=2025-03-31T13:47:47.649+02:00 level=INFO source=server.go:104 msg=\"system memory\" total=\"31.3 GiB\" free=\"28.8 GiB\" free_swap=\"8.0 GiB\"\ntime=2025-03-31T13:47:47.649+02:00 level=INFO source=memory.go:356 msg=\"offload to device\" layers.requested=-1 layers.model=29 layers.offload=0 layers.split=\"\" memory.available=\"[28.9 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"4.6 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"112.0 MiB\" memory.required.allocations=\"[4.6 GiB]\" memory.weights.total=\"3.8 GiB\" memory.weights.repeating=\"3.3 GiB\" memory.weights.nonrepeating=\"426.4 MiB\" memory.graph.full=\"304.0 MiB\" memory.graph.partial=\"730.4 MiB\"\ntime=2025-03-31T13:47:47.650+02:00 level=INFO source=server.go:392 msg=\"starting llama server\" cmd=\"/home/berend/ipex-llm/ollama-ipex-llm-2.2.0b20250318-ubuntu/ollama-bin runner --model /home/berend/.ollama/models/blobs/sha256-96c415656d377afbff962f6cdb2394ab092ccbcbaab4b82525bc4ca800fe8a49 --ctx-size 2048 --batch-size 512 --n-gpu-layers 999 --threads 6 --no-mmap --parallel 1 --port 38205\"\ntime=2025-03-31T13:47:47.650+02:00 level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\ntime=2025-03-31T13:47:47.650+02:00 level=INFO source=server.go:571 msg=\"waiting for llama runner to start responding\"\ntime=2025-03-31T13:47:47.650+02:00 level=INFO source=server.go:605 msg=\"waiting for server to become available\" status=\"llm server error\"\nggml_sycl_init: found 1 SYCL devices:\ntime=2025-03-31T13:47:47.706+02:00 level=INFO source=runner.go:967 msg=\"starting go runner\"\ntime=2025-03-31T13:47:47.707+02:00 level=INFO source=runner.go:968 msg=system info=\"CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | cgo(gcc)\" threads=6\ntime=2025-03-31T13:47:47.707+02:00 level=INFO source=runner.go:1026 msg=\"Server listening on 127.0.0.1:38205\"\nllama_load_model_from_file: using device SYCL0 (Intel(R) Arc(TM) A770 Graphics) - 15473 MiB free\nllama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /home/berend/.ollama/models/blobs/sha256-96c415656d377afbff962f6cdb2394ab092ccbcbaab4b82525bc4ca800fe8a49 (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 7B\nllama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen\nllama_model_loader: - kv   4:                         general.size_label str              = 7B\nllama_model_loader: - kv   5:                          qwen2.block_count u32              = 28\nllama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072\nllama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 3584\nllama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 18944\nllama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 28\nllama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 4\nllama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  13:                          general.file_type u32              = 15\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\ntime=2025-03-31T13:47:47.901+02:00 level=INFO source=server.go:605 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nllama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646\nllama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\nllama_model_loader: - kv  25:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  141 tensors\nllama_model_loader: - type q4_K:  169 tensors\nllama_model_loader: - type q6_K:   29 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 22\nllm_load_vocab: token to piece cache size = 0.9310 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = qwen2\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 152064\nllm_load_print_meta: n_merges         = 151387\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 131072\nllm_load_print_meta: n_embd           = 3584\nllm_load_print_meta: n_layer          = 28\nllm_load_print_meta: n_head           = 28\nllm_load_print_meta: n_head_kv        = 4\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 7\nllm_load_print_meta: n_embd_k_gqa     = 512\nllm_load_print_meta: n_embd_v_gqa     = 512\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: f_attn_scale     = 0.0e+00\nllm_load_print_meta: n_ff             = 18944\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 2\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 131072\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = Q4_K - Medium\nllm_load_print_meta: model params     = 7.62 B\nllm_load_print_meta: model size       = 4.36 GiB (4.91 BPW) \nllm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 7B\nllm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'\nllm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'\nllm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'\nllm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'\nllm_load_print_meta: LF token         = 148848 'ÄĬ'\nllm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'\nllm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'\nllm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'\nllm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'\nllm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'\nllm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'\nllm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'\nllm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'\nllm_load_print_meta: EOG token        = 151663 '<|repo_name|>'\nllm_load_print_meta: EOG token        = 151664 '<|file_sep|>'\nllm_load_print_meta: max token length = 256\nllm_load_tensors: offloading 28 repeating layers to GPU\nllm_load_tensors: offloading output layer to GPU\nllm_load_tensors: offloaded 29/29 layers to GPU\nllm_load_tensors:        SYCL0 model buffer size =  4168.09 MiB\nllm_load_tensors:          CPU model buffer size =   292.36 MiB\nSIGBUS: bus error\nPC=0x7beae8588d07 m=4 sigcode=2 addr=0x7be93ed13000\nsignal arrived during cgo execution\n\ngoroutine 11 gp=0xc0005048c0 m=4 mp=0xc000083508 [syscall]:\nruntime.cgocall(0x650f1ac5a400, 0xc000091b68)\n\truntime/cgocall.go:167 +0x4b fp=0xc000091b40 sp=0xc000091b08 pc=0x650f1a0b7feb\nollama/llama/llamafile._Cfunc_llama_load_model_from_file(0x7bea80000be0, {0x0, 0x3e7, 0x1, 0x0, 0x0, 0x0, 0x650f1ac59e10, 0xc000694010, 0x0, ...})\n\t_cgo_gotypes.go:701 +0x50 fp=0xc000091b68 sp=0xc000091b40 pc=0x650f1a47adb0\nollama/llama/llamafile.LoadModelFromFile.func1({0x7ffc7e987f8d?, 0xc00050cd20?}, {0x0, 0x3e7, 0x1, 0x0, 0x0, 0x0, 0x650f1ac59e10, 0xc000694010, ...})\n\tollama/llama/llamafile/llama.go:247 +0x127 fp=0xc000091c68 sp=0xc000091b68 pc=0x650f1a47e1e7\nollama/llama/llamafile.LoadModelFromFile({0x7ffc7e987f8d, 0x69}, {0x3e7, 0x0, 0x0, 0x0, {0x0, 0x0, 0x0}, 0xc00061e640, ...})\n\tollama/llama/llamafile/llama.go:247 +0x2d6 fp=0xc000091db8 sp=0xc000091c68 pc=0x650f1a47ded6\nollama/llama/runner.(*Server).loadModel(0xc0004cc120, {0x3e7, 0x0, 0x0, 0x0, {0x0, 0x0, 0x0}, 0xc00061e640, 0x0}, ...)\n\tollama/llama/runner/runner.go:859 +0xc5 fp=0xc000091f10 sp=0xc000091db8 pc=0x650f1a48b085\nollama/llama/runner.Execute.gowrap1()\n\tollama/llama/runner/runner.go:1001 +0xda fp=0xc000091fe0 sp=0xc000091f10 pc=0x650f1a48cc1a\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc000091fe8 sp=0xc000091fe0 pc=0x650f1a0c6ac1\ncreated by ollama/llama/runner.Execute in goroutine 1\n\tollama/llama/runner/runner.go:1001 +0xd0d\n\ngoroutine 1 gp=0xc0000061c0 m=nil [IO wait]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc000587560 sp=0xc000587540 pc=0x650f1a0be6ee\nruntime.netpollblock(0x21f80?, 0x1a055506?, 0xf?)\n\truntime/netpoll.go:575 +0xf7 fp=0xc000587598 sp=0xc000587560 pc=0x650f1a082357\ninternal/poll.runtime_pollWait(0x7beae9acd680, 0x72)\n\truntime/netpoll.go:351 +0x85 fp=0xc0005875b8 sp=0xc000587598 pc=0x650f1a0bd9e5\ninternal/poll.(*pollDesc).wait(0xc000118b00?, 0x2c?, 0x0)\n\tinternal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc0005875e0 sp=0xc0005875b8 pc=0x650f1a145007\ninternal/poll.(*pollDesc).waitRead(...)\n\tinternal/poll/fd_poll_runtime.go:89\ninternal/poll.(*FD).Accept(0xc000118b00)\n\tinternal/poll/fd_unix.go:620 +0x295 fp=0xc000587688 sp=0xc0005875e0 pc=0x650f1a14a3d5\nnet.(*netFD).accept(0xc000118b00)\n\tnet/fd_unix.go:172 +0x29 fp=0xc000587740 sp=0xc000587688 pc=0x650f1a1b2aa9\nnet.(*TCPListener).accept(0xc0006385c0)\n\tnet/tcpsock_posix.go:159 +0x1e fp=0xc000587790 sp=0xc000587740 pc=0x650f1a1c871e\nnet.(*TCPListener).Accept(0xc0006385c0)\n\tnet/tcpsock.go:372 +0x30 fp=0xc0005877c0 sp=0xc000587790 pc=0x650f1a1c75d0\nnet/http.(*onceCloseListener).Accept(0xc000020000?)\n\t<autogenerated>:1 +0x24 fp=0xc0005877d8 sp=0xc0005877c0 pc=0x650f1a440d24\nnet/http.(*Server).Serve(0xc000140960, {0x650f1b208ee0, 0xc0006385c0})\n\tnet/http/server.go:3330 +0x30c fp=0xc000587908 sp=0xc0005877d8 pc=0x650f1a418cac\nollama/llama/runner.Execute({0xc000036130?, 0x0?, 0x0?})\n\tollama/llama/runner/runner.go:1027 +0x11a9 fp=0xc000587ca8 sp=0xc000587908 pc=0x650f1a48c7e9\nollama/cmd.NewCLI.func2(0xc00050e500?, {0x650f1ac5ed1d?, 0x4?, 0x650f1ac5ed21?})\n\tollama/cmd/cmd.go:1430 +0x45 fp=0xc000587cd0 sp=0xc000587ca8 pc=0x650f1ac594e5\ngithub.com/spf13/cobra.(*Command).execute(0xc0004ca008, {0xc0001402d0, 0xf, 0xf})\n\tgithub.com/spf13/cobra@v1.8.1/command.go:985 +0xaaa fp=0xc000587e58 sp=0xc000587cd0 pc=0x650f1a24be8a\ngithub.com/spf13/cobra.(*Command).ExecuteC(0xc0001f5b08)\n\tgithub.com/spf13/cobra@v1.8.1/command.go:1117 +0x3ff fp=0xc000587f30 sp=0xc000587e58 pc=0x650f1a24c75f\ngithub.com/spf13/cobra.(*Command).Execute(...)\n\tgithub.com/spf13/cobra@v1.8.1/command.go:1041\ngithub.com/spf13/cobra.(*Command).ExecuteContext(...)\n\tgithub.com/spf13/cobra@v1.8.1/command.go:1034\nmain.main()\n\tollama/main.go:12 +0x4d fp=0xc000587f50 sp=0xc000587f30 pc=0x650f1ac59b4d\nruntime.main()\n\truntime/proc.go:272 +0x29d fp=0xc000587fe0 sp=0xc000587f50 pc=0x650f1a0899fd\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc000587fe8 sp=0xc000587fe0 pc=0x650f1a0c6ac1\n\ngoroutine 2 gp=0xc000006c40 m=nil [force gc (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc00007cfa8 sp=0xc00007cf88 pc=0x650f1a0be6ee\nruntime.goparkunlock(...)\n\truntime/proc.go:430\nruntime.forcegchelper()\n\truntime/proc.go:337 +0xb8 fp=0xc00007cfe0 sp=0xc00007cfa8 pc=0x650f1a089d38\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc00007cfe8 sp=0xc00007cfe0 pc=0x650f1a0c6ac1\ncreated by runtime.init.7 in goroutine 1\n\truntime/proc.go:325 +0x1a\n\ngoroutine 3 gp=0xc000007180 m=nil [GC sweep wait]:\nruntime.gopark(0x1?, 0x0?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc00007d780 sp=0xc00007d760 pc=0x650f1a0be6ee\nruntime.goparkunlock(...)\n\truntime/proc.go:430\nruntime.bgsweep(0xc00004c080)\n\truntime/mgcsweep.go:317 +0xdf fp=0xc00007d7c8 sp=0xc00007d780 pc=0x650f1a0743df\nruntime.gcenable.gowrap1()\n\truntime/mgc.go:204 +0x25 fp=0xc00007d7e0 sp=0xc00007d7c8 pc=0x650f1a068a25\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc00007d7e8 sp=0xc00007d7e0 pc=0x650f1a0c6ac1\ncreated by runtime.gcenable in goroutine 1\n\truntime/mgc.go:204 +0x66\n\ngoroutine 4 gp=0xc000007340 m=nil [GC scavenge wait]:\nruntime.gopark(0x10000?, 0x650f1ae04ed8?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc00007df78 sp=0xc00007df58 pc=0x650f1a0be6ee\nruntime.goparkunlock(...)\n\truntime/proc.go:430\nruntime.(*scavengerState).park(0x650f1b9a2da0)\n\truntime/mgcscavenge.go:425 +0x49 fp=0xc00007dfa8 sp=0xc00007df78 pc=0x650f1a071da9\nruntime.bgscavenge(0xc00004c080)\n\truntime/mgcscavenge.go:658 +0x59 fp=0xc00007dfc8 sp=0xc00007dfa8 pc=0x650f1a072339\nruntime.gcenable.gowrap2()\n\truntime/mgc.go:205 +0x25 fp=0xc00007dfe0 sp=0xc00007dfc8 pc=0x650f1a0689c5\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc00007dfe8 sp=0xc00007dfe0 pc=0x650f1a0c6ac1\ncreated by runtime.gcenable in goroutine 1\n\truntime/mgc.go:205 +0xa5\n\ngoroutine 5 gp=0xc000007c00 m=nil [finalizer wait]:\nruntime.gopark(0xc00007c648?, 0x650f1a05ef25?, 0xb0?, 0x1?, 0xc0000061c0?)\n\truntime/proc.go:424 +0xce fp=0xc00007c620 sp=0xc00007c600 pc=0x650f1a0be6ee\nruntime.runfinq()\n\truntime/mfinal.go:193 +0x107 fp=0xc00007c7e0 sp=0xc00007c620 pc=0x650f1a067aa7\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc00007c7e8 sp=0xc00007c7e0 pc=0x650f1a0c6ac1\ncreated by runtime.createfing in goroutine 1\n\truntime/mfinal.go:163 +0x3d\n\ngoroutine 6 gp=0xc0001f2e00 m=nil [chan receive]:\nruntime.gopark(0xc00007e760?, 0x650f1a19a125?, 0x40?, 0xe8?, 0x650f1b21c400?)\n\truntime/proc.go:424 +0xce fp=0xc00007e718 sp=0xc00007e6f8 pc=0x650f1a0be6ee\nruntime.chanrecv(0xc00004a310, 0x0, 0x1)\n\truntime/chan.go:639 +0x41c fp=0xc00007e790 sp=0xc00007e718 pc=0x650f1a05811c\nruntime.chanrecv1(0x0?, 0x0?)\n\truntime/chan.go:489 +0x12 fp=0xc00007e7b8 sp=0xc00007e790 pc=0x650f1a057cd2\nruntime.unique_runtime_registerUniqueMapCleanup.func1(...)\n\truntime/mgc.go:1781\nruntime.unique_runtime_registerUniqueMapCleanup.gowrap1()\n\truntime/mgc.go:1784 +0x2f fp=0xc00007e7e0 sp=0xc00007e7b8 pc=0x650f1a06ba8f\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc00007e7e8 sp=0xc00007e7e0 pc=0x650f1a0c6ac1\ncreated by unique.runtime_registerUniqueMapCleanup in goroutine 1\n\truntime/mgc.go:1779 +0x96\n\ngoroutine 7 gp=0xc0001f3a40 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc00007ef38 sp=0xc00007ef18 pc=0x650f1a0be6ee\nruntime.gcBgMarkWorker(0xc00004b730)\n\truntime/mgc.go:1412 +0xe9 fp=0xc00007efc8 sp=0xc00007ef38 pc=0x650f1a06ad89\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1328 +0x25 fp=0xc00007efe0 sp=0xc00007efc8 pc=0x650f1a06ac65\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc00007efe8 sp=0xc00007efe0 pc=0x650f1a0c6ac1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1328 +0x105\n\ngoroutine 18 gp=0xc000104380 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc000078738 sp=0xc000078718 pc=0x650f1a0be6ee\nruntime.gcBgMarkWorker(0xc00004b730)\n\truntime/mgc.go:1412 +0xe9 fp=0xc0000787c8 sp=0xc000078738 pc=0x650f1a06ad89\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1328 +0x25 fp=0xc0000787e0 sp=0xc0000787c8 pc=0x650f1a06ac65\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc0000787e8 sp=0xc0000787e0 pc=0x650f1a0c6ac1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1328 +0x105\n\ngoroutine 34 gp=0xc000504000 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc00050a738 sp=0xc00050a718 pc=0x650f1a0be6ee\nruntime.gcBgMarkWorker(0xc00004b730)\n\truntime/mgc.go:1412 +0xe9 fp=0xc00050a7c8 sp=0xc00050a738 pc=0x650f1a06ad89\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1328 +0x25 fp=0xc00050a7e0 sp=0xc00050a7c8 pc=0x650f1a06ac65\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc00050a7e8 sp=0xc00050a7e0 pc=0x650f1a0c6ac1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1328 +0x105\n\ngoroutine 8 gp=0xc0001f3c00 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc00007f738 sp=0xc00007f718 pc=0x650f1a0be6ee\nruntime.gcBgMarkWorker(0xc00004b730)\n\truntime/mgc.go:1412 +0xe9 fp=0xc00007f7c8 sp=0xc00007f738 pc=0x650f1a06ad89\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1328 +0x25 fp=0xc00007f7e0 sp=0xc00007f7c8 pc=0x650f1a06ac65\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc00007f7e8 sp=0xc00007f7e0 pc=0x650f1a0c6ac1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1328 +0x105\n\ngoroutine 19 gp=0xc000104540 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc000078f38 sp=0xc000078f18 pc=0x650f1a0be6ee\nruntime.gcBgMarkWorker(0xc00004b730)\n\truntime/mgc.go:1412 +0xe9 fp=0xc000078fc8 sp=0xc000078f38 pc=0x650f1a06ad89\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1328 +0x25 fp=0xc000078fe0 sp=0xc000078fc8 pc=0x650f1a06ac65\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc000078fe8 sp=0xc000078fe0 pc=0x650f1a0c6ac1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1328 +0x105\n\ngoroutine 35 gp=0xc0005041c0 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc00050af38 sp=0xc00050af18 pc=0x650f1a0be6ee\nruntime.gcBgMarkWorker(0xc00004b730)\n\truntime/mgc.go:1412 +0xe9 fp=0xc00050afc8 sp=0xc00050af38 pc=0x650f1a06ad89\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1328 +0x25 fp=0xc00050afe0 sp=0xc00050afc8 pc=0x650f1a06ac65\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc00050afe8 sp=0xc00050afe0 pc=0x650f1a0c6ac1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1328 +0x105\n\ngoroutine 9 gp=0xc0004ac1c0 m=nil [GC worker (idle)]:\nruntime.gopark(0x4c7ab775934?, 0x3?, 0x5f?, 0x92?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc00007ff38 sp=0xc00007ff18 pc=0x650f1a0be6ee\nruntime.gcBgMarkWorker(0xc00004b730)\n\truntime/mgc.go:1412 +0xe9 fp=0xc00007ffc8 sp=0xc00007ff38 pc=0x650f1a06ad89\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1328 +0x25 fp=0xc00007ffe0 sp=0xc00007ffc8 pc=0x650f1a06ac65\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc00007ffe8 sp=0xc00007ffe0 pc=0x650f1a0c6ac1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1328 +0x105\n\ngoroutine 10 gp=0xc0004ac380 m=nil [GC worker (idle)]:\nruntime.gopark(0x4c7ab777fae?, 0x3?, 0x73?, 0x83?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc000506738 sp=0xc000506718 pc=0x650f1a0be6ee\nruntime.gcBgMarkWorker(0xc00004b730)\n\truntime/mgc.go:1412 +0xe9 fp=0xc0005067c8 sp=0xc000506738 pc=0x650f1a06ad89\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1328 +0x25 fp=0xc0005067e0 sp=0xc0005067c8 pc=0x650f1a06ac65\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc0005067e8 sp=0xc0005067e0 pc=0x650f1a0c6ac1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1328 +0x105\n\ngoroutine 36 gp=0xc000504380 m=nil [GC worker (idle)]:\nruntime.gopark(0x4c7ab775cb8?, 0x0?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc00050b738 sp=0xc00050b718 pc=0x650f1a0be6ee\nruntime.gcBgMarkWorker(0xc00004b730)\n\truntime/mgc.go:1412 +0xe9 fp=0xc00050b7c8 sp=0xc00050b738 pc=0x650f1a06ad89\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1328 +0x25 fp=0xc00050b7e0 sp=0xc00050b7c8 pc=0x650f1a06ac65\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc00050b7e8 sp=0xc00050b7e0 pc=0x650f1a0c6ac1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1328 +0x105\n\ngoroutine 37 gp=0xc000504540 m=nil [GC worker (idle)]:\nruntime.gopark(0x4c7ab781541?, 0x0?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc00050bf38 sp=0xc00050bf18 pc=0x650f1a0be6ee\nruntime.gcBgMarkWorker(0xc00004b730)\n\truntime/mgc.go:1412 +0xe9 fp=0xc00050bfc8 sp=0xc00050bf38 pc=0x650f1a06ad89\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1328 +0x25 fp=0xc00050bfe0 sp=0xc00050bfc8 pc=0x650f1a06ac65\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc00050bfe8 sp=0xc00050bfe0 pc=0x650f1a0c6ac1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1328 +0x105\n\ngoroutine 38 gp=0xc000504700 m=nil [GC worker (idle)]:\nruntime.gopark(0x650f1b9cc920?, 0x1?, 0x12?, 0x3e?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc00050c738 sp=0xc00050c718 pc=0x650f1a0be6ee\nruntime.gcBgMarkWorker(0xc00004b730)\n\truntime/mgc.go:1412 +0xe9 fp=0xc00050c7c8 sp=0xc00050c738 pc=0x650f1a06ad89\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1328 +0x25 fp=0xc00050c7e0 sp=0xc00050c7c8 pc=0x650f1a06ac65\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc00050c7e8 sp=0xc00050c7e0 pc=0x650f1a0c6ac1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1328 +0x105\n\ngoroutine 20 gp=0xc000104700 m=nil [GC worker (idle)]:\nruntime.gopark(0x4c7ab7744da?, 0x0?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc000079738 sp=0xc000079718 pc=0x650f1a0be6ee\nruntime.gcBgMarkWorker(0xc00004b730)\n\truntime/mgc.go:1412 +0xe9 fp=0xc0000797c8 sp=0xc000079738 pc=0x650f1a06ad89\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1328 +0x25 fp=0xc0000797e0 sp=0xc0000797c8 pc=0x650f1a06ac65\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc0000797e8 sp=0xc0000797e0 pc=0x650f1a0c6ac1\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1328 +0x105\n\ngoroutine 12 gp=0xc000504a80 m=nil [semacquire]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0xa0?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc00050d618 sp=0xc00050d5f8 pc=0x650f1a0be6ee\nruntime.goparkunlock(...)\n\truntime/proc.go:430\nruntime.semacquire1(0xc0004cc128, 0x0, 0x1, 0x0, 0x12)\n\truntime/sema.go:178 +0x22c fp=0xc00050d680 sp=0xc00050d618 pc=0x650f1a09caac\nsync.runtime_Semacquire(0x0?)\n\truntime/sema.go:71 +0x25 fp=0xc00050d6b8 sp=0xc00050d680 pc=0x650f1a0bff05\nsync.(*WaitGroup).Wait(0x0?)\n\tsync/waitgroup.go:118 +0x48 fp=0xc00050d6e0 sp=0xc00050d6b8 pc=0x650f1a0d52e8\nollama/llama/runner.(*Server).run(0xc0004cc120, {0x650f1b20b1d0, 0xc0006240f0})\n\tollama/llama/runner/runner.go:315 +0x47 fp=0xc00050d7b8 sp=0xc00050d6e0 pc=0x650f1a487707\nollama/llama/runner.Execute.gowrap2()\n\tollama/llama/runner/runner.go:1006 +0x28 fp=0xc00050d7e0 sp=0xc00050d7b8 pc=0x650f1a48cb08\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc00050d7e8 sp=0xc00050d7e0 pc=0x650f1a0c6ac1\ncreated by ollama/llama/runner.Execute in goroutine 1\n\tollama/llama/runner/runner.go:1006 +0xde5\n\ngoroutine 50 gp=0xc000604700 m=nil [IO wait]:\nruntime.gopark(0x650f1a148605?, 0xc0001aa000?, 0x10?, 0x7a?, 0xb?)\n\truntime/proc.go:424 +0xce fp=0xc0002d7918 sp=0xc0002d78f8 pc=0x650f1a0be6ee\nruntime.netpollblock(0x650f1a0e1918?, 0x1a055506?, 0xf?)\n\truntime/netpoll.go:575 +0xf7 fp=0xc0002d7950 sp=0xc0002d7918 pc=0x650f1a082357\ninternal/poll.runtime_pollWait(0x7beae9acd568, 0x72)\n\truntime/netpoll.go:351 +0x85 fp=0xc0002d7970 sp=0xc0002d7950 pc=0x650f1a0bd9e5\ninternal/poll.(*pollDesc).wait(0xc0001aa000?, 0xc0001fc000?, 0x0)\n\tinternal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc0002d7998 sp=0xc0002d7970 pc=0x650f1a145007\ninternal/poll.(*pollDesc).waitRead(...)\n\tinternal/poll/fd_poll_runtime.go:89\ninternal/poll.(*FD).Read(0xc0001aa000, {0xc0001fc000, 0x1000, 0x1000})\n\tinternal/poll/fd_unix.go:165 +0x27a fp=0xc0002d7a30 sp=0xc0002d7998 pc=0x650f1a1462fa\nnet.(*netFD).Read(0xc0001aa000, {0xc0001fc000?, 0xc0002d7aa0?, 0x650f1a1454c5?})\n\tnet/fd_posix.go:55 +0x25 fp=0xc0002d7a78 sp=0xc0002d7a30 pc=0x650f1a1b0ae5\nnet.(*conn).Read(0xc000120088, {0xc0001fc000?, 0x0?, 0xc0003220c8?})\n\tnet/net.go:189 +0x45 fp=0xc0002d7ac0 sp=0xc0002d7a78 pc=0x650f1a1bf0e5\nnet.(*TCPConn).Read(0xc0003220c0?, {0xc0001fc000?, 0xc0001aa000?, 0xc0002d7af8?})\n\t<autogenerated>:1 +0x25 fp=0xc0002d7af0 sp=0xc0002d7ac0 pc=0x650f1a1d22e5\nnet/http.(*connReader).Read(0xc0003220c0, {0xc0001fc000, 0x1000, 0x1000})\n\tnet/http/server.go:798 +0x14b fp=0xc0002d7b40 sp=0xc0002d7af0 pc=0x650f1a40ea6b\nbufio.(*Reader).fill(0xc000112060)\n\tbufio/bufio.go:110 +0x103 fp=0xc0002d7b78 sp=0xc0002d7b40 pc=0x650f1a1d69e3\nbufio.(*Reader).Peek(0xc000112060, 0x4)\n\tbufio/bufio.go:148 +0x53 fp=0xc0002d7b98 sp=0xc0002d7b78 pc=0x650f1a1d6b13\nnet/http.(*conn).serve(0xc000020000, {0x650f1b20b198, 0xc0001c6420})\n\tnet/http/server.go:2127 +0x738 fp=0xc0002d7fb8 sp=0xc0002d7b98 pc=0x650f1a413db8\nnet/http.(*Server).Serve.gowrap3()\n\tnet/http/server.go:3360 +0x28 fp=0xc0002d7fe0 sp=0xc0002d7fb8 pc=0x650f1a4190a8\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc0002d7fe8 sp=0xc0002d7fe0 pc=0x650f1a0c6ac1\ncreated by net/http.(*Server).Serve in goroutine 1\n\tnet/http/server.go:3360 +0x485\n\nrax    0x7be93ed13000\nrbx    0x7bea8287b390\nrcx    0x3800\nrdx    0x3800\nrdi    0x7be93ed13000\nrsi    0x7bea82877b80\nrbp    0x7bea8affb580\nrsp    0x7bea8affb398\nr8     0x7be93ed13000\nr9     0x13330b000\nr10    0x1\nr11    0x246\nr12    0x7bea82877b80\nr13    0x7be93ed13000\nr14    0x8287e400\nr15    0x7bea80b25c60\nrip    0x7beae8588d07\nrflags 0x10206\ncs     0x33\nfs     0x0\ngs     0x0\ntime=2025-03-31T13:47:48.158+02:00 level=INFO source=server.go:605 msg=\"waiting for server to become available\" status=\"llm server error\"\ntime=2025-03-31T13:47:48.409+02:00 level=ERROR source=sched.go:455 msg=\"error loading llama server\" error=\"llama runner process has terminated: exit status 2\"```\n\n**Environment information**\nOutput of env-check.sh:\n```-----------------------------------------------------------------\n-----------------------------------------------------------------\nTransformers is not installed. \n-----------------------------------------------------------------\nPyTorch is not installed. \n-----------------------------------------------------------------\nipex-llm ./env-check.sh: line 41: pip: command not found\n-----------------------------------------------------------------\nIPEX is not installed. \n-----------------------------------------------------------------\nCPU Information: \nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        48 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               12\nOn-line CPU(s) list:                  0-11\nVendor ID:                            AuthenticAMD\nModel name:                           AMD Ryzen 5 5600 6-Core Processor\nCPU family:                           25\nModel:                                33\nThread(s) per core:                   2\nCore(s) per socket:                   6\nSocket(s):                            1\nStepping:                             2\nFrequency boost:                      enabled\nCPU(s) scaling MHz:                   52%\nCPU max MHz:                          4468.0000\n-----------------------------------------------------------------\nTotal CPU Memory: 31.2684 GB\n-----------------------------------------------------------------\nOperating System: \nUbuntu 24.04.2 LTS \\n \\l\n\n-----------------------------------------------------------------\nLinux adamantium 6.11.0-21-generic #21~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Feb 24 16:52:15 UTC 2 x86_64 x86_64 x86_64 GNU/Linux\n-----------------------------------------------------------------\n./env-check.sh: line 131: xpu-smi: command not found\n-----------------------------------------------------------------\n  Driver UUID                                     32352e30-352e-3332-3536-370000000000\n  Driver Version                                  25.05.32567\n-----------------------------------------------------------------\nDriver related package version:\n-----------------------------------------------------------------\n./env-check.sh: line 150: sycl-ls: command not found\nigpu not detected\n-----------------------------------------------------------------\nxpu-smi is not installed. Please install xpu-smi according to README.md\n```\n\n**Additional context**\nI previously also ran containers with ipex-llm, ollama and open-webui and would get the same result. When changing OLLAMA_NUM_GPU from 999 to 1 would not give an error, but would offload just 1 layer to the GPU.\n\nCould this be related to issue #12992 ?",
      "state": "closed",
      "author": "berendvosmer",
      "author_type": "User",
      "created_at": "2025-03-31T13:09:45Z",
      "updated_at": "2025-04-01T15:41:51Z",
      "closed_at": "2025-04-01T15:41:50Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13029/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13029",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13029",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:49.195710",
      "comments": [
        {
          "author": "berendvosmer",
          "body": "Resizable BAR was disabled in the BIOS. Will try again and report.",
          "created_at": "2025-04-01T13:56:03Z"
        },
        {
          "author": "berendvosmer",
          "body": "Resizable bar set to 'auto' in the BIOS solved this issue for me. #12992",
          "created_at": "2025-04-01T15:41:50Z"
        }
      ]
    },
    {
      "issue_number": 13025,
      "title": "Ollama ipex-llm Code Generation quality issue",
      "body": "**Describe the bug**\nIt is an performance issue, running 0llama-0.5.4-ipex-llm-2.2.0b20250226, the code generation quality is lower than others \n\n**How to reproduce**\nDS14B本地模型运行情况（以生成可以在网页上运行的俄罗斯方块游戏为例）：\nFlowy2.1.3-beta1：\n1、\t使用本地模型生成可以在网页上运行的俄罗斯方块代码，代码运行后游戏不能正常启动；\n2、\t基于以上生成从代码进行追问，模型不能结合之前的回答来解决追问的问题，而是会作为一次新的对话进行思考和回答。\n0llama-0.5.4-ipex-llm-2.2.0b20250226：\n1、\t使用本地模型生成可以在网页上运行的俄罗斯方块代码，代码运行后游戏不能正常启动；\n2、\t基于以上生成从代码进行追问，模型不能结合之前的回答来解决追问的问题，而是会作为一次新的对话进行思考和回答。\nUsing other tools：\n1、\t使用本地模型生成的游戏代码可以正常运行体验；\n2、\t对已生成的代码进行追问增加游戏难度，模型可以理解问题并结合之前的代码给出进阶版游戏完整代码。\n\n",
      "state": "open",
      "author": "juan-OY",
      "author_type": "User",
      "created_at": "2025-03-31T01:50:29Z",
      "updated_at": "2025-04-01T02:55:26Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13025/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13025",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13025",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:49.429465",
      "comments": [
        {
          "author": "juan-OY",
          "body": "使用0llama-0.5.4-ipex-llm-2.2.0b20250226的时候，前端使用的是openWebUI的界面",
          "created_at": "2025-03-31T02:14:58Z"
        },
        {
          "author": "qiuxin2012",
          "body": "Could you try latest [ollama-ipex-llm-2.2.0b20250318-win.zip](https://github.com/intel/ipex-llm/releases/download/v2.2.0-nightly/ollama-ipex-llm-2.2.0b20250318-win.zip) ?",
          "created_at": "2025-04-01T02:11:48Z"
        },
        {
          "author": "juan-OY",
          "body": "使用最新版本ollama-ipex-llm-2.2.0b20250318-win.zip的webui进行了14B模型质量测试，分别测试了①生成可以在HTML上正常运行的俄罗斯方块代码；②生成可以在HTML上正常运行的五子棋游戏代码；③帮我用HTML设计一个好玩的贪吃蛇游戏，均不能生成可以正常体验游戏的代码。",
          "created_at": "2025-04-01T02:55:24Z"
        }
      ]
    },
    {
      "issue_number": 13020,
      "title": "Documentation for running ipex-llm (at least Ollama) is incorrect and no device will be found",
      "body": "I found this issue on Ubuntu 24.10, if you are on a different version YMMV.\n\nThese instructions ultimately end up not working:\n\nhttps://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/bmg_quickstart.md\n\nafter running through the instructions start-ollama.sh (from the portable version) throws a golang error immediately upon starting.\n\nclinfo shows essentially nothing, no devices:\n\n> clinfo\n> Number of platforms                               0\n> \n> ICD loader properties\n>   ICD loader Name                                 OpenCL ICD Loader\n>   ICD loader Vendor                               OCL Icd free software\n>   ICD loader Version                              2.3.2\n>   ICD loader Profile                              OpenCL 3.0\n\nsudo clinfo does show the devices.\n\nSomething in the process in the bag_quickstart.md adds environment variables (3 of them):\n\nOCL_ICD_VENDORS_RESET=1\nOCL_ICD_VENDORS=/home/ubuntu/miniforge3/envs/battlemage/etc/OpenCL/vendors\nOCL_ICD_FILENAMES_RESET=1\n\nls /home/ubuntu/miniforge3/envs/battlemage/etc/OpenCL/vendors\n\n> intel-cpu.icd\n> intel-fpga_emu.icd\n\ncat intel-cpu.icd\n\n> /opt/anaconda1anaconda2anaconda3/lib/libintelocl.so\n\nEverything after /opt just doesn't exist on my system.\n\nunset OCL_ICD_VENDORS_RESET\nunset OCL_ICD_VENDORS\nunset OCL_ICD_FILENAMES_RESET\n\nSeems to fix the issue.\n\nWhat is inserting this:  /opt/anaconda1anaconda2anaconda3/lib/libintelocl.so into this file: /home/ubuntu/miniforge3/envs/battlemage/etc/OpenCL/vendors/intel-cpu.icd ?\n\nNow I see this when I run ./start-ollama.sh & ./ollama run deepseek-r1\n\n> ggml_sycl_init: found 3 SYCL devices:\n",
      "state": "open",
      "author": "Anonymous-Lurker",
      "author_type": "User",
      "created_at": "2025-03-28T01:02:48Z",
      "updated_at": "2025-03-31T22:33:30Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13020/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13020",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13020",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:49.614635",
      "comments": [
        {
          "author": "tpragasa",
          "body": "If you do not see the devices listed with `clinfo`, [ensure that you have the permissions to access `/dev/dri/renderD*`](https://dgpu-docs.intel.com/driver/client/overview.html#verifying-installation).\n\n```bash\nsudo gpasswd -a ${USER} render\nnewgrp render\n```",
          "created_at": "2025-03-30T03:35:17Z"
        },
        {
          "author": "Anonymous-Lurker",
          "body": "This is the crux of the problem - this file does not exist.  Something sets up a reference to the file, but the filesystem directory structure and file itself are not there: /opt/anaconda1anaconda2anaconda3/lib/libintelocl.so",
          "created_at": "2025-03-31T22:33:30Z"
        }
      ]
    },
    {
      "issue_number": 12643,
      "title": "How can I use NPU to run Ollama on Intel Ultra7 258V",
      "body": "How can I use NPU to run Ollama on Intel Ultra7 258V? Is there any relevant documentation?\r\nI don't know how to configure according to the following documentation.\r\n[npu_quickstart](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/npu_quickstart.md)",
      "state": "open",
      "author": "jackphj",
      "author_type": "User",
      "created_at": "2025-01-02T09:43:15Z",
      "updated_at": "2025-03-31T20:58:36Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12643/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12643",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12643",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:49.801452",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @jackphj, we do not currently support running Ollama on NPU. Please refer to our [Ollama document](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.md) for updates ;)",
          "created_at": "2025-01-03T01:22:48Z"
        },
        {
          "author": "DocMAX",
          "body": "i bought my laptop because of NPU and now i read it's not supported in linux nor by ollama? wtf?",
          "created_at": "2025-03-29T18:30:13Z"
        },
        {
          "author": "mecattaf",
          "body": "@sgwhat what about supporting ipex-llm on https://github.com/containers/ramalama ? I understand that ollama is out of the picture, and i think that ramalama is exactly the type of project that we could fold ipex-llm into for seamlessly using local LLMs on our devices!",
          "created_at": "2025-03-31T20:58:36Z"
        }
      ]
    },
    {
      "issue_number": 12994,
      "title": "llama_load_model_from_file: failed to load model with SYCL/Level Zero on Intel Arc B580 GPU",
      "body": "**Describe the bug**\nWhen running the Ollama server with an Intel Arc GPU (B580) using SYCL/Level Zero, the model fails to load with the error:\n\n```\nllama_load_model_from_file: failed to load model\npanic: unable to load model: /root/.ollama/models/blobs/sha256-04778965089b91318ad61d0995b7e44fad4b9a9f4e049d7be90932bf8812e828\n```\n\nThe logs indicate that the model metadata is loaded successfully, but the actual model tensors fail to load with an Input/output error. GPU offloading is attempted, but the process ultimately crashes.\n\n```\nllama_load_model_from_file: using device SYCL0 (Intel(R) Graphics [0xe20b]) - 11605 MiB free\nllama_model_loader: loaded meta data with 20 key-value pairs and 325 tensors from /root/.ollama/models/blobs/sha256-04778965089b91318ad61d0995b7e44fad4b9a9f4e049d7be90932bf8812e828 (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n...\nllm_load_tensors: offloading 32 repeating layers to GPU\nllm_load_tensors: offloading output layer to GPU\nllm_load_tensors: offloaded 33/33 layers to GPU\nllm_load_tensors:        SYCL0 model buffer size =  1456.19 MiB\nllm_load_tensors:    SYCL_Host model buffer size =    70.31 MiB\ntime=2025-03-23T10:12:40.886+08:00 level=INFO source=server.go:605 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nllama_model_load: error loading model: read error: Input/output error\nllama_load_model_from_file: failed to load model\npanic: unable to load model: /root/.ollama/models/blobs/sha256-04778965089b91318ad61d0995b7e44fad4b9a9f4e049d7be90932bf8812e828\n```\n\n**How to reproduce**\nI'm basically following this guide: https://syslynx.net/llm-intel-b580-linux/\n\nMy container run props aren't special:\n\n```\npodman run  -itd \\\n            --device=/dev/dri\\\n            --name=intel-llm \\\n            -v /srv/llm/ollama/models:/root/.ollama/models \\\n            -e no_proxy=localhost,127.0.0.1 \\\n            -e OLLAMA_HOST=0.0.0.0 \\\n            -e DEVICE=Arc \\\n            -e HOSTNAME=intel-llm \\\n            -e OLLAMA_NUM_GPU=999 \\\n            -e ZES_ENABLE_SYSMAN=1 \\\n            -e SYCL_DEVICE_FILTER=level_zero:gpu \\\n            --network ml_shared \\\n            --restart=always \\\n            docker.io/intelanalytics/ipex-llm-inference-cpp-xpu:latest \\\n            sh -c 'mkdir -p /llm/ollama && cd /llm/ollama && init-ollama && exec ./ollama serve'\n```\n\nI've trying to use open-webui as in the guide but it results in a 500, with the above logs in the intel-llm.\n\nI tried running a model manually, but I must be doing something wrong. Maybe the default pathing is incorrect for ollama to be run like this:\n\n```\n$ podman exec -it intel-llm ollama/ollama list\nggml_sycl_init: found 1 SYCL devices:\nNAME                                      ID              SIZE      MODIFIED\nphi:latest                                e2fd6321a5fe    1.6 GB    55 minutes ago\n\n$ podman exec -it intel-llm ollama/ollama run phi:latest \"test\" --verbose\nggml_sycl_init: found 1 SYCL devices:\nError: llama runner process has terminated: error loading model: read error: Input/output error\nllama_load_model_from_file: failed to load model\n\n# just to confirm the container envs passed in:\n$ podman exec -it intel-llm env\n...\nSYCL_DEVICE_FILTER=level_zero:gpu\nSYCL_CACHE_PERSISTENT=1\nOLLAMA_NUM_GPU=999\nDEVICE=Arc\nZES_ENABLE_SYSMAN=1\n...\n```\n\n\n**Environment information**\n\nMy environment is an Ubuntu 24.10 VM with the 6.13.7 kernel and raw PCI B580 GPU pass through.\n\nHost OS: Proxmox 8.3.5 w/ 6.11 kernel and Intel driver blacklisted\nGPU: Intel Arc B580 12GB\n\nContainer Image: docker.io/intelanalytics/ipex-llm-inference-cpp-xpu:latest\nOllama Version: 0.5.4-ipexllm-20250320\nEnvironment Variables:\nZES_ENABLE_SYSMAN=1\nSYCL_DEVICE_FILTER=level_zero:gpu\nOLLAMA_NUM_GPU=999\n\nThe model metadata is loaded successfully, but the actual model tensors fail to load with an Input/output error. GPU offloading is attempted, but the process crashes before completing. The issue persists even after re-downloading the model and verifying the model files.\n\n\n```\n$ ls -al /dev/dri/\ncrw-rw----+  1 root video  226,   0 Mar 22 19:03 card0\ncrw-rw----+  1 root video  226,   1 Mar 22 19:03 card1\ncrw-rw----+  1 root render 226, 128 Mar 22 19:03 renderD128\n\n# uname -a\nLinux 7aab3474ddb9 6.13.7-061307-generic #202503131244 SMP PREEMPT_DYNAMIC Fri Mar 14 02:34:39 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux\n\n# Ubuntu VM:\n$ cat /etc/os-release\nPRETTY_NAME=\"Ubuntu 24.10\"\nNAME=\"Ubuntu\"\nVERSION_ID=\"24.10\"\nVERSION=\"24.10 (Oracular Oriole)\"\n\n# CONTAINER: docker.io/intelanalytics/ipex-llm-inference-cpp-xpu:latest\n# cat /etc/os-release\nPRETTY_NAME=\"Ubuntu 22.04.5 LTS\"\nNAME=\"Ubuntu\"\nVERSION_ID=\"22.04\"\nVERSION=\"22.04.5 LTS (Jammy Jellyfish)\"\n\n# sycl-ls\n[level_zero:gpu][level_zero:0] Intel(R) oneAPI Unified Runtime over Level-Zero, Intel(R) Graphics [0xe20b] 20.1.0 [1.6.32224.500000]\n[opencl:cpu][opencl:0] Intel(R) OpenCL, 13th Gen Intel(R) Core(TM) i5-13400 OpenCL 3.0 (Build 0) [2024.18.12.0.05_160000]\n[opencl:gpu][opencl:1] Intel(R) OpenCL Graphics, Intel(R) Graphics [0xe20b] OpenCL 3.0 NEO  [24.52.32224.5]\n```\n\nTo confirm, the Intel(R) oneAPI Unified Runtime over Level-Zero, Intel(R) Graphics [0x**e20b**]  lines up with the lspci listed Intel Corporation Battlemage G21 [Intel Graphics] [8086:**e20b**] \n\n```\n$ sudo lspci -vnn | grep -i vga -A 12\n[sudo] password for user:\n00:02.0 VGA compatible controller [0300]: Device [1234:1111] (rev 02) (prog-if 00 [VGA controller])\n...\n00:10.0 VGA compatible controller [0300]: Intel Corporation Battlemage G21 [Intel Graphics] [8086:**e20b**] (prog-if 00 [VGA controller])\n\tSubsystem: Intel Corporation Device [8086:1100]\n\tPhysical Slot: 16\n\tFlags: bus master, fast devsel, latency 0, IRQ 38\n\tMemory at 7000000000 (64-bit, non-prefetchable) [size=16M]\n\tMemory at 7400000000 (64-bit, prefetchable) [size=16G]\n\tExpansion ROM at 000c0000 [disabled] [size=128K]\n\tCapabilities: [40] Vendor Specific Information: Len=0c <?>\n\tCapabilities: [70] Express Endpoint, IntMsgNum 0\n\tCapabilities: [ac] MSI: Enable+ Count=1/1 Maskable+ 64bit+\n\tCapabilities: [d0] Power Management version 3\n\tKernel driver in use: xe\n\tKernel modules: xe\n```\n\nAs requested I tried running the nv-check.sh in the container, but some things are missing from it. \n\n```\nroot@7aab3474ddb9:~# ./env-check.sh\n-----------------------------------------------------------------\nPYTHON_VERSION=3.11.11\n-----------------------------------------------------------------\n/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\ntransformers=4.36.2\n-----------------------------------------------------------------\ntorch=2.2.0+cu121\n-----------------------------------------------------------------\nipex-llm Version: 2.2.0b20250320\n-----------------------------------------------------------------\nIPEX is not installed.\n-----------------------------------------------------------------\nCPU Information:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        39 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               6\nOn-line CPU(s) list:                  0-5\nVendor ID:                            GenuineIntel\nModel name:                           13th Gen Intel(R) Core(TM) i5-13400\nCPU family:                           6\nModel:                                183\nThread(s) per core:                   1\nCore(s) per socket:                   6\nSocket(s):                            1\nStepping:                             1\nBogoMIPS:                             4992.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology cpuid tsc_known_freq pni pclmulqdq vmx ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves avx_vnni arat vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq rdpid movdiri movdir64b fsrm md_clear serialize flush_l1d arch_capabilities\nVirtualization:                       VT-x\n-----------------------------------------------------------------\nTotal CPU Memory: 15.117 GB\nMemory Type: sudo: dmidecode: command not found\n-----------------------------------------------------------------\nOperating System:\nUbuntu 22.04.5 LTS \\n \\l\n\n-----------------------------------------------------------------\nLinux 7aab3474ddb9 6.13.7-061307-generic #202503131244 SMP PREEMPT_DYNAMIC Fri Mar 14 02:34:39 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux\n-----------------------------------------------------------------\n./env-check.sh: line 148: xpu-smi: command not found\n-----------------------------------------------------------------\n./env-check.sh: line 154: clinfo: command not found\n-----------------------------------------------------------------\nDriver related package version:\nii  intel-level-zero-gpu                             1.6.32224.5                             amd64        Intel(R) Graphics Compute Runtime for oneAPI Level Zero.\nii  intel-level-zero-gpu-legacy1                     1.3.30872.22                            amd64        Intel(R) Graphics Compute Runtime for oneAPI Level Zero.\nii  level-zero-devel                                 1.20.2                                  amd64        oneAPI Level Zero\n-----------------------------------------------------------------\nigpu not detected\n-----------------------------------------------------------------\nxpu-smi is not installed. Please install xpu-smi according to README.md\n```\n\n**Additional context**\nAdd any other context about the problem here.\n",
      "state": "open",
      "author": "duhmojo",
      "author_type": "User",
      "created_at": "2025-03-23T02:48:36Z",
      "updated_at": "2025-03-31T17:20:34Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12994/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12994",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12994",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:50.354429",
      "comments": [
        {
          "author": "duhmojo",
          "body": "Here, I tried it with gemma3:4b as well (from Open WebUI). Error log is a bit different. To confirm, ZES_ENABLE_SYSMAN is set to 1.\n\n```\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\n\nllama_model_load: e",
          "created_at": "2025-03-23T02:56:32Z"
        },
        {
          "author": "duhmojo",
          "body": "Tried adding `IPEX_LLM_QUANTIZE_KV_CACHE=1` which I desperately noticed in another issue. Same error.",
          "created_at": "2025-03-23T03:07:54Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @duhmojo, could you please try to run `gemma3:1b` on your host node?",
          "created_at": "2025-03-25T01:15:49Z"
        },
        {
          "author": "duhmojo",
          "body": "I'll give it a try tonight first thing. I got your request a bit late. ",
          "created_at": "2025-03-25T14:31:35Z"
        },
        {
          "author": "duhmojo",
          "body": "Looks like I'm too late. gemma3:1b requires the \"latest version of ollama\". I have gemma3:4b from the weekend and didn't get the same out of date ollama error. I pulled the latest docker.io/intelanalytics/ipex-llm-inference-cpp-xpu:latest again and re-ran it, same problem. I don't think I can grab s",
          "created_at": "2025-03-26T02:54:20Z"
        }
      ]
    },
    {
      "issue_number": 12950,
      "title": "Please upgrade to Ollama latest version to support the model split between GPU and CPU.",
      "body": "I have a system with 96 GB of RAM and running with a Arc A770 gpu. But not able to load models more than 16GB with current 0.5.4 version.\n\nThanks ",
      "state": "open",
      "author": "baoduy",
      "author_type": "User",
      "created_at": "2025-03-07T02:41:53Z",
      "updated_at": "2025-03-31T16:32:47Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 17,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12950/reactions",
        "total_count": 4,
        "+1": 4,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12950",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12950",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:50.583237",
      "comments": [
        {
          "author": "warcow105",
          "body": "On top of that the newer models are requiring a much newer version of ollama.  phi4-mini and granite3.2-vision require 0.5.13.",
          "created_at": "2025-03-07T15:26:05Z"
        },
        {
          "author": "digitalextremist",
          "body": "Thanks for posting this @baoduy and giving more detail @warcow105. I came here for the exact same reasons.\n\n[I posted asking about this on the `Ollama` discord also](https://discord.com/channels/1128867683291627614/1347065569475559442/1347065569475559442) in case there were solutions coming to mind ",
          "created_at": "2025-03-10T00:22:51Z"
        },
        {
          "author": "qiuxin2012",
          "body": "The recommended option `OLLAMA_NUM_GPU=999`, it means to put all the models the GPU. You can reduce the `999` to a small number like 20, it will put only 20 layers in the model  to GPU, the rest layers will be on CPU.",
          "created_at": "2025-03-10T03:07:04Z"
        },
        {
          "author": "digitalextremist",
          "body": "Going to try this in the meantime, waiting for the newer `ollama` version:\n\n```sh\nOLLAMA_NUM_GPU=20\n```\n\n@qiuxin2012 do you have a reference on how many layers to use with an `Arc A770` ( 16gb VRAM ) by any chance?",
          "created_at": "2025-03-12T03:00:24Z"
        },
        {
          "author": "digitalextremist",
          "body": "Feeling the heat now: https://ollama.com/library/gemma3\n\nRequires `0.6.0` as the minimum version cut-off",
          "created_at": "2025-03-12T23:05:27Z"
        }
      ]
    },
    {
      "issue_number": 12844,
      "title": "Ollama can't run on 9th gen intel CPU or older, please use a newer version of Ollama",
      "body": "Hello!\n\nI am trying to run Ollama using 2x Intel Arc B580 running in a server with an i7-9700k, but it seems like it can't run with IPEX as the CPU is \"Too old\". I reported this issue to the Ollama project, and I was directed to submit it here as well. This is the issue I submitted at Ollama: https://github.com/ollama/ollama/issues/9176\n\nWould it be possible to update Ollama version to 0.5.8 or higher?",
      "state": "open",
      "author": "Ejo2001",
      "author_type": "User",
      "created_at": "2025-02-18T08:41:05Z",
      "updated_at": "2025-03-31T13:27:30Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 39,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12844/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12844",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12844",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:50.805780",
      "comments": []
    },
    {
      "issue_number": 13023,
      "title": "Harness does not work properly",
      "body": "**Describe the bug**\nWanna use Harness but does not work.\n**How to reproduce**\nSteps to reproduce the error:\n1. Using Conda:\n- conda create with python 3.11\n- conda activate& git clone harnesss\n- pip install -e .\n![Image](https://github.com/user-attachments/assets/b0fc65dd-4b51-4ff6-8f01-356eb0bdaa27)\n2.Using Docker b11:\n- pip install -e . successfully\n- python run_multi_llb.py --model ipex-llm --pretrained /model/DeepSeek-R1-Distill-Qwen-32B --precision sym_int4 --device xpu:0,1,2,3 --tasks mmlu --batch 1 --no_cache\n![Image](https://github.com/user-attachments/assets/3d913ff8-4bba-47a8-ad13-3a8e9444d54b)\n- pip install datasets==2.21.0\n-  python run_multi_llb.py --model ipex-llm --pretrained /model/DeepSeek-R1-Distill-Qwen-32B --precision sym_int4 --device xpu:0,1,2,3 --tasks mmlu --batch 1 --no_cache\n![Image](https://github.com/user-attachments/assets/52ed2e0c-cc4a-4437-bc7e-a5c0835d6af3)\n-  pip install accelerate==0.26.0 & python run_multi_llb.py again\n![Image](https://github.com/user-attachments/assets/41bf30ec-7b47-4db8-8834-11092b9dffc2)\n-pip install trl==0.11.0 & python run_multi_llb.py again\n\n![Image](https://github.com/user-attachments/assets/fcc1dca8-289e-4a94-873b-6dff4c219cf1)\n",
      "state": "open",
      "author": "RobinJing",
      "author_type": "User",
      "created_at": "2025-03-28T09:28:50Z",
      "updated_at": "2025-03-31T07:30:51Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13023/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hzjane"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13023",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13023",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:50.805807",
      "comments": [
        {
          "author": "RobinJing",
          "body": "For the conda environment, I have setup the environment and the deployment is ready to go, after this , I can start the script with:\n_python run_multi_llb.py --model ipex-llm --pretrained /model/DeepSeek-R1-Distill-Qwen-32B --precision sym_int4 --device xpu:0,1,2,3 --tasks hellaswag --batch 1 --no_c",
          "created_at": "2025-03-31T05:23:49Z"
        },
        {
          "author": "hzjane",
          "body": "Maybe you can try to run it again using latest b16 image, and we don't need to install conda again in docker container, you can use `pip install` directly to test.",
          "created_at": "2025-03-31T06:12:48Z"
        },
        {
          "author": "RobinJing",
          "body": "Hi, \nI use b16 image docker to perform the test, simply I start the backend server from vllm, and use harness 'local-completions' as the frontend, when I run the winogrande task, it works fine; however if I run mmlu task, the vllm server crashes:\n \n![Image](https://github.com/user-attachments/assets",
          "created_at": "2025-03-31T06:37:47Z"
        },
        {
          "author": "hkvision",
          "body": "If you want to run multi-card for one single large model, refer to this guide: https://github.com/intel/ipex-llm/tree/main/python/llm/dev/benchmark/ceval#multi-gpu-environment\n\n`run_multi_llb.py` is supposed to run multiple tasks across multi cards and one model & task on a single card.",
          "created_at": "2025-03-31T07:02:27Z"
        },
        {
          "author": "hzjane",
          "body": "> Hi, I use b16 image docker to perform the test, simply I start the backend server from vllm, and use harness 'local-completions' as the frontend, when I run the winogrande task, it works fine; however if I run mmlu task, the vllm server crashes:\n> \n> ![Image](https://github.com/user-attachments/as",
          "created_at": "2025-03-31T07:21:43Z"
        }
      ]
    },
    {
      "issue_number": 12723,
      "title": "non-stream is not supported",
      "body": "i use ipex-llm==2.1.0b20240805+vllm 0.4.2 to run Qwen2-7B-Instruct on CPU, the use curl to launch http request to call the api which is openai api compatible.\nThe server start command:\n```\npython -m ipex_llm.vllm.cpu.entrypoints.openai.api_server  \n--model /datamnt/Qwen2-7B-Instruct --port 8080   \n--served-model-name 'Qwen/Qwen2-7B-Instruct'  \n--load-format 'auto' --device cpu --dtype bfloat16  \n--load-in-low-bit sym_int4   \n--max-num-batched-tokens 32768\n```\nThe curl command:\n```\ntime curl http://172.16.30.28:8080/v1/chat/completions  -H \"Content-Type: application/json\" -d '{\n    \"model\": \"Qwen/Qwen2-7B-Instruct\",\n    \"messages\": [\n        {\"role\": \"system\", \"content\": \"你是一个写作助手\"},\n        {\"role\": \"user\", \"content\": \"请帮忙写一篇描述江南春天的小作文\"}\n    ],\n    \"top_k\": 1,\n    \"max_tokens\": 256,\n    \"stream\": false}'\n```\nThen the server raised error after the inference finished:\n```\nINFO 01-17 09:51:07 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%\nINFO 01-17 09:51:09 async_llm_engine.py:120] Finished request cmpl-a6703cc7cb0140adaebbfdd9dbf1f1e5.\nINFO:     172.16.30.28:47694 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\nERROR:    Exception in ASGI application\nTraceback (most recent call last):\n  File \"/data/qingfu.zeng/vllm-0.4.2-venv/lib/python3.10/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 409, in run_asgi\n    result = await app(  # type: ignore[func-returns-value]\n  File \"/data/qingfu.zeng/vllm-0.4.2-venv/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n    return await self.app(scope, receive, send)\n  File \"/data/qingfu.zeng/vllm-0.4.2-venv/lib/python3.10/site-packages/fastapi/applications.py\", line 1054, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/data/qingfu.zeng/vllm-0.4.2-venv/lib/python3.10/site-packages/starlette/applications.py\", line 113, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/data/qingfu.zeng/vllm-0.4.2-venv/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 187, in __call__\n    raise exc\n  File \"/data/qingfu.zeng/vllm-0.4.2-venv/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    await self.app(scope, receive, _send)\n  File \"/data/qingfu.zeng/vllm-0.4.2-venv/lib/python3.10/site-packages/starlette/middleware/cors.py\", line 85, in __call__\n    await self.app(scope, receive, send)\n  File \"/data/qingfu.zeng/vllm-0.4.2-venv/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/data/qingfu.zeng/vllm-0.4.2-venv/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/data/qingfu.zeng/vllm-0.4.2-venv/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/data/qingfu.zeng/vllm-0.4.2-venv/lib/python3.10/site-packages/starlette/routing.py\", line 715, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/data/qingfu.zeng/vllm-0.4.2-venv/lib/python3.10/site-packages/starlette/routing.py\", line 735, in app\n    await route.handle(scope, receive, send)\n  File \"/data/qingfu.zeng/vllm-0.4.2-venv/lib/python3.10/site-packages/starlette/routing.py\", line 288, in handle\n    await self.app(scope, receive, send)\n  File \"/data/qingfu.zeng/vllm-0.4.2-venv/lib/python3.10/site-packages/starlette/routing.py\", line 76, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"/data/qingfu.zeng/vllm-0.4.2-venv/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/data/qingfu.zeng/vllm-0.4.2-venv/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/data/qingfu.zeng/vllm-0.4.2-venv/lib/python3.10/site-packages/starlette/routing.py\", line 73, in app\n    response = await f(request)\n  File \"/data/qingfu.zeng/vllm-0.4.2-venv/lib/python3.10/site-packages/fastapi/routing.py\", line 301, in app\n    raw_response = await run_endpoint_function(\n  File \"/data/qingfu.zeng/vllm-0.4.2-venv/lib/python3.10/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n    return await dependant.call(**values)\n  File \"/data/qingfu.zeng/vllm-0.4.2-venv/lib/python3.10/site-packages/ipex_llm/vllm/cpu/entrypoints/openai/api_server.py\", line 117, in create_chat_completion\n    invalidInputError(isinstance(generator, ChatCompletionResponse))\nTypeError: invalidInputError() missing 1 required positional argument: 'errMsg'\n```",
      "state": "open",
      "author": "zengqingfu1442",
      "author_type": "User",
      "created_at": "2025-01-20T09:13:50Z",
      "updated_at": "2025-03-31T02:18:10Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 33,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12723/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "xiangyuT"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12723",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12723",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:51.025634",
      "comments": []
    },
    {
      "issue_number": 12993,
      "title": "cpu memory increased same size with GPU when run ollama with ipex_llm? is this expected?",
      "body": "Hi, I find the CPU memory increased same size with GPU when run ollama with ipex_llm, is it expected?\nI've set: set SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1, \nset  OLLAMA_INTEL_GPU=9999\n\n\nversion:ollama-ipex-llm-2.2.0b20250313-win",
      "state": "closed",
      "author": "sunyq1995",
      "author_type": "User",
      "created_at": "2025-03-22T15:10:51Z",
      "updated_at": "2025-03-29T03:14:52Z",
      "closed_at": "2025-03-29T03:14:52Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12993/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12993",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12993",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:51.025657",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "Are you using a dGPU or iGPU?",
          "created_at": "2025-03-24T01:50:51Z"
        },
        {
          "author": "sunyq1995",
          "body": "> Are you using a dGPU or iGPU?\n\ndGPU, got it, thanks!!",
          "created_at": "2025-03-29T03:14:52Z"
        }
      ]
    },
    {
      "issue_number": 13017,
      "title": "B580 Unable to run model larger than GPU memory",
      "body": "**Describe the bug**\nI am able to run regular deepseek-r1 model via ollama\nWhen trying to run deepseek-r1:70b via ollama errors occur. \n\n**How to reproduce**\nSteps to reproduce the error:\n1. Docker deploy intelanalytics/ipex-llm-inference-cpp-xpu:latest\n2. ./ollama run deekseek-r1 \n3. Works fine\n4. ./ollama run deekseek-r1:70b errors occur\n\n**Screenshots**\nIf applicable, add screenshots to help explain the problem\n\n**Environment information**\n\nDocker Compose \n``` \nservices:\n  ipex-llm-inference-cpp-xpu-container:\n    stdin_open: true\n    tty: true\n    restart: always\n    devices:\n      - /dev/dri\n    volumes:\n      - ./ollama-intel:/models\n      - ./ai-models/ollama:/root/.ollama/models\n    environment:\n      - no_proxy=localhost,127.0.0.1\n      - OLLAMA_HOST=0.0.0.0:11434\n      - ZES_ENABLE_SYSMAN=1\n      - OLLAMA_NUM_GPU=1\n      - ONEAPI_DEVICE_SELECTOR=level_zero:*\n    ports:\n      - 11434:11434\n    container_name: ipex-llm-inference-cpp-xpu-container\n    image: intelanalytics/ipex-llm-inference-cpp-xpu:latest\n    command: bash -c \"cd /llm/scripts/ && bash start-ollama.sh && tail -f\n      /llm/ollama/ollama.log\"\n```\n\n```# ./env-check.sh \n-----------------------------------------------------------------\nPYTHON_VERSION=3.11.11\n-----------------------------------------------------------------\n/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\ntransformers=4.36.2\n-----------------------------------------------------------------\ntorch=2.2.0+cu121\n-----------------------------------------------------------------\nipex-llm Version: 2.2.0b20250326\n-----------------------------------------------------------------\nIPEX is not installed. \n-----------------------------------------------------------------\nCPU Information: \nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        43 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               24\nOn-line CPU(s) list:                  0-23\nVendor ID:                            AuthenticAMD\nModel name:                           AMD Ryzen Threadripper 1920X 12-Core Processor\nCPU family:                           23\nModel:                                1\nThread(s) per core:                   2\nCore(s) per socket:                   12\nSocket(s):                            1\nStepping:                             1\nFrequency boost:                      enabled\nCPU max MHz:                          3500.0000\nCPU min MHz:                          2200.0000\n-----------------------------------------------------------------\nTotal CPU Memory: 251.582 GB\nMemory Type: sudo: dmidecode: command not found\n-----------------------------------------------------------------\nOperating System: \nUbuntu 22.04.5 LTS \\n \\l\n\n-----------------------------------------------------------------\nLinux ff637cabf2d2 6.12.19-Unraid #1 SMP PREEMPT_DYNAMIC Fri Mar 14 11:56:04 PDT 2025 x86_64 x86_64 x86_64 GNU/Linux\n-----------------------------------------------------------------\n./env-check.sh: line 148: xpu-smi: command not found\n-----------------------------------------------------------------\n./env-check.sh: line 154: clinfo: command not found\n-----------------------------------------------------------------\nDriver related package version:\nii  intel-level-zero-gpu                             1.6.32224.5                             amd64        Intel(R) Graphics Compute Runtime for oneAPI Level Zero.\nii  intel-level-zero-gpu-legacy1                     1.3.30872.22                            amd64        Intel(R) Graphics Compute Runtime for oneAPI Level Zero.\nii  level-zero-devel                                 1.20.2                                  amd64        oneAPI Level Zero\n-----------------------------------------------------------------\nINFO: Output filtered by ONEAPI_DEVICE_SELECTOR environment variable, which is set to level_zero:*.\nTo see device ids, use the --ignore-device-selectors CLI option.\n\nigpu not detected\n-----------------------------------------------------------------\nxpu-smi is not installed. Please install xpu-smi according to README.md\n```\n\n**Additional context**\nCompose logs attached\n\n[compose logs.txt](https://github.com/user-attachments/files/19486008/compose.logs.txt)",
      "state": "open",
      "author": "Undeadhunter",
      "author_type": "User",
      "created_at": "2025-03-27T12:17:38Z",
      "updated_at": "2025-03-29T02:08:02Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13017/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13017",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13017",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:51.256485",
      "comments": [
        {
          "author": "duhmojo",
          "body": "The only model I can run at all with my b580 is deepseek r1 7b. Can’t even run Phi 1b. I don’t know if it’s possible to rebuild IPEX for ollama 0.6.4 but the latest container is version 0.5.3. ",
          "created_at": "2025-03-29T02:08:01Z"
        }
      ]
    },
    {
      "issue_number": 13012,
      "title": "vllm failure in intelanalytics/ipex-llm-serving-xpu:2.2.0-b13",
      "body": "**Describe the bug**\nUse the Open-webui connect to the vllm server, the vllm server will crash with the following information.\n`\"'OpenAIServingTokenization' object has no attribute 'show_available_models'\"`\n\n**How to reproduce**\nSteps to reproduce the error:\n1. docker pull the intelanalytics/ipex-llm-serving-xpu:2.2.0-b13\n2. start the vllm serving for Qwen 14B\n3. start Open-webui connect to the vllm server, through OpenAI API.\n\n\n**Screenshots**\n\n![Image](https://github.com/user-attachments/assets/c28ed8b5-7554-4f3f-821c-fff36620f647)\n\n\n**Environment information**\nThis issue only exisit on the docker image release 2.2.0-b13. docker image release 2.2.0-b11 don't have this issue\nintelanalytics/ipex-llm-serving-xpu   2.2.0-b13\n\n\n**Additional context**\nAdd any other context about the problem here.\n",
      "state": "closed",
      "author": "oldmikeyang",
      "author_type": "User",
      "created_at": "2025-03-26T09:39:16Z",
      "updated_at": "2025-03-28T08:46:45Z",
      "closed_at": "2025-03-28T08:46:45Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13012/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13012",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13012",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:51.443137",
      "comments": [
        {
          "author": "glorysdj",
          "body": "hi @oldmikeyang, please try 2.2.0-b15. This is fixed in b15.",
          "created_at": "2025-03-27T02:13:08Z"
        },
        {
          "author": "oldmikeyang",
          "body": "Yes. b16 release don't have this issue.",
          "created_at": "2025-03-28T08:46:45Z"
        }
      ]
    },
    {
      "issue_number": 12874,
      "title": "ipex-llm will run slower and slower",
      "body": "SYSTEM: u265K(no igpu)+b580\nQUESTION:  ollama run qwen:7b for translate web ,After translating 300-500 sentences, the translation speed will gradually slow down, and the GPU utilization rate will drop from 90% to below 10%. Restarting ollama serve will speed it up again, but the slowdown will eventually occur once more.The same situation may also occur in chat services; after ipexllm runs for a while, the GPU utilization rate will become slower.",
      "state": "open",
      "author": "dttprofessor",
      "author_type": "User",
      "created_at": "2025-02-23T10:02:49Z",
      "updated_at": "2025-03-28T06:06:17Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12874/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12874",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12874",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:51.652221",
      "comments": [
        {
          "author": "dttprofessor",
          "body": " [Ollama Portable Zip](https://github.com/intel/ipex-llm/releases/tag/v2.2.0-nightly）\n@windows 11",
          "created_at": "2025-02-23T13:28:52Z"
        },
        {
          "author": "qiuxin2012",
          "body": "Similiar issue: https://github.com/intel/ipex-llm/issues/12852",
          "created_at": "2025-02-24T00:51:33Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @dttprofessor,  I can't reproduce your issue. Could you provide the ollama serve log, also the GPU performance and driver version from the Task Manager when the performance issue occurs?",
          "created_at": "2025-02-25T02:12:12Z"
        },
        {
          "author": "dttprofessor",
          "body": "B580 driver： 32.0.101.6559\n\nThis is a web-based translation case. After opening ollama for web translation and translating 300-500 sentences, the GPU utilization rate will continuously decrease and remain around 10% later on, unless ollama is restarted or the model is automatically flushed from the ",
          "created_at": "2025-02-25T03:34:17Z"
        },
        {
          "author": "Anonymous-Lurker",
          "body": "I'm having a similar or same issue.  Token generation slows  by sometimes more than 1/2.",
          "created_at": "2025-03-28T06:06:16Z"
        }
      ]
    },
    {
      "issue_number": 12852,
      "title": "2 x A770 with Ollama Linux , inference responses slow down dramatically",
      "body": "Hi,\nI use the latest cpp docker and the latest i915 dkms driver on Linux, inference with 2x A770 and Ollama, the model is Deepseek 32B int4. However, after 2 or 3 times we get 15~20 tokens, the rest of time we only get 6-8 tokens/s, also, the speed is very strange, it looks quite fast at the beginning, then drops dramatically after like 50-100 tokens, and sometimes it speeds up after 10-20 seconds..\n\nAlso, the gap between ollama and vllm is 5x times on the same machine, which is typically a bug performance.\nBR ",
      "state": "open",
      "author": "RobinJing",
      "author_type": "User",
      "created_at": "2025-02-19T07:04:40Z",
      "updated_at": "2025-03-28T02:50:22Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12852/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12852",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12852",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:51.940655",
      "comments": [
        {
          "author": "Anonymous-Lurker",
          "body": "same issue here, token generation on b580 starts fast, slows by about 50% at the end of token generation.",
          "created_at": "2025-03-28T02:50:21Z"
        }
      ]
    },
    {
      "issue_number": 13016,
      "title": "llama.cpp portable gemma3 sample - getting low GPU usage",
      "body": "Hi ,\nI am using portable llama.cpp on MTL 165H platform with iGPU having 2.3GHZ freq.\nI downloaded gemma3 gguf and tried running the inference using below command \n\n./llama-cli -m $model_path --no-context-shift -n 32 --prompt \"What is AI?\" -t 8 -e -ngl 50 --color -c 2048 --temp 0\n\noutput of gpu usage is: I see less GPU usage and also GPU freq is going only until 1.1GHZ \n\n![Image](https://github.com/user-attachments/assets/0ceab4c2-93b1-4a1b-ac6b-337e85fec625)\n",
      "state": "open",
      "author": "Mushtaq-BGA",
      "author_type": "User",
      "created_at": "2025-03-27T09:12:08Z",
      "updated_at": "2025-03-28T02:46:09Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13016/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13016",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13016",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:52.136510",
      "comments": [
        {
          "author": "rnwang04",
          "body": "Hi @Mushtaq-BGA , which gemm3 gguf are you used ? ",
          "created_at": "2025-03-28T02:46:08Z"
        }
      ]
    },
    {
      "issue_number": 12999,
      "title": "Access to Ollama from outside docker with no net:host",
      "body": "**Describe the bug**\nWant to make the ollama available from outside docker, without the use of  --net=host\n\n**How to reproduce**\nSteps to reproduce the error:\n1. Deploy docker as normal but replace \n --net=host\nwith\n-p 11434:11434\n\n\nWhen trying to do following from outside the docker\n\ncurl http://serverIP:11434/api/generate -d '\n{ \n   \"model\": \"deepseek-r1\", \n   \"prompt\": \"What is AI?\", \n   \"stream\": false\n}'\n\nOutcome:\ncurl: (7) Failed to connect to ServerIP port 11434 after 0 ms: Couldn't connect to server\n",
      "state": "closed",
      "author": "Undeadhunter",
      "author_type": "User",
      "created_at": "2025-03-24T13:20:41Z",
      "updated_at": "2025-03-27T11:56:34Z",
      "closed_at": "2025-03-27T11:56:22Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12999/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12999",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12999",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:52.314447",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "Can you share your ollama's log? It looks like ollama is inactived.",
          "created_at": "2025-03-25T02:20:16Z"
        },
        {
          "author": "dempsey-wen",
          "body": "maybe you can try to set the Environment of the container: \n`OLLAMA_HOST=0.0.0.0:11434`",
          "created_at": "2025-03-25T05:13:04Z"
        },
        {
          "author": "Undeadhunter",
          "body": " OLLAMA_HOST=0.0.0.0:11434\n\ndid the trick ",
          "created_at": "2025-03-27T11:56:33Z"
        }
      ]
    },
    {
      "issue_number": 13014,
      "title": "When using the NPU inference model, when the Prompt length exceeds a certain range, the model reports an error OSError",
      "body": "**Describe the bug**\nI developed a simple chatBot by myself. There are historical chat records in the chatBot (storing five rounds). When using the NPU inference model, when the length of the Prompt exceeds a certain range (about 700 tokens), the model reports an error OSError: exception: access violation writing 0x0000018F36FB7000.\n\nWhen using the Demo you provided for round-by-round conversations, there will be no errors. However, when adding the historical conversation function and making the length of the prompt larger, an error will be reported.\n\nI researched that the error may be a memory access exception (Access Violation) in the underlying library of the ipex framework during the generation stage, but when reasoning, the memory on my side is about 15~ 20G, and it is not full. Could you please help me see what is the reason? Optimize this input token prompt limit problem.\n\nThe model used is Qwen/Qwen 2.5-7B-Instruct, and the reference ipex-llm link is: https://github.com/intel/ipex-llm/tree/main/python/llm/example/NPU/HF-Transformers-AutoModels/LLM\n\n\nLooking forward to your reply, thank you very much!\n\n**Screenshots**\n\n![Image](https://github.com/user-attachments/assets/d3c3adc7-9a98-4f91-a6b6-46f02c024b34)\n\nError：\nThread Thread-14 (text_conversation_run) released the instance. Waiting threads: 2\nuser stop start\n2025-03-27 09:30:56,096 - INFO - 127.0.0.1 - - [27/Mar/2025 09:30:56] \"POST /LLM/chat HTTP/1.1\" 200 -\nuser stop finish. will clear temp resource!\nException in thread Thread-18 (stream_chat_generate):\nTraceback (most recent call last):\n  File \"C:\\Users\\Public\\App\\AIWorkbenchModel\\service\\env\\Deepseek_NPU\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n    self.run()\n  File \"C:\\Users\\Public\\App\\AIWorkbenchModel\\service\\env\\Deepseek_NPU\\Lib\\threading.py\", line 982, in run\n    self._target(*self._args, **self._kwargs)\n  File \"C:\\Users\\Public\\App\\AIWorkbenchModel\\service\\app\\services\\aigc\\llm_wrapper.py\", line 379, in stream_chat_generate\n    model.generate(\n  File \"C:\\Users\\Public\\App\\AIWorkbenchModel\\service\\env\\Deepseek_NPU\\Lib\\site-packages\\ipex_llm\\transformers\\npu_models\\convert.py\", line 338, in generate\n    return simple_generate(self, inputs=inputs, streamer=streamer, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Public\\App\\AIWorkbenchModel\\service\\env\\Deepseek_NPU\\Lib\\site-packages\\ipex_llm\\transformers\\npu_models\\convert.py\", line 404, in simple_generate\n    token = run_prefill(self.model_ptr, input_list, self.vocab_size,\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Public\\App\\AIWorkbenchModel\\service\\env\\Deepseek_NPU\\Lib\\site-packages\\ipex_llm\\transformers\\npu_models\\npu_llm_cpp.py\", line 82, in run_prefill\n    plogits = _lib.run_prefill(model_ptr, input_ptr, input_len, repetition_penalty, False)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nOSError: exception: access violation writing 0x00000261EEF15000\n\n\n\n**Environment information**\n\n![Image](https://github.com/user-attachments/assets/933ea1fd-b090-47ac-8fc1-62a62d675102)\n\n![Image](https://github.com/user-attachments/assets/38dbb863-9178-4f99-a154-fe4d4b0f68ed)\n\n\n**Additional context**\nAdd any other context about the problem here.\n",
      "state": "open",
      "author": "Bofuser",
      "author_type": "User",
      "created_at": "2025-03-27T01:38:37Z",
      "updated_at": "2025-03-27T01:42:52Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13014/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13014",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13014",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:52.535684",
      "comments": []
    },
    {
      "issue_number": 13003,
      "title": "ipex-llm vllm not support glm-edge-4b-chat model",
      "body": "**Describe the bug**\nUsing intelanalytics/ipex-llm-serving-xpu:2.2.0-b15 docker container, we started the vllm inference following the README in python/llm/example/GPU/vLLM-Serving. I encountered the following error:\n\n```shell\n****************************Usage Error************************\nCurrently, ipex-vllm does not support linear layers with skip_bias_add argument\n2025-03-24 21:45:11,347 - ERROR - \n```\n**How to reproduce**\nSteps to reproduce the error:\n1. vllm docker image: intelanalytics/ipex-llm-serving-xpu:2.2.0-b15\n2. huggingface model: THUDM/glm-edge-4b-chat\n3. start inference like this: https://github.com/intel/ipex-llm/tree/main/python/llm/example/GPU/vLLM-Serving#service\n\n**Screenshots**\n<details>\n```shell\nroot@vllm:/home/vllm# bash /usr/bin/vllm-server.sh \n \n:: WARNING: setvars.sh has already been run. Skipping re-execution.\n   To force a re-execution of setvars.sh, use the '--force' option.\n   Using '--force' can result in excessive use of your environment variables.\n  \nusage: source setvars.sh [--force] [--config=file] [--help] [...]\n  --force        Force setvars.sh to re-run, doing so may overload environment.\n  --config=file  Customize env vars using a setvars.sh configuration file.\n  --help         Display this help message and exit.\n  ...            Additional args are passed to individual env/vars.sh scripts\n                 and should follow this script's arguments.\n  \n  Some POSIX shells do not accept command-line options. In that case, you can pass\n  command-line options via the SETVARS_ARGS environment variable. For example:\n  \n  $ SETVARS_ARGS=\"--config=config.txt\" ; export SETVARS_ARGS\n  $ . path/to/setvars.sh\n  \n  The SETVARS_ARGS environment variable is cleared on exiting setvars.sh.\n  \nThe oneAPI toolkits no longer support 32-bit libraries, starting with the 2025.0 toolkit release. See the oneAPI release notes for more details.\n  \nINFO 03-24 21:44:58 __init__.py:180] Automatically detected platform xpu.\nWARNING 03-24 21:44:59 api_server.py:893] Warning: Please use `ipex_llm.vllm.xpu.entrypoints.openai.api_server` instead of `vllm.entrypoints.openai.api_server` to start the API server\nINFO 03-24 21:44:59 api_server.py:837] vLLM API server version 0.6.6+ipexllm\nINFO 03-24 21:44:59 api_server.py:838] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/vllm/AI-models/LLM/hf-model/glm-edge-4b-chat', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='float16', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=4096, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=8, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.75, num_gpu_blocks_override=None, max_num_batched_tokens=10240, max_num_seqs=12, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=True, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='xpu', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['glm-edge-4b-chat'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=True, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, low_bit_model_path=None, low_bit_save_path=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, load_in_low_bit='fp16')\nWARNING 03-24 21:44:59 utils.py:1920] Found ulimit of 32768 and failed to automatically increasewith error current limit exceeds maximum limit. This can cause fd limit errors like`OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n\nINFO 03-24 21:44:59 api_server.py:197] Started engine process with PID 531\nWARNING 03-24 21:44:59 config.py:2289] Casting torch.bfloat16 to torch.float16.\nINFO 03-24 21:45:03 __init__.py:180] Automatically detected platform xpu.\nWARNING 03-24 21:45:04 config.py:2289] Casting torch.bfloat16 to torch.float16.\nINFO 03-24 21:45:04 config.py:521] This model supports multiple tasks: {'generate', 'reward', 'embed', 'score', 'classify'}. Defaulting to 'generate'.\nINFO 03-24 21:45:09 config.py:521] This model supports multiple tasks: {'reward', 'embed', 'classify', 'generate', 'score'}. Defaulting to 'generate'.\nINFO 03-24 21:45:09 llm_engine.py:234] Initializing an LLM engine (v0.6.6+ipexllm) with config: model='/home/vllm/AI-models/LLM/hf-model/glm-edge-4b-chat', speculative_config=None, tokenizer='/home/vllm/AI-models/LLM/hf-model/glm-edge-4b-chat', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=xpu, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=glm-edge-4b-chat, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=True, \nINFO 03-24 21:45:09 xpu.py:27] Cannot use _Backend.FLASH_ATTN backend on XPU.\nINFO 03-24 21:45:09 selector.py:155] Using IPEX attention backend.\nWARNING 03-24 21:45:09 _ipex_ops.py:12] Import error msg: No module named 'intel_extension_for_pytorch'\nINFO 03-24 21:45:09 importing.py:14] Triton not installed or not compatible; certain GPU-related functions will not be available.\nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.09it/s]\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.21it/s]\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.19it/s]\n\n2025-03-24 21:45:11,070 - INFO - Converting the current model to fp16 format......\n2025-03-24 21:45:11,071 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\n2025-03-24 21:45:11,347 - ERROR - \n\n****************************Usage Error************************\nCurrently, ipex-vllm does not support linear layers with skip_bias_add argument\n2025-03-24 21:45:11,347 - ERROR - \n\n****************************Call Stack*************************\n2025-03-24 21:45:11,491 - ERROR - Currently, ipex-vllm does not support linear layers with skip_bias_add argument\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 234, in run_mp_engine\n    engine = IPEXLLMMQLLMEngine.from_engine_args(engine_args=engine_args,\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 221, in from_engine_args\n    return super().from_engine_args(engine_args, usage_context, ipc_path)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/multiprocessing/engine.py\", line 119, in from_engine_args\n    return cls(ipc_path=ipc_path,\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/multiprocessing/engine.py\", line 71, in __init__\n    self.engine = LLMEngine(*args, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py\", line 273, in __init__\n    self.model_executor = executor_class(vllm_config=vllm_config, )\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/executor_base.py\", line 36, in __init__\n    self._init_executor()\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/xpu_executor.py\", line 22, in _init_executor\n    GPUExecutor._init_executor(self)\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/gpu_executor.py\", line 35, in _init_executor\n    self.driver_worker.load_model()\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/worker.py\", line 155, in load_model\n    self.model_runner.load_model()\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/model_convert.py\", line 122, in _ipex_llm_load_model\n    optimize_model(self.model,\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/optimize.py\", line 254, in optimize_model\n    model = ggml_convert_low_bit(model,\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/transformers/convert.py\", line 1123, in ggml_convert_low_bit\n    model, has_been_replaced = _replace_with_low_bit_linear(\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/transformers/convert.py\", line 732, in _replace_with_low_bit_linear\n    _, _flag = _replace_with_low_bit_linear(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/transformers/convert.py\", line 732, in _replace_with_low_bit_linear\n    _, _flag = _replace_with_low_bit_linear(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/transformers/convert.py\", line 732, in _replace_with_low_bit_linear\n    _, _flag = _replace_with_low_bit_linear(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  [Previous line repeated 1 more time]\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/transformers/convert.py\", line 486, in _replace_with_low_bit_linear\n    is_linear, linear_args = is_linear_module(module)\n                             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/transformers/convert.py\", line 195, in is_linear_module\n    invalidInputError(module.skip_bias_add is not True, \"Currently, ipex-vllm does not\"\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/utils/common/log4Error.py\", line 32, in invalidInputError\n    raise RuntimeError(errMsg)\nRuntimeError: Currently, ipex-vllm does not support linear layers with skip_bias_add argument\nProcess SpawnProcess-1:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 242, in run_mp_engine\n    raise e  # noqa\n    ^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 234, in run_mp_engine\n    engine = IPEXLLMMQLLMEngine.from_engine_args(engine_args=engine_args,\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 221, in from_engine_args\n    return super().from_engine_args(engine_args, usage_context, ipc_path)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/multiprocessing/engine.py\", line 119, in from_engine_args\n    return cls(ipc_path=ipc_path,\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/multiprocessing/engine.py\", line 71, in __init__\n    self.engine = LLMEngine(*args, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py\", line 273, in __init__\n    self.model_executor = executor_class(vllm_config=vllm_config, )\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/executor_base.py\", line 36, in __init__\n    self._init_executor()\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/xpu_executor.py\", line 22, in _init_executor\n    GPUExecutor._init_executor(self)\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/gpu_executor.py\", line 35, in _init_executor\n    self.driver_worker.load_model()\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/worker.py\", line 155, in load_model\n    self.model_runner.load_model()\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/model_convert.py\", line 122, in _ipex_llm_load_model\n    optimize_model(self.model,\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/optimize.py\", line 254, in optimize_model\n    model = ggml_convert_low_bit(model,\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/transformers/convert.py\", line 1123, in ggml_convert_low_bit\n    model, has_been_replaced = _replace_with_low_bit_linear(\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/transformers/convert.py\", line 732, in _replace_with_low_bit_linear\n    _, _flag = _replace_with_low_bit_linear(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/transformers/convert.py\", line 732, in _replace_with_low_bit_linear\n    _, _flag = _replace_with_low_bit_linear(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/transformers/convert.py\", line 732, in _replace_with_low_bit_linear\n    _, _flag = _replace_with_low_bit_linear(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  [Previous line repeated 1 more time]\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/transformers/convert.py\", line 486, in _replace_with_low_bit_linear\n    is_linear, linear_args = is_linear_module(module)\n                             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/transformers/convert.py\", line 195, in is_linear_module\n    invalidInputError(module.skip_bias_add is not True, \"Currently, ipex-vllm does not\"\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/utils/common/log4Error.py\", line 32, in invalidInputError\n    raise RuntimeError(errMsg)\nRuntimeError: Currently, ipex-vllm does not support linear layers with skip_bias_add argument\n^CTraceback (most recent call last):\n  File \"/usr/lib/python3.11/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/usr/local/lib/python3.11/dist-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n           ^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/entrypoints/openai/api_server.py\", line 865, in run_server\n    async with build_async_engine_client(args) as engine_client:\n  File \"/usr/lib/python3.11/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/entrypoints/openai/api_server.py\", line 123, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n  File \"/usr/lib/python3.11/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/entrypoints/openai/api_server.py\", line 216, in build_async_engine_client_from_engine_args\n    await mq_engine_client.setup()\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/multiprocessing/client.py\", line 262, in setup\n    response = await self._wait_for_server_rpc(socket)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/multiprocessing/client.py\", line 369, in _wait_for_server_rpc\n    return await self._send_get_data_rpc_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/multiprocessing/client.py\", line 297, in _send_get_data_rpc_request\n    if await socket.poll(timeout=VLLM_RPC_TIMEOUT) == 0:\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nasyncio.exceptions.CancelledError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/entrypoints/openai/api_server.py\", line 906, in <module>\n    uvloop.run(run_server(args))\n  File \"/usr/local/lib/python3.11/dist-packages/uvloop/__init__.py\", line 105, in run\n    return runner.run(wrapper())\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/asyncio/runners.py\", line 123, in run\n    raise KeyboardInterrupt()\nKeyboardInterrupt\n^C\n```\n```\n</details>\n\n**Environment information**\ndocker image: intelanalytics/ipex-llm-serving-xpu:2.2.0-b15\n\n**Additional context**\n",
      "state": "closed",
      "author": "junruizh2021",
      "author_type": "User",
      "created_at": "2025-03-25T02:23:38Z",
      "updated_at": "2025-03-26T14:19:09Z",
      "closed_at": "2025-03-26T14:19:09Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13003/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "gc-fu"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13003",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13003",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:52.535707",
      "comments": [
        {
          "author": "gc-fu",
          "body": "Hi, I will investigate this issue.  Once we have found out the root-cause, we will get back to you.",
          "created_at": "2025-03-25T02:27:38Z"
        },
        {
          "author": "gc-fu",
          "body": "Hi, this should have been fixed by https://github.com/intel/ipex-llm/pull/13007",
          "created_at": "2025-03-26T01:29:31Z"
        }
      ]
    },
    {
      "issue_number": 13009,
      "title": "llama_server.exe  build  by llama.cpp d7cfe1f   crashed  when  using  ipex-llm   to  improve performance.",
      "body": "### enviroment\nos： win11   ， cpu： ultra7-155H  ，Intel(R) oneAPI DPC++/C++ Compiler 2025.0.4 (2025.0.4.20241205)\n\n###  Reproduce bug\n1.  clone llama.cpp  & checkout d7cfe1f   ，build  llama-server.exe  using DPC++ Compiler \n2.  copy  *.dll  to   llama.cpp/build/bin  overwirte  origin dll \n3.  start  llama_server  and crashed when load model\n\n",
      "state": "open",
      "author": "cjsdurj",
      "author_type": "User",
      "created_at": "2025-03-26T03:11:28Z",
      "updated_at": "2025-03-26T11:19:24Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13009/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "plusbang"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13009",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13009",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:57.788977",
      "comments": [
        {
          "author": "plusbang",
          "body": "Hi, we provide `llama_server.exe` in our nightly package, you could directly use it following our guide (https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/llama_cpp_quickstart.md).",
          "created_at": "2025-03-26T06:52:37Z"
        },
        {
          "author": "cjsdurj",
          "body": "in my use case:   I had added  openai  style  video & image  chat  api  using  vl   and  some  other  code  in  llama_server .  so  build from source code and  replace  ggml.dll & llama.dll   with  dlls in ipex-llm package  is useful. ",
          "created_at": "2025-03-26T11:19:23Z"
        }
      ]
    },
    {
      "issue_number": 12965,
      "title": "Failed to register worker to Raylet: IOError: [RayletClient]",
      "body": "## Test env\n\n```\nroot@arc-a770:~# lsb_release -a\nNo LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 22.04.5 LTS\nRelease:        22.04\nCodename:       jammy\n\nroot@arc-a770:~# uname -a\nLinux arc-a770 6.5.0-35-generic #35~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue May  7 09:00:52 UTC 2 x86_64 x86_64 x86_64 GNU/Linux\n\nroot@arc-a770:~# lshw -c display -businfo\nBus info          Device           Class          Description\n=============================================================\npci@0000:02:00.0  /dev/fb0         display        ASPEED Graphics Family\npci@0000:aa:00.0                   display        Intel Corporation\n\nairren@arc-a770:~$ xpu-smi discovery\n+-----------+--------------------------------------------------------------------------------------+\n| Device ID | Device Information                                                                   |\n+-----------+--------------------------------------------------------------------------------------+\n| 0         | Device Name: Intel(R) Arc(TM) A770 Graphics                                          |\n|           | Vendor Name: Intel(R) Corporation                                                    |\n|           | SOC UUID: 00000000-0000-00aa-0000-000856a08086                                       |\n|           | PCI BDF Address: 0000:aa:00.0                                                        |\n|           | DRM Device: /dev/dri/card1                                                           |\n|           | Function Type: physical                                                              |\n+-----------+--------------------------------------------------------------------------------------+\n\nairren@arc-a770:~$ apt list --installed |grep 915\n\nWARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n\nintel-i915-dkms/unknown,now 1.23.10.83.231129.91+i127-1 all [installed]\n```\n\n\n## Docker image and start script\n\n### create a container\n\n```sh\nexport DOCKER_IMAGE=intelanalytics/ipex-llm-serving-xpu:2.2.0-b14\nexport CONTAINER_NAME=ipex-llm-serving-xpu-container\n\nsudo docker run -itd \\\n        --net=host \\\n        --group-add video \\\n        --device=/dev/dri \\\n        -v /home/airren/models:/llm/models \\\n        -e no_proxy=localhost,127.0.0.1 \\\n        --memory=\"32G\" \\\n        --name=$CONTAINER_NAME \\\n        --shm-size=\"16g\" \\\n        --entrypoint /bin/bash \\\n        $DOCKER_IMAGE\n\n\nroot@arc-a770:~# docker ps\nCONTAINER ID  IMAGE                                                    COMMAND     CREATED        STATUS            PORTS       NAMES\ncef86b0a7dd0  docker.io/intelanalytics/ipex-llm-serving-xpu:2.2.0-b14              4 minutes ago  Up 4 minutes ago              ipex-llm-serving-xpu-container\n\nroot@arc-a770:~# docker exec -it  ipex-llm-serving-xpu-container bash\nroot@arc-a770:/llm# sycl-ls\n[level_zero:gpu][level_zero:0] Intel(R) oneAPI Unified Runtime over Level-Zero, Intel(R) Arc(TM) A770 Graphics 12.55.8 [1.6.32224.500000]\n[opencl:cpu][opencl:0] Intel(R) OpenCL, Intel(R) Xeon(R) Platinum 8480+ OpenCL 3.0 (Build 0) [2024.18.12.0.05_160000]\n[opencl:gpu][opencl:1] Intel(R) OpenCL Graphics, Intel(R) Arc(TM) A770 Graphics OpenCL 3.0 NEO  [24.52.32224.5]\n\n```\n\n### Start service \n\n```\nroot@arc-a770:/llm# ls models/deepseek-ai/\nDeepSeek-R1-Distill-Qwen-1.5B  DeepSeek-R1-Distill-Qwen-1___5B  DeepSeek-R1-Distill-Qwen-7B\n\nroot@arc-a770:/llm# export MODEL_PATH=\"/llm/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\nroot@arc-a770:/llm# export SERVED_MODEL_NAME=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n\nroot@arc-a770:/llm# bash start-vllm-service.sh\nStarting service with model: /llm/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\n```\n\n### Crash log\n\n```\nroot@arc-a770:/llm# bash start-vllm-service.sh\nStarting service with model: /llm/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\nServed model name: deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\nTensor parallel size: 1\nINFO 03-13 11:18:34 __init__.py:180] Automatically detected platform xpu.\nWARNING 03-13 11:18:35 api_server.py:849] Warning: Please use `ipex_llm.vllm.xpu.entrypoints.openai.api_server` instead of `vllm.entrypoints.openai.api_server` to start the API server\nINFO 03-13 11:18:35 api_server.py:793] vLLM API server version 0.6.6+ipexllm\nINFO 03-13 11:18:35 api_server.py:794] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/llm/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='float16', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=2000, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend='ray', worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=8, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.95, num_gpu_blocks_override=None, max_num_batched_tokens=3000, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=True, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='xpu', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['deepseek-ai/DeepSeek-R1-Distill-Qwen-7B'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=True, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, low_bit_model_path=None, low_bit_save_path=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, load_in_low_bit='fp8')\nINFO 03-13 11:18:35 api_server.py:197] Started engine process with PID 162\nWARNING 03-13 11:18:35 config.py:2289] Casting torch.bfloat16 to torch.float16.\n^[[AINFO 03-13 11:18:37 __init__.py:180] Automatically detected platform xpu.\nWARNING 03-13 11:18:39 config.py:2289] Casting torch.bfloat16 to torch.float16.\nINFO 03-13 11:18:39 config.py:521] This model supports multiple tasks: {'embed', 'score', 'classify', 'reward', 'generate'}. Defaulting to 'generate'.\nINFO 03-13 11:18:42 config.py:521] This model supports multiple tasks: {'embed', 'generate', 'score', 'classify', 'reward'}. Defaulting to 'generate'.\nWARNING 03-13 11:18:42 ray_utils.py:239] No existing RAY instance detected. A new instance will be launched with current node resources.\n2025-03-13 11:18:44,018 INFO worker.py:1841 -- Started a local Ray instance.\n[2025-03-13 11:18:45,133 E 162 162] core_worker.cc:496: Failed to register worker to Raylet: IOError: [RayletClient] Unable to register worker with raylet. Failed to read data from the socket: End of file worker_id=01000000ffffffffffffffffffffffffffffffffffffffffffffffff\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/entrypoints/openai/api_server.py\", line 862, in <module>\n    uvloop.run(run_server(args))\n  File \"/usr/local/lib/python3.11/dist-packages/uvloop/__init__.py\", line 105, in run\n    return runner.run(wrapper())\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/usr/local/lib/python3.11/dist-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n           ^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/entrypoints/openai/api_server.py\", line 821, in run_server\n    async with build_async_engine_client(args) as engine_client:\n  File \"/usr/lib/python3.11/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/entrypoints/openai/api_server.py\", line 123, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n  File \"/usr/lib/python3.11/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/entrypoints/openai/api_server.py\", line 221, in build_async_engine_client_from_engine_args\n    raise RuntimeError(\nRuntimeError: Engine process failed to start. See stack trace for the root cause.\n```",
      "state": "closed",
      "author": "Airren",
      "author_type": "User",
      "created_at": "2025-03-13T03:17:29Z",
      "updated_at": "2025-03-26T09:13:54Z",
      "closed_at": "2025-03-26T09:13:54Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12965/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hzjane"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12965",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12965",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:58.025571",
      "comments": [
        {
          "author": "Airren",
          "body": "In this environment, we only have one Arc A770 GPU, if I delete this line ` --distributed-executor-backend ray `, the vllm service can work normally.",
          "created_at": "2025-03-13T08:58:29Z"
        },
        {
          "author": "hzjane",
          "body": "We cannot reproduce your problem in a 1-ARC or 4-ARC environment. This issue may be caused by a network problem. You can continue to remove ray backend and run.",
          "created_at": "2025-03-17T02:04:34Z"
        }
      ]
    },
    {
      "issue_number": 10847,
      "title": "Running 2 x A770 with Ollama, inference responses slow down dramatically",
      "body": "System specs: 2 x A770, one on PCIE 3.0 x16, one on PCIE 3.0 x4, Ryzen 3600, 64GB RAM (EDIT: Host is Ubuntu 22.04, Docker base image is `intelanalytics/ipex-llm-xpu:latest`).\r\n\r\nWhen running Ollama in Docker, using the setup described here:\r\n\r\nhttps://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/ollama_quickstart.html\r\n\r\n...it loads large models just fine (eg Codellama 34b, with approx. 10GB on each GPU), but the response generation starts around 30t/s and gradually slows down to 5t/s within the same response.\r\n\r\nAt the same time, the GPU frequency slows from 1Ghz+ down to 100-300MHz.\r\n\r\nThe GPUs aren't power-starved, and they're running in open air (so no cooling problems, ambient temp is ~18C).\r\n\r\nWorth noting that I'm running with `source ipex-llm-init -c -g` in the container startup and `--no-mmap` on llama.cpp (it segfaults without the former, and hangs or outright crashes without the latter).\r\n\r\nI know it's not ideal running the second GPU on PCIE x4, but I'd have thought that'd cause general slowness rather than a gradual slowdown.\r\n\r\nIt shows the exact same behaviour with smaller models, too - eg LLama 3 8b and Mistral 7b. With those smaller models, running on a single GPU returns 50-60t/s.",
      "state": "closed",
      "author": "digitalscream",
      "author_type": "User",
      "created_at": "2024-04-22T13:36:47Z",
      "updated_at": "2025-03-26T07:20:47Z",
      "closed_at": "2024-05-01T13:47:39Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 30,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/10847/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/10847",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/10847",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:58.273331",
      "comments": []
    },
    {
      "issue_number": 13004,
      "title": "Failed to run Qwen2-vl with ipex-llm + a760",
      "body": "**Describe the bug**\nFollowing the wiki:\nhttps://github.com/intel/ipex-llm/tree/main/python/llm/example/GPU/HuggingFace/Multimodal/qwen2-vl\n\nAnd command as below:\n**python3 ./generate.py --repo-id-or-model-path /Qwen2-VL-2B/ --prompt \"how are you?\" --n-predict 128 --modelscope --image-url-or-path /home/llm/sample.jpg**\n\nBut meet below error:\n(myenv) root@intel-rpl-p-poc:/home/llm# python3 ./generate.py --repo-id-or-model-path /Qwen2-VL-2B/ --prompt \"how are you?\" --n-predict 128 --modelscope --image-url-or-path /home/llm/sample.jpg\n/home/llm/myenv/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n  warn(\n2025-03-25 02:59:10,581 - INFO - intel_extension_for_pytorch auto imported\n2025-03-25 02:59:10,636 - INFO - set VIDEO_TOTAL_PIXELS: 90316800\nThe argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\nUnrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\nLoading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 11.15it/s]\n2025-03-25 02:59:10,988 - INFO - Converting the current model to sym_int4 format......\n/home/llm/myenv/lib/python3.11/site-packages/torch/nn/init.py:412: UserWarning: Initializing zero-element tensors is a no-op\n  warnings.warn(\"Initializing zero-element tensors is a no-op\")\nTraceback (most recent call last):\n  File \"/home/llm/./generate.py\", line 102, in <module>\n    generated_ids = model.generate(\n                    ^^^^^^^^^^^^^^^\n  File \"/home/llm/myenv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/myenv/lib/python3.11/site-packages/ipex_llm/transformers/lookup.py\", line 125, in generate\n    return original_generate(self,\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/myenv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/myenv/lib/python3.11/site-packages/ipex_llm/transformers/speculative.py\", line 127, in generate\n    return original_generate(self,\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/myenv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/myenv/lib/python3.11/site-packages/ipex_llm/transformers/pipeline_parallel.py\", line 283, in generate\n    return original_generate(self,\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/myenv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/myenv/lib/python3.11/site-packages/transformers/generation/utils.py\", line 2048, in generate\n    result = self._sample(\n             ^^^^^^^^^^^^^\n  File \"/home/llm/myenv/lib/python3.11/site-packages/transformers/generation/utils.py\", line 3001, in _sample\n    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/myenv/lib/python3.11/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py\", line 1774, in prepare_inputs_for_generation\n    if cache_position is None or (cache_position is not None and cache_position[0] == 0):\n                                                                 ~~~~~~~~~~~~~~^^^\nIndexError: index 0 is out of bounds for dimension 0 with size 0\n\n\n**How to reproduce**\nAlmost following the wiki, but i'm in a docker env.\nhttps://github.com/intel/ipex-llm/tree/main/python/llm/example/GPU/HuggingFace/Multimodal/qwen2-vl\n\n",
      "state": "open",
      "author": "Zhiwei-Lii",
      "author_type": "User",
      "created_at": "2025-03-25T02:49:20Z",
      "updated_at": "2025-03-25T03:13:53Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/13004/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/13004",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/13004",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:58.273353",
      "comments": [
        {
          "author": "ATMxsp01",
          "body": "Hello, we are trying to reproduce your issue. Could you please share us your docker env's information? It will help us to reproduce your problem.",
          "created_at": "2025-03-25T03:13:52Z"
        }
      ]
    },
    {
      "issue_number": 12845,
      "title": "docker: a770: ollama crashes",
      "body": "After updating to the latest `intelanalytics/ipex-llm-inference-cpp-xpu` (intelanalytics/ipex-llm-inference-cpp-xpu@sha256:21f970942b9621790807a869e5661f3b0df50865d9c07db870acca7fb3cf7539), any model I try to run crashes with similar error.\n\nAn image before the oneAPI upgrade worked with llama3.2, llava and qwen2.5-coder.\n\nOS: Ubuntu 24.04.1\nKernel: 6.8.0-52-generic\nHw: Arc A770 16GB\n\nThe log is from qwen2.5-coder, but the same happens with llama3.2 and llava at least.\n\nError:\n```\nroot@dgpu-test:/llm/ollama# ./ollama run qwen2.5-coder\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\nggml_sycl_init: SYCL_USE_XMX: yes\nggml_sycl_init: found 1 SYCL devices:\npulling manifest\npulling 60e05f210007... 100% ▕███████████████████████████████████████▏ 4.7 GB\npulling 66b9ea09bd5b... 100% ▕███████████████████████████████████████▏   68 B\npulling e94a8ecb9327... 100% ▕███████████████████████████████████████▏ 1.6 KB\npulling 832dd9e00a68... 100% ▕███████████████████████████████████████▏  11 KB\npulling d9bb33f27869... 100% ▕███████████████████████████████████████▏  487 B\nverifying sha256 digest\nwriting manifest\nsuccess\ntime=2025-02-18T16:46:13.155+08:00 level=INFO source=server.go:104 msg=\"system memory\" total=\"31.3 GiB\" free=\"30.2 GiB\" free_swap=\"4.0 GiB\"\ntime=2025-02-18T16:46:13.155+08:00 level=INFO source=memory.go:356 msg=\"offload to device\" layers.requested=-1 layers.model=29 layers.offload=0 layers.split=\"\" memory.available=\"[30.2 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"6.0 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"896.0 MiB\" memory.required.allocations=\"[6.0 GiB]\" memory.weights.total=\"4.5 GiB\" memory.weights.repeating=\"4.1 GiB\" memory.weights.nonrepeating=\"426.4 MiB\" memory.graph.full=\"942.0 MiB\" memory.graph.partial=\"1.1 GiB\"\ntime=2025-02-18T16:46:13.155+08:00 level=INFO source=server.go:392 msg=\"starting llama server\" cmd=\"/usr/local/lib/python3.11/dist-packages/bigdl/cpp/libs/ollama runner --model /root/.ollama/models/blobs/sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --ctx-size 16384 --batch-size 512 --n-gpu-layers 999 --threads 4 --no-mmap --parallel 1 --port 44557\"\ntime=2025-02-18T16:46:13.156+08:00 level=INFO source=sched.go:449 msg=\"loaded runners\" count=2\ntime=2025-02-18T16:46:13.156+08:00 level=INFO source=server.go:571 msg=\"waiting for llama runner to start responding\"\ntime=2025-02-18T16:46:13.156+08:00 level=INFO source=server.go:605 msg=\"waiting for server to become available\" status=\"llm server error\"\n⠋ ggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\nggml_sycl_init: SYCL_USE_XMX: yes\nggml_sycl_init: found 1 SYCL devices:\ntime=2025-02-18T16:46:13.288+08:00 level=INFO source=runner.go:967 msg=\"starting go runner\"\ntime=2025-02-18T16:46:13.288+08:00 level=INFO source=runner.go:968 msg=system info=\"CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | cgo(gcc)\" threads=4\ntime=2025-02-18T16:46:13.289+08:00 level=INFO source=runner.go:1026 msg=\"Server listening on 127.0.0.1:44557\"\nllama_load_model_from_file: using device SYCL0 (Intel(R) Arc(TM) A770 Graphics) - 15473 MiB free\n⠙ llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from /root/.ollama/models/blobs/sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct\nllama_model_loader: - kv   3:                           general.finetune str              = Instruct\nllama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = apache-2.0\nllama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...\nllama_model_loader: - kv   8:                   general.base_model.count u32              = 1\nllama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B\nllama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen\nllama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...\nllama_model_loader: - kv  12:                               general.tags arr[str,6]       = [\"code\", \"codeqwen\", \"chat\", \"qwen\", ...\nllama_model_loader: - kv  13:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  14:                          qwen2.block_count u32              = 28\nllama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768\nllama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584\nllama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944\nllama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28\nllama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4\nllama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  22:                          general.file_type u32              = 15\nllama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  33:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  141 tensors\nllama_model_loader: - type q4_K:  169 tensors\nllama_model_loader: - type q6_K:   29 tensors\ntime=2025-02-18T16:46:13.407+08:00 level=INFO source=server.go:605 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n⠼ llm_load_vocab: special tokens cache size = 22\nllm_load_vocab: token to piece cache size = 0.9310 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = qwen2\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 152064\nllm_load_print_meta: n_merges         = 151387\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 32768\nllm_load_print_meta: n_embd           = 3584\nllm_load_print_meta: n_layer          = 28\nllm_load_print_meta: n_head           = 28\nllm_load_print_meta: n_head_kv        = 4\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 7\nllm_load_print_meta: n_embd_k_gqa     = 512\nllm_load_print_meta: n_embd_v_gqa     = 512\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 18944\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 2\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 1000000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 32768\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = Q4_K - Medium\nllm_load_print_meta: model params     = 7.62 B\nllm_load_print_meta: model size       = 4.36 GiB (4.91 BPW)\nllm_load_print_meta: general.name     = Qwen2.5 Coder 7B Instruct\nllm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\nllm_load_print_meta: EOS token        = 151645 '<|im_end|>'\nllm_load_print_meta: EOT token        = 151645 '<|im_end|>'\nllm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\nllm_load_print_meta: LF token         = 148848 'ÄĬ'\nllm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'\nllm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'\nllm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'\nllm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'\nllm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'\nllm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'\nllm_load_print_meta: EOG token        = 151643 '<|endoftext|>'\nllm_load_print_meta: EOG token        = 151645 '<|im_end|>'\nllm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'\nllm_load_print_meta: EOG token        = 151663 '<|repo_name|>'\nllm_load_print_meta: EOG token        = 151664 '<|file_sep|>'\nllm_load_print_meta: max token length = 256\n⠼ llm_load_tensors: offloading 28 repeating layers to GPU\nllm_load_tensors: offloading output layer to GPU\nllm_load_tensors: offloaded 29/29 layers to GPU\nllm_load_tensors:        SYCL0 model buffer size =  4168.09 MiB\nllm_load_tensors:          CPU model buffer size =   292.36 MiB\n⠧ llama_new_context_with_model: n_seq_max     = 1\nllama_new_context_with_model: n_ctx         = 16384\nllama_new_context_with_model: n_ctx_per_seq = 16384\nllama_new_context_with_model: n_batch       = 512\nllama_new_context_with_model: n_ubatch      = 512\nllama_new_context_with_model: flash_attn    = 0\nllama_new_context_with_model: freq_base     = 1000000.0\nllama_new_context_with_model: freq_scale    = 1\nllama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n[SYCL] call ggml_check_sycl\nggml_check_sycl: GGML_SYCL_DEBUG: 0\nggml_check_sycl: GGML_SYCL_F16: no\nFound 1 SYCL devices:\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\n| 0| [level_zero:gpu:0]|                Intel Arc A770 Graphics|  12.55|    512|    1024|   32| 16225M|     1.6.32224.500000|\nllama_kv_cache_init:      SYCL0 KV buffer size =   896.00 MiB\nllama_new_context_with_model: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB\nllama_new_context_with_model:  SYCL_Host  output buffer size =     0.59 MiB\nllama_new_context_with_model:      SYCL0 compute buffer size =   304.00 MiB\nllama_new_context_with_model:  SYCL_Host compute buffer size =    39.01 MiB\nllama_new_context_with_model: graph nodes  = 874\nllama_new_context_with_model: graph splits = 2\ntime=2025-02-18T16:46:32.867+08:00 level=WARN source=runner.go:892 msg=\"%s: warming up the model with an empty run - please wait ... \" !BADKEY=loadModel\n⠇ time=2025-02-18T16:46:32.980+08:00 level=INFO source=server.go:610 msg=\"llama runner started in 19.82 seconds\"\n>>> Hello\nllama_load_model_from_file: using device SYCL0 (Intel(R) Arc(TM) A770 Graphics) - 15473 MiB free\nllama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from /root/.ollama/models/blobs/sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct\nllama_model_loader: - kv   3:                           general.finetune str              = Instruct\nllama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                            general.license str              = apache-2.0\nllama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...\nllama_model_loader: - kv   8:                   general.base_model.count u32              = 1\nllama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B\nllama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen\nllama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...\nllama_model_loader: - kv  12:                               general.tags arr[str,6]       = [\"code\", \"codeqwen\", \"chat\", \"qwen\", ...\nllama_model_loader: - kv  13:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  14:                          qwen2.block_count u32              = 28\nllama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768\nllama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584\nllama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944\nllama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28\nllama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4\nllama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  22:                          general.file_type u32              = 15\nllama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  33:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  141 tensors\nllama_model_loader: - type q4_K:  169 tensors\nllama_model_loader: - type q6_K:   29 tensors\n⠹ llm_load_vocab: special tokens cache size = 22\n⠸ llm_load_vocab: token to piece cache size = 0.9310 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = qwen2\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 152064\nllm_load_print_meta: n_merges         = 151387\nllm_load_print_meta: vocab_only       = 1\nllm_load_print_meta: model type       = ?B\nllm_load_print_meta: model ftype      = all F32\nllm_load_print_meta: model params     = 7.62 B\nllm_load_print_meta: model size       = 4.36 GiB (4.91 BPW)\nllm_load_print_meta: general.name     = Qwen2.5 Coder 7B Instruct\nllm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\nllm_load_print_meta: EOS token        = 151645 '<|im_end|>'\nllm_load_print_meta: EOT token        = 151645 '<|im_end|>'\nllm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\nllm_load_print_meta: LF token         = 148848 'ÄĬ'\nllm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'\nllm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'\nllm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'\nllm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'\nllm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'\nllm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'\nllm_load_print_meta: EOG token        = 151643 '<|endoftext|>'\nllm_load_print_meta: EOG token        = 151645 '<|im_end|>'\nllm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'\nllm_load_print_meta: EOG token        = 151663 '<|repo_name|>'\nllm_load_print_meta: EOG token        = 151664 '<|file_sep|>'\nllm_load_print_meta: max token length = 256\nllama_model_load: vocab only - skipping tensors\nSIGILL: illegal instruction\nPC=0x70ef0960bc2f m=7 sigcode=2\nsignal arrived during cgo execution\ninstruction bytes: 0xf3 0xf 0xc7 0xf8 0x25 0xff 0x3 0x0 0x0 0x48 0x8b 0xd 0xe1 0xc2 0x2a 0x0\n\ngoroutine 36 gp=0xc000504700 m=7 mp=0xc000520008 [syscall]:\nruntime.cgocall(0x555b6f2584e0, 0xc000569b90)\n\truntime/cgocall.go:167 +0x4b fp=0xc000569b68 sp=0xc000569b30 pc=0x555b6e6b754b\nollama/llama/llamafile._Cfunc_llama_decode(0x70ee8686ef00, {0x1e, 0x70ee84029af0, 0x0, 0x0, 0x70ee8402a300, 0x70ee868797d0, 0x70ee86879fe0, 0x70ee840237c0})\n\t_cgo_gotypes.go:558 +0x4f fp=0xc000569b90 sp=0xc000569b68 pc=0x555b6ea7996f\nollama/llama/llamafile.(*Context).Decode.func1(0x555b6ea886eb?, 0x70ee8686ef00?)\n\tollama/llama/llamafile/llama.go:143 +0xf5 fp=0xc000569c80 sp=0xc000569b90 pc=0x555b6ea7c595\nollama/llama/llamafile.(*Context).Decode(0xc000075570?, 0x0?)\n\tollama/llama/llamafile/llama.go:143 +0x13 fp=0xc000569cc8 sp=0xc000569c80 pc=0x555b6ea7c413\nollama/llama/runner.(*Server).processBatch(0xc000532120, 0xc000112600, 0xc000075720)\n\tollama/llama/runner/runner.go:434 +0x23f fp=0xc000569ee0 sp=0xc000569cc8 pc=0x555b6ea873bf\nollama/llama/runner.(*Server).run(0xc000532120, {0x555b6f8089c0, 0xc00059b1d0})\n\tollama/llama/runner/runner.go:342 +0x1d5 fp=0xc000569fb8 sp=0xc000569ee0 pc=0x555b6ea86df5\nollama/llama/runner.Execute.gowrap2()\n\tollama/llama/runner/runner.go:1006 +0x28 fp=0xc000569fe0 sp=0xc000569fb8 pc=0x555b6ea8c068\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc000569fe8 sp=0xc000569fe0 pc=0x555b6e6c6021\ncreated by ollama/llama/runner.Execute in goroutine 1\n\tollama/llama/runner/runner.go:1006 +0xde5\n\ngoroutine 1 gp=0xc0000061c0 m=nil [IO wait]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc000121560 sp=0xc000121540 pc=0x555b6e6bdc4e\nruntime.netpollblock(0xc0001215b0?, 0x6e654a66?, 0x5b?)\n\truntime/netpoll.go:575 +0xf7 fp=0xc000121598 sp=0xc000121560 pc=0x555b6e6818b7\ninternal/poll.runtime_pollWait(0x70ef0a435680, 0x72)\n\truntime/netpoll.go:351 +0x85 fp=0xc0001215b8 sp=0xc000121598 pc=0x555b6e6bcf45\ninternal/poll.(*pollDesc).wait(0xc000490280?, 0x900000036?, 0x0)\n\tinternal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc0001215e0 sp=0xc0001215b8 pc=0x555b6e744567\ninternal/poll.(*pollDesc).waitRead(...)\n\tinternal/poll/fd_poll_runtime.go:89\ninternal/poll.(*FD).Accept(0xc000490280)\n\tinternal/poll/fd_unix.go:620 +0x295 fp=0xc000121688 sp=0xc0001215e0 pc=0x555b6e749935\nnet.(*netFD).accept(0xc000490280)\n\tnet/fd_unix.go:172 +0x29 fp=0xc000121740 sp=0xc000121688 pc=0x555b6e7b2009\nnet.(*TCPListener).accept(0xc0005a5300)\n\tnet/tcpsock_posix.go:159 +0x1e fp=0xc000121790 sp=0xc000121740 pc=0x555b6e7c7c7e\nnet.(*TCPListener).Accept(0xc0005a5300)\n\tnet/tcpsock.go:372 +0x30 fp=0xc0001217c0 sp=0xc000121790 pc=0x555b6e7c6b30\nnet/http.(*onceCloseListener).Accept(0xc000016cf0?)\n\t<autogenerated>:1 +0x24 fp=0xc0001217d8 sp=0xc0001217c0 pc=0x555b6ea40284\nnet/http.(*Server).Serve(0xc0005b2d20, {0x555b6f806700, 0xc0005a5300})\n\tnet/http/server.go:3330 +0x30c fp=0xc000121908 sp=0xc0001217d8 pc=0x555b6ea1820c\nollama/llama/runner.Execute({0xc000036130?, 0x0?, 0x0?})\n\tollama/llama/runner/runner.go:1027 +0x11a9 fp=0xc000121ca8 sp=0xc000121908 pc=0x555b6ea8bd49\nollama/cmd.NewCLI.func2(0xc000534400?, {0x555b6f25cf9d?, 0x4?, 0x555b6f25cfa1?})\n\tollama/cmd/cmd.go:1430 +0x45 fp=0xc000121cd0 sp=0xc000121ca8 pc=0x555b6f257765\ngithub.com/spf13/cobra.(*Command).execute(0xc000530008, {0xc0005b2690, 0xf, 0xf})\n\tgithub.com/spf13/cobra@v1.8.1/command.go:985 +0xaaa fp=0xc000121e58 sp=0xc000121cd0 pc=0x555b6e84b3ea\ngithub.com/spf13/cobra.(*Command).ExecuteC(0xc0004aa308)\n\tgithub.com/spf13/cobra@v1.8.1/command.go:1117 +0x3ff fp=0xc000121f30 sp=0xc000121e58 pc=0x555b6e84bcbf\ngithub.com/spf13/cobra.(*Command).Execute(...)\n\tgithub.com/spf13/cobra@v1.8.1/command.go:1041\ngithub.com/spf13/cobra.(*Command).ExecuteContext(...)\n\tgithub.com/spf13/cobra@v1.8.1/command.go:1034\nmain.main()\n\tollama/main.go:12 +0x4d fp=0xc000121f50 sp=0xc000121f30 pc=0x555b6f257dcd\nruntime.main()\n\truntime/proc.go:272 +0x29d fp=0xc000121fe0 sp=0xc000121f50 pc=0x555b6e688f5d\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc000121fe8 sp=0xc000121fe0 pc=0x555b6e6c6021\n\ngoroutine 2 gp=0xc000006c40 m=nil [force gc (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc000078fa8 sp=0xc000078f88 pc=0x555b6e6bdc4e\nruntime.goparkunlock(...)\n\truntime/proc.go:430\nruntime.forcegchelper()\n\truntime/proc.go:337 +0xb8 fp=0xc000078fe0 sp=0xc000078fa8 pc=0x555b6e689298\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc000078fe8 sp=0xc000078fe0 pc=0x555b6e6c6021\ncreated by runtime.init.7 in goroutine 1\n\truntime/proc.go:325 +0x1a\n\ngoroutine 3 gp=0xc000007180 m=nil [GC sweep wait]:\nruntime.gopark(0x1?, 0x0?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc000079780 sp=0xc000079760 pc=0x555b6e6bdc4e\nruntime.goparkunlock(...)\n\truntime/proc.go:430\nruntime.bgsweep(0xc0000a4000)\n\truntime/mgcsweep.go:317 +0xdf fp=0xc0000797c8 sp=0xc000079780 pc=0x555b6e67393f\nruntime.gcenable.gowrap1()\n\truntime/mgc.go:204 +0x25 fp=0xc0000797e0 sp=0xc0000797c8 pc=0x555b6e667f85\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc0000797e8 sp=0xc0000797e0 pc=0x555b6e6c6021\ncreated by runtime.gcenable in goroutine 1\n\truntime/mgc.go:204 +0x66\n\ngoroutine 4 gp=0xc000007340 m=nil [GC scavenge wait]:\nruntime.gopark(0x10000?, 0x555b6f403070?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc000079f78 sp=0xc000079f58 pc=0x555b6e6bdc4e\nruntime.goparkunlock(...)\n\truntime/proc.go:430\nruntime.(*scavengerState).park(0x555b6ff9ed80)\n\truntime/mgcscavenge.go:425 +0x49 fp=0xc000079fa8 sp=0xc000079f78 pc=0x555b6e671309\nruntime.bgscavenge(0xc0000a4000)\n\truntime/mgcscavenge.go:658 +0x59 fp=0xc000079fc8 sp=0xc000079fa8 pc=0x555b6e671899\nruntime.gcenable.gowrap2()\n\truntime/mgc.go:205 +0x25 fp=0xc000079fe0 sp=0xc000079fc8 pc=0x555b6e667f25\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc000079fe8 sp=0xc000079fe0 pc=0x555b6e6c6021\ncreated by runtime.gcenable in goroutine 1\n\truntime/mgc.go:205 +0xa5\n\ngoroutine 5 gp=0xc000007c00 m=nil [finalizer wait]:\nruntime.gopark(0xc000078648?, 0x555b6e65e485?, 0xb0?, 0x1?, 0xc0000061c0?)\n\truntime/proc.go:424 +0xce fp=0xc000078620 sp=0xc000078600 pc=0x555b6e6bdc4e\nruntime.runfinq()\n\truntime/mfinal.go:193 +0x107 fp=0xc0000787e0 sp=0xc000078620 pc=0x555b6e667007\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc0000787e8 sp=0xc0000787e0 pc=0x555b6e6c6021\ncreated by runtime.createfing in goroutine 1\n\truntime/mfinal.go:163 +0x3d\n\ngoroutine 6 gp=0xc0001ece00 m=nil [chan receive]:\nruntime.gopark(0xc00007a760?, 0x555b6e799685?, 0x40?, 0x28?, 0x555b6f819c00?)\n\truntime/proc.go:424 +0xce fp=0xc00007a718 sp=0xc00007a6f8 pc=0x555b6e6bdc4e\nruntime.chanrecv(0xc000044310, 0x0, 0x1)\n\truntime/chan.go:639 +0x41c fp=0xc00007a790 sp=0xc00007a718 pc=0x555b6e65767c\nruntime.chanrecv1(0x0?, 0x0?)\n\truntime/chan.go:489 +0x12 fp=0xc00007a7b8 sp=0xc00007a790 pc=0x555b6e657232\nruntime.unique_runtime_registerUniqueMapCleanup.func1(...)\n\truntime/mgc.go:1781\nruntime.unique_runtime_registerUniqueMapCleanup.gowrap1()\n\truntime/mgc.go:1784 +0x2f fp=0xc00007a7e0 sp=0xc00007a7b8 pc=0x555b6e66afef\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc00007a7e8 sp=0xc00007a7e0 pc=0x555b6e6c6021\ncreated by unique.runtime_registerUniqueMapCleanup in goroutine 1\n\truntime/mgc.go:1779 +0x96\n\ngoroutine 7 gp=0xc0001eda40 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc00007af38 sp=0xc00007af18 pc=0x555b6e6bdc4e\nruntime.gcBgMarkWorker(0xc0000458f0)\n\truntime/mgc.go:1412 +0xe9 fp=0xc00007afc8 sp=0xc00007af38 pc=0x555b6e66a2e9\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1328 +0x25 fp=0xc00007afe0 sp=0xc00007afc8 pc=0x555b6e66a1c5\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc00007afe8 sp=0xc00007afe0 pc=0x555b6e6c6021\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1328 +0x105\n\ngoroutine 18 gp=0xc000104380 m=nil [GC worker (idle)]:\nruntime.gopark(0x12bb42237f9?, 0x3?, 0xcf?, 0x6d?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc000074738 sp=0xc000074718 pc=0x555b6e6bdc4e\nruntime.gcBgMarkWorker(0xc0000458f0)\n\truntime/mgc.go:1412 +0xe9 fp=0xc0000747c8 sp=0xc000074738 pc=0x555b6e66a2e9\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1328 +0x25 fp=0xc0000747e0 sp=0xc0000747c8 pc=0x555b6e66a1c5\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc0000747e8 sp=0xc0000747e0 pc=0x555b6e6c6021\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1328 +0x105\n\ngoroutine 8 gp=0xc0001edc00 m=nil [GC worker (idle)]:\nruntime.gopark(0x12bb421ee0e?, 0x3?, 0x26?, 0x3e?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc00007b738 sp=0xc00007b718 pc=0x555b6e6bdc4e\nruntime.gcBgMarkWorker(0xc0000458f0)\n\truntime/mgc.go:1412 +0xe9 fp=0xc00007b7c8 sp=0xc00007b738 pc=0x555b6e66a2e9\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1328 +0x25 fp=0xc00007b7e0 sp=0xc00007b7c8 pc=0x555b6e66a1c5\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc00007b7e8 sp=0xc00007b7e0 pc=0x555b6e6c6021\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1328 +0x105\n\ngoroutine 34 gp=0xc000504000 m=nil [GC worker (idle)]:\nruntime.gopark(0x12bb421f752?, 0x0?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:424 +0xce fp=0xc00050a738 sp=0xc00050a718 pc=0x555b6e6bdc4e\nruntime.gcBgMarkWorker(0xc0000458f0)\n\truntime/mgc.go:1412 +0xe9 fp=0xc00050a7c8 sp=0xc00050a738 pc=0x555b6e66a2e9\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1328 +0x25 fp=0xc00050a7e0 sp=0xc00050a7c8 pc=0x555b6e66a1c5\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc00050a7e8 sp=0xc00050a7e0 pc=0x555b6e6c6021\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1328 +0x105\n\ngoroutine 107 gp=0xc000504540 m=nil [IO wait]:\nruntime.gopark(0x555b6e662965?, 0x0?, 0x0?, 0x0?, 0xb?)\n\truntime/proc.go:424 +0xce fp=0xc0004c9da8 sp=0xc0004c9d88 pc=0x555b6e6bdc4e\nruntime.netpollblock(0x555b6e6e0e78?, 0x6e654a66?, 0x5b?)\n\truntime/netpoll.go:575 +0xf7 fp=0xc0004c9de0 sp=0xc0004c9da8 pc=0x555b6e6818b7\ninternal/poll.runtime_pollWait(0x70ef0a435568, 0x72)\n\truntime/netpoll.go:351 +0x85 fp=0xc0004c9e00 sp=0xc0004c9de0 pc=0x555b6e6bcf45\ninternal/poll.(*pollDesc).wait(0xc000591d00?, 0xc0001c3f61?, 0x0)\n\tinternal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc0004c9e28 sp=0xc0004c9e00 pc=0x555b6e744567\ninternal/poll.(*pollDesc).waitRead(...)\n\tinternal/poll/fd_poll_runtime.go:89\ninternal/poll.(*FD).Read(0xc000591d00, {0xc0001c3f61, 0x1, 0x1})\n\tinternal/poll/fd_unix.go:165 +0x27a fp=0xc0004c9ec0 sp=0xc0004c9e28 pc=0x555b6e74585a\nnet.(*netFD).Read(0xc000591d00, {0xc0001c3f61?, 0xc0004c9f48?, 0x555b6e6bf8d0?})\n\tnet/fd_posix.go:55 +0x25 fp=0xc0004c9f08 sp=0xc0004c9ec0 pc=0x555b6e7b0045\nnet.(*conn).Read(0xc0000e4000, {0xc0001c3f61?, 0x0?, 0x555b6ffc6680?})\n\tnet/net.go:189 +0x45 fp=0xc0004c9f50 sp=0xc0004c9f08 pc=0x555b6e7be645\nnet.(*TCPConn).Read(0x555b6ff030a0?, {0xc0001c3f61?, 0x0?, 0x0?})\n\t<autogenerated>:1 +0x25 fp=0xc0004c9f80 sp=0xc0004c9f50 pc=0x555b6e7d1845\nnet/http.(*connReader).backgroundRead(0xc0001c3f50)\n\tnet/http/server.go:690 +0x37 fp=0xc0004c9fc8 sp=0xc0004c9f80 pc=0x555b6ea0db37\nnet/http.(*connReader).startBackgroundRead.gowrap2()\n\tnet/http/server.go:686 +0x25 fp=0xc0004c9fe0 sp=0xc0004c9fc8 pc=0x555b6ea0da65\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc0004c9fe8 sp=0xc0004c9fe0 pc=0x555b6e6c6021\ncreated by net/http.(*connReader).startBackgroundRead in goroutine 121\n\tnet/http/server.go:686 +0xb6\n\ngoroutine 121 gp=0xc000504a80 m=nil [select]:\nruntime.gopark(0xc00005da68?, 0x2?, 0xce?, 0x36?, 0xc00005d834?)\n\truntime/proc.go:424 +0xce fp=0xc00005d650 sp=0xc00005d630 pc=0x555b6e6bdc4e\nruntime.selectgo(0xc00005da68, 0xc00005d830, 0x1e?, 0x0, 0x1?, 0x1)\n\truntime/select.go:335 +0x7a5 fp=0xc00005d778 sp=0xc00005d650 pc=0x555b6e69af45\nollama/llama/runner.(*Server).completion(0xc000532120, {0x555b6f806910, 0xc0004b7500}, 0xc00041aa00)\n\tollama/llama/runner/runner.go:696 +0xab6 fp=0xc00005dac0 sp=0xc00005d778 pc=0x555b6ea89236\nollama/llama/runner.(*Server).completion-fm({0x555b6f806910?, 0xc0004b7500?}, 0x555b6ea21fe7?)\n\t<autogenerated>:1 +0x36 fp=0xc00005daf0 sp=0xc00005dac0 pc=0x555b6ea8c916\nnet/http.HandlerFunc.ServeHTTP(0xc00019e1c0?, {0x555b6f806910?, 0xc0004b7500?}, 0x0?)\n\tnet/http/server.go:2220 +0x29 fp=0xc00005db18 sp=0xc00005daf0 pc=0x555b6ea14809\nnet/http.(*ServeMux).ServeHTTP(0x555b6e65e485?, {0x555b6f806910, 0xc0004b7500}, 0xc00041aa00)\n\tnet/http/server.go:2747 +0x1ca fp=0xc00005db68 sp=0xc00005db18 pc=0x555b6ea1670a\nnet/http.serverHandler.ServeHTTP({0x555b6f803510?}, {0x555b6f806910?, 0xc0004b7500?}, 0x6?)\n\tnet/http/server.go:3210 +0x8e fp=0xc00005db98 sp=0xc00005db68 pc=0x555b6ea33c6e\nnet/http.(*conn).serve(0xc000016cf0, {0x555b6f808988, 0xc0005f0510})\n\tnet/http/server.go:2092 +0x5d0 fp=0xc00005dfb8 sp=0xc00005db98 pc=0x555b6ea131b0\nnet/http.(*Server).Serve.gowrap3()\n\tnet/http/server.go:3360 +0x28 fp=0xc00005dfe0 sp=0xc00005dfb8 pc=0x555b6ea18608\nruntime.goexit({})\n\truntime/asm_amd64.s:1700 +0x1 fp=0xc00005dfe8 sp=0xc00005dfe0 pc=0x555b6e6c6021\ncreated by net/http.(*Server).Serve in goroutine 1\n\tnet/http/server.go:3360 +0x485\n\nrax    0x0\nrbx    0x0\nrcx    0x70ee840257c0\nrdx    0x70eea89febf0\nrdi    0x6\nrsi    0x555b8508ace0\nrbp    0x70eea89fe960\nrsp    0x70eea89fe3f0\nr8     0x70ee84025940\nr9     0x0\nr10    0x70ef0b69bf28\nr11    0x70ee84025940\nr12    0x70ee84025940\nr13    0x70ee84058de8\nr14    0x70ee840257c0\nr15    0x70ee84025940\nrip    0x70ef0960bc2f\nrflags 0x10202\ncs     0x33\nfs     0x0\ngs     0x0\nError: POST predict: Post \"http://127.0.0.1:44557/completion\": EOF\n```\n",
      "state": "open",
      "author": "tkatila",
      "author_type": "User",
      "created_at": "2025-02-18T08:57:16Z",
      "updated_at": "2025-03-24T18:53:27Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12845/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12845",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12845",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:58.453668",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Could you show me `clinfo | grep \"Device Name\"` and your oneapi version? You may also follow https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/install_linux_gpu.md#install-gpu-driver to reinstall intel driver and oneapi.",
          "created_at": "2025-02-19T01:53:55Z"
        },
        {
          "author": "tkatila",
          "body": "clinfo:\n```\nroot@dgpu-test:/# clinfo | grep \"Device Name\"\n  Device Name                                     Intel(R) Core(TM) i7-8700K CPU @ 3.70GHz\n  Device Name                                     Intel(R) Arc(TM) A770 Graphics\n    Device Name                                   Intel(R) Core(TM) i7",
          "created_at": "2025-02-19T08:13:44Z"
        },
        {
          "author": "JKlesmith",
          "body": "Seeing the exact same issue from the  ollama-0.5.4-ipex-llm-2.2.0b20250218-ubuntu.tgz release.\n\n\nSIGILL: illegal instruction\nPC=0x749a53e0bc2f m=10 sigcode=2\nsignal arrived during cgo execution\ninstruction bytes: 0xf3 0xf 0xc7 0xf8 0x25 0xff 0x3 0x0 0x0 0x48 0x8b 0xd 0xe1 0xc2 0x2a 0x0\n\n\n  Device Na",
          "created_at": "2025-02-20T01:20:09Z"
        },
        {
          "author": "qiuxin2012",
          "body": "similar issue: https://github.com/intel/ipex-llm/issues/12844",
          "created_at": "2025-02-20T02:38:36Z"
        },
        {
          "author": "hollisticated-horse",
          "body": "This problem seems to have been fixed by @qiuxin2012 in #12844. ",
          "created_at": "2025-03-24T18:53:26Z"
        }
      ]
    },
    {
      "issue_number": 12998,
      "title": "failed to load Qwen2-VL with ipex-llm[xpu] under a760",
      "body": "**Describe the bug**\n/usr/local/lib/python3.11/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n  warn(\nTraceback (most recent call last):\n  File \"/home/llm/llama3_8b_ipex.py\", line 7, in <module>\n    from ipex_llm import optimize_model\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/__init__.py\", line 38, in <module>\n    ipex_importer.import_ipex()\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/utils/ipex_importer.py\", line 136, in import_ipex\n    self.directly_import_ipex()\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/utils/ipex_importer.py\", line 156, in directly_import_ipex\n    import intel_extension_for_pytorch as ipex\n  File \"/usr/local/lib/python3.11/dist-packages/intel_extension_for_pytorch/__init__.py\", line 122, in <module>\n    from . import _dynamo\n  File \"/usr/local/lib/python3.11/dist-packages/intel_extension_for_pytorch/_dynamo/__init__.py\", line 5, in <module>\n    from torch._inductor import codecache  # noqa\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/codecache.py\", line 1437, in <module>\n    AsyncCompile.warm_pool()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/codecache.py\", line 1356, in warm_pool\n    pool = cls.process_pool()\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/codecache.py\", line 1341, in process_pool\n    pool = ProcessPoolExecutor(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/concurrent/futures/process.py\", line 706, in __init__\n    self._call_queue = _SafeQueue(\n                       ^^^^^^^^^^^\n  File \"/usr/lib/python3.11/concurrent/futures/process.py\", line 168, in __init__\n    super().__init__(max_size, ctx=ctx)\n  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 43, in __init__\n    self._rlock = ctx.Lock()\n                  ^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/context.py\", line 68, in Lock\n    return Lock(ctx=self.get_context())\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/synchronize.py\", line 162, in __init__\n    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)\n  File \"/usr/lib/python3.11/multiprocessing/synchronize.py\", line 57, in __init__\n    sl = self._semlock = _multiprocessing.SemLock(\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory\n\n\n**How to reproduce**\nfail at \"from ipex_llm import optimize_model\"\ndon't know which file is required here\n\n**Environment information**\nFollowing https://github.com/intel/ipex-llm/blob/main/python/llm/example/GPU/HuggingFace/Multimodal/qwen2-vl/README.md\nexcept \"conda create -n llm python=3.11; conda activate llm\"\n\n**Additional context**\nAdd any other context about the problem here.\n",
      "state": "closed",
      "author": "Zhiwei-Lii",
      "author_type": "User",
      "created_at": "2025-03-24T08:44:47Z",
      "updated_at": "2025-03-24T13:04:25Z",
      "closed_at": "2025-03-24T13:04:25Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12998/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12998",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12998",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:58.646360",
      "comments": [
        {
          "author": "Zhiwei-Lii",
          "body": "In my linux, it does not such path \"/dev/shm\", which used by python library. so it crashed.\ncritical log: newfstatat(AT_FDCWD, \"/dev/shm/sem.EplO0X\", 0x7ffdc05ed780, AT_SYMLINK_NOFOLLOW) = -1 ENOENT (No such file or directory)\nopenat(AT_FDCWD, \"/dev/shm/sem.EplO0X\", O_RDWR|O_CREAT|O_EXCL, 0600) = -1",
          "created_at": "2025-03-24T12:58:13Z"
        }
      ]
    },
    {
      "issue_number": 12980,
      "title": "Ollama Portable Zip SIGSEV",
      "body": "Running `./ollama` and `./start-ollama.sh` results in `SIGSEGV: segmentation violation`.\n\n\n**How to reproduce**\nFollow the [quickstart docs](https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_portable_zip_quickstart.md#linux-quickstart) to download and start Ollama.\n\nIntel oneAPI is installed in my system. Running `setvars.sh` beforehand prevents this behavior. \n\nA text file containing the error is attached.\n\n**Environment information**\n\nOutputs of the script before and after running `setvars.sh` is provided.\n\n[env_after_setvars.txt](https://github.com/user-attachments/files/19317614/env_after_setvars.txt)\n[env_before_setvars.txt](https://github.com/user-attachments/files/19317616/env_before_setvars.txt)\n[error.txt](https://github.com/user-attachments/files/19317615/error.txt)",
      "state": "open",
      "author": "idkSeth",
      "author_type": "User",
      "created_at": "2025-03-18T11:34:02Z",
      "updated_at": "2025-03-24T07:58:09Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12980/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12980",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12980",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:58.826023",
      "comments": [
        {
          "author": "1823616178",
          "body": "So do I. I hope to get it fixed soon",
          "created_at": "2025-03-19T01:20:43Z"
        },
        {
          "author": "qiuxin2012",
          "body": "Please do not `source setvars.sh`, as you are using protable zip. The oneapi libs are included in the tgz.",
          "created_at": "2025-03-19T02:30:18Z"
        },
        {
          "author": "idkSeth",
          "body": "> Please do not `source setvars.sh`, as you are using protable zip. The oneapi libs are included in the tgz.\n\nI initially ran Ollama without setting the variables/running `source setvars.sh` and the error occurred.\n\nRunning `source setvars.sh` prevents the error and Ollama works. It does not work if",
          "created_at": "2025-03-19T17:20:50Z"
        },
        {
          "author": "qiuxin2012",
          "body": "@idkSeth Can you share your cpu and gpu information to me?",
          "created_at": "2025-03-24T01:18:09Z"
        },
        {
          "author": "idkSeth",
          "body": "OS: Ubuntu oracular 24.10 x86_64\nKernel: Linux 6.11.0-19-generic\nCPU: 11th Gen Intel(R) Core(TM) i7-11390H (8) @ 5.00 GHz\nGPU: Intel Iris Xe Graphics @ 1.40 GHz [Integrated]",
          "created_at": "2025-03-24T07:58:08Z"
        }
      ]
    },
    {
      "issue_number": 12990,
      "title": "vllm on tensor parallel - RuntimeError: oneCCL: ze_fd_manager.cpp:144 init_device_fds: EXCEPTION: opendir failed: could not open device directory",
      "body": "**Describe the bug**\nwhile using vllm in docker and using tensor parallel i get:\n`RuntimeError: oneCCL: ze_fd_manager.cpp:144 init_device_fds: EXCEPTION: opendir failed: could not open device directory`\n\nsingle card serving works \n\nLlama.cpp works also for multi gpus\n\ni remember some months ago i tried it on intel system with 2 GPUs and it worked.\n\n\n**How to reproduce**\nSteps to reproduce the error:\n1. multi gpus\n2. try to run\n\n**Screenshots**\nIf applicable, add screenshots to help explain the problem\n\n**Environment information**\nI'm building myown docker image based on this:\n\n```\nFROM intelanalytics/ipex-llm-serving-xpu:latest\nWORKDIR /temp\n\nSHELL [\"/bin/bash\", \"-c\"] \nRUN apt update && apt install -y libpng16-16\nRUN wget http://mirrors.kernel.org/ubuntu/pool/main/libj/libjpeg-turbo/libjpeg-turbo8_2.1.2-0ubuntu1_amd64.deb  \nRUN apt install ./libjpeg-turbo8_2.1.2-0ubuntu1_amd64.deb\n\nWORKDIR /llm\nRUN . /opt/intel/1ccl-wks/setvars.sh\nENTRYPOINT python -m  ipex_llm.vllm.xpu.entrypoints.openai.api_server \\ \n  --served-model-name ${served_model_name} \\\n  --quantization $quantization \\\n  --model $model \\\n  --port $port \\\n  --trust-remote-code \\\n  --block-size 8 \\\n  --gpu-memory-utilization ${gpu_memory_utilization} \\\n  --device xpu \\\n  --dtype $dtype \\\n  --enforce-eager \\\n  --load-in-low-bit ${load_in_low_bit} \\\n  --max-model-len ${max_model_len} \\\n  --max-num-batched-tokens ${max_num_batched_tokens} \\\n  --max-num-seqs ${max_num_seqs} \\\n  --tensor-parallel-size ${tensor_parallel_size} \\ \n  --disable-async-output-proc \\\n  --distributed-executor-backend ray\n```\n\nThe docker-compose file looks like below:\n\n```\nservices:\n  vllm-ipex:\n    image: intelanalytics/ipex-llm-serving-xpu:latest\n    container_name: vllm-ipex\n    build:\n      dockerfile: ./dockerfile/dockerfile\n    volumes:\n      - \"/models/huggingface:/root/.cache/huggingface\"\n      - \"/models/vllm:/llm/models\"\n      - /etc/timezone:/etc/timezone:ro\n      - /etc/localtime:/etc/localtime:ro\n    # restart: unless-stopped\n    devices:\n      - /dev/dri:/dev/dri\n    tty: true\n    ports:\n    - 8000:8000\n    shm_size: \"64g\"\n    environment:\n      - model=Qwen/Qwen2.5-32B-Instruct-AWQ\n      - served_model_name=Qwen2.5-32B-Instruct-AWQ\n      - quantization=awq\n      - TZ=Europe/Berlin\n      - SYCL_CACHE_PERSISTENT=1\n      - CCL_WORKER_COUNT=2\n      - FI_PROVIDER=shm\n      - CCL_ATL_TRANSPORT=ofi\n      - CCL_ZE_IPC_EXCHANGE=sockets\n      - CCL_ATL_SHM=1\n      - USE_XETLA=OFF\n      - SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=2\n      - TORCH_LLM_ALLREDUCE=0\n      - CCL_SAME_STREAM=1\n      - CCL_BLOCKING_WAIT=0\n      - port=8000\n      - gpu_memory_utilization=0.95\n      - dtype=float16  \n      - load_in_low_bit=asym_int4\n      - max_model_len=2048\n      - max_num_batched_tokens=4000\n      - max_num_seqs=256\n      - tensor_parallel_size=2\n      - pipeline_parallel_size=1\n      - VLLM_LOGGING_LEVEL=DEBUG\n      - VLLM_TRACE_FUNCTION=1\n```\n\nSystem: Ubuntu 24.04\nCPU: EPYC 7282\nMB: Supermicro h12ssl\nRAM: 256GB \nGPUs: 4xARC 770 LE\n\n**logs**\n\n[dmesg.txt](https://github.com/user-attachments/files/19389380/dmesg.txt)\n[log.txt](https://github.com/user-attachments/files/19389379/log.txt)\n",
      "state": "closed",
      "author": "flekol",
      "author_type": "User",
      "created_at": "2025-03-21T12:45:36Z",
      "updated_at": "2025-03-24T05:58:31Z",
      "closed_at": "2025-03-24T05:58:30Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12990/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "gc-fu"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12990",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12990",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:59.059844",
      "comments": [
        {
          "author": "gc-fu",
          "body": "Hi, can you try to add `privileged: true` into the docker compose file and see if this error persists?",
          "created_at": "2025-03-24T02:07:29Z"
        },
        {
          "author": "flekol",
          "body": "Thanks a lot.\n\nIt works, now i feel stupid that i did not come up with this on my own :)\n\n",
          "created_at": "2025-03-24T05:58:30Z"
        }
      ]
    },
    {
      "issue_number": 12989,
      "title": "ipex-llm(ollama)是否有一些可设置环境变量供参考",
      "body": "比如set IPEX_LLM_MODEL_SOURCE=modelscope\n我们也许会有调整IP、PORT、并行计算数量等需求",
      "state": "open",
      "author": "kelvennn",
      "author_type": "User",
      "created_at": "2025-03-21T09:50:19Z",
      "updated_at": "2025-03-24T02:28:57Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12989/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12989",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12989",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:59.237584",
      "comments": [
        {
          "author": "suomi2024",
          "body": "set OLLAMA_NUM_GPU=999\n\nset no_proxy=localhost,127.0.0.1\n\nset ZES_ENABLE_SYSMAN=1\n\nset SYCL_CACHE_PERSISTENT=1\n\nset SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1\n\nset OLLAMA_KEEP_ALIVE=-1\n\nset OLLAMA_NUM_PARALLEL=1\n\nset OLLAMA_PARAMETER num_ctx 16384\n\nset OLLAMA_PARAMETER num_predict 8192\n\nset PAR",
          "created_at": "2025-03-21T10:43:54Z"
        }
      ]
    },
    {
      "issue_number": 12982,
      "title": "Unable to fully load model into Vram using ollama zip gpu",
      "body": "SYSTEM：U265K(igpu off)+48G ram+B580(12g)\n\ndeepseek-r1:14b (Q4)：\nB580 video memory is enough to load deepseek-r1:14b (Q4) model, but segmentation error occurs, less than 7G is loaded into VRAM，and the rest is loaded into shared GPU memory。\n\ndeepseek-r1:32b (Q4)：\n12G of the model is loaded into the dedicated GPU memory, and the remaining 8G is loaded into the shared GPU memory. The system RAM is basically not occupied and the CPU cannot participate in reasoning.\n\n\n",
      "state": "open",
      "author": "dttprofessor",
      "author_type": "User",
      "created_at": "2025-03-19T14:54:34Z",
      "updated_at": "2025-03-21T11:57:29Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12982/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12982",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12982",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:59.441566",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Could you check your GPU's VRAM usage before loading the model?",
          "created_at": "2025-03-21T02:00:31Z"
        },
        {
          "author": "dttprofessor",
          "body": "11GB（12GB total）is free.\r\n\r\n\r\n\r\n发送自我的盖乐世\r\n\r\n\r\n\r\n-------- 原始信息 --------\r\n发件人： SONG Ge ***@***.***>\r\n日期: 2025/3/21 10:00 (GMT+08:00)\r\n收件人： intel/ipex-llm ***@***.***>\r\n抄送： dttprofessor ***@***.***>, Author ***@***.***>\r\n主题： Re: [intel/ipex-llm] Unable to fully load model into Vram using ollama zip gpu",
          "created_at": "2025-03-21T02:17:25Z"
        },
        {
          "author": "suomi2024",
          "body": "set OLLAMA_NUM_GPU=999\n",
          "created_at": "2025-03-21T10:45:28Z"
        },
        {
          "author": "suomi2024",
          "body": "set OLLAMA_NUM_GPU=999\n\nset no_proxy=localhost,127.0.0.1\n\nset ZES_ENABLE_SYSMAN=1\n\nset SYCL_CACHE_PERSISTENT=1\n\nset SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1\n\nset OLLAMA_KEEP_ALIVE=-1\n\nset OLLAMA_NUM_PARALLEL=1\n\nset OLLAMA_PARAMETER num_ctx 16384\n\nset OLLAMA_PARAMETER num_predict 8192\n\nset PAR",
          "created_at": "2025-03-21T10:45:37Z"
        },
        {
          "author": "dttprofessor",
          "body": "I will try it\r\n\r\n\r\n\r\n\r\n发送自我的盖乐世\r\n\r\n\r\n\r\n-------- 原始信息 --------\r\n发件人： suomi2024 ***@***.***>\r\n日期: 2025/3/21 18:46 (GMT+08:00)\r\n收件人： intel/ipex-llm ***@***.***>\r\n抄送： dttprofessor ***@***.***>, Author ***@***.***>\r\n主题： Re: [intel/ipex-llm] Unable to fully load model into Vram using ollama zip gpu (Issue",
          "created_at": "2025-03-21T11:57:29Z"
        }
      ]
    },
    {
      "issue_number": 12986,
      "title": "RuntimeError: UR error with LlamaIndex",
      "body": "**Describe the bug**\nWhen trying to use IPEX LLM with LlamaIndex by following their [example](https://docs.llamaindex.ai/en/stable/examples/llm/ipex_llm_gpu/#for-linux-users-with-intel-arc-a-series-gpu) `RuntimeError: UR error` is raised.\n\n```\nTraceback (most recent call last):\n  File \"/media/seth/Second/llama_index/ipex_test.py\", line 52, in <module>\n    resp = llm.chat(messages)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/media/seth/Second/conda_llama-index-1/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 322, in wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/media/seth/Second/conda_llama-index-1/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py\", line 173, in wrapped_llm_chat\n    f_return_val = f(_self, messages, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/media/seth/Second/conda_llama-index-1/lib/python3.11/site-packages/llama_index/llms/ipex_llm/base.py\", line 472, in chat\n    completion_response = self.complete(prompt, formatted=True, **kwargs)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/media/seth/Second/conda_llama-index-1/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 322, in wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/media/seth/Second/conda_llama-index-1/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py\", line 431, in wrapped_llm_predict\n    f_return_val = f(_self, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/media/seth/Second/conda_llama-index-1/lib/python3.11/site-packages/llama_index/llms/ipex_llm/base.py\", line 506, in complete\n    tokens = self._model.generate(\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/media/seth/Second/conda_llama-index-1/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/media/seth/Second/conda_llama-index-1/lib/python3.11/site-packages/ipex_llm/transformers/pipeline_parallel.py\", line 283, in generate\n    return original_generate(self,\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/media/seth/Second/conda_llama-index-1/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/media/seth/Second/conda_llama-index-1/lib/python3.11/site-packages/transformers/generation/utils.py\", line 1359, in generate\n    and torch.sum(inputs_tensor[:, -1] == generation_config.pad_token_id) > 0\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: UR error\n```\n\n\n**How to reproduce**\nFollow the LlamaIndex example by importing `IpexLLM` and loading a model.\n\n```\n# Transform a string into input zephyr-specific input\ndef completion_to_prompt(completion):\n    return f\"<|system|>\\n</s>\\n<|user|>\\n{completion}</s>\\n<|assistant|>\\n\"\n\n\n# Transform a list of chat messages into zephyr-specific input\ndef messages_to_prompt(messages):\n    prompt = \"\"\n    for message in messages:\n        if message.role == \"system\":\n            prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n        elif message.role == \"user\":\n            prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n        elif message.role == \"assistant\":\n            prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n\n    # ensure we start with a system prompt, insert blank if needed\n    if not prompt.startswith(\"<|system|>\\n\"):\n        prompt = \"<|system|>\\n</s>\\n\" + prompt\n\n    # add final assistant prompt\n    prompt = prompt + \"<|assistant|>\\n\"\n\n    return prompt\n\nfrom llama_index.llms.ipex_llm import IpexLLM\n\nllm = IpexLLM.from_model_id(\n    model_name=\"HuggingFaceH4/zephyr-7b-alpha\",\n    tokenizer_name=\"HuggingFaceH4/zephyr-7b-alpha\",\n    context_window=512,\n    max_new_tokens=128,\n    generate_kwargs={\"do_sample\": False},\n    completion_to_prompt=completion_to_prompt,\n    messages_to_prompt=messages_to_prompt,\n    device_map=\"xpu\",\n)\n```\n\n**Screenshots**\nIf applicable, add screenshots to help explain the problem\n\n**Environment information**\nAttached output of `env-check.sh`. I have an iGPU that is not being detected but works(unrelated).\n\n[conda_llama-index-1.txt](https://github.com/user-attachments/files/19363762/conda_llama-index-1.txt)\n\n**Additional context**\nIt seems this issue stems from PyTorch IPEX, trying to perform various tensor operations on XPU results in `RuntimeError: UR error`. The code that causes the error can be fixed by moving the tensor to CPU before the sum operation, but the same error is raised elsewhere.\n\n```\nTraceback (most recent call last):\n  File \"/media/seth/Second/llama_index/ipex_test.py\", line 52, in <module>\n    resp = llm.chat(messages)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/media/seth/Second/conda_llama-index-1/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 322, in wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/media/seth/Second/conda_llama-index-1/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py\", line 173, in wrapped_llm_chat\n    f_return_val = f(_self, messages, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/media/seth/Second/conda_llama-index-1/lib/python3.11/site-packages/llama_index/llms/ipex_llm/base.py\", line 472, in chat\n    completion_response = self.complete(prompt, formatted=True, **kwargs)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/media/seth/Second/conda_llama-index-1/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 322, in wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/media/seth/Second/conda_llama-index-1/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py\", line 431, in wrapped_llm_predict\n    f_return_val = f(_self, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/media/seth/Second/conda_llama-index-1/lib/python3.11/site-packages/llama_index/llms/ipex_llm/base.py\", line 506, in complete\n    tokens = self._model.generate(\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/media/seth/Second/conda_llama-index-1/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/media/seth/Second/conda_llama-index-1/lib/python3.11/site-packages/ipex_llm/transformers/pipeline_parallel.py\", line 283, in generate\n    return original_generate(self,\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/media/seth/Second/conda_llama-index-1/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/media/seth/Second/conda_llama-index-1/lib/python3.11/site-packages/transformers/generation/utils.py\", line 1474, in generate\n    return self.greedy_search(\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/media/seth/Second/conda_llama-index-1/lib/python3.11/site-packages/transformers/generation/utils.py\", line 2388, in greedy_search\n    next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)\nRuntimeError: UR error\n```\n\n",
      "state": "open",
      "author": "idkSeth",
      "author_type": "User",
      "created_at": "2025-03-20T15:37:08Z",
      "updated_at": "2025-03-20T15:37:08Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12986/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12986",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12986",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:59.627962",
      "comments": []
    },
    {
      "issue_number": 12985,
      "title": "RuntimeErrror ipex_duplicate_import_error",
      "body": "**Describe the bug**\nWhen trying to use IPEX LLM with LlamaIndex by following their [example](https://docs.llamaindex.ai/en/stable/examples/llm/ipex_llm_gpu/#for-linux-users-with-intel-arc-a-series-gpu), a usage error occurs as IPEX is already imported.\n\n\n**How to reproduce**\nFollow the LlamaIndex example by importing `IpexLLM` and loading a model.\n\n```\n# Transform a string into input zephyr-specific input\ndef completion_to_prompt(completion):\n    return f\"<|system|>\\n</s>\\n<|user|>\\n{completion}</s>\\n<|assistant|>\\n\"\n\n\n# Transform a list of chat messages into zephyr-specific input\ndef messages_to_prompt(messages):\n    prompt = \"\"\n    for message in messages:\n        if message.role == \"system\":\n            prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n        elif message.role == \"user\":\n            prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n        elif message.role == \"assistant\":\n            prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n\n    # ensure we start with a system prompt, insert blank if needed\n    if not prompt.startswith(\"<|system|>\\n\"):\n        prompt = \"<|system|>\\n</s>\\n\" + prompt\n\n    # add final assistant prompt\n    prompt = prompt + \"<|assistant|>\\n\"\n\n    return prompt\n\nfrom llama_index.llms.ipex_llm import IpexLLM\n\nllm = IpexLLM.from_model_id(\n    model_name=\"HuggingFaceH4/zephyr-7b-alpha\",\n    tokenizer_name=\"HuggingFaceH4/zephyr-7b-alpha\",\n    context_window=512,\n    max_new_tokens=128,\n    generate_kwargs={\"do_sample\": False},\n    completion_to_prompt=completion_to_prompt,\n    messages_to_prompt=messages_to_prompt,\n    device_map=\"xpu\",\n)\n```\n\n**Screenshots**\nIf applicable, add screenshots to help explain the problem\n\n**Environment information**\nAttached output of `env-check.sh`. I have an iGPU that is not being detected but works(unrelated).\n\n[conda_llama-index-1.txt](https://github.com/user-attachments/files/19363762/conda_llama-index-1.txt)\n\n**Additional context**\nReplacing [`ipex_importer.py`](https://github.com/intel/ipex-llm/blob/main/python/llm/src/ipex_llm/utils/ipex_importer.py) with the version from [BigDL](https://github.com/intel/BigDL/blob/main/python/llm/src/bigdl/llm/utils/ipex_importer.py) fixes this issue.\n\n\n",
      "state": "open",
      "author": "idkSeth",
      "author_type": "User",
      "created_at": "2025-03-20T11:06:02Z",
      "updated_at": "2025-03-20T11:20:18Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12985/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12985",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12985",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:59.627983",
      "comments": [
        {
          "author": "qiyuangong",
          "body": "Setting `BIGDL_CHECK_DUPLICATE_IMPORT=0` in env can also disable this duplicate checker. For example,\n```bash\nexport BIGDL_CHECK_DUPLICATE_IMPORT=0\n```",
          "created_at": "2025-03-20T11:20:17Z"
        }
      ]
    },
    {
      "issue_number": 12975,
      "title": "Docker Image not updated",
      "body": "I'm seeing the Docker images for GPU inference being updated on the Docker Hub, but the images do not cointain any significant change, like updates to Ollama or old Pytorch libraries.\n\nIs it gonna receive a significant update that brings up Ollama to 0.6?",
      "state": "open",
      "author": "tomalta",
      "author_type": "User",
      "created_at": "2025-03-15T17:54:18Z",
      "updated_at": "2025-03-20T02:06:26Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12975/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12975",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12975",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:30:59.836284",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @tomalta, we are currently working on supporting 0.6.x ollama.",
          "created_at": "2025-03-17T02:06:57Z"
        },
        {
          "author": "tomalta",
          "body": "There will be a notification about it? Either on the Github main page or any others? As mentioned there are many updates pushed to the Docker image and they don't have any details in their page. Thanks for your work.",
          "created_at": "2025-03-18T08:28:06Z"
        },
        {
          "author": "sgwhat",
          "body": "Yeah, we will post a notification on the IPEX-LLM GitHub main page :)",
          "created_at": "2025-03-20T02:06:25Z"
        }
      ]
    },
    {
      "issue_number": 12974,
      "title": "TTFT of the distill qwen model is worser than the base model, is it a expected behavior?",
      "body": "Hi, ipex team\n\nI tested TTFT and TPOT for these 2 pairs of models on Core ultra5 125h: \n\nDeepSeek-R1-Distill-Qwen-1.5B vs Qwen2.5-1.5B\nDeepSeek-R1-Distill-Qwen-7B vs Qwen2.5-7B\n\nHow to test:   https://github.com/intel/ipex-llm/tree/main/python/llm/dev/benchmark/all-in-one\nEnv:   Win11  + GPU driver: 32.0.101.6632 + ipex-llm2.2.0b20250228\nConfig: \ninput-output: 1024-512\ntest_api: \"transformer_int4_fp16_gpu_win\"\n\n--------Name----------------------||----1st token avg latency(ms)----||--2+ avg latency(ms/token)--\nDeepSeek-R1-Distill-Qwen-1.5B           622.92                                          20.84\nQwen2.5-1.5B                                        514.1                                            20.44\nDeepSeek-R1-Distill-Qwen-7B              2799.25                                        54.27\nQwen2.5-7B                                           2212.94                                        53.39\n\nFrom my knowledge,  the performance of the distill model should be the same with the base model, but the data shows they are different. Not sure if the testing is valid. Do you know the root cause?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "state": "open",
      "author": "dan20210809",
      "author_type": "User",
      "created_at": "2025-03-14T11:16:11Z",
      "updated_at": "2025-03-20T01:14:21Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12974/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "qiyuangong"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12974",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12974",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:00.067163",
      "comments": [
        {
          "author": "qiyuangong",
          "body": "> Hi, ipex team\n> \n> I tested TTFT and TPOT for these 2 pairs of models on Core ultra5 125h:\n> \n> DeepSeek-R1-Distill-Qwen-1.5B vs Qwen2.5-1.5B DeepSeek-R1-Distill-Qwen-7B vs Qwen2.5-7B\n> \n> How to test: https://github.com/intel/ipex-llm/tree/main/python/llm/dev/benchmark/all-in-one Env: Win11 + GPU",
          "created_at": "2025-03-14T12:58:39Z"
        },
        {
          "author": "dan20210809",
          "body": "@qiyuangong , I retested the models you mentioned, but gap still exists:\n\n--------Name----------------------||----1st token avg latency(ms)----||--2+ avg latency(ms/token)\nDeepSeek-R1-Distill-Qwen-1.5B --------591.69----------------------------32.3\nQwen2.5-Math-1.5B ---------------------489.53------",
          "created_at": "2025-03-16T09:41:06Z"
        },
        {
          "author": "qiyuangong",
          "body": "> [@qiyuangong](https://github.com/qiyuangong) , I retested the models you mentioned, but gap still exists:\n> \n> --------Name----------------------||----1st token avg latency(ms)----||--2+ avg latency(ms/token) DeepSeek-R1-Distill-Qwen-1.5B --------591.69----------------------------32.3 Qwen2.5-Math",
          "created_at": "2025-03-16T10:06:34Z"
        }
      ]
    },
    {
      "issue_number": 12961,
      "title": "torch depend on cuda-libs when installed bigdl-core-cpp in linux",
      "body": "Can fix bigdl-core-cpp to depend torch-cpu to avoid to install cuda-libs?",
      "state": "open",
      "author": "shichang00",
      "author_type": "User",
      "created_at": "2025-03-12T02:41:04Z",
      "updated_at": "2025-03-20T01:14:20Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12961/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "qiyuangong"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12961",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12961",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:00.294313",
      "comments": [
        {
          "author": "qiyuangong",
          "body": "Thank you for your suggestion! :)\n\nWe will specify Torch version (XPU/CPU) in https://github.com/intel/ipex-llm/blob/main/python/llm/setup.py#L327 to avoid depending on cuda-libs.",
          "created_at": "2025-03-12T11:59:46Z"
        }
      ]
    },
    {
      "issue_number": 12978,
      "title": "Gemma 3 Context Shift Causes Gibberish Output (llama.cpp IPEX build)",
      "body": "**Describe the bug**\nThe Intel IPEX-LLM llama.cpp portable build exhibits a bug with Gemma 3 models where the output becomes gibberish after a context shift.  This is the same issue as [ggml-org/llama.cpp#12357](https://github.com/ggerganov/llama.cpp/issues/12357), which has been fixed in the main llama.cpp repository, but the fix does not seem to appear to be incorporated into the current IPEX-LLM build.  The issue occurs when the model reaches its context length and attempts to shift the context window.  The generated text becomes nonsensical, consisting of repeated characters and phrases.\n**How to reproduce**\nSteps to reproduce the error:\n1. Use the current IPEX-LLM llama.cpp build with Gemma 3 support\n2. Start the `llama-server` with the following command (adjust paths as necessary):\n```bash\n    set SYCL_CACHE_PERSISTENT=1\n    set SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=0\n    set ONEAPI_DEVICE_SELECTOR=level_zero:1\n    llama-server -m C:\\LLM\\google_gemma-3-12b-it-Q4_0.gguf -ngl 99 -c 512 -b 512 --temp 0 --seed 0 -n 1000\n ```\n4. I used the built-in web UI, but you can also use the API (as shown in the original issue).  Send the following prompt: `Tell me a 1000 word math proof`\n5. Observe the response. The output text is clear at first but becomes nonsensical and repetitive after some point:\n```\nOkay, let's construct a relatively involved proof, aiming for approximately 1000 words. We'll prove a theorem concerning the prime number theorem, specifically a slightly refined version:\n\nTheorem: Let π(x) be the prime-counting function, which gives the number of primes less than or equal to x. The prime number theorem states that π(x) is asymptotic to x / ln(x) as x approaches infinity. We'll prove a slightly stronger result:\n\nFor x ≥ 17, |π(x) - x / ln(x)| < 2x / ln²(x).\n\nIn simpler terms, we're showing that the difference between the actual number of primes and the approximation x/ln(x) is bounded by 2x/ln²(x). This is a tighter bound than just stating that x/ln(x) is a good approximation, as it provides a measurable error bound.\n\nProof Outline:\n\nBackground - The Prime Number Theorem (PNT) and Chebyshev Functions: We'll introduce the Chebyshev functions θ(x) and ψ(x), which are related to the distribution of primes and are more amenable to analysis than π(x) directly. θ(x) is the sum of the natural logarithms of the primes less than or equal to x, and ψ(x) is the sum of the natural logarithms of all primes less than or equal to x, including with multiplicity (i.e., if p is a prime, it appears in the sum as many times as it appears in the prime factorization of numbers up to x). These functions are closely related to π(x).\n\nKey Lemma – Chebyshev Function Bound: We'll prove a lemma stating that ψ(x) is bounded by x and -x/2, and that x - ψ(x) is not too large. This is a crucial step in establishing the error bound.\n\nConnecting ψ(x) to π(x): We'll use known relationships between ψ(x) and π(x) to express π(x) in terms of ψ(x).\n\nThe Error Bound: Finally, we'll combine the bounds on ψ(x) with the expression for π(x) to obtain the ψ(and ψ(x - ψ(x - ψ(x andψ(ψ(and ψ( ψ( ψ(ψ(and to and ψ(ψ(ψ(ψ( ψ(ψ(ψ(ψ(ψ(ψ( ψ(ψ(ψ(ψ(ψ(ψ(ψ(ψ( ψ(ψ(ψ(ψ(ψ(x and ψ(ψ(ψ(ψ(ψ( ψ(ψ( ψ(ψ(ψ(ψ( ψ(ψ(ψ( and ψ( ψ and ψ(ψ(ψ ψ and ψ(ψ( ψ( ψ and the error( and ψ and, and of ψ and ψ(ψ ψ(ψ( and ψ π(ψ ψ ψ and of ψ( ψ ψ ψ and ψ ofψ and ψ and of and ψ and to derive the error,ψ and to and ψ and to get the error and of ψ and ψ and ψ and to and ψψ and and to ofψ of the errorψ and to obtain the desired error and of and and to and to derive and of and of to get and of to derive the desired error of the error bound.\n\nProof:\n\nChebyshev Functions:\nθ and θ, θ and θ the actual value, the.\n2, θ, value\n\n2\n\n2, and θ and\n\n2,\n\n2,\n\n2\n\n2, which is,\n\n2,\n\n2 and the,\n\n2\n\n2\n\n2 and θ\n\n2\n\n2\n\n2\n\n2\n\n2\n\n2\n\n2\n```\n\n**Server log output**\n```\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\nggml_sycl_init: SYCL_USE_XMX: yes\nggml_sycl_init: found 1 SYCL devices:\nbuild: 1 (4cfa0b8) with MSVC 19.38.33133.0 for\nsystem info: n_threads = 14, n_threads_batch = 14, total_threads = 20\n\nsystem_info: n_threads = 14 (n_threads_batch = 14) / 20 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 |\n\nmain: HTTP server is listening, hostname: 127.0.0.1, port: 8080, http threads: 19\nmain: loading model\nsrv    load_model: loading model 'C:\\LLM\\google_gemma-3-12b-it-Q4_0.gguf'\nllama_load_model_from_file: using device SYCL0 (Intel(R) Arc(TM) A770M Graphics) - 15473 MiB free\nllama_model_loader: loaded meta data with 44 key-value pairs and 626 tensors from C:\\LLM\\google_gemma-3-12b-it-Q4_0.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Gemma 3 12b It\nllama_model_loader: - kv   3:                           general.finetune str              = it\nllama_model_loader: - kv   4:                           general.basename str              = gemma-3\nllama_model_loader: - kv   5:                         general.size_label str              = 12B\nllama_model_loader: - kv   6:                            general.license str              = gemma\nllama_model_loader: - kv   7:                   general.base_model.count u32              = 1\nllama_model_loader: - kv   8:                  general.base_model.0.name str              = Gemma 3 12b Pt\nllama_model_loader: - kv   9:          general.base_model.0.organization str              = Google\nllama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/google/gemma-3...\nllama_model_loader: - kv  11:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\nllama_model_loader: - kv  12:                      gemma3.context_length u32              = 131072\nllama_model_loader: - kv  13:                    gemma3.embedding_length u32              = 3840\nllama_model_loader: - kv  14:                         gemma3.block_count u32              = 48\nllama_model_loader: - kv  15:                 gemma3.feed_forward_length u32              = 15360\nllama_model_loader: - kv  16:                gemma3.attention.head_count u32              = 16\nllama_model_loader: - kv  17:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  18:                gemma3.attention.key_length u32              = 256\nllama_model_loader: - kv  19:              gemma3.attention.value_length u32              = 256\nllama_model_loader: - kv  20:                      gemma3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  21:            gemma3.attention.sliding_window u32              = 1024\nllama_model_loader: - kv  22:             gemma3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  23:                   gemma3.rope.scaling.type str              = linear\nllama_model_loader: - kv  24:                 gemma3.rope.scaling.factor f32              = 8.000000\nllama_model_loader: - kv  25:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  26:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  27:                      tokenizer.ggml.tokens arr[str,262144]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\nllama_model_loader: - kv  28:                      tokenizer.ggml.scores arr[f32,262144]  = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  31:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  32:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  36:                    tokenizer.chat_template str              = {{ bos_token }}\\n{%- if messages[0]['r...\nllama_model_loader: - kv  37:            tokenizer.ggml.add_space_prefix bool             = false\nllama_model_loader: - kv  38:               general.quantization_version u32              = 2\nllama_model_loader: - kv  39:                          general.file_type u32              = 2\nllama_model_loader: - kv  40:                      quantize.imatrix.file str              = /models_out/gemma-3-12b-it-GGUF/googl...\nllama_model_loader: - kv  41:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\nllama_model_loader: - kv  42:             quantize.imatrix.entries_count i32              = 336\nllama_model_loader: - kv  43:              quantize.imatrix.chunks_count i32              = 129\nllama_model_loader: - type  f32:  289 tensors\nllama_model_loader: - type q4_0:  330 tensors\nllama_model_loader: - type q4_1:    6 tensors\nllama_model_loader: - type q6_K:    1 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 6414\nllm_load_vocab: token to piece cache size = 1.9446 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = gemma3\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 262144\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 131072\nllm_load_print_meta: n_embd           = 3840\nllm_load_print_meta: n_layer          = 48\nllm_load_print_meta: n_head           = 16\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_rot            = 256\nllm_load_print_meta: n_swa            = 1024\nllm_load_print_meta: n_embd_head_k    = 256\nllm_load_print_meta: n_embd_head_v    = 256\nllm_load_print_meta: n_gqa            = 2\nllm_load_print_meta: n_embd_k_gqa     = 2048\nllm_load_print_meta: n_embd_v_gqa     = 2048\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: f_attn_scale     = 6.2e-02\nllm_load_print_meta: n_ff             = 15360\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 2\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 1000000.0\nllm_load_print_meta: freq_scale_train = 0.125\nllm_load_print_meta: n_ctx_orig_yarn  = 131072\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 12B\nllm_load_print_meta: model ftype      = Q4_0\nllm_load_print_meta: model params     = 11.77 B\nllm_load_print_meta: model size       = 6.43 GiB (4.69 BPW)\nllm_load_print_meta: general.name     = Gemma 3 12b It\nllm_load_print_meta: BOS token        = 2 '<bos>'\nllm_load_print_meta: EOS token        = 1 '<eos>'\nllm_load_print_meta: EOT token        = 106 '<end_of_turn>'\nllm_load_print_meta: UNK token        = 3 '<unk>'\nllm_load_print_meta: PAD token        = 0 '<pad>'\nllm_load_print_meta: LF token         = 248 '<0x0A>'\nllm_load_print_meta: EOG token        = 1 '<eos>'\nllm_load_print_meta: EOG token        = 106 '<end_of_turn>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: offloading 48 repeating layers to GPU\nllm_load_tensors: offloading output layer to GPU\nllm_load_tensors: offloaded 49/49 layers to GPU\nllm_load_tensors:        SYCL0 model buffer size =  6582.82 MiB\nllm_load_tensors:   CPU_Mapped model buffer size =   787.50 MiB\n.................................................................................\nllama_new_context_with_model: n_seq_max     = 1\nllama_new_context_with_model: n_ctx         = 512\nllama_new_context_with_model: n_ctx_per_seq = 512\nllama_new_context_with_model: n_batch       = 512\nllama_new_context_with_model: n_ubatch      = 512\nllama_new_context_with_model: flash_attn    = 0\nllama_new_context_with_model: freq_base     = 1000000.0\nllama_new_context_with_model: freq_scale    = 0.125\nllama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n[SYCL] call ggml_check_sycl\nggml_check_sycl: GGML_SYCL_DEBUG: 0\nggml_check_sycl: GGML_SYCL_F16: no\nFound 1 SYCL devices:\n|  |                   |                                       |       |Max    |        |Max  |Global |\n    |\n|  |                   |                                       |       |compute|Max work|sub  |mem    |\n    |\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\n| 0| [level_zero:gpu:0]|               Intel Arc A770M Graphics|  12.55|    512|    1024|   32| 16704M|            1.6.32413|\nllama_kv_cache_init:      SYCL0 KV buffer size =   192.00 MiB\nllama_new_context_with_model: KV self size  =  192.00 MiB, K (f16):   96.00 MiB, V (f16):   96.00 MiB\nllama_new_context_with_model:  SYCL_Host  output buffer size =     1.00 MiB\nllama_new_context_with_model:      SYCL0 compute buffer size =   519.50 MiB\nllama_new_context_with_model:  SYCL_Host compute buffer size =     9.51 MiB\nllama_new_context_with_model: graph nodes  = 1975 (with bs=512), 1831 (with bs=1)\nllama_new_context_with_model: graph splits = 2\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nsrv          init: initializing slots, n_slots = 1\nslot         init: id  0 | task -1 | new slot n_ctx_slot = 512\nmain: model loaded\nmain: chat template, built_in: 1, chat_example: '<start_of_turn>user\nYou are a helpful assistant\n\nHello<end_of_turn>\n<start_of_turn>model\nHi there<end_of_turn>\n<start_of_turn>user\nHow are you?<end_of_turn>\n<start_of_turn>model\n'\nmain: server is listening on http://127.0.0.1:8080 - starting the main loop\nsrv  update_slots: all slots are idle\nrequest: GET /v1/chat/completions 127.0.0.1 404\nrequest: GET / 127.0.0.1 200\nslot launch_slot_: id  0 | task 0 | processing task\nslot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 20\nslot update_slots: id  0 | task 0 | kv cache rm [0, end)\nslot update_slots: id  0 | task 0 | prompt processing progress, n_past = 20, n_tokens = 20, progress = 1.000000\nslot update_slots: id  0 | task 0 | prompt done, n_past = 20, n_tokens = 20\nslot update_slots: id  0 | task 0 | slot context shift, n_keep = 1, n_left = 510, n_discard = 255\nslot update_slots: id  0 | task 0 | slot context shift, n_keep = 1, n_left = 510, n_discard = 255\nsrv  cancel_tasks: cancel task, id_task = 0\nrequest: POST /v1/chat/completions 127.0.0.1 200\nslot      release: id  0 | task 0 | stop processing: n_past = 328, truncated = 1\nsrv  update_slots: all slots are idle\n```\n",
      "state": "closed",
      "author": "Sketchfellow",
      "author_type": "User",
      "created_at": "2025-03-17T06:56:20Z",
      "updated_at": "2025-03-18T03:13:19Z",
      "closed_at": "2025-03-18T03:13:18Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12978/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12978",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12978",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:00.472877",
      "comments": [
        {
          "author": "cyita",
          "body": "Hi Sketchfellow,\n\nThank you for reporting this issue. We're working on a fix and will provide updates once it's ready. In the meantime, you can try setting a larger `-c` as a workaround.",
          "created_at": "2025-03-17T09:04:58Z"
        },
        {
          "author": "cyita",
          "body": "Hi Sketchfellow,\n\nThe fix is now available in bigdl-core-cpp 2.6.0b20250317. You can follow the instructions in [this guide](https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/llama_cpp_quickstart.md) to download the latest ipex-llm[cpp] support and give it a try.",
          "created_at": "2025-03-18T02:21:47Z"
        },
        {
          "author": "Sketchfellow",
          "body": "Thank you! The model now runs coherently after a context shift. ",
          "created_at": "2025-03-18T03:13:18Z"
        }
      ]
    },
    {
      "issue_number": 12914,
      "title": "Ollama Portable Zip on Windows with Intel ARC B580 and nomic-embed-text wsarecv an existing connection was forcibly closed by the remote host",
      "body": "Ollama Portable Zip on Windows with Intel ARC B580 and nomic-embed-text:\n\nERROR source=routes.go:479 msg=\"embedding generation failed\" error=\"do embedding request: Post \\\"http://127.0.0.1:55895/embedding\\\": read tcp 127.0.0.1:55905->127.0.0.1:55895: wsarecv: Eine vorhandene Verbindung wurde vom Remotehost geschlossen.\"\n\n\n\nFound 1 SYCL devices:\n|  |                   |                                       |       |Max    |        |Max  |Global |\n    |\n|  |                   |                                       |       |compute|Max work|sub  |mem    |\n    |\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\n| 0| [level_zero:gpu:0]|                Intel Arc B580 Graphics|   20.1|    160|    1024|   32| 12508M|            1.6.31896|\nllama_kv_cache_init:      SYCL0 KV buffer size =   288.00 MiB\nllama_new_context_with_model: KV self size  =  288.00 MiB, K (i8):  144.00 MiB, V (i8):  144.00 MiB\nllama_new_context_with_model:  SYCL_Host  output buffer size =     0.00 MiB\nllama_new_context_with_model:      SYCL0 compute buffer size =    17.50 MiB\nllama_new_context_with_model:  SYCL_Host compute buffer size =     3.50 MiB\nllama_new_context_with_model: graph nodes  = 429\nllama_new_context_with_model: graph splits = 4 (with bs=512), 2 (with bs=1)\ntime=2025-02-28T17:21:20.882+01:00 level=WARN source=runner.go:892 msg=\"%s: warming up the model with an empty run - please wait ... \" !BADKEY=loadModel\ntime=2025-02-28T17:21:20.922+01:00 level=INFO source=server.go:610 msg=\"llama runner started in 3.02 seconds\"\nllama_load_model_from_file: using device SYCL0 (Intel(R) Arc(TM) B580 Graphics) - 8392 MiB free\nllama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from C:\\Users\\Admin\\.ollama\\models\\blobs\\sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = nomic-bert\nllama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5\nllama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12\nllama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048\nllama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768\nllama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072\nllama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12\nllama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000\nllama_model_loader: - kv   8:                          general.file_type u32              = 1\nllama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false\nllama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1\nllama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000\nllama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2\nllama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101\nllama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102\nllama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert\nllama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = [\"[PAD]\", \"[unused0]\", \"[unused1]\", \"...\nllama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1,1, 1, 1, ...\nllama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100\nllama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101\nllama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103\nllama_model_loader: - type  f32:   51 tensors\nllama_model_loader: - type  f16:   61 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 5\nllm_load_vocab: token to piece cache size = 0.2032 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = nomic-bert\nllm_load_print_meta: vocab type       = WPM\nllm_load_print_meta: n_vocab          = 30522\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 1\nllm_load_print_meta: model type       = ?B\nllm_load_print_meta: model ftype      = all F32\nllm_load_print_meta: model params     = 136.73 M\nllm_load_print_meta: model size       = 260.86 MiB (16.00 BPW)\nllm_load_print_meta: general.name     = nomic-embed-text-v1.5\nllm_load_print_meta: BOS token        = 101 '[CLS]'\nllm_load_print_meta: EOS token        = 102 '[SEP]'\nllm_load_print_meta: UNK token        = 100 '[UNK]'\nllm_load_print_meta: SEP token        = 102 '[SEP]'\nllm_load_print_meta: PAD token        = 0 '[PAD]'\nllm_load_print_meta: CLS token        = 101 '[CLS]'\nllm_load_print_meta: MASK token       = 103 '[MASK]'\nllm_load_print_meta: LF token         = 0 '[PAD]'\nllm_load_print_meta: EOG token        = 102 '[SEP]'\nllm_load_print_meta: max token length = 21\nllama_model_load: vocab only - skipping tensors\nException 0xc000001d 0x0 0x0 0x7ffa1a5c2a16\nPC=0x7ffa1a5c2a16\nsignal arrived during external code execution\n\nruntime.cgocall(0x7ff77812c940, 0xc00040bb90)\n        runtime/cgocall.go:167 +0x3e fp=0xc00040bb68 sp=0xc00040bb00 pc=0x7ff777579c1e\nollama/llama/llamafile._Cfunc_llama_decode(0x21cfa01d520, {0x1, 0x21cf9ea45d0, 0x0, 0x0, 0x21cfa0c62e0, 0x21cfa24c550, 0x21cf9ea7620, 0x21cf9e5dd70})\n        _cgo_gotypes.go:550 +0x55 fp=0xc00040bb90 sp=0xc00040bb68 pc=0x7ff777950735\nollama/llama/llamafile.(*Context).Decode.func1(0x7ff77795f8eb?, 0x21cfa01d520?)\n        ollama/llama/llamafile/llama.go:143 +0xf5 fp=0xc00040bc80 sp=0xc00040bb90 pc=0x7ff777953715\nollama/llama/llamafile.(*Context).Decode(0x7ff778da3400?, 0x0?)\n        ollama/llama/llamafile/llama.go:143 +0x13 fp=0xc00040bcc8 sp=0xc00040bc80 pc=0x7ff777953593\nollama/llama/runner.(*Server).processBatch(0xc000121560, 0xc0002002a0, 0xc00040bf20)\n        ollama/llama/runner/runner.go:434 +0x23f fp=0xc00040bee0 sp=0xc00040bcc8 pc=0x7ff77795e5bf\nollama/llama/runner.(*Server).run(0xc000121560, {0x7ff77856c360, 0xc0000f12c0})\n        ollama/llama/runner/runner.go:342 +0x1d5 fp=0xc00040bfb8 sp=0xc00040bee0 pc=0x7ff77795dff5\nollama/llama/runner.Execute.gowrap2()\n        ollama/llama/runner/runner.go:1006 +0x28 fp=0xc00040bfe0 sp=0xc00040bfb8 pc=0x7ff777963268\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00040bfe8 sp=0xc00040bfe0 pc=0x7ff777588901\ncreated by ollama/llama/runner.Execute in goroutine 1\n        ollama/llama/runner/runner.go:1006 +0xde5\n\ngoroutine 1 gp=0xc00005c000 m=nil [IO wait]:\nruntime.gopark(0x7ff77758a0c0?, 0x7ff778d32ac0?, 0x20?, 0xf?, 0xc000530fcc?)\n        runtime/proc.go:424 +0xce fp=0xc00037d418 sp=0xc00037d3f8 pc=0x7ff7775803ce\nruntime.netpollblock(0x3ec?, 0x77518366?, 0xf7?)\n        runtime/netpoll.go:575 +0xf7 fp=0xc00037d450 sp=0xc00037d418 pc=0x7ff777544f97\ninternal/poll.runtime_pollWait(0x21c9e5db670, 0x72)\n        runtime/netpoll.go:351 +0x85 fp=0xc00037d470 sp=0xc00037d450 pc=0x7ff77757f645\ninternal/poll.(*pollDesc).wait(0x7ff777612bd5?, 0x7ff77757ae7d?, 0x0)\n        internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc00037d498 sp=0xc00037d470 pc=0x7ff777614207\ninternal/poll.execIO(0xc000530f20, 0xc000375540)\n        internal/poll/fd_windows.go:177 +0x105 fp=0xc00037d510 sp=0xc00037d498 pc=0x7ff777615645\ninternal/poll.(*FD).acceptOne(0xc000530f08, 0x3d4, {0xc0005580f0?, 0xc0003755a0?, 0x7ff77761d3c5?}, 0xc0003755d4?)\n        internal/poll/fd_windows.go:946 +0x65 fp=0xc00037d570 sp=0xc00037d510 pc=0x7ff777619c85\ninternal/poll.(*FD).Accept(0xc000530f08, 0xc00037d720)\n        internal/poll/fd_windows.go:980 +0x1b6 fp=0xc00037d628 sp=0xc00037d570 pc=0x7ff777619fb6\nnet.(*netFD).accept(0xc000530f08)\n        net/fd_windows.go:182 +0x4b fp=0xc00037d740 sp=0xc00037d628 pc=0x7ff77768082b\nnet.(*TCPListener).accept(0xc0002b6dc0)\n        net/tcpsock_posix.go:159 +0x1e fp=0xc00037d790 sp=0xc00037d740 pc=0x7ff77769699e\nnet.(*TCPListener).Accept(0xc0002b6dc0)\n        net/tcpsock.go:372 +0x30 fp=0xc00037d7c0 sp=0xc00037d790 pc=0x7ff777695750\nnet/http.(*onceCloseListener).Accept(0xc000121b00?)\n        <autogenerated>:1 +0x24 fp=0xc00037d7d8 sp=0xc00037d7c0 pc=0x7ff777910044\nnet/http.(*Server).Serve(0xc0003c53b0, {0x7ff77856a0d0, 0xc0002b6dc0})\n        net/http/server.go:3330 +0x30c fp=0xc00037d908 sp=0xc00037d7d8 pc=0x7ff7778e7fcc\nollama/llama/runner.Execute({0xc0000c6010?, 0x0?, 0x0?})\n        ollama/llama/runner/runner.go:1027 +0x11a9 fp=0xc00037dca8 sp=0xc00037d908 pc=0x7ff777962f49\nollama/cmd.NewCLI.func2(0xc0004ca008?, {0x7ff7783ac40e?, 0x4?, 0x7ff7783ac412?})\n        ollama/cmd/cmd.go:1430 +0x45 fp=0xc00037dcd0 sp=0xc00037dca8 pc=0x7ff77812bda5\ngithub.com/spf13/cobra.(*Command).execute(0xc0004ca008, {0xc0004ee0f0, 0xf, 0xf})\n        github.com/spf13/cobra@v1.8.1/command.go:985 +0xaaa fp=0xc00037de58 sp=0xc00037dcd0 pc=0x7ff77771a4ea\ngithub.com/spf13/cobra.(*Command).ExecuteC(0xc00053c308)\n        github.com/spf13/cobra@v1.8.1/command.go:1117 +0x3ff fp=0xc00037df30 sp=0xc00037de58 pc=0x7ff77771adbf\ngithub.com/spf13/cobra.(*Command).Execute(...)\n        github.com/spf13/cobra@v1.8.1/command.go:1041\ngithub.com/spf13/cobra.(*Command).ExecuteContext(...)\n        github.com/spf13/cobra@v1.8.1/command.go:1034\nmain.main()\n        ollama/main.go:12 +0x4d fp=0xc00037df50 sp=0xc00037df30 pc=0x7ff77812c40d\nruntime.main()\n        runtime/proc.go:272 +0x27d fp=0xc00037dfe0 sp=0xc00037df50 pc=0x7ff77754df9d\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00037dfe8 sp=0xc00037dfe0 pc=0x7ff777588901\n\ngoroutine 2 gp=0xc00005c700 m=nil [force gc (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc00005ffa8 sp=0xc00005ff88 pc=0x7ff7775803ce\nruntime.goparkunlock(...)\n        runtime/proc.go:430\nruntime.forcegchelper()\n        runtime/proc.go:337 +0xb8 fp=0xc00005ffe0 sp=0xc00005ffa8 pc=0x7ff77754e2b8\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00005ffe8 sp=0xc00005ffe0 pc=0x7ff777588901\ncreated by runtime.init.7 in goroutine 1\n        runtime/proc.go:325 +0x1a\n\ngoroutine 3 gp=0xc00005ca80 m=nil [GC sweep wait]:\nruntime.gopark(0x1?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc000061f80 sp=0xc000061f60 pc=0x7ff7775803ce\nruntime.goparkunlock(...)\n        runtime/proc.go:430\nruntime.bgsweep(0xc00006c000)\n        runtime/mgcsweep.go:317 +0xdf fp=0xc000061fc8 sp=0xc000061f80 pc=0x7ff777536f9f\nruntime.gcenable.gowrap1()\n        runtime/mgc.go:204 +0x25 fp=0xc000061fe0 sp=0xc000061fc8 pc=0x7ff77752b5c5\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000061fe8 sp=0xc000061fe0 pc=0x7ff777588901\ncreated by runtime.gcenable in goroutine 1\n        runtime/mgc.go:204 +0x66\n\ngoroutine 4 gp=0xc00005cc40 m=nil [GC scavenge wait]:\nruntime.gopark(0x10000?, 0x7ff778559520?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc000073f78 sp=0xc000073f58 pc=0x7ff7775803ce\nruntime.goparkunlock(...)\n        runtime/proc.go:430\nruntime.(*scavengerState).park(0x7ff778d56940)\n        runtime/mgcscavenge.go:425 +0x49 fp=0xc000073fa8 sp=0xc000073f78 pc=0x7ff777534969\nruntime.bgscavenge(0xc00006c000)\n        runtime/mgcscavenge.go:658 +0x59 fp=0xc000073fc8 sp=0xc000073fa8 pc=0x7ff777534ef9\nruntime.gcenable.gowrap2()\n        runtime/mgc.go:205 +0x25 fp=0xc000073fe0 sp=0xc000073fc8 pc=0x7ff77752b565\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000073fe8 sp=0xc000073fe0 pc=0x7ff777588901\ncreated by runtime.gcenable in goroutine 1\n        runtime/mgc.go:205 +0xa5\n\ngoroutine 18 gp=0xc000086380 m=nil [finalizer wait]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc00006fe20 sp=0xc00006fe00 pc=0x7ff7775803ce\nruntime.runfinq()\n        runtime/mfinal.go:193 +0x107 fp=0xc00006ffe0 sp=0xc00006fe20 pc=0x7ff77752a687\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00006ffe8 sp=0xc00006ffe0 pc=0x7ff777588901\ncreated by runtime.createfing in goroutine 1\n        runtime/mfinal.go:163 +0x3d\n\ngoroutine 19 gp=0xc000087500 m=nil [chan receive]:\nruntime.gopark(0xc000063f60?, 0x7ff77766a2e5?, 0xe0?, 0x27?, 0x7ff778580260?)\n        runtime/proc.go:424 +0xce fp=0xc000063f18 sp=0xc000063ef8 pc=0x7ff7775803ce\nruntime.chanrecv(0xc000088380, 0x0, 0x1)\n        runtime/chan.go:639 +0x41e fp=0xc000063f90 sp=0xc000063f18 pc=0x7ff77751ac9e\nruntime.chanrecv1(0x7ff77754e100?, 0xc000063f76?)\n        runtime/chan.go:489 +0x12 fp=0xc000063fb8 sp=0xc000063f90 pc=0x7ff77751a852\nruntime.unique_runtime_registerUniqueMapCleanup.func1(...)\n        runtime/mgc.go:1781\nruntime.unique_runtime_registerUniqueMapCleanup.gowrap1()\n        runtime/mgc.go:1784 +0x2f fp=0xc000063fe0 sp=0xc000063fb8 pc=0x7ff77752e6af\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000063fe8 sp=0xc000063fe0 pc=0x7ff777588901\ncreated by unique.runtime_registerUniqueMapCleanup in goroutine 1\n        runtime/mgc.go:1779 +0x96\n\ngoroutine 34 gp=0xc0003ce1c0 m=nil [GC worker (idle)]:\nruntime.gopark(0x44ed30f908?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc00040df38 sp=0xc00040df18 pc=0x7ff7775803ce\nruntime.gcBgMarkWorker(0xc000406000)\n        runtime/mgc.go:1412 +0xe9 fp=0xc00040dfc8 sp=0xc00040df38 pc=0x7ff77752d9a9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc00040dfe0 sp=0xc00040dfc8 pc=0x7ff77752d885\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00040dfe8 sp=0xc00040dfe0 pc=0x7ff777588901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 5 gp=0xc00005d180 m=nil [GC worker (idle)]:\nruntime.gopark(0x44ed2a5bac?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc000075f38 sp=0xc000075f18 pc=0x7ff7775803ce\nruntime.gcBgMarkWorker(0xc000406000)\n        runtime/mgc.go:1412 +0xe9 fp=0xc000075fc8 sp=0xc000075f38 pc=0x7ff77752d9a9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc000075fe0 sp=0xc000075fc8 pc=0x7ff77752d885\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000075fe8 sp=0xc000075fe0 pc=0x7ff777588901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 20 gp=0xc0000876c0 m=nil [GC worker (idle)]:\nruntime.gopark(0x44ed2a5bac?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc000071f38 sp=0xc000071f18 pc=0x7ff7775803ce\nruntime.gcBgMarkWorker(0xc000406000)\n        runtime/mgc.go:1412 +0xe9 fp=0xc000071fc8 sp=0xc000071f38 pc=0x7ff77752d9a9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc000071fe0 sp=0xc000071fc8 pc=0x7ff77752d885\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000071fe8 sp=0xc000071fe0 pc=0x7ff777588901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 35 gp=0xc0003ce380 m=nil [GC worker (idle)]:\nruntime.gopark(0x44ed2a5bac?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc00040ff38 sp=0xc00040ff18 pc=0x7ff7775803ce\nruntime.gcBgMarkWorker(0xc000406000)\n        runtime/mgc.go:1412 +0xe9 fp=0xc00040ffc8 sp=0xc00040ff38 pc=0x7ff77752d9a9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc00040ffe0 sp=0xc00040ffc8 pc=0x7ff77752d885\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00040ffe8 sp=0xc00040ffe0 pc=0x7ff777588901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 15 gp=0xc0003ce000 m=nil [chan receive]:\nruntime.gopark(0x7ff777586917?, 0xc00024b898?, 0xd6?, 0xfd?, 0xc00024b880?)\n        runtime/proc.go:424 +0xce fp=0xc00024b860 sp=0xc00024b840 pc=0x7ff7775803ce\nruntime.chanrecv(0xc000354150, 0xc00024ba10, 0x1)\n        runtime/chan.go:639 +0x41e fp=0xc00024b8d8 sp=0xc00024b860 pc=0x7ff77751ac9e\nruntime.chanrecv1(0xc000146240?, 0xc00020c808?)\n        runtime/chan.go:489 +0x12 fp=0xc00024b900 sp=0xc00024b8d8 pc=0x7ff77751a852\nollama/llama/runner.(*Server).embeddings(0xc000121560, {0x7ff77856a2b0, 0xc0000bc0e0}, 0xc000014140)\n        ollama/llama/runner/runner.go:791 +0x746 fp=0xc00024bac0 sp=0xc00024b900 pc=0x7ff777961086\nollama/llama/runner.(*Server).embeddings-fm({0x7ff77856a2b0?, 0xc0000bc0e0?}, 0x7ff7778f1da7?)\n        <autogenerated>:1 +0x36 fp=0xc00024baf0 sp=0xc00024bac0 pc=0x7ff777963a96\nnet/http.HandlerFunc.ServeHTTP(0xc0004376c0?, {0x7ff77856a2b0?, 0xc0000bc0e0?}, 0x67c1e282?)\n        net/http/server.go:2220 +0x29 fp=0xc00024bb18 sp=0xc00024baf0 pc=0x7ff7778e45c9\nnet/http.(*ServeMux).ServeHTTP(0x7ff777521b65?, {0x7ff77856a2b0, 0xc0000bc0e0}, 0xc000014140)\n        net/http/server.go:2747 +0x1ca fp=0xc00024bb68 sp=0xc00024bb18 pc=0x7ff7778e64ca\nnet/http.serverHandler.ServeHTTP({0x7ff778566e50?}, {0x7ff77856a2b0?, 0xc0000bc0e0?}, 0x6?)\n        net/http/server.go:3210 +0x8e fp=0xc00024bb98 sp=0xc00024bb68 pc=0x7ff777903a2e\nnet/http.(*conn).serve(0xc000121b00, {0x7ff77856c328, 0xc000456c30})\n        net/http/server.go:2092 +0x5d0 fp=0xc00024bfb8 sp=0xc00024bb98 pc=0x7ff7778e2f70\nnet/http.(*Server).Serve.gowrap3()\n        net/http/server.go:3360 +0x28 fp=0xc00024bfe0 sp=0xc00024bfb8 pc=0x7ff7778e83c8\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00024bfe8 sp=0xc00024bfe0 pc=0x7ff777588901\ncreated by net/http.(*Server).Serve in goroutine 1\n        net/http/server.go:3360 +0x485\n\ngoroutine 66 gp=0xc000174000 m=nil [IO wait]:\nruntime.gopark(0x0?, 0xc0005311a0?, 0x48?, 0x12?, 0xc00053124c?)\n        runtime/proc.go:424 +0xce fp=0xc0004f7d20 sp=0xc0004f7d00 pc=0x7ff7775803ce\nruntime.netpollblock(0x3d8?, 0x77518366?, 0xf7?)\n        runtime/netpoll.go:575 +0xf7 fp=0xc0004f7d58 sp=0xc0004f7d20 pc=0x7ff777544f97\ninternal/poll.runtime_pollWait(0x21c9e5db558, 0x72)\n        runtime/netpoll.go:351 +0x85 fp=0xc0004f7d78 sp=0xc0004f7d58 pc=0x7ff77757f645\ninternal/poll.(*pollDesc).wait(0xc0004f7dd8?, 0x7ff777526065?, 0x0)\n        internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc0004f7da0 sp=0xc0004f7d78 pc=0x7ff777614207\ninternal/poll.execIO(0xc0005311a0, 0x7ff77842e0a0)\n        internal/poll/fd_windows.go:177 +0x105 fp=0xc0004f7e18 sp=0xc0004f7da0 pc=0x7ff777615645\ninternal/poll.(*FD).Read(0xc000531188, {0xc000192fd1, 0x1, 0x1})\n        internal/poll/fd_windows.go:438 +0x2a7 fp=0xc0004f7ec0 sp=0xc0004f7e18 pc=0x7ff777616347\nnet.(*netFD).Read(0xc000531188, {0xc000192fd1?, 0xc0004f7f48?, 0x7ff777581d10?})\n        net/fd_posix.go:55 +0x25 fp=0xc0004f7f08 sp=0xc0004f7ec0 pc=0x7ff77767e945\nnet.(*conn).Read(0xc000498518, {0xc000192fd1?, 0x0?, 0x7ff778da3400?})\n        net/net.go:189 +0x45 fp=0xc0004f7f50 sp=0xc0004f7f08 pc=0x7ff77768df25\nnet.(*TCPConn).Read(0x7ff778d0aa50?, {0xc000192fd1?, 0x0?, 0x0?})\n        <autogenerated>:1 +0x25 fp=0xc0004f7f80 sp=0xc0004f7f50 pc=0x7ff77769f945\nnet/http.(*connReader).backgroundRead(0xc000192fc0)\n        net/http/server.go:690 +0x37 fp=0xc0004f7fc8 sp=0xc0004f7f80 pc=0x7ff7778dd8f7\nnet/http.(*connReader).startBackgroundRead.gowrap2()\n        net/http/server.go:686 +0x25 fp=0xc0004f7fe0 sp=0xc0004f7fc8 pc=0x7ff7778dd825\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc0004f7fe8 sp=0xc0004f7fe0 pc=0x7ff777588901\ncreated by net/http.(*connReader).startBackgroundRead in goroutine 15\n        net/http/server.go:686 +0xb6\nrax     0x21c87c118e4\nrbx     0x300\nrcx     0x0\nrdx     0xfffffffffffff400\nrdi     0x21c87c10ce0\nrsi     0x0\nrbp     0x56f10fe6e0\nrsp     0x56f10fe6b0\nr8      0x21c87c100e0\nr9      0x300\nr10     0x0\nr11     0x0\nr12     0x0\nr13     0x300\nr14     0x0\nr15     0x300\nrip     0x7ffa1a5c2a16\nrflags  0x10203\ncs      0x33\nfs      0x53\ngs      0x2b\ntime=2025-02-28T17:21:23.030+01:00 level=ERROR source=routes.go:479 msg=\"embedding generation failed\" error=\"do embedding request: Post \\\"http://127.0.0.1:55895/embedding\\\": read tcp 127.0.0.1:55905->127.0.0.1:55895: wsarecv: Eine vorhandene Verbindung wurde vom Remotehost geschlossen.\"\n[GIN] 2025/02/28 - 17:21:23 | 500 |    5.1642929s |  172.27.100.205 | POST     \"/api/embed\"\ntime=2025-02-28T17:21:24.625+01:00 level=INFO source=server.go:104 msg=\"system memory\" total=\"31.9 GiB\" free=\"20.4 GiB\"free_swap=\"17.9 GiB\"\ntime=2025-02-28T17:21:24.626+01:00 level=INFO source=memory.go:356 msg=\"offload to device\" layers.requested=-1 layers.model=13 layers.offload=0 layers.split=\"\" memory.available=\"[20.4 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"352.9 MiB\" memory.required.partial=\"0 B\" memory.required.kv=\"24.0 MiB\" memory.required.allocations=\"[352.9 MiB]\" memory.weights.total=\"240.1 MiB\" memory.weights.repeating=\"195.4 MiB\" memory.weights.nonrepeating=\"44.7 MiB\" memory.graph.full=\"48.0 MiB\" memory.graph.partial=\"48.0 MiB\"\ntime=2025-02-28T17:21:24.631+01:00 level=INFO source=server.go:392 msg=\"starting llama server\" cmd=\"C:\\\\Users\\\\Admin\\\\ollama-0.5.4-ipex-llm\\\\ollama-lib.exe runner --model C:\\\\Users\\\\Admin\\\\.ollama\\\\models\\\\blobs\\\\sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 16384 --batch-size 512 --n-gpu-layers 999 --threads 4 --no-mmap--parallel 1 --port 55960\"\ntime=2025-02-28T17:21:24.639+01:00 level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\ntime=2025-02-28T17:21:24.640+01:00 level=INFO source=server.go:571 msg=\"waiting for llama runner to start responding\"\ntime=2025-02-28T17:21:24.640+01:00 level=INFO source=server.go:605 msg=\"waiting for server to become available\" status=\"llm server error\"",
      "state": "open",
      "author": "DediCATeD88",
      "author_type": "User",
      "created_at": "2025-02-28T16:27:33Z",
      "updated_at": "2025-03-17T16:49:57Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12914/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12914",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12914",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:00.699383",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Which version of protable zip are you running?",
          "created_at": "2025-03-03T01:52:00Z"
        },
        {
          "author": "DediCATeD88",
          "body": "> Which version of protable zip are you running?\n\nWith ollama-0.5.4-ipex-llm-2.2.0b20250226-win.zip i get:\n\nlevel=ERROR source=routes.go:479 msg=\"embedding generation failed\" error=\"do embedding request: Post \\\"http://127.0.0.1:51510/embedding\\\": read tcp 127.0.0.1:51518->127.0.0.1:51510: wsarecv: E",
          "created_at": "2025-03-03T09:54:14Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @DediCATeD88 , I cannot reproduce your issue, nomic-embed-text works well on my Intel B580 windows desktop, and we get exactly the same  version of gpu driver.\n\nBtw, how did you run this model, like windows cmd?",
          "created_at": "2025-03-04T02:29:08Z"
        },
        {
          "author": "DediCATeD88",
          "body": "> Hi [@DediCATeD88](https://github.com/DediCATeD88) , I cannot reproduce your issue, nomic-embed-text works well on my Intel B580 windows desktop, and we get exactly the same version of gpu driver.\n> \n> Btw, how did you run this model, like windows cmd?\n\nStrange. For example DeepSeek R1 7B/8B is run",
          "created_at": "2025-03-04T07:56:55Z"
        },
        {
          "author": "DediCATeD88",
          "body": "> > Hi [@DediCATeD88](https://github.com/DediCATeD88) , I cannot reproduce your issue, nomic-embed-text works well on my Intel B580 windows desktop, and we get exactly the same version of gpu driver.\n> > Btw, how did you run this model, like windows cmd?\n> \n> Strange. For example DeepSeek R1 7B/8B i",
          "created_at": "2025-03-04T17:24:31Z"
        }
      ]
    },
    {
      "issue_number": 12916,
      "title": "error output on NPU",
      "body": "When I run **qwen2.5-7b-sym-int8 and asym-int4** using quickstart, there is an **error output:** perdida perdida perdida perdida perdida perdida perdida perdida perdida perdida perdida perdida perdida perdida perdida perdida perdida perdida perdida perdida perdida perdida perdida perdida perdida perdida perdida perdida perdida perdida perdida perdida.The **llama3-8b** also have same problem.\n However, the output of **sym-int4** is correct. \nMy ipex-llm[npu] version is `2.2.0b20250120`.",
      "state": "open",
      "author": "jzw02",
      "author_type": "User",
      "created_at": "2025-03-01T12:32:35Z",
      "updated_at": "2025-03-17T05:10:08Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12916/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12916",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12916",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:00.924143",
      "comments": [
        {
          "author": "plusbang",
          "body": "Could you please provide more details about your hardware (which CPU model) and NPU driver version?",
          "created_at": "2025-03-04T02:20:02Z"
        },
        {
          "author": "jzw02",
          "body": "CPU：intel Core Ultra 7 155H\n\nNPU：intel AI Boost  / driver version:`32.0.100.3104`",
          "created_at": "2025-03-04T06:31:18Z"
        },
        {
          "author": "MingJerry",
          "body": "<img width=\"690\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/dbb00c38-1d66-4244-af32-1ecc30cc8ca5\" />\nCPU：intel Core Ultra 7 155H\nNPU：intel AI Boost / driver version:32.0.100.3104\nipex-llm[npu] version is 2.2.0b20250315.\nThe model conversion and inference process using the qwen.py sc",
          "created_at": "2025-03-17T05:10:06Z"
        }
      ]
    },
    {
      "issue_number": 12897,
      "title": "ollama-0.5.4-ipex-llm A770 16G Deepseek-R1:14b Deepseek-R1:32b 配置问题",
      "body": "**标题**: 关于Intel GPU运行Ollama-ipex-llm的性能表现与功能咨询\n### 问题背景\n基于开发组发布的 ollama-0.5.4-ipex-llm（20250222 版本），用户通过 Intel GPU（如 A770 16G）在 Windows 11 下实现了免安装部署 DeepSeek 本地大模型。该方案降低了I卡用户在win11下部署DeepSeek本地大模型的门槛，对小白来说十分友好（尤其在 B 站引发广泛关注）。这也使得I卡中A770 16G大显存的优势在蒸馏版DeepSeek本地大模型部署上得到体现。\n\n### 环境描述\n**硬件配置**：\n- CPU: AMD Ryzen 5 5600G (超频)\n- GPU: \n  - 主显卡: Intel Arc A770 16GB (Driver 6460)\n  - 副显卡: NVIDIA P106-100 6GB (未启用)\n- 内存: 48GB DDR4 3200MHz (16+8+16+8 非对称双通道超频)\n\n**软件版本**：\n- 使用编译版本: ollama-0.5.4-ipex-llm (20250222)\n- 操作系统: Windows 11\n\n### 当前配置方案\n```batch\n:: start-ollama.bat 参数设置\n@echo off\nsetlocal\nset OLLAMA_NUM_GPU=999      :: GPU层数调控(A770总层数显示65)\nset no_proxy=localhost,127.0.0.1  :: 本地部署\nset IPEX_LLM_NUM_CTX=16384  :: 扩展上下文长度\nset ZES_ENABLE_SYSMAN=1     :: GPU资源调用\nset SYCL_CACHE_PERSISTENT=1 :: 持久化代码缓存\n@REM set SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1 ::？\nset OLLAMA_KEEP_ALIVE=-1    :: 避免模型重复加载\nset ONEAPI_DEVICE_SELECTOR=level_zero:0 :: 指定GPU0\ncd /d %~dp0\nstart \"\" ollama.exe serve\ncmd /k\n```\n\n### 性能测试数据\n| 模型/参数           | GPU层数 | Prompt速率(t/s) | Eval速率(t/s) | CPU占用 | GPU占用 | 内存占用 |\n|---------------------|---------|-----------------|---------------|---------|---------|----------|\n| DeepSeek-14b(Q4_KM) | 999     | 232.24          | 10.34         | 30%     | 85%     | 22%      |\n| DeepSeek-32b(Q4_KM) | 999     | 8.19            | 1.99          | 18%     | 100%    | 39%      |\n| DeepSeek-14b(Q4_KM) | 26      | 6.24            | 5.98          | 85%     | 30%     | 37%      |\n| DeepSeek-32b(Q4_KM) | 26      | 48.62           | 2.75          | 80%     | 17%     | 54%      |\n| DeepSeek-14b(Q4_KM) | 0       | 11.95           | 4.16          | 100%    | 1%      | 48%      |\n| DeepSeek-32b(Q4_KM) | 0       | 5.12            | 1.92          | 100%    | 1%      | 72%      |\n\n*注：空闲状态资源占用为CPU 15%/GPU 1%/内存 16%*\n\n### 观察总结\n1. 当GPU显存充足时(A770 16G运行14b模型)，推理效率显著提升(eval速率10.34t/s)\n2. 显存不足时(32b模型)，系统自动降级为CPU+内存混合计算，此时性能大幅下降\n\n### 功能咨询\n1. **文档完善请求**  \n   能否提供更详细的参数调优指南？特别是关于：\n   - `OLLAMA_NUM_GPU` 与显存占用的量化关系\n   - `SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS` 的实际作用\n   - 内存/显存分配策略的配置方法等\n\n2. **多GPU支持**  \n   如果配置双A770 16G显卡：\n   - 是否支持显存叠加运行32b模型？\n   - 能否通过SLI/NVLink类技术实现算力聚合？\n   - 多卡环境下`ONEAPI_DEVICE_SELECTOR`的正确配置方式\n\n3. **硬件兼容性**  \n   - 当前版本是否限定仅支持Intel GPU？未来是否计划支持：\n     * AMD GPU (通过ROCm)\n     * NVIDIA GPU (通过CUDA)\n   - 异构显卡(P106+A770)的协同计算可能性\n\n4. **资源监控疑问**  \n   当设置`OLLAMA_NUM_GPU=0`时：\n   - `ollama ps`显示GPU占用19%，但Intel驱动面板无活动\n   - 是否存在监控指标错位？19%是否实际为共享显存占用？\n   - 能否强制禁用内存回退，保持纯CPU+GPU计算模式？\n\n5. **硬件路线图**  \n   据传Intel将推出B770 24GB型号，请问：\n   - 该产品是否在官方路线图中？\n   - 预计何时上市？\n   - 现有架构是否预留了对此新硬件的支持？\n\n6. **单卡性能优化方案咨询**  \n   除调整 GPU 层数外，是否还有其他方式可提升 A770 16G 单卡的推理能力：\n   - 例如？",
      "state": "open",
      "author": "XL-Qing",
      "author_type": "User",
      "created_at": "2025-02-25T14:55:59Z",
      "updated_at": "2025-03-17T03:45:44Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12897/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Oscilloscope98"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12897",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12897",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:01.165106",
      "comments": [
        {
          "author": "Oscilloscope98",
          "body": "Hi @XL-Qing,\n\nThank you for your detailed feedback!\n\nFor problem 1 & 2 (文档完善请求 & 多GPU支持), we will update our documentation soon for detailed information :)\n\nFor problem 3 (硬件兼容性), Ollama with IPEX-LLM optimizations is designed for Intel GPU.\n\nFor problem 4 (资源监控疑问), `ollama ps` is not yet supported ",
          "created_at": "2025-02-26T09:34:46Z"
        },
        {
          "author": "XL-Qing",
          "body": "**软件版本**：\n- 使用编译版本: ollama-0.5.4-ipex-llm (20250222)\n- 操作系统: Windows 11\n- GPU驱动版本: 6078（驱动由6460变为6078后14B模型推理效果提升显著）\n\n### 性能测试数据\n| 模型/参数           | GPU层数 | Prompt速率(t/s) | Eval速率(t/s) | CPU占用 | GPU占用 | 内存占用 |\n|---------------------|---------|-----------------|---------------|---------|---------|---",
          "created_at": "2025-02-27T12:25:21Z"
        },
        {
          "author": "XL-Qing",
          "body": "**软件版本**：\n- 使用编译版本: ollama-0.5.4-ipex-llm2.2.0b (20250226)\n- 操作系统: Windows 11\n- GPU驱动版本: 6078\n\n### 性能测试数据\n| 模型/参数           | GPU层数 | Prompt速率(t/s) | Eval速率(t/s) | CPU占用 | GPU占用 | 内存占用 |\n|---------------------|---------|-----------------|---------------|---------|---------|----------|\n| DeepSeek-14b",
          "created_at": "2025-02-27T12:25:47Z"
        },
        {
          "author": "XL-Qing",
          "body": "> Hi [@XL-Qing](https://github.com/XL-Qing),\n> \n> Thank you for your detailed feedback!\n> \n> For problem 1 & 2 (文档完善请求 & 多GPU支持), we will update our documentation soon for detailed information :)\n> \n> For problem 3 (硬件兼容性), Ollama with IPEX-LLM optimizations is designed for Intel GPU.\n> \n> For probl",
          "created_at": "2025-02-27T12:44:37Z"
        },
        {
          "author": "XL-Qing",
          "body": "**软件版本**：\n- 使用编译版本: ollama-ipex-llm-2.2.0b 20250313-win\n- 操作系统: Windows 11\n- GPU驱动版本: 6647\n\n### 性能测试数据\n| 模型/参数           | GPU层数 | Prompt速率(t/s) | Eval速率(t/s) | CPU占用 | GPU占用 | 内存占用 |\n|---------------------|---------|-----------------|---------------|---------|---------|----------|\n| DeepSeek-14b(Q4",
          "created_at": "2025-03-17T03:45:43Z"
        }
      ]
    },
    {
      "issue_number": 12952,
      "title": "After running a model with high context window normally, offloading it and pulling another model significantly hampers inference speed.",
      "body": "I ran mistral-nemo:12b-instruct-2407-q8_0 with num_ctx = 65536 normally. The dedicated memory usage was full for my Intel Arc A770 16GB, and some shared GPU memory usage was observed (around 8GB). Inference speed was normal. \n\nAfter offloading the model and then running another model, Ollama uses a significant amount of shared memory before fully utilizing dedicated gpu memory, hampering inference speed very significantly. Only restarting the computer resolved the issue. Sometimes flooding the gpu with a 32B model works too. It's a really strange bug.\n\nSteps to produce issue:\n\n1. Run mistral-nemo:12b-instruct-2407-q8_0 with num_ctx = 65536\n2. Generate a few responses. Normal speed and full dedicated GPU memory usage observed. \n3. Offload the model. \n4. Run any model.\n5. Generate a few responses. Extremely slow speed, shared GPU memory usage before dedicated GPU memory was full.\n\nserver.log output:\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\nggml_sycl_init: SYCL_USE_XMX: yes\nggml_sycl_init: found 1 SYCL devices:\n2025/03/07 17:46:08 routes.go:1259: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:true OLLAMA_KEEP_ALIVE:1m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:1 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\\\Users\\\\gabri\\\\.ollama\\\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[* http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]\"\ntime=2025-03-07T17:46:08.503+08:00 level=INFO source=images.go:757 msg=\"total blobs: 87\"\ntime=2025-03-07T17:46:08.511+08:00 level=INFO source=images.go:764 msg=\"total unused blobs removed: 0\"\n[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.\n\n[GIN-debug] [WARNING] Running in \"debug\" mode. Switch to \"release\" mode in production.\n - using env:\texport GIN_MODE=release\n - using code:\tgin.SetMode(gin.ReleaseMode)\n\n[GIN-debug] POST   /api/pull                 --> ollama/server.(*Server).PullHandler-fm (5 handlers)\n[GIN-debug] POST   /api/generate             --> ollama/server.(*Server).GenerateHandler-fm (5 handlers)\n[GIN-debug] POST   /api/chat                 --> ollama/server.(*Server).ChatHandler-fm (5 handlers)\n[GIN-debug] POST   /api/embed                --> ollama/server.(*Server).EmbedHandler-fm (5 handlers)\n[GIN-debug] POST   /api/embeddings           --> ollama/server.(*Server).EmbeddingsHandler-fm (5 handlers)\n[GIN-debug] POST   /api/create               --> ollama/server.(*Server).CreateHandler-fm (5 handlers)\n[GIN-debug] POST   /api/push                 --> ollama/server.(*Server).PushHandler-fm (5 handlers)\n[GIN-debug] POST   /api/copy                 --> ollama/server.(*Server).CopyHandler-fm (5 handlers)\n[GIN-debug] DELETE /api/delete               --> ollama/server.(*Server).DeleteHandler-fm (5 handlers)\n[GIN-debug] POST   /api/show                 --> ollama/server.(*Server).ShowHandler-fm (5 handlers)\n[GIN-debug] POST   /api/blobs/:digest        --> ollama/server.(*Server).CreateBlobHandler-fm (5 handlers)\n[GIN-debug] HEAD   /api/blobs/:digest        --> ollama/server.(*Server).HeadBlobHandler-fm (5 handlers)\n[GIN-debug] GET    /api/ps                   --> ollama/server.(*Server).PsHandler-fm (5 handlers)\n[GIN-debug] POST   /v1/chat/completions      --> ollama/server.(*Server).ChatHandler-fm (6 handlers)\n[GIN-debug] POST   /v1/completions           --> ollama/server.(*Server).GenerateHandler-fm (6 handlers)\n[GIN-debug] POST   /v1/embeddings            --> ollama/server.(*Server).EmbedHandler-fm (6 handlers)\n[GIN-debug] GET    /v1/models                --> ollama/server.(*Server).ListHandler-fm (6 handlers)\n[GIN-debug] GET    /v1/models/:model         --> ollama/server.(*Server).ShowHandler-fm (6 handlers)\n[GIN-debug] GET    /                         --> ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\n[GIN-debug] GET    /api/tags                 --> ollama/server.(*Server).ListHandler-fm (5 handlers)\n[GIN-debug] GET    /api/version              --> ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\n[GIN-debug] HEAD   /                         --> ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\n[GIN-debug] HEAD   /api/tags                 --> ollama/server.(*Server).ListHandler-fm (5 handlers)\n[GIN-debug] HEAD   /api/version              --> ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\ntime=2025-03-07T17:46:08.518+08:00 level=INFO source=routes.go:1310 msg=\"Listening on 127.0.0.1:11434 (version 0.5.4-ipexllm-20250226)\"\ntime=2025-03-07T17:46:08.518+08:00 level=INFO source=routes.go:1339 msg=\"Dynamic LLM libraries\" runners=[ipex_llm]\n[GIN] 2025/03/07 - 17:46:17 | 200 |    127.5508ms |       127.0.0.1 | GET      \"/api/tags\"\n[GIN] 2025/03/07 - 17:46:17 | 200 |       543.5µs |       127.0.0.1 | GET      \"/api/version\"\ntime=2025-03-07T17:46:46.768+08:00 level=WARN source=types.go:509 msg=\"invalid option provided\" option=reasoning_effort\ntime=2025-03-07T17:46:46.768+08:00 level=INFO source=gpu.go:226 msg=\"looking for compatible GPUs\"\ntime=2025-03-07T17:46:46.768+08:00 level=INFO source=gpu_windows.go:167 msg=packages count=1\ntime=2025-03-07T17:46:46.768+08:00 level=INFO source=gpu_windows.go:214 msg=\"\" package=0 cores=8 efficiency=0 threads=8\ntime=2025-03-07T17:46:47.019+08:00 level=INFO source=server.go:104 msg=\"system memory\" total=\"31.9 GiB\" free=\"16.4 GiB\" free_swap=\"34.7 GiB\"\ntime=2025-03-07T17:46:47.019+08:00 level=INFO source=memory.go:356 msg=\"offload to device\" layers.requested=-1 layers.model=41 layers.offload=0 layers.split=\"\" memory.available=\"[16.4 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"26.4 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"10.0 GiB\" memory.required.allocations=\"[15.9 GiB]\" memory.weights.total=\"20.8 GiB\" memory.weights.repeating=\"20.1 GiB\" memory.weights.nonrepeating=\"680.0 MiB\" memory.graph.full=\"4.2 GiB\" memory.graph.partial=\"4.5 GiB\"\ntime=2025-03-07T17:46:47.032+08:00 level=INFO source=server.go:392 msg=\"starting llama server\" cmd=\"C:\\\\Users\\\\gabri\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\ollama-lib.exe runner --model C:\\\\Users\\\\gabri\\\\.ollama\\\\models\\\\blobs\\\\sha256-824229be17606dd8177fc91c1d330b065bc4f3de2873eab614376b988dcbf48a --ctx-size 65536 --batch-size 512 --n-gpu-layers 999 --threads 8 --no-mmap --parallel 1 --port 51315\"\ntime=2025-03-07T17:46:47.084+08:00 level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\ntime=2025-03-07T17:46:47.084+08:00 level=INFO source=server.go:571 msg=\"waiting for llama runner to start responding\"\ntime=2025-03-07T17:46:47.085+08:00 level=INFO source=server.go:605 msg=\"waiting for server to become available\" status=\"llm server error\"\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\nggml_sycl_init: SYCL_USE_XMX: yes\nggml_sycl_init: found 1 SYCL devices:\ntime=2025-03-07T17:46:48.283+08:00 level=INFO source=runner.go:967 msg=\"starting go runner\"\ntime=2025-03-07T17:46:48.284+08:00 level=INFO source=runner.go:968 msg=system info=\"CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | cgo(clang)\" threads=8\ntime=2025-03-07T17:46:48.285+08:00 level=INFO source=runner.go:1026 msg=\"Server listening on 127.0.0.1:51315\"\ntime=2025-03-07T17:46:48.337+08:00 level=INFO source=server.go:605 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nllama_load_model_from_file: using device SYCL0 (Intel(R) Arc(TM) A770 Graphics) - 14765 MiB free\nllama_model_loader: loaded meta data with 35 key-value pairs and 363 tensors from C:\\Users\\gabri\\.ollama\\models\\blobs\\sha256-824229be17606dd8177fc91c1d330b065bc4f3de2873eab614376b988dcbf48a (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Mistral Nemo Instruct 2407\nllama_model_loader: - kv   3:                            general.version str              = 2407\nllama_model_loader: - kv   4:                           general.finetune str              = Instruct\nllama_model_loader: - kv   5:                           general.basename str              = Mistral-Nemo\nllama_model_loader: - kv   6:                         general.size_label str              = 12B\nllama_model_loader: - kv   7:                            general.license str              = apache-2.0\nllama_model_loader: - kv   8:                          general.languages arr[str,9]       = [\"en\", \"fr\", \"de\", \"es\", \"it\", \"pt\", ...\nllama_model_loader: - kv   9:                          llama.block_count u32              = 40\nllama_model_loader: - kv  10:                       llama.context_length u32              = 1024000\nllama_model_loader: - kv  11:                     llama.embedding_length u32              = 5120\nllama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                 llama.attention.key_length u32              = 128\nllama_model_loader: - kv  18:               llama.attention.value_length u32              = 128\nllama_model_loader: - kv  19:                          general.file_type u32              = 7\nllama_model_loader: - kv  20:                           llama.vocab_size u32              = 131072\nllama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  22:            tokenizer.ggml.add_space_prefix bool             = false\nllama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = tekken\nllama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,131072]  = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\nllama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,131072]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,269443]  = [\"Ġ Ġ\", \"Ġ t\", \"e r\", \"i n\", \"Ġ  ...\nllama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  32:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  33:                    tokenizer.chat_template str              = {%- if messages[0]['role'] == 'system...\nllama_model_loader: - kv  34:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   81 tensors\nllama_model_loader: - type q8_0:  282 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 1000\nllm_load_vocab: token to piece cache size = 0.8498 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 131072\nllm_load_print_meta: n_merges         = 269443\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 1024000\nllm_load_print_meta: n_embd           = 5120\nllm_load_print_meta: n_layer          = 40\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 1000000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 1024000\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 13B\nllm_load_print_meta: model ftype      = Q8_0\nllm_load_print_meta: model params     = 12.25 B\nllm_load_print_meta: model size       = 12.12 GiB (8.50 BPW) \nllm_load_print_meta: general.name     = Mistral Nemo Instruct 2407\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 1196 'Ä'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 150\nllm_load_tensors: offloading 40 repeating layers to GPU\nllm_load_tensors: offloading output layer to GPU\nllm_load_tensors: offloaded 41/41 layers to GPU\nllm_load_tensors:        SYCL0 model buffer size = 11731.58 MiB\nllm_load_tensors:    SYCL_Host model buffer size =   680.00 MiB\nllama_new_context_with_model: n_seq_max     = 1\nllama_new_context_with_model: n_ctx         = 65536\nllama_new_context_with_model: n_ctx_per_seq = 65536\nllama_new_context_with_model: n_batch       = 512\nllama_new_context_with_model: n_ubatch      = 512\nllama_new_context_with_model: flash_attn    = 0\nllama_new_context_with_model: freq_base     = 1000000.0\nllama_new_context_with_model: freq_scale    = 1\nllama_new_context_with_model: n_ctx_per_seq (65536) < n_ctx_train (1024000) -- the full capacity of the model will not be utilized\n[SYCL] call ggml_check_sycl\nggml_check_sycl: GGML_SYCL_DEBUG: 0\nggml_check_sycl: GGML_SYCL_F16: no\nFound 1 SYCL devices:\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\n| 0| [level_zero:gpu:0]|                Intel Arc A770 Graphics|  12.55|    512|    1024|   32| 16704M|            1.6.31896|\nllama_kv_cache_init:      SYCL0 KV buffer size = 10240.00 MiB\nllama_new_context_with_model: KV self size  = 10240.00 MiB, K (f16): 5120.00 MiB, V (f16): 5120.00 MiB\nllama_new_context_with_model:  SYCL_Host  output buffer size =     0.52 MiB\nllama_new_context_with_model:      SYCL0 compute buffer size =   266.00 MiB\nllama_new_context_with_model:  SYCL_Host compute buffer size =   138.01 MiB\nllama_new_context_with_model: graph nodes  = 1126\nllama_new_context_with_model: graph splits = 2\ntime=2025-03-07T17:47:08.246+08:00 level=WARN source=runner.go:892 msg=\"%s: warming up the model with an empty run - please wait ... \" !BADKEY=loadModel\ntime=2025-03-07T17:47:08.444+08:00 level=INFO source=server.go:610 msg=\"llama runner started in 21.36 seconds\"\n[GIN] 2025/03/07 - 17:47:13 | 200 |   26.2989845s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/03/07 - 17:47:17 | 200 |       526.2µs |       127.0.0.1 | HEAD     \"/\"\ntime=2025-03-07T17:47:18.827+08:00 level=WARN source=types.go:509 msg=\"invalid option provided\" option=reasoning_effort\n[GIN] 2025/03/07 - 17:47:22 | 200 |    4.2452125s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2025/03/07 - 17:47:43 | 200 |     19.2436ms |       127.0.0.1 | GET      \"/api/tags\"\n[GIN] 2025/03/07 - 17:49:08 | 200 |       522.5µs |       127.0.0.1 | GET      \"/api/version\"\ntime=2025-03-07T17:49:11.448+08:00 level=WARN source=types.go:509 msg=\"invalid option provided\" option=reasoning_effort\ntime=2025-03-07T17:49:11.479+08:00 level=INFO source=server.go:104 msg=\"system memory\" total=\"31.9 GiB\" free=\"15.6 GiB\" free_swap=\"32.6 GiB\"\ntime=2025-03-07T17:49:11.480+08:00 level=INFO source=memory.go:356 msg=\"offload to device\" layers.requested=-1 layers.model=41 layers.offload=0 layers.split=\"\" memory.available=\"[15.6 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"26.4 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"10.0 GiB\" memory.required.allocations=\"[15.4 GiB]\" memory.weights.total=\"20.8 GiB\" memory.weights.repeating=\"20.1 GiB\" memory.weights.nonrepeating=\"680.0 MiB\" memory.graph.full=\"4.2 GiB\" memory.graph.partial=\"4.5 GiB\"\ntime=2025-03-07T17:49:11.487+08:00 level=INFO source=server.go:392 msg=\"starting llama server\" cmd=\"C:\\\\Users\\\\gabri\\\\AppData\\\\Local\\\\Programs\\\\Ollama\\\\ollama-lib.exe runner --model C:\\\\Users\\\\gabri\\\\.ollama\\\\models\\\\blobs\\\\sha256-824229be17606dd8177fc91c1d330b065bc4f3de2873eab614376b988dcbf48a --ctx-size 65536 --batch-size 512 --n-gpu-layers 999 --threads 8 --no-mmap --parallel 1 --port 51861\"\ntime=2025-03-07T17:49:11.491+08:00 level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\ntime=2025-03-07T17:49:11.491+08:00 level=INFO source=server.go:571 msg=\"waiting for llama runner to start responding\"\ntime=2025-03-07T17:49:11.491+08:00 level=INFO source=server.go:605 msg=\"waiting for server to become available\" status=\"llm server error\"\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\nggml_sycl_init: SYCL_USE_XMX: yes\nggml_sycl_init: found 1 SYCL devices:\ntime=2025-03-07T17:49:11.690+08:00 level=INFO source=runner.go:967 msg=\"starting go runner\"\ntime=2025-03-07T17:49:11.690+08:00 level=INFO source=runner.go:968 msg=system info=\"CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | cgo(clang)\" threads=8\ntime=2025-03-07T17:49:11.690+08:00 level=INFO source=runner.go:1026 msg=\"Server listening on 127.0.0.1:51861\"\ntime=2025-03-07T17:49:11.742+08:00 level=INFO source=server.go:605 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nllama_load_model_from_file: using device SYCL0 (Intel(R) Arc(TM) A770 Graphics) - 14600 MiB free\nllama_model_loader: loaded meta data with 35 key-value pairs and 363 tensors from C:\\Users\\gabri\\.ollama\\models\\blobs\\sha256-824229be17606dd8177fc91c1d330b065bc4f3de2873eab614376b988dcbf48a (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Mistral Nemo Instruct 2407\nllama_model_loader: - kv   3:                            general.version str              = 2407\nllama_model_loader: - kv   4:                           general.finetune str              = Instruct\nllama_model_loader: - kv   5:                           general.basename str              = Mistral-Nemo\nllama_model_loader: - kv   6:                         general.size_label str              = 12B\nllama_model_loader: - kv   7:                            general.license str              = apache-2.0\nllama_model_loader: - kv   8:                          general.languages arr[str,9]       = [\"en\", \"fr\", \"de\", \"es\", \"it\", \"pt\", ...\nllama_model_loader: - kv   9:                          llama.block_count u32              = 40\nllama_model_loader: - kv  10:                       llama.context_length u32              = 1024000\nllama_model_loader: - kv  11:                     llama.embedding_length u32              = 5120\nllama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                 llama.attention.key_length u32              = 128\nllama_model_loader: - kv  18:               llama.attention.value_length u32              = 128\nllama_model_loader: - kv  19:                          general.file_type u32              = 7\nllama_model_loader: - kv  20:                           llama.vocab_size u32              = 131072\nllama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  22:            tokenizer.ggml.add_space_prefix bool             = false\nllama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = tekken\nllama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,131072]  = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\nllama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,131072]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,269443]  = [\"Ġ Ġ\", \"Ġ t\", \"e r\", \"i n\", \"Ġ  ...\nllama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  32:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  33:                    tokenizer.chat_template str              = {%- if messages[0]['role'] == 'system...\nllama_model_loader: - kv  34:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   81 tensors\nllama_model_loader: - type q8_0:  282 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 1000\nllm_load_vocab: token to piece cache size = 0.8498 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 131072\nllm_load_print_meta: n_merges         = 269443\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 1024000\nllm_load_print_meta: n_embd           = 5120\nllm_load_print_meta: n_layer          = 40\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 1000000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 1024000\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 13B\nllm_load_print_meta: model ftype      = Q8_0\nllm_load_print_meta: model params     = 12.25 B\nllm_load_print_meta: model size       = 12.12 GiB (8.50 BPW) \nllm_load_print_meta: general.name     = Mistral Nemo Instruct 2407\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 1196 'Ä'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 150\nllm_load_tensors: offloading 40 repeating layers to GPU\nllm_load_tensors: offloading output layer to GPU\nllm_load_tensors: offloaded 41/41 layers to GPU\nllm_load_tensors:        SYCL0 model buffer size = 11731.58 MiB\nllm_load_tensors:    SYCL_Host model buffer size =   680.00 MiB\nllama_new_context_with_model: n_seq_max     = 1\nllama_new_context_with_model: n_ctx         = 65536\nllama_new_context_with_model: n_ctx_per_seq = 65536\nllama_new_context_with_model: n_batch       = 512\nllama_new_context_with_model: n_ubatch      = 512\nllama_new_context_with_model: flash_attn    = 0\nllama_new_context_with_model: freq_base     = 1000000.0\nllama_new_context_with_model: freq_scale    = 1\nllama_new_context_with_model: n_ctx_per_seq (65536) < n_ctx_train (1024000) -- the full capacity of the model will not be utilized\n[SYCL] call ggml_check_sycl\nggml_check_sycl: GGML_SYCL_DEBUG: 0\nggml_check_sycl: GGML_SYCL_F16: no\nFound 1 SYCL devices:\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\n| 0| [level_zero:gpu:0]|                Intel Arc A770 Graphics|  12.55|    512|    1024|   32| 16704M|            1.6.31896|\nllama_kv_cache_init:      SYCL0 KV buffer size = 10240.00 MiB\nllama_new_context_with_model: KV self size  = 10240.00 MiB, K (f16): 5120.00 MiB, V (f16): 5120.00 MiB\nllama_new_context_with_model:  SYCL_Host  output buffer size =     0.52 MiB\nllama_new_context_with_model:      SYCL0 compute buffer size =   266.00 MiB\nllama_new_context_with_model:  SYCL_Host compute buffer size =   138.01 MiB\nllama_new_context_with_model: graph nodes  = 1126\nllama_new_context_with_model: graph splits = 2\ntime=2025-03-07T17:49:32.182+08:00 level=WARN source=runner.go:892 msg=\"%s: warming up the model with an empty run - please wait ... \" !BADKEY=loadModel\ntime=2025-03-07T17:49:32.277+08:00 level=INFO source=server.go:610 msg=\"llama runner started in 20.79 seconds\"\n[GIN] 2025/03/07 - 17:50:08 | 200 |   57.0347614s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "state": "open",
      "author": "gabriel278",
      "author_type": "User",
      "created_at": "2025-03-07T09:55:54Z",
      "updated_at": "2025-03-16T16:15:00Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12952/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12952",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12952",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:01.364095",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Currently, you may restart `ollama serve` instead of offloading loaded model in VRAM.",
          "created_at": "2025-03-11T07:45:12Z"
        },
        {
          "author": "gabriel278",
          "body": "Restarting ollama serve often does not work. I noticed for VRAM leakage into shared memory, the occupied shared memory is often 4.0GB. Don't know if this piece of info is useful.",
          "created_at": "2025-03-16T16:14:59Z"
        }
      ]
    },
    {
      "issue_number": 12825,
      "title": "Error: POST predict: Post \"http://127.0.0.1:38093/completion\": EOF",
      "body": "I met the Error: POST predict: Post \"http://127.0.0.1:38093/completion\": EOF, when I type:   \nollama run llava \n\nit's ok when I type : \n./ollama run llama2-chinese\n\nexperiment environment: ARC 770 16G\nwhen I type: ollama run llava, the memory is:\nchunk_size: 4KiB, total: 16288MiB, free: 8449MiB\nwhen I input image and question, the message occurs and ollama run llava command quit and the memory becomes at once:\nchunk_size: 4KiB, total: 16288MiB, free: 11479MiB\n\n\n",
      "state": "closed",
      "author": "aiwanwan",
      "author_type": "User",
      "created_at": "2025-02-14T06:02:59Z",
      "updated_at": "2025-03-15T09:47:48Z",
      "closed_at": "2025-02-17T03:10:54Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 11,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12825/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12825",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12825",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:01.544220",
      "comments": [
        {
          "author": "aiwanwan",
          "body": "When I set llm_load_tensors: offloaded 0/33 layers to GPU, all run on cpu, it's ok,  but when I offloaded 33/33 layers to GPU, it go wrong, but I checked that gpu ram still left 10G\n\n",
          "created_at": "2025-02-14T07:01:46Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @aiwanwan, I cannot reproduce this issue, which version of ipex-llm ollama are you using? Also Could you provide more detailed ollama serve logs?",
          "created_at": "2025-02-14T07:44:59Z"
        },
        {
          "author": "aiwanwan",
          "body": "oneapi version 2025\nipex-llm version is ipex-llm[cpp]==2.2.0b20250213\nollama version is ollama version is 0.5.4-ipexllm-20250213\nthe ollama serve logs are:\n/home/runner/_work/llm.cpp/llm.cpp/llm.cpp/bigdl-core-xe/llama_backend/sdp_xmx_kernel.cpp:439: auto ggml_sycl_op_sdp_xmx_casual(fp16 *, fp16 *, ",
          "created_at": "2025-02-14T08:28:11Z"
        },
        {
          "author": "aiwanwan",
          "body": "or can you suggest a suitable ipex-llm version and ollama version ",
          "created_at": "2025-02-14T08:32:03Z"
        },
        {
          "author": "sgwhat",
          "body": "This issue has been resolved.",
          "created_at": "2025-02-14T10:11:43Z"
        }
      ]
    },
    {
      "issue_number": 12966,
      "title": "EOF POST predict error",
      "body": "Hello\n\nI installed, with a lot of effort, ollama ready zip for ubuntu on fedora. \n\nI tried to load a model like this: \n\nollama run dolphin-mistral-24b-gpu:latest\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\nggml_sycl_init: SYCL_USE_XMX: yes\nggml_sycl_init: found 1 SYCL devices:\n>>> hi\nError: POST predict: Post \"http://127.0.0.1:44119/completion\": EOF\n\nI get this error.\n\nPlease help me to troubleshoot this! \n\n\n",
      "state": "open",
      "author": "yabo-boye",
      "author_type": "User",
      "created_at": "2025-03-13T03:21:12Z",
      "updated_at": "2025-03-14T13:08:00Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12966/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12966",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12966",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:03.598706",
      "comments": [
        {
          "author": "yabo-boye",
          "body": "[log.txt](https://github.com/user-attachments/files/19238207/log.txt)",
          "created_at": "2025-03-13T23:36:58Z"
        },
        {
          "author": "yabo-boye",
          "body": "Segmentation Fault with Intel Arc GPU (SYCL) Even When GPU Offloading Disabled\nDescription\n\nI’ve encountered a persistent segmentation fault (SIGSEGV) when running Ollama with the ipex-llm nightly build (20250226) on an Intel Arc GPU (Meteor Lake), using oneAPI 2025.0 on Nobara 41. The crash occurs ",
          "created_at": "2025-03-14T01:10:43Z"
        },
        {
          "author": "1823616178",
          "body": "+1 double A770 has this error",
          "created_at": "2025-03-14T13:02:15Z"
        }
      ]
    },
    {
      "issue_number": 12868,
      "title": "ipex-llm run DeepSeek-R1-Q4_K_M on hybrid CPU+ARC770 raiss coredump",
      "body": "follow the guide https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/llama_cpp_quickstart.md to run DeepSeek-R1-Q4_K_M on hybrid model (INTEL(R) XEON(R) PLATINUM 8592 + ARC770) occurs coredump\n\nThe prepare work is \n\n```\npip install --pre --upgrade ipex-llm[cpp]\nmkdir llama-cpp\ncd llama-cpp\ninit-llama-cpp\n```\n\nThe env has been set \n\n```\n(ipex-llm) root@emr15195:/home/xtang/llama-cpp# echo $SYCL_CACHE_PERSISTENT\n1\n(ipex-llm) root@emr15195:/home/xtang/llama-cpp# echo $SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS\n1\n(ipex-llm) root@emr15195:/home/xtang/llama-cpp# echo $ONEAPI_DEVICE_SELECTOR\nlevel_zero:0\n```\n\nThe run command is \n\n`numactl -N 0 -m 0 ./llama-cli -m /nvme0/models/DeepSeek-R1-GGUF/DeepSeek-R1-Q4_K_M/DeepSeek-R1-Q4_K_M-00001-of-00009.gguf -n 128 --prompt \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: Question: If \\( a > 1 \\), then the sum of the real solutions of \\( \\sqrt{a} - \\sqrt{a + x} = x \\) is equal to:. Assistant: <think>\" -t 48 -e -ngl 2 --color -c 512 --temp 0 --no-context-shift -ot exps=CPU`\n\nthe output is\n```\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\nggml_sycl_init: SYCL_USE_XMX: yes\nggml_sycl_init: found 1 SYCL devices:\nbuild: 1 (e66308a) with Intel(R) oneAPI DPC++/C++ Compiler 2025.0.4 (2025.0.4.20241205) for x86_64-unknown-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_load_model_from_file: using device SYCL0 (Intel(R) Arc(TM) A770 Graphics) - 15473 MiB free\nllama_model_loader: additional 8 GGUFs metadata loaded.\nllama_model_loader: loaded meta data with 48 key-value pairs and 1025 tensors from /nvme0/models/DeepSeek-R1-GGUF/DeepSeek-R1-Q4_K_M/DeepSeek-R1-Q4_K_M-00001-of-00009.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = deepseek2\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 BF16\nllama_model_loader: - kv   3:                       general.quantized_by str              = Unsloth\nllama_model_loader: - kv   4:                         general.size_label str              = 256x20B\nllama_model_loader: - kv   5:                           general.repo_url str              = https://huggingface.co/unsloth\nllama_model_loader: - kv   6:                      deepseek2.block_count u32              = 61\nllama_model_loader: - kv   7:                   deepseek2.context_length u32              = 163840\nllama_model_loader: - kv   8:                 deepseek2.embedding_length u32              = 7168\nllama_model_loader: - kv   9:              deepseek2.feed_forward_length u32              = 18432\nllama_model_loader: - kv  10:             deepseek2.attention.head_count u32              = 128\nllama_model_loader: - kv  11:          deepseek2.attention.head_count_kv u32              = 128\nllama_model_loader: - kv  12:                   deepseek2.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  13: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  14:                deepseek2.expert_used_count u32              = 8\nllama_model_loader: - kv  15:        deepseek2.leading_dense_block_count u32              = 3\nllama_model_loader: - kv  16:                       deepseek2.vocab_size u32              = 129280\nllama_model_loader: - kv  17:            deepseek2.attention.q_lora_rank u32              = 1536\nllama_model_loader: - kv  18:           deepseek2.attention.kv_lora_rank u32              = 512\nllama_model_loader: - kv  19:             deepseek2.attention.key_length u32              = 192\nllama_model_loader: - kv  20:           deepseek2.attention.value_length u32              = 128\nllama_model_loader: - kv  21:       deepseek2.expert_feed_forward_length u32              = 2048\nllama_model_loader: - kv  22:                     deepseek2.expert_count u32              = 256\nllama_model_loader: - kv  23:              deepseek2.expert_shared_count u32              = 1\nllama_model_loader: - kv  24:             deepseek2.expert_weights_scale f32              = 2.500000\nllama_model_loader: - kv  25:              deepseek2.expert_weights_norm bool             = true\nllama_model_loader: - kv  26:               deepseek2.expert_gating_func u32              = 2\nllama_model_loader: - kv  27:             deepseek2.rope.dimension_count u32              = 64\nllama_model_loader: - kv  28:                deepseek2.rope.scaling.type str              = yarn\nllama_model_loader: - kv  29:              deepseek2.rope.scaling.factor f32              = 40.000000\nllama_model_loader: - kv  30: deepseek2.rope.scaling.original_context_length u32              = 4096\nllama_model_loader: - kv  31: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.100000\nllama_model_loader: - kv  32:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  33:                         tokenizer.ggml.pre str              = deepseek-v3\nllama_model_loader: - kv  34:                      tokenizer.ggml.tokens arr[str,129280]  = [\"<｜begin▁of▁sentence｜>\", \"<▒...\nllama_model_loader: - kv  35:                  tokenizer.ggml.token_type arr[i32,129280]  = [3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  36:                      tokenizer.ggml.merges arr[str,127741]  = [\"Ġ t\", \"Ġ a\", \"i n\", \"Ġ Ġ\", \"h e...\nllama_model_loader: - kv  37:                tokenizer.ggml.bos_token_id u32              = 0\nllama_model_loader: - kv  38:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  39:            tokenizer.ggml.padding_token_id u32              = 128815\nllama_model_loader: - kv  40:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  41:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  42:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\nllama_model_loader: - kv  43:               general.quantization_version u32              = 2\nllama_model_loader: - kv  44:                          general.file_type u32              = 15\nllama_model_loader: - kv  45:                                   split.no u16              = 0\nllama_model_loader: - kv  46:                        split.tensors.count i32              = 1025\nllama_model_loader: - kv  47:                                split.count u16              = 9\nllama_model_loader: - type  f32:  361 tensors\nllama_model_loader: - type q4_K:  606 tensors\nllama_model_loader: - type q6_K:   58 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 819\nllm_load_vocab: token to piece cache size = 0.8223 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = deepseek2\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 129280\nllm_load_print_meta: n_merges         = 127741\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 163840\nllm_load_print_meta: n_embd           = 7168\nllm_load_print_meta: n_layer          = 61\nllm_load_print_meta: n_head           = 128\nllm_load_print_meta: n_head_kv        = 128\nllm_load_print_meta: n_rot            = 64\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 192\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 24576\nllm_load_print_meta: n_embd_v_gqa     = 16384\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 18432\nllm_load_print_meta: n_expert         = 256\nllm_load_print_meta: n_expert_used    = 8\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = yarn\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 0.025\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 671B\nllm_load_print_meta: model ftype      = Q4_K - Medium\nllm_load_print_meta: model params     = 671.03 B\nllm_load_print_meta: model size       = 376.65 GiB (4.82 BPW)\nllm_load_print_meta: general.name     = DeepSeek R1 BF16\nllm_load_print_meta: BOS token        = 0 '<｜begin▁of▁sentence｜>'\nllm_load_print_meta: EOS token        = 1 '<｜end▁of▁sentence｜>'\nllm_load_print_meta: EOT token        = 1 '<｜end▁of▁sentence｜>'\nllm_load_print_meta: PAD token        = 128815 '<｜PAD▁TOKEN｜>'\nllm_load_print_meta: LF token         = 131 'Ä'\nllm_load_print_meta: FIM PRE token    = 128801 '<｜fim▁begin｜>'\nllm_load_print_meta: FIM SUF token    = 128800 '<｜fim▁hole｜>'\nllm_load_print_meta: FIM MID token    = 128802 '<｜fim▁end｜>'\nllm_load_print_meta: EOG token        = 1 '<｜end▁of▁sentence｜>'\nllm_load_print_meta: max token length = 256\nllm_load_print_meta: n_layer_dense_lead   = 3\nllm_load_print_meta: n_lora_q             = 1536\nllm_load_print_meta: n_lora_kv            = 512\nllm_load_print_meta: n_ff_exp             = 2048\nllm_load_print_meta: n_expert_shared      = 1\nllm_load_print_meta: expert_weights_scale = 2.5\nllm_load_print_meta: expert_weights_norm  = 1\nllm_load_print_meta: expert_gating_func   = sigmoid\nllm_load_print_meta: rope_yarn_log_mul    = 0.1000\nllm_load_tensors: offloading 2 repeating layers to GPU\nllm_load_tensors: offloaded 2/62 layers to GPU\nllm_load_tensors:   CPU_Mapped model buffer size = 46095.40 MiB\nllm_load_tensors:   CPU_Mapped model buffer size = 47139.54 MiB\nllm_load_tensors:   CPU_Mapped model buffer size = 47232.92 MiB\nllm_load_tensors:   CPU_Mapped model buffer size = 46036.25 MiB\nllm_load_tensors:   CPU_Mapped model buffer size = 47132.51 MiB\nllm_load_tensors:   CPU_Mapped model buffer size = 46036.25 MiB\nllm_load_tensors:   CPU_Mapped model buffer size = 47139.54 MiB\nllm_load_tensors:   CPU_Mapped model buffer size = 44663.89 MiB\nllm_load_tensors:   CPU_Mapped model buffer size = 14105.06 MiB\nllm_load_tensors:        SYCL0 model buffer size =   269.34 MiB\n...................................................................................................Bus error (core dumped)\n\n```",
      "state": "closed",
      "author": "xtangxtang",
      "author_type": "User",
      "created_at": "2025-02-21T05:51:41Z",
      "updated_at": "2025-03-14T09:47:45Z",
      "closed_at": "2025-03-10T02:39:57Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12868/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "rnwang04"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12868",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12868",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:03.808998",
      "comments": [
        {
          "author": "rnwang04",
          "body": "Hi @xtangxtang , I can't reproduce this error with the same command on our machine (INTEL(R) XEON(R) PLATINUM 8558P + ARC A770).\nHere are some tips or questions that I want to further check with you :\n- Have you ever tried running it on CPU with `-ngl 0` ? It is successful?\n- Have you ever successfu",
          "created_at": "2025-02-24T01:42:04Z"
        },
        {
          "author": "xtangxtang",
          "body": "@rnwang04 I finally follow the guide https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/llamacpp_portable_zip_gpu_quickstart.md#flashmoe-for-deepseek-v3r1 to run DS R1 by ipex-llm with this command  \"LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$(cd \"$(dirname \"$0\")\";pwd) $(cd \"$(dirname \"$0\")\";",
          "created_at": "2025-03-10T02:39:57Z"
        },
        {
          "author": "Ramzee-S",
          "body": "Hi xtangxtang, sorry to write in your issue here. But i see that you have an 8592+ How is your performance with llm's. \nI have an Intel ES Q30G, which is similar architecture, just 48 cores instead of 64.  On 1 cpu was doing quite great, but in a dual-socket setup, it was about 4 times slower on the",
          "created_at": "2025-03-14T09:47:44Z"
        }
      ]
    },
    {
      "issue_number": 12111,
      "title": "Running llama3.1 in ollama/langchain fails.",
      "body": "After updating ipex-llm, running llama3.1 through langchain and ollama no longer works.\r\nA simple reproducer:\r\n\r\n```python\r\n# pip install langchain langchain_community\r\nfrom langchain_community.llms import Ollama\r\n\r\n# ollama pull llama3.1:70b-instruct-q4_K_M\r\nllm = Ollama(model=\"llama3.1:70b-instruct-q4_K_M\")\r\nresponse = llm.invoke(\"What is the capital of France?\")\r\nprint(response)\r\n```\r\n\r\nLast know working ipex-llm version is 2.2.0b20240826.\r\nTested on Ubuntu 22.04, oneAPI 2024.02 (intel-basekit 2024.2.1-98) with two Intel(R) Data Center GPU Max 1100 GPUs.\r\n\r\nError message:\r\n\r\n```txt\r\n[1727090840] warming up the model with an empty run\r\nollama_llama_server: /home/runner/_work/llm.cpp/llm.cpp/llm.cpp/bigdl-core-xe/llama_backend/sdp_xmx_kernel.cpp:428: auto ggml_sycl_op_sdp_xmx_casual(fp16 *, fp16 *, fp16 *, fp16 *, fp16 *, float *, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, float *, float, bool, sycl::queue &)::(anonymous class)::operator()() const: Assertion `false' failed.\r\ntime=2024-09-23T11:27:23.172Z level=INFO source=server.go:629 msg=\"waiting for server to become available\" status=\"llm server error\"\r\ntime=2024-09-23T11:27:23.423Z level=ERROR source=sched.go:456 msg=\"error loading llama server\" error=\"llama runner process has terminated: signal: aborted (core dumped)\"\r\n```\r\n",
      "state": "closed",
      "author": "tkarna",
      "author_type": "User",
      "created_at": "2024-09-23T11:50:53Z",
      "updated_at": "2025-03-14T06:51:48Z",
      "closed_at": "2025-03-14T06:51:48Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12111/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [
        "leonardozcm"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12111",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12111",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:04.007274",
      "comments": [
        {
          "author": "leonardozcm",
          "body": "hi, I think we have fix this in latest pr, may you try ipex-llm[cpp] >=2.2.0b20240924 tomorrow?",
          "created_at": "2024-09-24T07:48:18Z"
        },
        {
          "author": "tkarna",
          "body": "Thanks, I confirm that the simple example works now. However, when running a larger langchain agents workflow I'm still getting an error:\r\n```\r\n/home/runner/_work/llm.cpp/llm.cpp/ollama-internal/llm/llama.cpp/ggml/src/ggml-backend.c:96: GGML_ASSERT(base != NULL && \"backend buffer base cannot be NULL",
          "created_at": "2024-09-26T16:30:15Z"
        },
        {
          "author": "tklengyel",
          "body": "I still have this issue using Ollama and Open WebUI with llama3.1 as of 2.2.0b20240927. \r\n\r\n```\r\nollama_llama_server: /home/runner/_work/llm.cpp/llm.cpp/llm.cpp/bigdl-core-xe/llama_backend/sdp_xmx_kernel.cpp:429: auto ggml_sycl_op_sdp_xmx_casual(fp16 *, fp16 *, fp16 *, fp16 *, fp16 *, float *, size_",
          "created_at": "2024-09-27T22:29:49Z"
        }
      ]
    },
    {
      "issue_number": 12780,
      "title": "Ollama llama3.x models do not work with LangChain chat/tool integration",
      "body": "Llama3.x models ran through ipex-llm's ollama do not work with LangChain chat/tool integration.\n\nHere's a minimal example of a chat model with tools:\n\n```python\n# pip install langchain langchain-ollama\n# ollama pull llama3.2:3b-instruct-q4_K_M\nfrom langchain_core.tools import tool\nfrom langchain_ollama.chat_models import ChatOllama\n\n\n@tool\ndef get_weather(location: str):\n    \"\"\"Call to get the current weather.\"\"\"\n    if location.lower() in [\"sf\", \"san francisco\"]:\n        return \"It's 60 degrees and foggy.\"\n    else:\n        return \"It's 90 degrees and sunny.\"\n\n\nmodel = ChatOllama(\n    model= \"llama3.2:3b-instruct-q4_K_M\",\n    num_predict=50,  # limit number of tokens to stop hallucination\n)\n\ntools = [get_weather]\nmodel_with_tools = model.bind_tools(tools)\n\nres = model_with_tools.invoke(\"what's the weather in sf?\")\nres.pretty_print()\n```\n\nThis example runs correctly with standard ollama. Expected response with tool arguments:\n\n```txt\n================================== Ai Message ==================================\nTool Calls:\n  get_weather (328879e5-247c-48d1-9013-39f0e1b65539)\n Call ID: 328879e5-247c-48d1-9013-39f0e1b65539\n  Args:\n    location: sf\n```\n\nActual output shows that the model just hallucinates:\n\n```txt\n================================== Ai Message ==================================\n\nI hope that a new\n$ has several times this would be =_._ _-level was an item 8/<< is not only to the best\nTo view= (or is a significant but also knows are still allow [or)\n```\n\nTested with:\n\nUbuntu 22.04.5 LTS\noneapi/2025.0\npython 3.10.0\nipex-llm 2.2.0b20250105, 2.2.0b20250123\nlangchain 0.3.17\nlangchain-ollama 0.2.3\nGPU: Intel(R) Data Center GPU Max 1100\n",
      "state": "closed",
      "author": "tkarna",
      "author_type": "User",
      "created_at": "2025-02-06T11:28:20Z",
      "updated_at": "2025-03-14T06:50:28Z",
      "closed_at": "2025-03-14T06:50:27Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12780/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12780",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12780",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:04.196800",
      "comments": [
        {
          "author": "tkarna",
          "body": "Ollama server is able to generate JSON output however. This command\n```bash\ncurl -X POST http://localhost:11434/api/chat -H \"Content-Type: application/json\" -d '{\n  \"model\": \"llama3.2:1b-instruct-q4_K_M\",\n  \"messages\": [{\"role\": \"user\", \"content\": \"Tell me about Canada.\"}],\n  \"stream\": false,\n  \"for",
          "created_at": "2025-02-06T13:33:14Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @tkarna , May I ask what \"standard ollama\" refers to? Is it \"ollama run\" or the community version of ollama?",
          "created_at": "2025-02-07T02:22:55Z"
        },
        {
          "author": "tkarna",
          "body": "> Hi [@tkarna](https://github.com/tkarna) , May I ask what \"standard ollama\" refers to? Is it \"ollama run\" or the community version of ollama?\n\nCommunity ollama running on CPU. I have also compiled ollama 3.13 with Intel GPU support. The above test case works with both of these.",
          "created_at": "2025-02-07T06:37:52Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @tkarna，I cannot reproduce your issue, you may install our latest version of ipex-llm ollama via `pip install --pre --upgrade ipex-llm[cpp]`.",
          "created_at": "2025-02-11T01:30:23Z"
        },
        {
          "author": "tkarna",
          "body": "> Hi [@tkarna](https://github.com/tkarna)，I cannot reproduce your issue, you may install our latest version of ipex-llm ollama via `pip install --pre --upgrade ipex-llm[cpp]`.\n\nI am still getting the same issue with latest ipex-llm version 2.2.0b20250210.\n\nCan you please share the stack you tested w",
          "created_at": "2025-02-11T06:02:16Z"
        }
      ]
    },
    {
      "issue_number": 12946,
      "title": "deepseek-r1 model cannot stream token generation",
      "body": "ipex-llm version: 2.2.0b20250122\ntransformers version: 4.37.0\n\nRun the qwen_generate.py, `AttributeError: 'Qwen2ForCausalLM' object has no attribute 'chat_stream'` is happened.\n```shell\nroot@c4c4623afae0:/home/llm# python3 Qwen_generate.py -m /home/llm/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/\n/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n  warn(\n2025-03-06 08:25:50,334 - INFO - intel_extension_for_pytorch auto imported\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 13.99it/s]2025-03-06 08:25:52,050 - INFO - Converting the current model to sym_int4 format......\n/usr/local/lib/python3.10/dist-packages/torch/nn/init.py:412: UserWarning: Initializing zero-element tensors is a no-op\n  warnings.warn(\"Initializing zero-element tensors is a no-op\")\nmodel load time 9503.011703491211 ms\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nPlease input your question:你好\ninput_token_count: 1\n-------------------- stream chat output --------------------\nQwen-ipex assistant: Traceback (most recent call last):\n  File \"/home/llm/Qwen_generate.py\", line 121, in <module>\n    output = model.chat_stream(tokenizer=tokenizer, query=input_text, history=history, max_new_tokens=args.max_sequence_length)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1695, in __getattr__\n    raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\nAttributeError: 'Qwen2ForCausalLM' object has no attribute 'chat_stream'\n```\n\nThen, I upgraded the transformers version to 4.40.0, `AttributeError: module 'transformers.models.qwen2.modeling_qwen2' has no attribute 'Qwen2SdpaAttention'. Did you mean: 'Qwen2Attention'?` is happened.\n\n```shell\nroot@c4c4623afae0:/home/llm# python3 Qwen_generate.py -m /home/llm/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/\n/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n  warn(\n2025-03-06 08:55:37,631 - INFO - intel_extension_for_pytorch auto imported\nSliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 14.13it/s]2025-03-06 08:55:37,894 - INFO - Converting the current model to sym_int4 format......\n/usr/local/lib/python3.10/dist-packages/torch/nn/init.py:412: UserWarning: Initializing zero-element tensors is a no-op\n  warnings.warn(\"Initializing zero-element tensors is a no-op\")\nTraceback (most recent call last):\n  File \"/home/llm/Qwen_generate.py\", line 69, in <module>\n    model = AutoModelForCausalLM.from_pretrained(model_path,\n  File \"/usr/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"/usr/local/lib/python3.10/dist-packages/ipex_llm/transformers/model.py\", line 350, in from_pretrained\n    model = cls.load_convert(q_k, optimize_model, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/ipex_llm/transformers/model.py\", line 503, in load_convert\n    model = ggml_convert_low_bit(model, qtype, optimize_model,\n  File \"/usr/local/lib/python3.10/dist-packages/ipex_llm/transformers/convert.py\", line 1142, in ggml_convert_low_bit\n    model = _optimize_post(model)\n  File \"/usr/local/lib/python3.10/dist-packages/ipex_llm/transformers/convert.py\", line 1615, in _optimize_post\n    module.Qwen2SdpaAttention,\nAttributeError: module 'transformers.models.qwen2.modeling_qwen2' has no attribute 'Qwen2SdpaAttention'. Did you mean: 'Qwen2Attention'?\n```\n\nThe qwen_generate.py that we used is as follow:\n```python\nimport torch\nimport time\nimport argparse\nimport os\nimport json\nfrom ipex_llm.transformers import AutoModelForCausalLM\nfrom transformers import AutoTokenizer\n\n# you could tune the prompt based on your own model\nQWEN_PROMPT_FORMAT = \"\"\"\n<|im_start|>system\nYou are a helpful assistant.\n<|im_end|>\n<|im_start|>user\n{prompt}\n<|im_end|>\n<|im_start|>assistant\n\"\"\"\nRESULT_FILE = '/result.txt'\nINPUT_FILE = \"/ipex_input_text.json\"\n\nif __name__ == '__main__':\n    os.environ['SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS'] = '1'\n    parser = argparse.ArgumentParser(description='Predict Tokens using `generate()` API for Qwen model')\n    parser.add_argument('--repo-id-or-model-path', type=str, default=\"/home/lqh/llm/LeBron/models/Qwen_7B_int4\",\n                        help='The huggingface repo id for the Qwen model to be downloaded'\n                             ', or the path to the huggingface checkpoint folder')\n    parser.add_argument('--prompt', type=str, default=\"Please introduce Beijing\",\n                        help='Prompt to infer')\n    parser.add_argument('--n-predict', type=int, default=2048,\n                        help='Max tokens to predict')\n    parser.add_argument('-a',\n                        '--add_history',\n                        type=bool,\n                        default=False,\n                        help='Whether to add history.')\n    parser.add_argument('-s',\n                        '--sleep_ms',\n                        default=0,\n                        type=int,\n                        help='Sleep time/ms.')\n    parser.add_argument('-i',\n                        '--input_len',\n                        default=-1,\n                        choices=[32,256,512,1024],\n                        type=int,\n                        help='The length of predefined input text')\n    parser.add_argument('-m',\n                        '--model_path',\n                        default='/qwen-14b-int4',\n                        type=str,\n                        help='Model path')\n    parser.add_argument('-l',\n                        '--max_sequence_length',\n                        default=1024,\n                        required=False,\n                        type=int,\n                        help='Maximun length of output')\n    args = parser.parse_args()\n    sleep_time = args.sleep_ms\n    model_path = args.model_path\n    input_len = args.input_len\n    if input_len != -1:\n        with open(INPUT_FILE, 'r') as json_file:\n            data = json.load(json_file)\n        input_text=data[\"qwen-\"+str(input_len)]\n    # Load the Qwen model\n    load_st = time.time()\n    model = AutoModelForCausalLM.from_pretrained(model_path,\n                                                 load_in_4bit=True,\n                                                 optimize_model=True,\n                                                 trust_remote_code=True,\n                                                 use_cache=True\n                                                 ).to('xpu').eval()\n    #model = model.to(\"xpu\")\n    #model = AutoModelForCausalLM.load_low_bit(model_path,\n    #                                             optimize_model=True,\n    #                                             trust_remote_code=True,\n    #                                             use_cache=True,\n    #                                             use_cache_quantization=False,\n    #                                             use_cache_kernel=False,\n    #                                             use_flash_attn=False,\n    #                                             ).half()\n    #model.to(\"xpu\").eval()\n    load_end = time.time()\n    module_load_ms = 1000 * (load_end - load_st)\n    print(f'model load time {module_load_ms} ms')\n\n    tokenizer = AutoTokenizer.from_pretrained(model_path,\n                                              trust_remote_code=True)\n\n    # Generate predicted tokens\n    with torch.inference_mode():\n        os.environ['SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS'] = '1'\n        history = []\n        count = 0\n        while 1:\n            if not args.add_history:\n                history = []\n            if input_len == -1:\n                text = input(\"Please input your question:\")\n                input_text = text\n            else:\n                text = input(\"Press the Enter key then start!\")\n            if text.lower() == 'stop':\n                print(\"exit!\")\n                break\n            if text.lower() == 'clear':\n                history = []\n                print(\"AI assistant: history has cleared!\")\n                continue\n            input_token_count = len(tokenizer.tokenize(input_text))\n            print(\"input_token_count:\", input_token_count)\n            print('-' * 20, \"stream chat output\", '-' * 20)\n            print(\"Qwen-ipex assistant: \", end=\"\")\n            time_start = time.time()\n            time_first_record = False  # measure the first Token\n            output_token_count = 0\n            time_first_token = time.time()\n            time_total_token = time.time()\n            output = model.chat_stream(tokenizer=tokenizer, query=input_text, history=history, max_new_tokens=args.max_sequence_length)\n            partial_text = \"\"\n            size = 0\n            for text in output:\n                token = text[size:]\n                print(token, end=\"\", flush=True)\n                size = len(text)\n                output_token_count += 1\n                if not time_first_record:\n                    time_first_token = time.time()\n                    time_first_record = True\n                partial_text += text\n                if sleep_time > 0:\n                    time.sleep(sleep_time/1000)\n            if args.add_history:\n                history.append((input_text, partial_text))\n            time_total_token = time.time()\n            llm_ms_first_token = (time_first_token - time_start) * 1000\n            llm_ms_after_token = 0\n            if output_token_count == 0 or output_token_count == 1:\n                llm_ms_after_token = (time_total_token - time_first_token) * 1000\n            elif output_token_count > 1:\n                llm_ms_after_token = (time_total_token - time_first_token) / (output_token_count - 1) * 1000\n            llm_time = time_total_token - time_start\n            count += 1\n            result = \"<LLM summary> sleep time = {} ms\\n\".format(sleep_time)\n            result += \"<LLM summary> module load = {} ms\\n\".format(module_load_ms)\n            result += \"<LLM summary> tests count = {} times\\n\".format(count)\n            result += \"<LLM summary> input length = {} tokens\\n\".format(input_token_count)\n            result += \"<LLM summary> output length = {} tokens\\n\".format(output_token_count)\n            result += \"<LLM summary> First token latency = {} ms\\n\".format(llm_ms_first_token)\n            result += \"<LLM summary> After token latency = {} ms/token\\n\".format(llm_ms_after_token)\n            result += \"<LLM summary> total conversation time = {} s\\n\".format(llm_time)\n            print(result)\n            with open(RESULT_FILE, 'w') as file:\n                file.write(result)\n\n```",
      "state": "open",
      "author": "junruizh2021",
      "author_type": "User",
      "created_at": "2025-03-06T09:28:55Z",
      "updated_at": "2025-03-14T06:36:25Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12946/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12946",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12946",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:04.447998",
      "comments": [
        {
          "author": "MeouSker77",
          "body": "`DeepSeek-R1-Distill-Qwen-7B` is based on Qwen2, [Qwen2 model](https://github.com/huggingface/transformers/blob/53fad641cfdb5105e2470bcf3ef17ea8e25cc300/src/transformers/models/qwen2/modeling_qwen2.py#L1082) has no method named `chat_stream`, and it doesn't provide any streaming chat methods.\n\nEven ",
          "created_at": "2025-03-07T03:07:37Z"
        },
        {
          "author": "junruizh2021",
          "body": "@MeouSker77 Thanks for your reply. I'll try it.",
          "created_at": "2025-03-07T06:39:05Z"
        },
        {
          "author": "junruizh2021",
          "body": "@MeouSker77 I compared the performance beween TextIteratorStream and ipex-llm.util.benchmarkWrapper. The performance gap is huge. Is that reasonable? which is the standard benchmark method of LLM inference?",
          "created_at": "2025-03-14T03:16:28Z"
        }
      ]
    },
    {
      "issue_number": 12842,
      "title": "With Intel chips, how to fine-tune models with LoRA and use it on inference?",
      "body": "In openvino, I see Intel have OpenVINO notebook samples for llm agent React.\nhttps://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/llm-agent-react\n\nBesides, OpenVINO is able to use lora adapter to enhance LLM. \nhttps://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/text_generation/lora.py\n\nCould I know if IPEX-llm has the similiar samples?\n\n\nAnd as I know OpenVINO is an inference SDK but not for model fine-tuning and training. \nI have Intel CPU, iGPU, and dGPU. \nIs the IPEX or IPEX-llm SDK able to do llm fine-tune with LoRA on Intel hardware components? Thanks!",
      "state": "open",
      "author": "JamieVC",
      "author_type": "User",
      "created_at": "2025-02-18T03:16:59Z",
      "updated_at": "2025-03-13T11:44:50Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12842/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12842",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12842",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:04.656005",
      "comments": [
        {
          "author": "qiyuangong",
          "body": "Hi @JamieVC \n\nWe have LoRA/QLora finetune examples for dGPU and Qlora examples for CPU. Please refer to these examples\n1. GPU finetune https://github.com/intel/ipex-llm/tree/main/python/llm/example/GPU/LLM-Finetuning\n2. CPU finetune https://github.com/intel/ipex-llm/tree/main/python/llm/example/CPU/",
          "created_at": "2025-02-18T04:39:35Z"
        },
        {
          "author": "JamieVC",
          "body": "hi @qiyuangong \nDoes IPEX-llm has samples to inference with LoRA adapters?",
          "created_at": "2025-03-13T07:33:12Z"
        },
        {
          "author": "qiyuangong",
          "body": "> hi [@qiyuangong](https://github.com/qiyuangong) Does IPEX-llm has samples to inference with LoRA adapters?\n\nHi @JamieVC \nNo. However, we provide a script for merging LoRA weight into the original model. https://github.com/intel/ipex-llm/tree/main/python/llm/example/GPU/LLM-Finetuning/QLoRA/alpaca-",
          "created_at": "2025-03-13T11:44:40Z"
        }
      ]
    },
    {
      "issue_number": 12848,
      "title": "Feature Request: Implementing a No-Install Runtime Package for Ollama on Intel NPU using IPEX-LLM",
      "body": "Hello,\nI would like to propose a feature request to develop a no-install runtime package for running Ollama on Intel NPU using Intel PyTorch Extension for Large Language Models (IPEX-LLM). This feature will significantly simplify the deployment process and enhance user experience by eliminating the need for manual setup and configuration.\nDetails:\nObjective:\nDevelop a self-contained, no-install runtime package that allows users to run Ollama models seamlessly on Intel NPU hardware using IPEX-LLM.\nScope:\nThe package should include all necessary dependencies and configurations pre-installed.\nEnsure compatibility with the latest versions of Ollama, IPEX-LLM, and Intel NPU drivers.\nProvide a simple command-line interface or script to launch the environment without requiring additional installations or configurations.\nBenefits:\nSimplify the deployment process for end-users.\nReduce the time and effort required to set up the environment.\nEnhance the accessibility of Ollama models on Intel NPU hardware.\nRequirements:\nSupport for Intel NPU hardware acceleration.\nIntegration with IPEX-LLM for optimized performance.\nPre-configured environment with all necessary libraries and dependencies.\nDocumentation for easy setup and usage instructions.\nAdditional Information:\nTarget platforms: intel ultra 7 258v\nBest regards",
      "state": "open",
      "author": "ChenZeiShuai",
      "author_type": "User",
      "created_at": "2025-02-19T05:43:57Z",
      "updated_at": "2025-03-13T09:31:30Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12848/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12848",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12848",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:04.854581",
      "comments": [
        {
          "author": "dmbuil",
          "body": "+1!",
          "created_at": "2025-03-13T09:31:27Z"
        }
      ]
    },
    {
      "issue_number": 12959,
      "title": "flash-moe  for deepseek portable zip, crash as Illegal instruction.",
      "body": "**Describe the bug**\n\nflash-moe  for deepseek portable zip, crash as Illegal instruction.\n\n**How to reproduce**\n\nrun flash-moe with deepseek q4\n\n**Screenshots**\n\nFound 1 SYCL devices:\n| 0| [level_zero:gpu:0]|                Intel Graphics [0xe20b]|   20.1|    160|    1024|   32| 12168M|         1.6.32224+14|\nllama_kv_cache_init:        CPU KV buffer size =  4240.00 MiB\nllama_kv_cache_init:      SYCL0 KV buffer size =   640.00 MiB\nllama_new_context_with_model: KV self size  = 4880.00 MiB, K (f16): 2928.00 MiB, V (f16): 1952.00 MiB\nllama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\nllama_new_context_with_model:      SYCL0 compute buffer size =   852.00 MiB\nllama_new_context_with_model:  SYCL_Host compute buffer size =   774.01 MiB\nllama_new_context_with_model: graph nodes  = 5489 (with bs=1024), 5025 (with bs=1)\nllama_new_context_with_model: graph splits = 756 (with bs=1024), 19 (with bs=1)\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n./flash-moe: line 7: 14534 Illegal instruction\n\n**Environment information**\n\nfrom env_check.sh\n\nCPU Information: \nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        43 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               64\nOn-line CPU(s) list:                  0-63\nVendor ID:                            AuthenticAMD\nModel name:                           AMD Ryzen Threadripper 3970X 32-Core Processor\nCPU family:                           23\nModel:                                49\nThread(s) per core:                   2\nCore(s) per socket:                   32\nSocket(s):                            1\nStepping:                             0\nFrequency boost:                      enabled\nCPU(s) scaling MHz:                   22%\nCPU max MHz:                          4550.0000\n\nTotal CPU Memory: 251.555 GB\nMemory Type: DDR4 \n\nOperating System: \nUbuntu 24.04.2 LTS \\n \\l\n\n\nLinux ai 6.11.0-19-generic #19~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Feb 17 11:51:52 UTC 2 x86_64 x86_64 x86_64 GNU/Linux\n\n  Driver Version                                  2024.18.12.0.05_160000\n  Driver UUID                                     32342e35-322e-3332-3232-340000000000\n  Driver Version                                  24.52.32224\n\nDriver related package version:\n\n\n**Additional context**\n\nfrom sycl-ls:\n[level_zero:gpu][level_zero:0] Intel(R) oneAPI Unified Runtime over Level-Zero, Intel(R) Graphics [0xe20b] 20.1.0 [1.6.32224+14]\n[opencl:cpu][opencl:0] Intel(R) OpenCL, AMD Ryzen Threadripper 3970X 32-Core Processor  OpenCL 3.0 (Build 0) [2024.18.12.0.05_160000]\n[opencl:gpu][opencl:1] Intel(R) OpenCL Graphics, Intel(R) Graphics [0xe20b] OpenCL 3.0 NEO  [24.52.32224]\n\nSo it seems a local build is best way to solve this issue.     any info for how to build flash-moe from source ?\n\n",
      "state": "open",
      "author": "ParallelDqn",
      "author_type": "User",
      "created_at": "2025-03-11T03:36:14Z",
      "updated_at": "2025-03-13T00:32:03Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12959/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "qiuxin2012"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12959",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12959",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:05.049898",
      "comments": [
        {
          "author": "ParallelDqn",
          "body": "I tried  flash-moe from another machine which is intel i7 12700K and it works there.  \nWhen compare the IS diff from 12700k to Threadripper, the answer is   avx_vnni,   which make sense here as it's intel project.\n\nBut if anybody can help show me a light where is the code and I can still build it fo",
          "created_at": "2025-03-12T12:56:06Z"
        },
        {
          "author": "qiuxin2012",
          "body": "Yes, lacking of avx_vnni will get `Ollegal instruction` error. We are removing this requirement. ",
          "created_at": "2025-03-13T00:32:01Z"
        }
      ]
    },
    {
      "issue_number": 12964,
      "title": "ipex-llm-2.2.0b20250312 silently crash",
      "body": "**Describe the bug**\nipex-llm-2.2.0b20250312 silently crash without error message. \n\nI see this in the message : \nllm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n\nIs this ok ?\n\n\n**How to reproduce**\nFor example this command : \nllama-bench.exe -m F:\\models\\llama-2-7b.Q4_0.gguf -v\n\n**Screenshots**\n\n```\nllama-bench.exe -m F:\\models\\llama-2-7b.Q4_0.gguf -v\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\nggml_sycl_init: SYCL_USE_XMX: yes\nggml_sycl_init: found 1 SYCL devices:\n| model                          |       size |     params | backend    | ngl |          test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | ------------: | -------------------: |\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\nllama_load_model_from_file: using device SYCL0 (Intel(R) Iris(R) Xe Graphics) - 30140 MiB free\nllama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from F:\\models\\llama-2-7b.Q4_0.gguf (version GGUF V2)\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = LLaMA v2\nllama_model_loader: - kv   2:                       llama.context_length u32              = 4096\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  10:                          general.file_type u32              = 2\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  18:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q4_0:  225 tensors\nllama_model_loader: - type q6_K:    1 tensors\nllm_load_vocab: control token:      2 '</s>' is not marked as EOG\nllm_load_vocab: control token:      1 '<s>' is not marked as EOG\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V2\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = Q4_0\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 3.56 GiB (4.54 BPW)\nllm_load_print_meta: general.name     = LLaMA v2\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\nllm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\nllm_load_tensors: offloading 32 repeating layers to GPU\nllm_load_tensors: offloading output layer to GPU\nllm_load_tensors: offloaded 33/33 layers to GPU\nllm_load_tensors:        SYCL0 model buffer size =  3577.56 MiB\nllm_load_tensors:   CPU_Mapped model buffer size =    70.31 MiB\n..................................................................................................\nllama_new_context_with_model: n_seq_max     = 1\nllama_new_context_with_model: n_ctx         = 512\nllama_new_context_with_model: n_ctx_per_seq = 512\nllama_new_context_with_model: n_batch       = 512\nllama_new_context_with_model: n_ubatch      = 512\nllama_new_context_with_model: flash_attn    = 0\nllama_new_context_with_model: freq_base     = 10000.0\nllama_new_context_with_model: freq_scale    = 1\nllama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n[SYCL] call ggml_check_sycl\nggml_check_sycl: GGML_SYCL_DEBUG: 0\nggml_check_sycl: GGML_SYCL_F16: no\nFound 1 SYCL devices:\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\n| 0| [level_zero:gpu:0]|                 Intel Iris Xe Graphics|   12.0|     96|     512|   32| 31604M|            1.6.32413|\n\n(llm-cpp) C:\\>\n```\n\n\n\n**Environment information**\nWindows and docker image tested \n\n",
      "state": "open",
      "author": "easyfab",
      "author_type": "User",
      "created_at": "2025-03-12T19:48:49Z",
      "updated_at": "2025-03-13T00:23:39Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12964/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12964",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12964",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:05.267130",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "For the silently crash, you can run `set IPEX_LLM_QUANTIZE_KV_CACHE=1`. We will fix this bug later.",
          "created_at": "2025-03-13T00:23:38Z"
        }
      ]
    },
    {
      "issue_number": 12960,
      "title": "When starting with a sh script, it prompts 'illegal instruction'.",
      "body": "**Describe the bug**\nA clear and concise description of what the bug or error is.\n\n**How to reproduce**\nSteps to reproduce the error:\n1. Start a container using docker run\n```bash\ndocker run -it --net=host --cpuset-cpus=\"0-7\" --cpuset-mems=\"0\" -v ~/1506/models/:/llm/models -e no_proxy=localhost,127.0.0.1 --memory=\"4G\" --shm-size=\"1g\" docker.m.daocloud.io/intelanalytics/ipex-llm-serving-cpu:latest\n```\n2.Start the model using the following command.\nroot@docker-desktop:/llm# python -m ipex_llm.vllm.cpu.entrypoints.openai.api_server --served-model-name qwen2.5-1.5b --port 8000 --model /llm/models/Qwen2.5-1.5B-Instruct --trust-remote-code --device cpu --dtype bfloat16 --enforce-eager --load-in-low-bit bf16 --max-model-len 4096 --max-num-batched-tokens 10240 --max-num-seqs 12 --tensor-parallel-size 1\n**the output**\nIllegal instruction\nroot@docker-desktop:/llm# python -m ipex_llm.vllm.cpu.entrypoints.openai.api_server --help\nIllegal instruction\nroot@docker-desktop:/llm# \n**Screenshots**\n\n<img width=\"1060\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/c4b5e916-af61-46bd-a700-e05844f605dc\" />\n\n**Environment information**\nroot@docker-desktop:/llm/models/git/ipex-llm/python/llm/scripts# ./env-check.sh \n-----------------------------------------------------------------\nPYTHON_VERSION=3.11.11\n-----------------------------------------------------------------\ntransformers=4.49.0\n-----------------------------------------------------------------\ntorch=2.5.1+cpu\n-----------------------------------------------------------------\nipex-llm DEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/vllm-0.6.6.post1+cpu-py3.11-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\nVersion: 2.2.0b20250227\n-----------------------------------------------------------------\nIPEX is not installed. \n-----------------------------------------------------------------\nCPU Information: \nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        39 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               8\nOn-line CPU(s) list:                  0-7\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Core(TM) i5-8259U CPU @ 2.30GHz\nCPU family:                           6\nModel:                                142\nThread(s) per core:                   1\nCore(s) per socket:                   8\nSocket(s):                            1\nStepping:                             10\nBogoMIPS:                             4607.73\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch pti intel_ppin fsgsbase bmi1 avx2 smep bmi2 erms rdseed adx smap clflushopt xsaveopt xsavec xgetbv1 arat\nL1d cache:                            256 KiB (8 instances)\n-----------------------------------------------------------------\nTotal CPU Memory: 3.8216 GB\n-----------------------------------------------------------------\nOperating System: \nUbuntu 22.04.3 LTS \\n \\l\n\n-----------------------------------------------------------------\nLinux docker-desktop 6.10.14-linuxkit #1 SMP PREEMPT_DYNAMIC Fri Nov 29 17:24:06 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux\n-----------------------------------------------------------------\n./env-check.sh: line 148: xpu-smi: command not found\n-----------------------------------------------------------------\n./env-check.sh: line 154: clinfo: command not found\n-----------------------------------------------------------------\nDriver related package version:\n-----------------------------------------------------------------\n./env-check.sh: line 167: sycl-ls: command not found\nigpu not detected\n-----------------------------------------------------------------\nxpu-smi is not installed. Please install xpu-smi according to README.md\nroot@docker-desktop:/llm/models/git/ipex-llm/python/llm/scripts# \n\n**Additional context**\n...\n",
      "state": "closed",
      "author": "linhao622",
      "author_type": "User",
      "created_at": "2025-03-11T15:51:49Z",
      "updated_at": "2025-03-12T17:22:44Z",
      "closed_at": "2025-03-12T17:22:44Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12960/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "xiangyuT"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12960",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12960",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:05.487262",
      "comments": [
        {
          "author": "xiangyuT",
          "body": "Hi @linhao622,\n\nThe issue seems because your CPU  (`i5-8259U`) lacking support for the `AVX512` ISA. Our Docker image was compiled with AVX512 optimizations enabled.\n\nYou can try:\n1. Testing in a modern environment with AVX512-supported hardware, **or**\n2. Recompiling vLLM CPU in your current enviro",
          "created_at": "2025-03-12T01:59:03Z"
        },
        {
          "author": "linhao622",
          "body": "thankyou，i will give it a try\n> Hi @linhao622,\n> \n> The issue seems because your CPU  (`i5-8259U`) lacking support for the `AVX512` ISA. Our Docker image was compiled with AVX512 optimizations enabled.\n> \n> You can try:\n> 1. Testing in a modern environment with AVX512-supported hardware, **or**\n> 2.",
          "created_at": "2025-03-12T03:55:26Z"
        }
      ]
    },
    {
      "issue_number": 12954,
      "title": "Docker container crash due to \"No supported config format found in default_model_path\"",
      "body": "\n#### Environment\n\n```\n➜ airren@arc-a770  ~  docker images\nREPOSITORY                                     TAG         IMAGE ID      CREATED       SIZE\ndocker.io/intelanalytics/ipex-llm-serving-xpu  latest      8ff908c82b3f  31 hours ago  23.7 GB\n➜ airren@arc-a770  ~  hwinfo --display\n15: PCI 6700.0: 0300 VGA compatible controller (VGA)\n  [Created at pci.386]\n  Unique ID: KMWP.A9wBFoNnX+6\n  Parent ID: kcMk.mr2N3fBJq5F\n  SysFS ID: /devices/pci0000:64/0000:64:00.0/0000:65:00.0/0000:66:01.0/0000:67:00.0\n  SysFS BusID: 0000:67:00.0\n  Hardware Class: graphics card\n  Model: \"Intel VGA compatible controller\"\n  Vendor: pci 0x8086 \"Intel Corporation\"\n  Device: pci 0x56a0\n  SubVendor: pci 0x1ef7\n  SubDevice: pci 0x1684\n  Revision: 0x08\n  Driver: \"i915\"\n  Driver Modules: \"i915\"\n  Memory Range: 0xd7000000-0xd7ffffff (rw,non-prefetchable)\n  Memory Range: 0xc0000000-0xcfffffff (ro,non-prefetchable)\n  Memory Range: 0x000c0000-0x000dffff (rw,non-prefetchable,disabled)\n  IRQ: 65 (52054 events)\n  Module Alias: \"pci:v00008086d000056A0sv00001EF7sd00001684bc03sc00i00\"\n  Driver Info #0:\n    Driver Status: i915 is active\n    Driver Activation Cmd: \"modprobe i915\"\n  Driver Info #1:\n    Driver Status: xe is active\n    Driver Activation Cmd: \"modprobe xe\"\n  Config Status: cfg=new, avail=yes, need=no, active=unknown\n  Attached to: #113 (PCI bridge)\n\nPrimary display adapter: #15\n\n```\n\nmodel\n```\n\n➜ airren@arc-a770  ~/models  pwd\n/home/airren/models\n➜ airren@arc-a770  ~/models  tree -L 3\n.\n└── Qwen\n    └── Qwen2.5-7B-Instruct\n        ├── config.json\n        ├── configuration.json\n        ├── generation_config.json\n        ├── LICENSE\n        ├── merges.txt\n        ├── model-00001-of-00004.safetensors\n        ├── model-00002-of-00004.safetensors\n        ├── model-00003-of-00004.safetensors\n        ├── model-00004-of-00004.safetensors\n        ├── model.safetensors.index.json\n        ├── README.md\n        ├── tokenizer_config.json\n        ├── tokenizer.json\n        └── vocab.json\n\n3 directories, 14 files\n\nexport DOCKER_IMAGE=intelanalytics/ipex-llm-serving-xpu:latest\nexport CONTAINER_NAME=ipex-llm-serving-xpu-container\nexport MODEL_PATH=\"/llm/models/Qwen/Qwen2.5-7B-Instruct\"\nexport SERVED_MODEL_NAME=\"Qwen/Qwen2.5-7B-Instruct\"\nsudo docker run -itd \\\n        --net=host \\\n        --group-add video \\\n        --device=/dev/dri \\\n        -v /home/airren/models:/llm/models \\\n        -e no_proxy=localhost,127.0.0.1 \\\n        --memory=\"32G\" \\\n        --name=$CONTAINER_NAME \\\n        --shm-size=\"16g\" \\\n        $DOCKER_IMAGE\n```\n\n### Crash log\n\n```log\nINFO 03-08 00:26:12 api_server.py:197] Started engine process with PID 73\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/entrypoints/openai/api_server.py\", line 862, in <module>\n    uvloop.run(run_server(args))\n  File \"/usr/local/lib/python3.11/dist-packages/uvloop/__init__.py\", line 105, in run\n    return runner.run(wrapper())\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/usr/local/lib/python3.11/dist-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n           ^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/entrypoints/openai/api_server.py\", line 821, in run_server\n    async with build_async_engine_client(args) as engine_client:\n  File \"/usr/lib/python3.11/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/entrypoints/openai/api_server.py\", line 123, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n  File \"/usr/lib/python3.11/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/entrypoints/openai/api_server.py\", line 208, in build_async_engine_client_from_engine_args\n    engine_config = engine_args.create_engine_config()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/arg_utils.py\", line 1066, in create_engine_config\n    model_config = self.create_model_config()\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/arg_utils.py\", line 983, in create_model_config\n    return ModelConfig(\n           ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/config.py\", line 286, in __init__\n    hf_config = get_config(self.model, trust_remote_code, revision,\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/transformers_utils/config.py\", line 196, in get_config\n    raise ValueError(f\"No supported config format found in {model}\")\nValueError: No supported config format found in default_model_path\nINFO 03-08 00:26:15 __init__.py:180] Automatically detected platform xpu.\n2025-03-08 00:26:19,533 - ERROR - No supported config format found in default_model_path\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 234, in run_mp_engine\n    engine = IPEXLLMMQLLMEngine.from_engine_args(engine_args=engine_args,\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 221, in from_engine_args\n    return super().from_engine_args(engine_args, usage_context, ipc_path)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/multiprocessing/engine.py\", line 114, in from_engine_args\n    engine_config = engine_args.create_engine_config(usage_context)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/arg_utils.py\", line 1066, in create_engine_config\n    model_config = self.create_model_config()\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/arg_utils.py\", line 983, in create_model_config\n    return ModelConfig(\n           ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/config.py\", line 286, in __init__\n    hf_config = get_config(self.model, trust_remote_code, revision,\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/transformers_utils/config.py\", line 196, in get_config\n    raise ValueError(f\"No supported config format found in {model}\")\nValueError: No supported config format found in default_model_path\nProcess SpawnProcess-1:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 242, in run_mp_engine\n    raise e  # noqa\n    ^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 234, in run_mp_engine\n    engine = IPEXLLMMQLLMEngine.from_engine_args(engine_args=engine_args,\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 221, in from_engine_args\n    return super().from_engine_args(engine_args, usage_context, ipc_path)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/multiprocessing/engine.py\", line 114, in from_engine_args\n    engine_config = engine_args.create_engine_config(usage_context)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/arg_utils.py\", line 1066, in create_engine_config\n    model_config = self.create_model_config()\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/arg_utils.py\", line 983, in create_model_config\n    return ModelConfig(\n           ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/config.py\", line 286, in __init__\n    hf_config = get_config(self.model, trust_remote_code, revision,\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/transformers_utils/config.py\", line 196, in get_config\n    raise ValueError(f\"No supported config format found in {model}\")\nValueError: No supported config format found in default_model_path\n\n```",
      "state": "closed",
      "author": "Airren",
      "author_type": "User",
      "created_at": "2025-03-07T13:11:05Z",
      "updated_at": "2025-03-12T05:57:42Z",
      "closed_at": "2025-03-12T05:57:14Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12954/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12954",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12954",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:05.715157",
      "comments": [
        {
          "author": "liu-shaojun",
          "body": "Could you try starting the container with the following command?\n\n```\nsudo docker run -td \\\n    --privileged \\\n    --net=host \\\n    --device=/dev/dri \\\n    --name=test-container \\\n    -v /home/airren/models:/llm/models \\\n    -e no_proxy=localhost,127.0.0.1 \\\n    --shm-size=\"16g\" \\\n    --entrypoint /",
          "created_at": "2025-03-10T02:25:53Z"
        },
        {
          "author": "Airren",
          "body": "something wrong about my platform",
          "created_at": "2025-03-12T05:57:14Z"
        }
      ]
    },
    {
      "issue_number": 12937,
      "title": "Arc A770 MiniCPM-o-2_6 performance issue",
      "body": "I'm evaluating multimodal image/video understanding on A770. Had tested `MimiCPM-o-2_6/MiniCPU-V-2_6/Janus-Pro-7B/Qwen2-VL-7B-Instruct` on A770, environment variable **SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS** has big performance impact, and it's also related to different task.\n\n```\nsource /opt/intel/oneapi/setvars.sh\nexport USE_XETLA=OFF\nexport SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1\nexport SYCL_CACHE_PERSISTENT=1\n```  \nFor image chat task, the image is http://farm6.staticflickr.com/5268/5602445367_3504763978_z.jpg in ipex-llm example.\n```\nipex-llm/python/llm/example/GPU/HuggingFace/Multimodal/MiniCPM-o-2_6# python chat.py --repo-id-or-model-path /models/MiniCPM-o-2_6/ --image-path 5602445367_3504763978_z.jpg --prompt \"请详细描述图像内容\" --n-predict 512\n\nInference time: 10.233431100845337 s\n```\nAfter `export SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=0` the result is `Inference time: 3.5721495151519775 s`.  Seems for image understanding, should unset SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS.\n\nBut when I test video understanding task (see below video_chat.py)\n```python\n import os\nimport time\nimport torch\nimport librosa\nimport argparse\nfrom PIL import Image\nfrom transformers import AutoTokenizer\nfrom ipex_llm.transformers import AutoModel\nfrom decord import VideoReader, cpu    # pip install decord\n\nMAX_NUM_FRAMES=10 # if cuda OOM set a smaller number\n\ndef encode_video(video_path):\n    def uniform_sample(l, n):\n        gap = len(l) / n\n        idxs = [int(i * gap + gap / 2) for i in range(n)]\n        return [l[i] for i in idxs]\n\n    #vr = VideoReader(video_path, ctx=cpu(0), width=960, height=540)\n    vr = VideoReader(video_path, ctx=cpu(0))\n    sample_fps = round(vr.get_avg_fps() / 1)  # FPS\n    frame_idx = [i for i in range(0, len(vr), sample_fps)]\n    print(frame_idx)\n    if len(frame_idx) > MAX_NUM_FRAMES:\n        frame_idx = uniform_sample(frame_idx, MAX_NUM_FRAMES)\n    frames = vr.get_batch(frame_idx).asnumpy()\n    frames = [Image.fromarray(v.astype('uint8')) for v in frames]\n    print('num frames:', len(frames))\n    return frames\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Chat with MiniCPM-o-2_6 with text/audio/image')\n    parser.add_argument('--repo-id-or-model-path', type=str, default=\"openbmb/MiniCPM-o-2_6\",\n                        help='The Hugging Face or ModelScope repo id for the MiniCPM-o-2_6 model to be downloaded'\n                             ', or the path to the checkpoint folder')\n    parser.add_argument('--video-path', type=str,\n                        help='The path to the image for inference.')\n    parser.add_argument('--audio-path', type=str,\n                        help='The path to the audio for inference.')\n    parser.add_argument('--prompt', type=str,\n                        help='Prompt for inference.')\n    parser.add_argument('--n-predict', type=int, default=32,\n                        help='Max tokens to predict')\n    \n    args = parser.parse_args()\n\n    model_path = args.repo_id_or_model_path\n    video_path = args.video_path\n    audio_path = args.audio_path\n\n    modules_to_not_convert = []\n    init_vision = False\n    init_audio = False\n    if video_path is not None and os.path.exists(video_path):\n        init_vision = True\n        modules_to_not_convert += [\"vpm\", \"resampler\"]\n    if audio_path is not None and os.path.exists(audio_path):\n        init_audio = True\n        modules_to_not_convert += [\"apm\"]\n\n    # Load model in 4 bit,\n    # which convert the relevant layers in the model into INT4 format\n    model = AutoModel.from_pretrained(model_path, \n                                      load_in_low_bit=\"sym_int4\",\n                                      optimize_model=True,\n                                      trust_remote_code=True,\n                                      attn_implementation='sdpa',\n                                      use_cache=True,\n                                      init_vision=init_vision,\n                                      init_audio=init_audio,\n                                      init_tts=False,\n                                      modules_to_not_convert=modules_to_not_convert)\n    \n    model = model.half().to('xpu')\n\n    tokenizer = AutoTokenizer.from_pretrained(model_path,\n                                              trust_remote_code=True)\n    \n\n    # The following code for generation is adapted from \n    # https://huggingface.co/openbmb/MiniCPM-o-2_6#addressing-various-audio-understanding-tasks and\n    # https://huggingface.co/openbmb/MiniCPM-o-2_6#chat-with-single-image\n    content = []\n    if init_vision:\n        frames = encode_video(video_path)\n    if args.prompt is not None:\n        content.append(args.prompt)\n    if init_audio:\n        audio_input, _ = librosa.load(audio_path, sr=16000, mono=True)\n        content.append(audio_input)\n    messages = [{'role': 'user', 'content': frames + [args.prompt]}]\n\n\n    with torch.inference_mode():\n        # ipex_llm model needs a warmup, then inference time can be accurate\n        model.chat(\n            msgs=messages,\n            tokenizer=tokenizer,\n            sampling=True,\n            max_new_tokens=args.n_predict,\n        )\n\n        st = time.time()\n        response = model.chat(\n            msgs=messages,\n            tokenizer=tokenizer,\n            sampling=True,\n            max_new_tokens=args.n_predict,\n        )\n        torch.xpu.synchronize()\n        end = time.time()\n\n    output_ids = tokenizer.encode(response, return_tensors=\"pt\").to('xpu')\n    print(f'Output lengh: {len(output_ids[0])}')\n\n    print(f'Inference time: {end-st} s')\n    print('-'*20, 'Input Video Path', '-'*20)\n    print(video_path)\n    print('-'*20, 'Input Audio Path', '-'*20)\n    print(audio_path)\n    print('-'*20, 'Input Prompt', '-'*20)\n    print(args.prompt)\n    print('-'*20, 'Chat Output', '-'*20)\n    print(response)\n```\n```\npython video_chat.py --repo-id-or-model-path /models/MiniCPM-o-2_6/ --video-path Skiing-1080p.mp4 --prompt \"请描述这个视频,使用中文\" --n-predict 512\n\nexport SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=0\nInference time: 105.26203894615173 s\n\nexport SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1\nInference time: 16.64881181716919 s\n```\n`Skiing-1080p.mp4`'s resolution is 1920x1080, it's transcoded from original 960x540 `https://huggingface.co/openbmb/MiniCPM-o-2_6/blob/main/assets/Skiing.mp4`\n`ffmpeg -i Skiing.mp4 -c:v h264 -s 1920x1080 Skiing-1080p.mp4`\n\nUsing original Skiing.mp4\n```\nexport SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=0\nInference time: 6.2952880859375 s\n\nexport SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1\nInference time: 12.700184106826782 s\n```\nSeems it's also related to resolution. It's difficult to choose **SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS** value.",
      "state": "closed",
      "author": "caijimin",
      "author_type": "User",
      "created_at": "2025-03-05T06:55:20Z",
      "updated_at": "2025-03-12T00:43:07Z",
      "closed_at": "2025-03-12T00:43:05Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12937/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12937",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12937",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:05.955829",
      "comments": [
        {
          "author": "ATMxsp01",
          "body": "We are trying to reproduce this issue, and update here for any progress :)",
          "created_at": "2025-03-07T08:25:17Z"
        },
        {
          "author": "ATMxsp01",
          "body": "After trying, we did not reproduce the extremely long time cost in `video_chat.py`  `Skiing-1080p.mp4` with `SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=0`.\n\nCould you retry your test to make sure the previous result is not an outlier?",
          "created_at": "2025-03-10T01:25:39Z"
        },
        {
          "author": "caijimin",
          "body": "I upgraded the kernel to 6.5.0-35-generic and also upgraded package \"intel-i915-dkms intel-fw-gpu\", this issue is disappeared, seems it's driver version problem. Thanks.",
          "created_at": "2025-03-12T00:43:05Z"
        }
      ]
    },
    {
      "issue_number": 12958,
      "title": "ollama-0.5.4-ipex-llm-2.2.0b20250226-win model xxx not found",
      "body": "Error “model xxx not found” occurs when running DS model in ollama-0.5.4-ipex-llm-2.2.0b20250226-win.\n\nStep:\n1. Download \"ollama-0.5.4-ipex-llm-2.2.0b20250226-win\" from https://www.modelscope.cn/models/ipexllm/ollama-ipex-llm/files\n2. Extract the zip file to a folder\n3. Start Ollama serve as follows:\n   - Open \"Command Prompt\" (cmd), enter the extracted folder by \"cd /d PATH\\TO\\EXTRACTED\\FOLDER\"\n   - Run \"start-ollama.bat\" in the \"Command Prompt\", and then a window will pop up for Ollama serve\n4. In the same \"Command Prompt\" (not the pop-up window), run \"curl http://localhost:11434/api/generate -d \"{\\\"model\\\": \\\"DeepSeek-R1-Distill-Llama-8B-GGUF\\\", \\\"prompt\\\": \\\"Hello\\\" }\"\" (you may use any other model)\n\n\nerror message：\nC:\\Users\\X>curl http://localhost:11434/api/generate -d \"{\\\"model\\\": \\\"deepseek-r1:8b\\\", \\\"prompt\\\": \\\"Hello\\\" }\"\n{\"error\":\"model 'deepseek-r1:8b' not found\"}\nC:\\Users\\X>cd C:\\Users\\X\\Documents\\ollama-0.5.4-ipex-llm-2.2.0b20250226-win\n\nC:\\Users\\X\\Documents\\ollama-0.5.4-ipex-llm-2.2.0b20250226-win>ollama list\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\nggml_sycl_init: SYCL_USE_XMX: yes\nggml_sycl_init: found 1 SYCL devices:\nNAME                                                              ID              SIZE      MODIFIED\nmodelscope.cn/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF:Q4_K_M    4c337bc8b461    4.9 GB    4 days ago\nmodelscope.cn/unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF:Q4_K_M    5782868bf1bb    9.0 GB    5 days ago\n\nC:\\Users\\X\\Documents\\ollama-0.5.4-ipex-llm-2.2.0b20250226-win>curl http://localhost:11434/api/generate -d \"{\\\"model\\\": \\\"DeepSeek-R1-Distill-Llama-8B-GGUF\\\", \\\"prompt\\\": \\\"Hello\\\" }\"\n{\"error\":\"model 'DeepSeek-R1-Distill-Llama-8B-GGUF' not found\"}\nC:\\Users\\X\\Documents\\ollama-0.5.4-ipex-llm-2.2.0b20250226-win>curl http://localhost:11434/api/generate -d \"{\\\"model\\\": \\\"DeepSeek-R1-Distill-Qwen-14B-GGUF\\\", \\\"prompt\\\": \\\"Hello\\\" }\"\n{\"error\":\"model 'DeepSeek-R1-Distill-Qwen-14B-GGUF' not found\"}",
      "state": "closed",
      "author": "Yaquan2",
      "author_type": "User",
      "created_at": "2025-03-10T06:52:51Z",
      "updated_at": "2025-03-11T04:45:17Z",
      "closed_at": "2025-03-11T02:58:24Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12958/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12958",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12958",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:06.168466",
      "comments": [
        {
          "author": "Yaquan2",
          "body": "```\nC:\\Users\\X\\Documents\\ollama-0.5.4-ipex-llm-2.2.0b20250226-win>curl http://localhost:11434/api/generate -d \"{ \\\"model\\\": \\\"modelscope.cn/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF:Q4_K_M\\\",  \\\"prompt\\\": \\\"Hello\\\" }\"\n{\"model\":\"modelscope.cn/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF:Q4_K_M\",\"created",
          "created_at": "2025-03-10T08:30:39Z"
        },
        {
          "author": "KiwiHana",
          "body": "curl http://localhost:11434/api/generate -d \"{ \\\"model\\\": \\\"modelscope.cn/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF:Q4_K_M\\\",  \\\"prompt\\\": \\\"Hello\\\",  \\\"stream\\\": false}\"",
          "created_at": "2025-03-10T09:59:59Z"
        },
        {
          "author": "KiwiHana",
          "body": "怀疑是modelscope和ipex-llm下载模型源地址不同导致的。请下载以下链接的 20250226，\n\nhttps://github.com/intel/ipex-llm/releases/tag/v2.2.0-nightly\n\n运行 ollama run deepseek-r1:8b\n\n然后 curl http://localhost:11434/api/generate -d \"{\"model\": \"deepseek-r1:8b\", \"prompt\": \"Hello\" }\"",
          "created_at": "2025-03-10T10:02:11Z"
        },
        {
          "author": "Oscilloscope98",
          "body": "Hi @Yaquan2,\n\n`DeepSeek-R1-Distill-Llama-8B-GGUF` is not a correct model id on either Ollama library or ModelScope.\n\nOllama portable zip downloaded from ModelScope will use ModelScope as default model source. Model downloaded with ModelScope as model source will still show actual model id in `ollama",
          "created_at": "2025-03-10T10:13:29Z"
        },
        {
          "author": "Yaquan2",
          "body": "We found that the problem was due to different model names being downloaded from github and modelscope, it's working as expected now, hence closing this issue, thank you for your help!\n\n`\nC:\\Users\\X\\Documents\\ollama-0.5.4-ipex-llm-2.2.0b20250226-win (1)>ollama list\nggml_sycl_init: GGML_SYCL_FORCE_MM",
          "created_at": "2025-03-11T02:58:24Z"
        }
      ]
    },
    {
      "issue_number": 12736,
      "title": "vpux-compiler error occured when using qwen2.5-7B in large content or prompt",
      "body": "First error is occured when  max-context-len >= 10000\n![Image](https://github.com/user-attachments/assets/510adb4b-120a-4cb7-9d73-1f393c8f224b)\nThe second error is occured when max-prompt-len >=6000\n![Image](https://github.com/user-attachments/assets/6f879a68-0a93-4e77-9395-c0daac393e04)\n\n\nDevice : Nuc 14 pro\nCPU: Ultra5 125H\nMemory: 96G whole\nNPU driver: 32.0.100.3104\nOthers:\n![Image](https://github.com/user-attachments/assets/514f5447-f779-47c0-a966-46c7ceb8803b)",
      "state": "open",
      "author": "shichang00",
      "author_type": "User",
      "created_at": "2025-01-22T10:23:31Z",
      "updated_at": "2025-03-10T11:37:15Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12736/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12736",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12736",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:06.457097",
      "comments": [
        {
          "author": "plusbang",
          "body": "Hi, @dockerg , `max-context-len>=10000` is not supported yet, maybe you could try 2K.",
          "created_at": "2025-01-23T02:47:34Z"
        },
        {
          "author": "shichang00",
          "body": "how about \"DDR\" memory allocate issue?",
          "created_at": "2025-01-23T03:18:34Z"
        },
        {
          "author": "plusbang",
          "body": "> how about \"DDR\" memory allocate issue?\n\nDoes this issue exist for smaller value of `max-prompt-len` (such as 1024)? Too long prompt is also not supported yet.",
          "created_at": "2025-01-23T08:21:27Z"
        },
        {
          "author": "shichang00",
          "body": "> > how about \"DDR\" memory allocate issue?\n> \n> Does this issue exist for smaller value of `max-prompt-len` (such as 1024)? Too long prompt is also not supported yet.\n\nIt occured when max-prompt-len > 4000;\nWhat limit the prompt? Hardware or npu-driver or Ipex-llm not support it?\n",
          "created_at": "2025-01-23T08:27:39Z"
        },
        {
          "author": "plusbang",
          "body": "\n> \n> It occured when max-prompt-len > 4000; What limit the prompt? Hardware or npu-driver or Ipex-llm not support it?\n\nThe issue relates to hardware and npu driver.",
          "created_at": "2025-01-24T02:26:43Z"
        }
      ]
    },
    {
      "issue_number": 12934,
      "title": "Qwen2-VL-7B can not run TP=2 on 2*ARC",
      "body": "\n4*arc workstation\nstart docker command:\n`\n#/bin/bash\nexport DOCKER_IMAGE=intelanalytics/ipex-llm-serving-xpu:latest\nexport CONTAINER_NAME=ipex-llm-serving-xpu-container_1\nsudo docker run -it \\\n        --privileged \\\n        --net=host \\\n        --device=/dev/dri \\\n        -v /home/test/:/llm/models \\\n        -e no_proxy=localhost,127.0.0.1 \\\n        --memory=\"32G\" \\\n        --name=$CONTAINER_NAME \\\n        --shm-size=\"16g\"  \\\n        --entrypoint /bin/bash \\\n        $DOCKER_IMAGE\n`\n\nMODEL_PATH=\"/llm/models/LLM/Qwen2-VL-7B-Instruct/\"\nSERVED_MODEL_NAME=\"Qwen2-VL-7B-Instruct/\"\nTENSOR_PARALLEL_SIZE=2  # Default to 1 if not set\n\n`(WrapperWithLoadBit pid=786) -----> current rank: 1, world size: 2, byte_count: 21504000\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469] Error executing method determine_num_available_blocks. This might cause deadlock in distributed execution.\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469] Traceback (most recent call last):\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/worker_base.py\", line 461, in execute_method\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]     return executor(*args, **kwargs)\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]     return func(*args, **kwargs)\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]            ^^^^^^^^^^^^^^^^^^^^^\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/xpu_worker.py\", line 106, in determine_num_available_blocks\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]     self.model_runner.profile_run()\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]     return func(*args, **kwargs)\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]            ^^^^^^^^^^^^^^^^^^^^^\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/xpu_model_runner.py\", line 821, in profile_run\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]     self.execute_model(model_input, kv_caches, intermediate_tensors)\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]     return func(*args, **kwargs)\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]            ^^^^^^^^^^^^^^^^^^^^^\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/xpu_model_runner.py\", line 931, in execute_model\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]     hidden_or_intermediate_states = model_executable(\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]                                     ^^^^^^^^^^^^^^^^^\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]     return self._call_impl(*args, **kwargs)\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]     return forward_call(*args, **kwargs)\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen2_vl.py\", line 1327, in forward\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]     hidden_states = self.language_model.model(\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/vllm/compilation/decorators.py\", line 168, in __call__\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]     return self.forward(*args, **kwargs)\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen2.py\", line 340, in forward\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]     hidden_states, residual = layer(\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]                               ^^^^^^\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]     return self._call_impl(*args, **kwargs)\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]     return forward_call(*args, **kwargs)\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen2.py\", line 247, in forward\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]     hidden_states = self.self_attn(\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]                     ^^^^^^^^^^^^^^^\n(WrapperWithLoadBit pid=786) ERROR 03-05 14:13:23 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_implERROR 03-05 14:13:42 worker_base.py:469] Error executing method determine_num_available_blocks. This might cause deadlock in distributed execution.\nERROR 03-05 14:13:42 worker_base.py:469] Traceback (most recent call last):\nERROR 03-05 14:13:42 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/worker_base.py\", line 461, in execute_method\nERROR 03-05 14:13:42 worker_base.py:469]     return executor(*args, **kwargs)\nERROR 03-05 14:13:42 worker_base.py:469]            ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-05 14:13:42 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 03-05 14:13:42 worker_base.py:469]     return func(*args, **kwargs)\nERROR 03-05 14:13:42 worker_base.py:469]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 03-05 14:13:42 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/xpu_worker.py\", line 106, in determine_num_available_blocks\nERROR 03-05 14:13:42 worker_base.py:469]     self.model_runner.profile_run()\nERROR 03-05 14:13:42 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 03-05 14:13:42 worker_base.py:469]     return func(*args, **kwargs)\nERROR 03-05 14:13:42 worker_base.py:469]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 03-05 14:13:42 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/xpu_model_runner.py\", line 821, in profile_run\nERROR 03-05 14:13:42 worker_base.py:469]     self.execute_model(model_input, kv_caches, intermediate_tensors)\nERROR 03-05 14:13:42 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 03-05 14:13:42 worker_base.py:469]     return func(*args, **kwargs)\nERROR 03-05 14:13:42 worker_base.py:469]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 03-05 14:13:42 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/xpu_model_runner.py\", line 931, in execute_model\nERROR 03-05 14:13:42 worker_base.py:469]     hidden_or_intermediate_states = model_executable(\nERROR 03-05 14:13:42 worker_base.py:469]                                     ^^^^^^^^^^^^^^^^^\nERROR 03-05 14:13:42 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\nERROR 03-05 14:13:42 worker_base.py:469]     return self._call_impl(*args, **kwargs)\nERROR 03-05 14:13:42 worker_base.py:469]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-05 14:13:42 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\nERROR 03-05 14:13:42 worker_base.py:469]     return forward_call(*args, **kwargs)\nERROR 03-05 14:13:42 worker_base.py:469]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-05 14:13:42 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen2_vl.py\", line 1327, in forward\nERROR 03-05 14:13:42 worker_base.py:469]     hidden_states = self.language_model.model(\nERROR 03-05 14:13:42 worker_base.py:469]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-05 14:13:42 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/vllm/compilation/decorators.py\", line 168, in __call__\nERROR 03-05 14:13:42 worker_base.py:469]     return self.forward(*args, **kwargs)\nERROR 03-05 14:13:42 worker_base.py:469]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-05 14:13:42 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen2.py\", line 340, in forward\nERROR 03-05 14:13:42 worker_base.py:469]     hidden_states, residual = layer(\nERROR 03-05 14:13:42 worker_base.py:469]                               ^^^^^^\nERROR 03-05 14:13:42 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\nERROR 03-05 14:13:42 worker_base.py:469]     return self._call_impl(*args, **kwargs)\nERROR 03-05 14:13:42 worker_base.py:469]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-05 14:13:42 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\nERROR 03-05 14:13:42 worker_base.py:469]     return forward_call(*args, **kwargs)\nERROR 03-05 14:13:42 worker_base.py:469]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-05 14:13:42 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen2.py\", line 247, in forward\nERROR 03-05 14:13:42 worker_base.py:469]     hidden_states = self.self_attn(\nERROR 03-05 14:13:42 worker_base.py:469]                     ^^^^^^^^^^^^^^^\nERROR 03-05 14:13:42 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\nERROR 03-05 14:13:42 worker_base.py:469]     return self._call_impl(*args, **kwargs)\nERROR 03-05 14:13:42 worker_base.py:469]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-05 14:13:42 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\nERROR 03-05 14:13:42 worker_base.py:469]     return forward_call(*args, **kwargs)\nERROR 03-05 14:13:42 worker_base.py:469]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-05 14:13:42 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen2.py\", line 176, in forward\nERROR 03-05 14:13:42 worker_base.py:469]     attn_output = self.attn(q,\nERROR 03-05 14:13:42 worker_base.py:469]                   ^^^^^^^^^^^^\nERROR 03-05 14:13:42 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\nERROR 03-05 14:13:42 worker_base.py:469]     return self._call_impl(*args, **kwargs)\nERROR 03-05 14:13:42 worker_base.py:469]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-05 14:13:42 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\nERROR 03-05 14:13:42 worker_base.py:469]     return forward_call(*args, **kwargs)\nERROR 03-05 14:13:42 worker_base.py:469]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-05 14:13:42 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/vllm/attention/layer.py\", line 134, in forward\nERROR 03-05 14:13:42 worker_base.py:469]     return self.impl.forward(query,\nERROR 03-05 14:13:42 worker_base.py:469]            ^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-05 14:13:42 worker_base.py:469]   File \"/usr/local/lib/python3.11/dist-packages/vllm/attention/backends/ipex_attn.py\", line 449, in forward\nERROR 03-05 14:13:42 worker_base.py:469]     sub_out = xe_addons.sdp_causal(\nERROR 03-05 14:13:42 worker_base.py:469]               ^^^^^^^^^^^^^^^^^^^^^\nERROR 03-05 14:13:42 worker_base.py:469] RuntimeError: UR backend failed. UR backend returns:40 (UR_RESULT_ERROR_OUT_OF_RESOURCES)\n\n`\n",
      "state": "open",
      "author": "Zjq9409",
      "author_type": "User",
      "created_at": "2025-03-05T06:19:22Z",
      "updated_at": "2025-03-10T11:36:50Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12934/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12934",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12934",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:06.661093",
      "comments": [
        {
          "author": "gc-fu",
          "body": "Hi, can you try to comment out `source /opt/intel/1ccl-wks/setvars.sh` and run again?",
          "created_at": "2025-03-05T06:53:24Z"
        }
      ]
    },
    {
      "issue_number": 9829,
      "title": "Starting from bigdl-llm[xpu]==2.5.0b20231227, the 32-32 output of gguf models on Arc770 contains <unk>",
      "body": "## Issue description\r\n\r\nDuring benchmarking GGUF models on Arc 770, from bigdl-llm[xpu]==2.5.0b20231227, the 32-32 output of gguf models contains <unk> randomly. I tested baichuan and llama and both had this issue. There is no such issue with bigdl-llm[xpu]==2.5.0b20231226.\r\n\r\n<img width=\"766\" alt=\"MicrosoftTeams-image (2)\" src=\"https://github.com/intel-analytics/BigDL/assets/61072813/a749b2b2-e223-402d-b5a3-6d5827e0d319\">\r\n\r\n\r\n## To reproduce\r\n\r\nWe are using all-in-one/run.py to test it:\r\n\r\nhttps://github.com/intel-analytics/BigDL/blob/main/python/llm/dev/benchmark/all-in-one/run.py\r\n\r\nupdating run_transformer_int4_gpu() in run.py to use from_gguf() to load model\r\n\r\n```bash\r\ndef run_transformer_int4_gpu(repo_id,\r\n                             local_model_hub,\r\n                             in_out_pairs,\r\n                             warm_up,\r\n                             num_trials,\r\n                             num_beams,\r\n                             low_bit):\r\n    from bigdl.llm.transformers import AutoModel, AutoModelForCausalLM\r\n    from transformers import AutoTokenizer, GPTJForCausalLM, LlamaTokenizer\r\n    import intel_extension_for_pytorch as ipex\r\n    reserved_mem_list = []\r\n    #model_path = get_model_path(repo_id, local_model_hub)\r\n    model_path = \"/mnt/disk1/models/gguf/llama-2-7b-chat.Q4_0.gguf\"\r\n    # Load model in 4 bit,\r\n    # which convert the relevant layers in the model into INT4 format\r\n    st = time.perf_counter()\r\n    model, tokenizer = AutoModelForCausalLM.from_gguf(model_path)\r\n    model = model.to('xpu')\r\n    end = time.perf_counter()\r\n    print(\">> loading of model costs {}s\".format(end - st))\r\n    reserved_mem_list.append(torch.xpu.memory.memory_reserved()/(1024**3))\r\n\r\n    model = BenchmarkWrapper(model)\r\n\r\n    result = {}\r\n    with torch.inference_mode():\r\n        for in_out in in_out_pairs:\r\n            in_out_len = in_out.split(\"-\")\r\n            in_len = int(in_out_len[0])\r\n            out_len = int(in_out_len[1])\r\n            # As different tokenizer has different encodings,\r\n            # in_len.txt maybe shorter than we need,\r\n            # use much longer context to make sure input length\r\n            test_length = min(in_len*2, 8192)\r\n            while test_length not in [32, 256, 1024, 2048, 8192]:\r\n                test_length = test_length * 2\r\n            input_str = open(f\"prompt/{test_length}.txt\", 'r').read()\r\n            input_ids = tokenizer.encode(input_str, return_tensors=\"pt\")\r\n            input_ids = input_ids[:, :in_len]\r\n            true_str = tokenizer.batch_decode(input_ids)[0]\r\n            input_ids = tokenizer.encode(true_str, return_tensors=\"pt\").to('xpu')\r\n            actual_in_len = input_ids.shape[1]\r\n            result[in_out] = []\r\n            thread = threading.Thread(target=run_model_in_thread, args=(model, in_out, tokenizer, result, warm_up, num_beams, input_ids, out_len, actual_in_len, num_trials, reserved_mem_list))\r\n            thread.start()\r\n            thread.join()\r\n    model.to('cpu')\r\n    torch.xpu.synchronize()\r\n    torch.xpu.empty_cache()\r\n    del model\r\n    gc.collect()\r\n    return result\r\n```\r\n\r\n",
      "state": "open",
      "author": "liu-shaojun",
      "author_type": "User",
      "created_at": "2024-01-04T02:06:14Z",
      "updated_at": "2025-03-10T11:35:48Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/9829/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/9829",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/9829",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:06.886069",
      "comments": [
        {
          "author": "MeouSker77",
          "body": "**This bug is caused by using XMX kernel in a new thread**, it won't happen if running model in current thread. And I think its root cause is a bug of oneapi 2023.2 or related to machine or driver.\r\nWhen using ipex 2.1 and oneapi 2024.0, this bug won't happen.\r\nBefore 1227 version, 32 input won't us",
          "created_at": "2024-01-04T09:17:51Z"
        },
        {
          "author": "jason-dai",
          "body": "> **This bug is caused by using XMX kernel in a new thread**, it won't happen if running model in current thread. And I think its root cause is a bug of oneapi 2023.2 or related to machine or driver. When using ipex 2.1 and oneapi 2024.0, this bug won't happen. Before 1227 version, 32 input won't us",
          "created_at": "2024-01-04T09:32:12Z"
        }
      ]
    },
    {
      "issue_number": 9790,
      "title": "chatglm3-6b can't use batch_size to generate",
      "body": "I use [generate.py](https://github.com/intel-analytics/BigDL/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/chatglm3/generate.py) to test, and update it to use batch_size to generate.\r\n```bash\r\n#input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\r\n#prompt = [prompt] * batch_size\r\nprompt = [prompt] * 2\r\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\r\n\r\n#output_str = tokenizer.decode(output[0], skip_special_tokens=True)\r\noutput_str = tokenizer.batch_decode(output, skip_special_tokens=True)\r\n```\r\nAnd it will report an error:\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"/root/wangjian/project/bigdl-llm/BigDL/python/llm/dev/benchmark/all-in-one/generate/generate-chatglm2.py\", line 65, in <module>\r\n    output = model.generate(input_ids,\r\n  File \"/root/anaconda3/envs/wangjian-llm/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/root/anaconda3/envs/wangjian-llm/lib/python3.9/site-packages/transformers/generation/utils.py\", line 1538, in generate\r\n    return self.greedy_search(\r\n  File \"/root/anaconda3/envs/wangjian-llm/lib/python3.9/site-packages/transformers/generation/utils.py\", line 2362, in greedy_search\r\n    outputs = self(\r\n  File \"/root/anaconda3/envs/wangjian-llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/root/anaconda3/envs/wangjian-llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/root/.cache/huggingface/modules/transformers_modules/chatglm3-6b/modeling_chatglm.py\", line 937, in forward\r\n    transformer_outputs = self.transformer(\r\n  File \"/root/anaconda3/envs/wangjian-llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/root/anaconda3/envs/wangjian-llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/root/anaconda3/envs/wangjian-llm/lib/python3.9/site-packages/bigdl/llm/transformers/models/chatglm2.py\", line 152, in chatglm2_model_forward\r\n    hidden_states, presents, all_hidden_states, all_self_attentions = self.encoder(\r\n  File \"/root/anaconda3/envs/wangjian-llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/root/anaconda3/envs/wangjian-llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/root/.cache/huggingface/modules/transformers_modules/chatglm3-6b/modeling_chatglm.py\", line 640, in forward\r\n    layer_ret = layer(\r\n  File \"/root/anaconda3/envs/wangjian-llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/root/anaconda3/envs/wangjian-llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/root/.cache/huggingface/modules/transformers_modules/chatglm3-6b/modeling_chatglm.py\", line 544, in forward\r\n    attention_output, kv_cache = self.self_attention(\r\n  File \"/root/anaconda3/envs/wangjian-llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/root/anaconda3/envs/wangjian-llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/root/anaconda3/envs/wangjian-llm/lib/python3.9/site-packages/bigdl/llm/transformers/models/chatglm2.py\", line 359, in chatglm2_attention_forward_8eb45c\r\n    output = self.dense(context_layer)\r\n  File \"/root/anaconda3/envs/wangjian-llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/root/anaconda3/envs/wangjian-llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/root/anaconda3/envs/wangjian-llm/lib/python3.9/site-packages/bigdl/llm/transformers/low_bit_linear.py\", line 466, in forward\r\n    x_2d = x.view(-1, x_shape[-1])\r\nRuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\r\n```\r\nIf using hf model (need to set the chatglm-6b config.json's torch_dtype from `float16` to `float`), and the output will be normal:\r\n```bash\r\n    #model = AutoModel.from_pretrained(model_path,\r\n    #                                  load_in_4bit=True,\r\n    #                                  trust_remote_code=True)\r\n\r\n    from transformers import AutoModel\r\n    model = AutoModel.from_pretrained(model_path, trust_remote_code=True)\r\n\r\n-------------------- Output --------------------\r\n['[gMASK]sop <|user|>\\nAI是什么？\\n<|assistant|> AI是人工智能（Artificial Intelligence）的缩写，指的是由计算机系统和软件实现的智能。AI旨在使计算机能够执行需要人类智能才能完成的各种', '[gMASK]sop <|user|>\\nAI是什么？\\n<|assistant|> AI是人工智能（Artificial Intelligence）的缩写，指的是由计算机系统和软件实现的智能。AI旨在使计算机能够执行需要人类智能才能完成的各种']\r\n\r\n```\r\n",
      "state": "open",
      "author": "hzjane",
      "author_type": "User",
      "created_at": "2023-12-27T02:02:40Z",
      "updated_at": "2025-03-10T11:35:47Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/9790/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/9790",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/9790",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:07.073242",
      "comments": []
    },
    {
      "issue_number": 10249,
      "title": "Run llama2-chat-hf with tranformers 4.38.1 failed",
      "body": "Get below error：\r\n```\r\n<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\r\nKeyword arguments {'add_special_tokens': False} not recognized.\r\nKeyword arguments {'add_special_tokens': False} not recognized.\r\n/home/arda/xin/BigDL-xin/python/llm/dev/benchmark/all-in-one/../benchmark_util.py:1295: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\r\n  warnings.warn(\r\n2024-02-27 10:27:43,904 - ERROR - \r\n\r\n****************************Usage Error************************\r\nAttention mask should be of size (1, 1, 33, 33), but is torch.Size([1, 1, 4096, 4096])\r\n2024-02-27 10:27:43,904 - ERROR - \r\n\r\n****************************Call Stack*************************\r\nException in thread Thread-4:\r\nTraceback (most recent call last):\r\n  File \"/home/arda/anaconda3/envs/xin-llm/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\r\n    self.run()\r\n  File \"/home/arda/anaconda3/envs/xin-llm/lib/python3.9/threading.py\", line 917, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/arda/xin/BigDL-xin/python/llm/dev/benchmark/all-in-one/run.py\", line 62, in run_model_in_thread\r\n    output_ids = model.generate(input_ids, do_sample=False, max_new_tokens=out_len,\r\n  File \"/home/arda/anaconda3/envs/xin-llm/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/arda/xin/BigDL-xin/python/llm/dev/benchmark/all-in-one/../benchmark_util.py\", line 1563, in generate\r\n    return self.greedy_search(\r\n  File \"/home/arda/xin/BigDL-xin/python/llm/dev/benchmark/all-in-one/../benchmark_util.py\", line 2385, in greedy_search\r\n    outputs = self(\r\n  File \"/home/arda/xin/BigDL-xin/python/llm/dev/benchmark/all-in-one/../benchmark_util.py\", line 533, in __call__\r\n    return self.model(*args, **kwargs)\r\n  File \"/home/arda/anaconda3/envs/xin-llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/arda/anaconda3/envs/xin-llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/arda/anaconda3/envs/xin-llm/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\", line 1168, in forward\r\n    outputs = self.model(\r\n  File \"/home/arda/anaconda3/envs/xin-llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/arda/anaconda3/envs/xin-llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/arda/anaconda3/envs/xin-llm/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\", line 1008, in forward\r\n    layer_outputs = decoder_layer(\r\n  File \"/home/arda/anaconda3/envs/xin-llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/arda/anaconda3/envs/xin-llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/arda/anaconda3/envs/xin-llm/lib/python3.9/site-packages/bigdl/llm/transformers/models/llama.py\", line 190, in llama_decoder_forward\r\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\n  File \"/home/arda/anaconda3/envs/xin-llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/arda/anaconda3/envs/xin-llm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/arda/anaconda3/envs/xin-llm/lib/python3.9/site-packages/bigdl/llm/transformers/models/llama.py\", line 1047, in llama_attention_forward_4_36\r\n    attn_output, attn_weights = native_sdp(query_states, key_states, value_states,\r\n  File \"/home/arda/anaconda3/envs/xin-llm/lib/python3.9/site-packages/bigdl/llm/transformers/models/llama.py\", line 1090, in native_sdp\r\n    invalidInputError(False,\r\n  File \"/home/arda/anaconda3/envs/xin-llm/lib/python3.9/site-packages/bigdl/llm/utils/common/log4Error.py\", line 32, in invalidInputError\r\n    raise RuntimeError(errMsg)\r\nRuntimeError: Attention mask should be of size (1, 1, 33, 33), but is torch.Size([1, 1, 4096, 4096])\r\n\r\n```",
      "state": "open",
      "author": "qiuxin2012",
      "author_type": "User",
      "created_at": "2024-02-27T02:28:56Z",
      "updated_at": "2025-03-10T11:35:15Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/10249/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/10249",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/10249",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:07.073265",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "Caused by https://github.com/huggingface/transformers/blob/a0857740c0e6127485c11476650314df3accc2b6/src/transformers/models/llama/modeling_llama.py#L369\r\n",
          "created_at": "2024-02-27T03:22:54Z"
        }
      ]
    },
    {
      "issue_number": 10552,
      "title": "Update installation instructions to install oneapi 2024.0",
      "body": "Currently instructions do not have version qualifications. \r\n\r\nWe also need to give instructions on how to upgrade and downgrade. ",
      "state": "open",
      "author": "yangw1234",
      "author_type": "User",
      "created_at": "2024-03-26T22:05:31Z",
      "updated_at": "2025-03-10T11:35:00Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/10552/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "chtanch"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/10552",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/10552",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:07.293057",
      "comments": [
        {
          "author": "chtanch",
          "body": "#### PR\r\nSpecify oneAPI 2024.0 - https://github.com/intel-analytics/ipex-llm/pull/10561\r\n\r\n#### Notes\r\n- Currently offline installer [download site](https://www.intel.com/content/www/us/en/developer/tools/oneapi/base-toolkit-download.html) only shows oneAPI 2024.1.0. Previous versions are not availa",
          "created_at": "2024-03-27T09:19:53Z"
        },
        {
          "author": "hkvision",
          "body": "The first issue, @Oscilloscope98 is helping to check and change the url for 2024.0\r\nFor the second issue, @qiuxin2012 Does Gemma require 2024.0.1? If so, do we need to modify the download url for the linux offline installer to install 2024.0.1?",
          "created_at": "2024-03-27T10:08:15Z"
        }
      ]
    },
    {
      "issue_number": 12343,
      "title": "[NPU] Typo in npu_model.py causes error when perform load_low_bit function",
      "body": "Syntax error in https://github.com/intel-analytics/ipex-llm/blob/899a30331abbfae62cdf126b9d62e8d0469c715f/python/llm/src/ipex_llm/transformers/npu_model.py#L552C32-L552C47\r\n\r\nwhere **_max_output_len_** suppose to be **_max_context_len_**\r\n\r\n",
      "state": "closed",
      "author": "climh",
      "author_type": "User",
      "created_at": "2024-11-06T01:23:23Z",
      "updated_at": "2025-03-10T11:34:37Z",
      "closed_at": "2024-11-06T02:07:01Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12343/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "plusbang"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12343",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12343",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:07.514683",
      "comments": [
        {
          "author": "plusbang",
          "body": "Hi, @climh , thank you for pointing out this bug! We will fix it soon : )",
          "created_at": "2024-11-06T01:49:15Z"
        }
      ]
    },
    {
      "issue_number": 12304,
      "title": "A770 run harness.RuntimeError: unsupported dtype, only fp32 and fp16 are supported",
      "body": "\r\n(llm) test@test-Z590-VISION-D:~/ipexllm_whowhat/ipex-llm/python/llm/dev/benchmark/harness$ python run_llb.py --model ipex-llm --pretrained /home/test/models/LLM/baichuan2-7b/pytorch/ --precision sym_int4 --device xpu --tasks winogrande --batch 1 --no_cache\r\n/home/test/miniforge3/envs/llm/lib/python3.11/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\r\n  warnings.warn(\r\n/home/test/miniforge3/envs/llm/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n  warn(\r\n2024-10-31 14:44:27,844 - INFO - intel_extension_for_pytorch auto imported\r\nSelected Tasks: ['winogrande']\r\nThe repository for /home/test/models/LLM/baichuan2-7b/pytorch/ contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co//home/test/models/LLM/baichuan2-7b/pytorch/.\r\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\r\n\r\nDo you wish to run the custom code? [y/N] y\r\n2024-10-31 14:44:30,657 - WARNING - Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\r\npip install xformers.\r\n/home/test/miniforge3/envs/llm/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\n2024-10-31 14:44:39,311 - INFO - Converting the current model to sym_int4 format......\r\nDownloading builder script: 5.65kB [00:00, 7.26MB/s]\r\nDownloading readme: 9.97kB [00:00, 14.1MB/s]\r\nDownloading data: 100%|█████████████████████████████████████████████████| 3.40M/3.40M [00:19<00:00, 172kB/s]\r\nGenerating train split: 100%|███████████████████████████████| 40398/40398 [00:00<00:00, 69234.88 examples/s]\r\nGenerating test split: 100%|██████████████████████████████████| 1767/1767 [00:00<00:00, 69010.06 examples/s]\r\nGenerating validation split: 100%|████████████████████████████| 1267/1267 [00:00<00:00, 67524.56 examples/s]\r\nTask: winogrande; number of docs: 1267\r\nTask: winogrande; document 0; context prompt (starting on next line):\r\nNatalie took basic French lessons from Betty after school because Betty is strong at that language.\r\n\r\nMy friends tried to drive the car through the alleyway but the car was too wide.\r\n\r\nSarah didn't practice ballet much but Mary practiced all the time. Sarah wasn't chosen to be a lead dancer.\r\n\r\nThe trainer tried to put the exercise equipment in the van but it wouldn't fit; the van was too small.\r\n\r\nNatalie never checks the air in the tires while Tanya does and you just knew Natalie would have flat tires.\r\n\r\nPeople think Rebecca\r\n(end of prompt on previous line)\r\nRequests: [Req_loglikelihood(\"Natalie took basic French lessons from Betty after school because Betty is strong at that language.\\n\\nMy friends tried to drive the car through the alleyway but the car was too wide.\\n\\nSarah didn't practice ballet much but Mary practiced all the time. Sarah wasn't chosen to be a lead dancer.\\n\\nThe trainer tried to put the exercise equipment in the van but it wouldn't fit; the van was too small.\\n\\nNatalie never checks the air in the tires while Tanya does and you just knew Natalie would have flat tires.\\n\\nPeople think Samantha\", ' is embarassed, because Samantha made snide comments about the shirt Rebecca was wearing.')[0]\r\n, Req_loglikelihood(\"Natalie took basic French lessons from Betty after school because Betty is strong at that language.\\n\\nMy friends tried to drive the car through the alleyway but the car was too wide.\\n\\nSarah didn't practice ballet much but Mary practiced all the time. Sarah wasn't chosen to be a lead dancer.\\n\\nThe trainer tried to put the exercise equipment in the van but it wouldn't fit; the van was too small.\\n\\nNatalie never checks the air in the tires while Tanya does and you just knew Natalie would have flat tires.\\n\\nPeople think Rebecca\", ' is embarassed, because Samantha made snide comments about the shirt Rebecca was wearing.')[0]\r\n]\r\nRunning loglikelihood requests\r\n  0%|                                                                              | 0/2534 [00:00<?, ?it/s]/home/test/miniforge3/envs/llm/lib/python3.11/site-packages/ipex_llm/transformers/models/utils.py:79: UserWarning: `BIGDL_QUANTIZE_KV_CACHE` is deprecated and will be removed in future releases. Please use `IPEX_LLM_QUANTIZE_KV_CACHE` instead.\r\n  warnings.warn(\r\n  0%|                                                                              | 0/2534 [00:00<?, ?it/s]\r\nTraceback (most recent call last):\r\n  File \"/home/test/ipexllm_whowhat/ipex-llm/python/llm/dev/benchmark/harness/run_llb.py\", line 147, in <module>\r\n    main()\r\n  File \"/home/test/ipexllm_whowhat/ipex-llm/python/llm/dev/benchmark/harness/run_llb.py\", line 101, in main\r\n    results = evaluator.simple_evaluate(\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/test/ipexllm_whowhat/ipex-llm/python/llm/dev/benchmark/harness/lm-evaluation-harness/lm_eval/utils.py\", line 243, in _wrapper\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/test/ipexllm_whowhat/ipex-llm/python/llm/dev/benchmark/harness/lm-evaluation-harness/lm_eval/evaluator.py\", line 94, in simple_evaluate\r\n    results = evaluate(\r\n              ^^^^^^^^^\r\n  File \"/home/test/ipexllm_whowhat/ipex-llm/python/llm/dev/benchmark/harness/lm-evaluation-harness/lm_eval/utils.py\", line 243, in _wrapper\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/test/ipexllm_whowhat/ipex-llm/python/llm/dev/benchmark/harness/lm-evaluation-harness/lm_eval/evaluator.py\", line 289, in evaluate\r\n    resps = getattr(lm, reqtype)([req.args for req in reqs])\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/test/ipexllm_whowhat/ipex-llm/python/llm/dev/benchmark/harness/lm-evaluation-harness/lm_eval/base.py\", line 221, in loglikelihood\r\n    return self._loglikelihood_tokens(new_reqs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/test/ipexllm_whowhat/ipex-llm/python/llm/dev/benchmark/harness/lm-evaluation-harness/lm_eval/base.py\", line 357, in _loglikelihood_tokens\r\n    self._model_call(batched_inps), dim=-1\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/test/ipexllm_whowhat/ipex-llm/python/llm/dev/benchmark/harness/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 514, in _model_call\r\n    return self.model(inputs)[\"logits\"]\r\n           ^^^^^^^^^^^^^^^^^^\r\n  File \"/home/test/miniforge3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/test/miniforge3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/test/.cache/huggingface/modules/transformers_modules/modeling_baichuan.py\", line 686, in forward\r\n    outputs = self.model(\r\n              ^^^^^^^^^^^\r\n  File \"/home/test/miniforge3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/test/miniforge3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/test/miniforge3/envs/llm/lib/python3.11/site-packages/ipex_llm/transformers/models/baichuan.py\", line 198, in baichuan_model_7b_forward\r\n    layer_outputs = decoder_layer(\r\n                    ^^^^^^^^^^^^^^\r\n  File \"/home/test/miniforge3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/test/miniforge3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/test/.cache/huggingface/modules/transformers_modules/modeling_baichuan.py\", line 273, in forward\r\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\n                                                          ^^^^^^^^^^^^^^^\r\n  File \"/home/test/miniforge3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/test/miniforge3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/test/miniforge3/envs/llm/lib/python3.11/site-packages/ipex_llm/transformers/models/baichuan.py\", line 271, in baichuan_attention_forward_7b\r\n    xe_addons.rotary_half_inplaced(self.rotary_emb.inv_freq, position_ids,\r\nRuntimeError: unsupported dtype, only fp32 and fp16 are supported\r\n\r\n",
      "state": "closed",
      "author": "tao-ov",
      "author_type": "User",
      "created_at": "2024-10-31T06:49:12Z",
      "updated_at": "2025-03-10T11:34:37Z",
      "closed_at": "2024-11-04T05:52:07Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12304/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "cranechu0131"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12304",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12304",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:07.701304",
      "comments": [
        {
          "author": "cranechu0131",
          "body": "Hi tao-ov,\r\n\r\nWe have reproduced the issue and are looking into it. We'll update if there's any solution.",
          "created_at": "2024-10-31T11:12:23Z"
        },
        {
          "author": "cranechu0131",
          "body": "Hi tao-ov,\r\n\r\nWe have found a tentative solution to this issue. Please change the function in `<path to your ipex-llm package source code>/transformers/models/utils.py` line 163 to the following one:\r\n\r\n```python\r\ndef should_use_fuse_rope(hidden_states, position_ids, training):\r\n    return (\r\n      ",
          "created_at": "2024-11-01T07:29:36Z"
        },
        {
          "author": "cranechu0131",
          "body": "Hi tao-ov,\r\n\r\nWe have found another convenient solution to this issue. Please switch harness command to the following one:\r\n\r\n```\r\npython run_llb.py --model ipex-llm --pretrained /home/test/models/LLM/baichuan2-7b/pytorch/ --precision sym_int4 --device xpu --tasks winogrande --batch 1 --no_cache --m",
          "created_at": "2024-11-04T02:04:33Z"
        },
        {
          "author": "tao-ov",
          "body": "yes, this command add --model_args dtype=float16 it success, thanks for your supporting~",
          "created_at": "2024-11-04T02:48:33Z"
        }
      ]
    },
    {
      "issue_number": 12280,
      "title": "Questions about performance gap between benchmark scripts and llama-bench from ipex-llm[cpp]",
      "body": "**Background**\r\n\r\nWe evaluate the performance with llama-bench from ipex-llm[cpp] and [the benchmark script](https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/dev/benchmark) , to compare with the benchmark results from [this image](https://llm-assets.readthedocs.io/en/latest/_images/MTL_perf.jpg).\r\n\r\nWe found the benchmark script, which use transformers pipeline and pytorch backend achieves better performance than using `llama-bench` (llama-bench evaluate the prefill and decode speed repesctively and no sampling during decoding at all, it should have been faster than normal LLM generate pipeline).\r\n\r\nWe run the benchmarks on Ubuntu 22.04 and Intel Ultra 7 155H.\r\n\r\n**The steps and our results**\r\n\r\nThe *llama-bench* (the original version) results:\r\n\r\n```\r\n./llama-bench -m model.gguf -n 128 -p 365,876,3376 -t 16 -ub 2048 -b 2048 -r 5  \r\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nggml_sycl_init: SYCL_USE_XMX: yes\r\nggml_sycl_init: found 1 SYCL devices:\r\n| model                          |       size |     params | backend    | ngl | threads | n_ubatch |          test |              t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -------: | ------------: | ---------------: |\r\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\r\n[SYCL] call ggml_check_sycl\r\nggml_check_sycl: GGML_SYCL_DEBUG: 0\r\nggml_check_sycl: GGML_SYCL_F16: no\r\nfound 1 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|                     Intel Arc Graphics|    1.5|    128|    1024|   32| 30655M|            1.3.30872|\r\n| phi3 3B Q4_K - Medium          |   2.23 GiB |     3.82 B | SYCL       |  99 |      16 |     2048 |         pp365 |    438.78 ± 4.06 |\r\n| phi3 3B Q4_K - Medium          |   2.23 GiB |     3.82 B | SYCL       |  99 |      16 |     2048 |         pp876 |    563.76 ± 9.62 |\r\n| phi3 3B Q4_K - Medium          |   2.23 GiB |     3.82 B | SYCL       |  99 |      16 |     2048 |        pp3376 |    418.48 ± 2.02 |\r\n| phi3 3B Q4_K - Medium          |   2.23 GiB |     3.82 B | SYCL       |  99 |      16 |     2048 |         tg128 |     26.42 ± 0.32 |\r\n| phi3 3B Q4_K - Medium          |   2.23 GiB |     3.82 B | SYCL       |  99 |      16 |     2048 |         tg256 |     26.38 ± 0.08 |\r\n| phi3 3B Q4_K - Medium          |   2.23 GiB |     3.82 B | SYCL       |  99 |      16 |     2048 |         tg512 |     25.35 ± 0.89 |\r\n| phi3 3B Q4_K - Medium          |   2.23 GiB |     3.82 B | SYCL       |  99 |      16 |     2048 |        tg1024 |     25.17 ± 0.69 |\r\n\r\nbuild: d33728a (1)\r\n```\r\n\r\nAs mentioned above, first, we made some modification to llama-bench, to make it run decode after prefilling, and show prefill and decode speed respectively.\r\n\r\nCode at here: https://github.com/acane77/llama.cpp/tree/dev_ipex_mod\r\n\r\nWe build the llama-bench with the following scripts\r\n```\r\ncmake -B build -DBUILD_SHARED_LIBS=ON -DLLAMA_SYCL=1 -DLLAMA_CLBLAST=1 -DGGML_SYCL=ON -DCMAKE_C_COMPILER=gcc -DCMAKE_CXX_COMPILER=icx  -DCMAKE_BUILD_TYPE=Release\r\ncmake --build build --config Release -j\r\ncmake --install build --prefix install\r\ncp ./install/bin/llama-bench ~/projects/llama-cpp/llama-bench-emb\r\n```\r\n\r\nwhere the `~/projects/llama-cpp/llama-bench-emb` is created by `llama-init`, and the libs are linked to ipex-llm venv.\r\n\r\nThen, we run this *llama-bench-emb* (our modified version), the results is as following.\r\n\r\n```\r\n./llama-bench-emb -m model.gguf -n 128 -p 365,876,3376 -t 16 -ub 2048 -b 2048 -r 5  \r\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nggml_sycl_init: SYCL_USE_XMX: yes\r\nggml_sycl_init: found 1 SYCL devices:\r\n| model                          |       size |     params | backend    | ngl |    threads |   n_ubatch |          test |    prefill (t/s) |     decode (t/s) |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | ---------: | ---------: | ------------: | ---------------: | ---------------: |\r\n-- Note: Use embedding as model input  >> found prompt: 365\r\n  >> found prompt: 876\r\n  >> found prompt: 3376\r\n  << found decode: 128\r\n****** Add test: prompt 365   decode: 128\r\n****** Add test: prompt 876   decode: 128\r\n****** Add test: prompt 3376   decode: 128\r\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\r\n[SYCL] call ggml_check_sycl\r\nggml_check_sycl: GGML_SYCL_DEBUG: 0\r\nggml_check_sycl: GGML_SYCL_F16: no\r\nfound 1 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|                     Intel Arc Graphics|    1.5|    128|    1024|   32| 30655M|            1.3.30872|\r\n----> test case: n_prompt=365, n_gen=128\r\n| phi3 3B Q4_K - Medium          |   2.23 GiB |     3.82 B | SYCL       |  99 |         16 |       2048 |   pp365+tg128 |  479.02 ± 11.30 |    27.60 ± 0.22 |\r\n----> test case: n_prompt=876, n_gen=128\r\n| phi3 3B Q4_K - Medium          |   2.23 GiB |     3.82 B | SYCL       |  99 |         16 |       2048 |   pp876+tg128 |   562.92 ± 3.75 |    26.76 ± 0.14 |\r\n----> test case: n_prompt=3376, n_gen=128\r\n| phi3 3B Q4_K - Medium          |   2.23 GiB |     3.82 B | SYCL       |  99 |         16 |       2048 |  pp3376+tg128 |  419.32 ± 27.55 |    21.75 ± 1.43 |\r\n\r\nbuild: 1da2df74 (3009)\r\n```\r\n\r\nWhile the following table is generated by ipex benchmark script.\r\n\r\n```\r\n,model,1st token avg latency (ms),2+ avg latency (ms/token),encoder time (ms),input/output tokens,batch_size,actual input/output tokens,num_beams,low_bit,cpu_embedding,model loading time (s),peak mem (GB),streaming,use_fp16_torch_dtype\r\n0,microsoft/Phi-3-mini-128k-instruct,729.85,31.58,0.0,365-128,1,366-128,1,sym_int4,,4.33,2.431640625,N/A,N/A\r\n1,microsoft/Phi-3-mini-128k-instruct,1383.27,32.73,0.0,778-128,1,779-128,1,sym_int4,,4.33,2.576171875,N/A,N/A\r\n2,microsoft/Phi-3-mini-128k-instruct,8095.65,32.73,0.0,3667-128,1,3668-128,1,sym_int4,,4.33,4.080078125,N/A,N/A\r\n,model,1st token avg latency (ms),2+ avg latency (ms/token),encoder time (ms),input/output tokens,batch_size,actual input/output tokens,num_beams,low_bit,cpu_embedding,model loading time (s),peak mem (GB),streaming,use_fp16_torch_dtype\r\n0,microsoft/Phi-3-mini-128k-instruct,241.83,30.39,0.0,32-32,1,33-32,1,sym_int4,,3.92,2.474609375,N/A,N/A\r\n1,microsoft/Phi-3-mini-128k-instruct,1852.87,32.42,0.0,960-64,1,961-64,1,sym_int4,,3.92,2.98046875,N/A,N/A\r\n2,microsoft/Phi-3-mini-128k-instruct,2162.56,32.94,0.0,1024-128,1,1025-128,1,sym_int4,,3.92,2.94921875,N/A,N/A\r\n,model,1st token avg latency (ms),2+ avg latency (ms/token),encoder time (ms),input/output tokens,batch_size,actual input/output tokens,num_beams,low_bit,cpu_embedding,model loading time (s),peak mem (GB),streaming,use_fp16_torch_dtype\r\n0,microsoft/Phi-3-mini-128k-instruct,783.59,30.04,0.0,365-128,1,366-128,1,sym_int4,,3.87,2.681640625,N/A,N/A\r\n1,microsoft/Phi-3-mini-128k-instruct,1614.28,31.18,0.0,778-128,1,779-128,1,sym_int4,,3.87,2.82421875,N/A,N/A\r\n2,microsoft/Phi-3-mini-128k-instruct,12719.86,32.0,0.0,3667-128,1,3668-128,1,sym_int4,,3.87,5.5,N/A,N/A\r\n3,microsoft/Phi-3-mini-128k-instruct,810.81,32.42,0.0,365-128,1,366-128,1,sym_int4,,10.96,2.681640625,N/A,N/A\r\n4,microsoft/Phi-3-mini-128k-instruct,1618.89,33.6,0.0,778-128,1,779-128,1,sym_int4,,10.96,2.8125,N/A,N/A\r\n```\r\n\r\n**Questions**\r\n\r\nIs there any reason for this significant performance gap between the python transformers benchmark and llama-bench?\r\n\r\nThe difference is that the pytorch benchmark uses `xpu` device while llama-bench uses `gpu`.",
      "state": "closed",
      "author": "acane77",
      "author_type": "User",
      "created_at": "2024-10-28T05:19:33Z",
      "updated_at": "2025-03-10T11:34:36Z",
      "closed_at": "2024-10-31T02:47:52Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12280/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12280",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12280",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:07.983543",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "Please make sure your performance data are the same format. For example, `python transformers benchmark`'s 1st is total time of 1st token, 2nd is ms per token. What's the format of llama-bench?  It looks like number of tokens per second.",
          "created_at": "2024-10-29T02:23:14Z"
        },
        {
          "author": "qiuxin2012",
          "body": "I just notice llama-bench's format is `prefill (t/s) |     decode (t/s) |`\r\nFor 365 tokens, 1st total time is  365 / 479 = 0.792 s, 2nd is 1000 / 27.6 = 36.23 ms/token. It shows `python transformers` is a little faster.",
          "created_at": "2024-10-29T02:54:33Z"
        },
        {
          "author": "acane77",
          "body": "Yes, we also noticed this. We also tried different configuations (batch size, ubatch size, thread numbers), but all these performance are lower than the `transformers` results.",
          "created_at": "2024-10-29T03:36:41Z"
        }
      ]
    },
    {
      "issue_number": 12158,
      "title": "Inference hangs on LNL iGPU with large input prompts.",
      "body": "## Type of issue\r\n\r\n- I conducted a benchmark on LNL iGPU for [Arcee-lite](https://huggingface.co/arcee-ai/arcee-lite) model, which is based on the Qwen2 Architecture, and obtained via LLM distillation techniques. It turns out the model runs perfectly for [6x128], [6x256] configs, but when given large input prompts (in my case [1000x512]), it hangs without any error logs.\r\n\r\n- Same issue happened also for [Supernova-lite](https://huggingface.co/arcee-ai/Llama-3.1-SuperNova-Lite) model which is based on the Llama 3.1 architecture, except that none of: [6x128], [6x256] or [1000x512] configurations worked.\r\n\r\n- The model generation step hungs exactly on this line of code:\r\n\r\n```\r\n  output_ids = model.generate(input_ids, do_sample=False, max_new_tokens=out_len, min_new_tokens=out_len, num_beams=num_beams)\r\n```\r\n\r\n- For benchmarking, I'm using all-in-one benchmark scripts, and here is my config.yml file:\r\n\r\n```\r\nrepo_id:\r\n  - 'arcee-ai/arcee-lite'\r\n  # - 'arcee-ai/Llama-3.1-SuperNova-Lite'test_api now\r\nlocal_model_hub: 'C:\\Users\\Intel\\.cache\\huggingface\\hub\\models--arcee-ai--arcee-lite\\snapshots\\c5cb9c38be16b64757f785f0df36dca87f76d5e2'\r\nwarm_up: 1 # must set >=2 when run \"pipeline_parallel_gpu\" test_api\r\nnum_trials: 1\r\nnum_beams: 1 # default to greedy search\r\nlow_bit: 'sym_int4' # default to use 'sym_int4' (i.e. symmetric int4)\r\nbatch_size: 1 # default to 1\r\nin_out_pairs:\r\n    # - '6-128'\r\n    # - '6-256'\r\n    - '1000-512'\r\ntest_api:\r\n  # - \"transformer_int4_fp16_gpu\"             # on Intel GPU, transformer-like API, (qtype=int4), (dtype=fp16)\r\n  # - \"transformer_int4_fp16_gpu_win\"       # on Intel GPU for Windows, transformer-like API, (qtype=int4), (dtype=fp16)\r\n  # - \"transformer_int4_gpu\"                # on Intel GPU, transformer-like API, (qtype=int4), (dtype=fp32)\r\n  - \"transformer_int4_gpu_win\"            # on Intel GPU for Windows, transformer-like API, (qtype=int4), (dtype=fp32)\r\n  # - \"transformer_int4_loadlowbit_gpu_win\" # on Intel GPU for Windows, transformer-like API, (qtype=int4), use load_low_bit API. Please make sure you have used the save.py to save the converted low bit model\r\n  # - \"transformer_int4_fp16_loadlowbit_gpu_win\" # on Intel GPU for Windows, transformer-like API, (qtype=int4), (dtype=fp16), use load_low_bit API. Please make sure you have used the save.py to save the converted low bit model\r\n  # - \"bigdl_fp16_gpu\"                      # on Intel GPU, use ipex-llm transformers API, (dtype=fp16), (qtype=fp16)\r\n  # - \"optimize_model_gpu\"                  # on Intel GPU, can optimize any pytorch models include transformer model\r\n  # - \"deepspeed_optimize_model_gpu\"        # on Intel GPU, deepspeed autotp inference\r\n  # - \"pipeline_parallel_gpu\"               # on Intel GPU, pipeline parallel inference\r\n  # - \"speculative_gpu\"                     # on Intel GPU, inference with self-speculative decoding\r\n  # - \"transformer_int4\"                    # on Intel CPU, transformer-like API, (qtype=int4)\r\n  # - \"native_int4\"                         # on Intel CPU\r\n  # - \"optimize_model\"                      # on Intel CPU, can optimize any pytorch models include transformer model\r\n  # - \"pytorch_autocast_bf16\"               # on Intel CPU\r\n  # - \"transformer_autocast_bf16\"           # on Intel CPU\r\n  # - \"bigdl_ipex_bf16\"                     # on Intel CPU, (qtype=bf16)\r\n  # - \"bigdl_ipex_int4\"                     # on Intel CPU, (qtype=int4)\r\n  # - \"bigdl_ipex_int8\"                     # on Intel CPU, (qtype=int8)\r\n  # - \"speculative_cpu\"                     # on Intel CPU, inference with self-speculative decoding\r\n  # - \"deepspeed_transformer_int4_cpu\"      # on Intel CPU, deepspeed autotp inference\r\n  # - \"transformers_int4_npu_win\"           # on Intel NPU for Windows,  transformer-like API, (qtype=int4)\r\n  # - \"transformers_int4_loadlowbit_npu_win\" # on Intel NPU for Windows, transformer-like API, (qtype=int4), use load_low_bit API. Please make sure you have used the save_npu.py to save the converted low bit model\r\ncpu_embedding: False # whether put embedding to CPU\r\nstreaming: False # whether output in streaming way (only available now for gpu win related test_api)\r\noptimize_model: False # whether apply further optimization on NPU (only available now for transformers_int4_npu_win test_api)\r\nuse_fp16_torch_dtype: True # whether use fp16 for non-linear layer (only available now for \"pipeline_parallel_gpu\" test_api)\r\ntask: 'continuation' # task can be 'continuation', 'QA' and 'summarize'\r\ntranspose_value_cache: True # whether apply transposed v_cache optimization on NPU (only available now for transformers_int4_npu_win test_api)\r\n```\r\n\r\n\r\n## GPU Driver version\r\n\r\n32.0.101.5737\r\n\r\n## What operating system are you seeing the problem on?\r\n\r\nWindows 11",
      "state": "closed",
      "author": "aahouzi",
      "author_type": "User",
      "created_at": "2024-10-03T14:23:02Z",
      "updated_at": "2025-03-10T11:34:35Z",
      "closed_at": "2024-10-17T14:42:30Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12158/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12158",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12158",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:08.167339",
      "comments": [
        {
          "author": "Oscilloscope98",
          "body": "Hi @aahouzi ,\r\n\r\nWe recently update `ipex-llm` for Lunar Lake (LNL) support. You could refer to [here](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/install_windows_gpu.md#install-ipex-llm) regarding how to install `ipex-llm` for LNL iGPU on Windows.\r\n\r\nBesides, for `a",
          "created_at": "2024-10-11T02:12:23Z"
        },
        {
          "author": "aahouzi",
          "body": "@Oscilloscope98 Thanks for your support.\r\n\r\nThis fix works only for Arcee-lite, but doesn't work for Supernova-Lite, which is based on the Llama 3.1 architecture. The weirdest thing here is that during inference time it picks llama 3.2 instead of llama 3.1 as shown in the logs:\r\n\r\n```\r\n(ipex) C:\\Use",
          "created_at": "2024-10-11T13:21:24Z"
        },
        {
          "author": "Oscilloscope98",
          "body": "Hi @aahouzi,\r\n\r\nWe have reproduced this issue on Supernova-Lite model, and will update here once we have fixed it :)",
          "created_at": "2024-10-12T06:45:05Z"
        },
        {
          "author": "Oscilloscope98",
          "body": "Hi @aahouzi,\r\n\r\nWe have fixed this issue. You could install `ipex-llm[xpu_lnl]>=2.2.0b20241014` through:\r\n\r\n```cmd\r\npip install --pre --upgrade ipex-llm[xpu_lnl] --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/lnl/us/\r\n```\r\nand have a try again.\r\n\r\nPlease let us know for any",
          "created_at": "2024-10-15T01:45:57Z"
        },
        {
          "author": "aahouzi",
          "body": "Thanks @Oscilloscope98, it's working now :)\r\n",
          "created_at": "2024-10-17T14:42:30Z"
        }
      ]
    },
    {
      "issue_number": 12152,
      "title": "Cannot use gpu",
      "body": "Hello developers! I'm trying to run ollama on my intel iGPU, and I set up the environment as follows, but ollama still runs on CPU.\r\n\r\n1. Update GPU driver from intel\r\n![image](https://github.com/user-attachments/assets/d49acefc-5272-48bb-9f62-adfe5fde98a3)\r\n2. (suggested by [intel](https://www.intel.com.tw/content/www/tw/zh/content-details/826081/running-ollama-with-open-webui-on-intel-hardware-platform.html) )install oneAPI toolkit\r\n3. create venv and install ipex-llm[cpp]\r\n![image](https://github.com/user-attachments/assets/bd894f87-970b-48cf-8544-b8308053e235)\r\n4. initialize it with init-ollama\r\n![image](https://github.com/user-attachments/assets/d1b00cee-752c-4721-a30e-cd1952fd5169)\r\n5. run it with following env\r\n![image](https://github.com/user-attachments/assets/ac783593-c570-4268-b75e-914230c9bb28)\r\n\r\nbut it turned out still run ollama on CPU if I understand the output correctly.\r\n![image](https://github.com/user-attachments/assets/c0149caa-7374-4fed-9cc1-41d88ae78f40)\r\n\r\nHow to solve this?\r\n",
      "state": "closed",
      "author": "yuyalun-allen",
      "author_type": "User",
      "created_at": "2024-09-30T11:42:32Z",
      "updated_at": "2025-03-10T11:34:35Z",
      "closed_at": "2024-10-01T09:15:29Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12152/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12152",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12152",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:08.434378",
      "comments": [
        {
          "author": "user7z",
          "body": "@yuyalun-allen that does not mean it runs on cpu , it will always use cpu , but just the serving service i.e ollama serve , to know if it runs on gpu you have to first run a model.and go ro task manager to see if the igpu is  bieng used , that if the model runs witbout any issue",
          "created_at": "2024-09-30T14:13:25Z"
        },
        {
          "author": "yuyalun-allen",
          "body": "Okay, then it seems that I ran into another problem.\r\nWhen I run the system ollama, it works fine, but on cpu, rather than gpu.\r\n![image](https://github.com/user-attachments/assets/87a88566-2344-479b-aed4-23116b964b5b)\r\n\r\nthen when I use ollama.exe which generated by ipex-llm, then the program crash",
          "created_at": "2024-09-30T14:43:53Z"
        },
        {
          "author": "user7z",
          "body": "@yuyalun-allen you need to understand that ollama well always run on cpu , but when you run a model , the ollama that is runing on the cpu well use the gpu for the model , that what accelersting ollama by thr gpu means , how to know if the gpu is utiliezed or know , you should see task manager.\r\n\r\nD",
          "created_at": "2024-09-30T14:55:21Z"
        },
        {
          "author": "yuyalun-allen",
          "body": "alright I got to know that ollama will always use cpu and use gpu to accelerate. And I am still not clear with what ` init-ollama.bat` do to generate a `ollama.exe` file under current directory, since the guide says on windows we should still run `ollama serve` rather than `.\\ollama.exe serve` (and ",
          "created_at": "2024-09-30T15:08:03Z"
        },
        {
          "author": "user7z",
          "body": "Every thing that uses the gpu well use also cpu , you cant skip the cpu entirelly.\r\nYeah those guys here really have slpit docs there & there there not orgnized in friendlly way , iam glad it helped , have a nice day",
          "created_at": "2024-09-30T15:11:49Z"
        }
      ]
    },
    {
      "issue_number": 10854,
      "title": "RuntimeError: \"fused_dropout\" not implemented for 'Byte' when running trl ppo finetuning",
      "body": "**Machine: MAX1100**\r\n**ipex-llm: 2.1.0b20240421**\r\n**bigdl-core-xe-21            2.5.0b20240421\r\nbigdl-core-xe-esimd-21      2.5.0b20240421**\r\n[Related PR](https://github.com/intel-analytics/ipex-llm/pull/10841)\r\nWhen trying to run trl PPO finetuning on MAX1100, I got the following error.\r\n```\r\n(ppo) (base) wangyishuo@7cc25526b7ac:~/ziteng$ python ppo.py --model_name \"/mnt/disk1/Llama-2-7b-chat-hf\" --dataset_name \"HuggingFaceH4/helpful_instructions\"\r\n/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n  warn(\r\n/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/datasets/load.py:1461: FutureWarning: The repository for HuggingFaceH4/helpful_instructions contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/HuggingFaceH4/helpful_instructions\r\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\r\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\r\n  warnings.warn(\r\n2024-04-22 19:34:28,707 - root - INFO - intel_extension_for_pytorch auto imported\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.15s/it]\r\n/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\r\n  warnings.warn(\r\n/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\r\n  warnings.warn(\r\n2024-04-22 19:34:31,311 - root - INFO - peft adapter initialised\r\n2024-04-22 19:34:31,315 - ipex_llm.transformers.utils - INFO - Converting the current model to fp4 format......\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.00it/s]\r\nSome weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /mnt/disk1/Llama-2-7b-chat-hf and are newly initialized: ['score.weight']\r\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n0it [00:00, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n0it [00:02, ?it/s]\r\nTraceback (most recent call last):\r\n  File \"/home/wangyishuo/ziteng/ppo.py\", line 248, in <module>\r\n    response_tensors = ppo_trainer.generate(\r\n                       ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py\", line 469, in generate\r\n    response = self._generate_batched(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py\", line 556, in _generate_batched\r\n    generations = unwrapped_model.generate(**padded_inputs, **generation_kwargs)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/trl/models/modeling_value_head.py\", line 204, in generate\r\n    return self.pretrained_model.generate(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/peft/peft_model.py\", line 1190, in generate\r\n    outputs = self.base_model.generate(*args, **kwargs)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/ipex_llm/transformers/lookup.py\", line 86, in generate\r\n    return original_generate(self,\r\n           ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/ipex_llm/transformers/speculative.py\", line 103, in generate\r\n    return original_generate(self,\r\n           ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/transformers/generation/utils.py\", line 1520, in generate\r\n    return self.sample(\r\n           ^^^^^^^^^^^^\r\n  File \"/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/transformers/generation/utils.py\", line 2617, in sample\r\n    outputs = self(\r\n              ^^^^^\r\n  File \"/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 1183, in forward\r\n    outputs = self.model(\r\n              ^^^^^^^^^^^\r\n  File \"/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 1070, in forward\r\n    layer_outputs = decoder_layer(\r\n                    ^^^^^^^^^^^^^^\r\n  File \"/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 798, in forward\r\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\n                                                          ^^^^^^^^^^^^^^^\r\n  File \"/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 386, in forward\r\n    query_states = self.q_proj(hidden_states)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/peft/tuners/lora/layer.py\", line 509, in forward\r\n    result = result + lora_B(lora_A(dropout(x))) * scaling\r\n                                    ^^^^^^^^^^\r\n  File \"/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/torch/nn/modules/dropout.py\", line 58, in forward\r\n    return F.dropout(input, self.p, self.training, self.inplace)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/wangyishuo/miniconda3/envs/ppo/lib/python3.11/site-packages/torch/nn/functional.py\", line 1266, in dropout\r\n    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\r\n                                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: \"fused_dropout\" not implemented for 'Byte'\r\n```\r\n",
      "state": "open",
      "author": "Jasonzzt",
      "author_type": "User",
      "created_at": "2024-04-23T03:42:57Z",
      "updated_at": "2025-03-10T11:34:02Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/10854/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/10854",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/10854",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:08.659590",
      "comments": [
        {
          "author": "Uxito-Ada",
          "body": "@leonardozcm pls take a look, whether it is not supported by our kernel? tks.",
          "created_at": "2024-04-23T05:32:08Z"
        },
        {
          "author": "leonardozcm",
          "body": "hi, I think the `VF.drop` is not implemented by our kernels, instead I suppose this error indicates that `input ` is in 8-bit data format which is not a supported dtype for torch.nn.functional.dropout",
          "created_at": "2024-04-23T14:04:38Z"
        },
        {
          "author": "Uxito-Ada",
          "body": "@Jasonzzt From the log, it is found that PPO also applies PEFT LoRA.\r\nTherefore, like QLoRA, rather than `from_pretrained` a peft model with lora config, we should first load the base model, and then use `get_peft_model`, `prepare_model_for_kbit_training` etc. methods  in [qlora.py](https://github.c",
          "created_at": "2024-04-24T01:24:29Z"
        }
      ]
    },
    {
      "issue_number": 12465,
      "title": "init-ollama.bat Not Working",
      "body": "I unable to run ipex with llama.cpp, it is not working for ollama.\r\n\r\nDevice: Lenovo ThinkPad\r\nOS: Windows 11\r\nGPU: Iris Xe\r\nGPU Driver: 32.0.101.6299\r\n\r\nI followed the step by step\r\n\r\n```\r\nconda create -n llm-cpp python=3.11\r\nconda activate llm-cpp\r\npip install --pre --upgrade ipex-llm[cpp]\r\n```\r\n\r\n```\r\nmkdir ipex-ollama\r\ncd ipex-ollama\r\n```\r\n\r\nBut when I try `init-ollama.bat` it is saying that 'init-ollama.bat' is not recognized as an internal or external command,\r\noperable program or batch file.",
      "state": "closed",
      "author": "imabdul-dev",
      "author_type": "User",
      "created_at": "2024-11-28T21:33:40Z",
      "updated_at": "2025-03-10T11:33:48Z",
      "closed_at": "2024-11-30T18:05:15Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12465/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12465",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12465",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:08.872594",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @imabdul-dev, please run `init-ollama.bat` with administrator privilege in Miniforge Prompt.",
          "created_at": "2024-11-29T02:34:21Z"
        },
        {
          "author": "imabdul-dev",
          "body": "As you can see in the screenshot I am Miniforge prompt is in admin mode but still getting error.\r\n![image](https://github.com/user-attachments/assets/a6109714-1824-49d4-ad68-6e894f2ff1b7)\r\n",
          "created_at": "2024-11-29T11:08:04Z"
        },
        {
          "author": "imabdul-dev",
          "body": "first I have to create a conda environment and activate it and then install ipex-llm[cpp] and then.  And after activating the previously created conda environment as admin and then initialize the ollama with `.init-ollama.bat` I already have installed the intel oneapi base. Am I right?",
          "created_at": "2024-11-29T11:10:22Z"
        },
        {
          "author": "imabdul-dev",
          "body": "@sgwhat  It is working now. Actually, at first I use python 3.13 instead of 3.11, and after figuring out that it is not working then I use python 3.11, but it uses the cached pip packages, and that's why some packages were missing.\r\nBut now after clearing all the cache I've successfully installed th",
          "created_at": "2024-11-30T09:40:25Z"
        },
        {
          "author": "imabdul-dev",
          "body": "It is working now. Thanks!",
          "created_at": "2024-11-30T18:05:12Z"
        }
      ]
    },
    {
      "issue_number": 12463,
      "title": "AssertionError: Speculative decoding not yet supported for XPU backend",
      "body": "``` shell\r\n#!/bin/bash\r\nmodel=\"/llm/models/Qwen2.5-32B-Instruct\"\r\nserved_model_name=\"Qwen2.5-32B-FP8\"\r\n\r\nexport CCL_WORKER_COUNT=4\r\nexport FI_PROVIDER=shm\r\nexport CCL_ATL_TRANSPORT=ofi\r\nexport CCL_ZE_IPC_EXCHANGE=sockets\r\nexport CCL_ATL_SHM=1\r\n \r\nexport USE_XETLA=OFF\r\nexport SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=2\r\nexport TORCH_LLM_ALLREDUCE=0\r\n \r\nsource /opt/intel/1ccl-wks/setvars.sh\r\n\r\npython -m ipex_llm.vllm.xpu.entrypoints.openai.api_server \\\r\n  --served-model-name $served_model_name \\\r\n  --port 8000 \\\r\n  --model $model \\\r\n  --trust-remote-code \\\r\n  --block-size 8 \\\r\n  --gpu-memory-utilization 0.85 \\\r\n  --device xpu \\\r\n  --dtype auto \\\r\n  --enforce-eager \\\r\n  --use-v2-block-manager \\\r\n  --speculative-model \"/llm/models/Qwen2.5-0.5B-Instruct\"  \\\r\n  --speculative-draft-tensor-parallel-size 1 \\\r\n  --num-speculative-tokens 5 \\\r\n  --load-in-low-bit sym_int8 \\\r\n  --max-model-len 2048 \\\r\n  --max-num-batched-tokens 4000 \\\r\n  --max-num-seqs 12 \\\r\n  --tensor-parallel-size 4 \\\r\n  --disable-async-output-proc \\\r\n  --distributed-executor-backend ray\r\n```\r\nWhen I setting Speculative decoding via ipex vllm docker contariner , It show me this :\r\n``` bash\r\nINFO 11-28 21:37:01 llm_engine.py:226] Initializing an LLM engine (v0.6.2+ipexllm) with config: model='/llm/models/Qwen2.5-32B-Instruct', speculative_config=SpeculativeConfig(draft_model='/llm/models/Qwen2.5-0.5B-Instruct', num_spec_tokens=5), tokenizer='/llm/models/Qwen2.5-32B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=xpu, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen2.5-32B-FP8, use_v2_block_manager=True, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\r\nProcess SpawnProcess-49:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 145, in run_mp_engine\r\n    engine = IPEXLLMMQLLMEngine.from_engine_args(engine_args=engine_args,\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 133, in from_engine_args\r\n    return super().from_engine_args(engine_args, usage_context, ipc_path)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/multiprocessing/engine.py\", line 138, in from_engine_args\r\n    return cls(\r\n           ^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/multiprocessing/engine.py\", line 78, in __init__\r\n    self.engine = LLMEngine(*args,\r\n                  ^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py\", line 325, in __init__\r\n    self.model_executor = executor_class(\r\n                          ^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/distributed_gpu_executor.py\", line 26, in __init__\r\n    super().__init__(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/xpu_executor.py\", line 38, in __init__\r\n    assert (not speculative_config\r\n            ^^^^^^^^^^^^^^^^^^^^^^\r\nAssertionError: Speculative decoding not yet supported for XPU backend\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/entrypoints/openai/api_server.py\", line 574, in <module>\r\n    uvloop.run(run_server(args))\r\n  File \"/usr/local/lib/python3.11/dist-packages/uvloop/__init__.py\", line 105, in run\r\n    return runner.run(wrapper())\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib/python3.11/asyncio/runners.py\", line 118, in run\r\n    return self._loop.run_until_complete(task)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\r\n  File \"/usr/local/lib/python3.11/dist-packages/uvloop/__init__.py\", line 61, in wrapper\r\n    return await main\r\n           ^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/entrypoints/openai/api_server.py\", line 541, in run_server\r\n    async with build_async_engine_client(args) as engine_client:\r\n  File \"/usr/lib/python3.11/contextlib.py\", line 210, in __aenter__\r\n    return await anext(self.gen)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/entrypoints/openai/api_server.py\", line 105, in build_async_engine_client\r\n    async with build_async_engine_client_from_engine_args(\r\n  File \"/usr/lib/python3.11/contextlib.py\", line 210, in __aenter__\r\n    return await anext(self.gen)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/entrypoints/openai/api_server.py\", line 195, in build_async_engine_client_from_engine_args\r\n    raise RuntimeError(\r\nRuntimeError: Engine process failed to start\r\n```",
      "state": "closed",
      "author": "thomas-hiddenpeak",
      "author_type": "User",
      "created_at": "2024-11-28T13:41:31Z",
      "updated_at": "2025-03-10T11:33:48Z",
      "closed_at": "2024-12-04T01:38:49Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12463/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "gc-fu"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12463",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12463",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:09.171311",
      "comments": [
        {
          "author": "gc-fu",
          "body": "Hi, this feature is not supported yet on xpu, we will see if we can support this feature.",
          "created_at": "2024-11-29T02:29:34Z"
        },
        {
          "author": "HumerousGorgon",
          "body": "+1 to this, was just thinking about it earlier on today. Went to set it up and realised that it's not supported on the XPU backend. Would massively speed up model performance.\r\nThanks!",
          "created_at": "2024-11-29T07:36:07Z"
        },
        {
          "author": "thomas-hiddenpeak",
          "body": "this feature is very useful for my applications.\nIpex serve qwen32b-int8，too slow to use on 4 Arc 770 cards.\nI hope that I can follow your update， Testting and use it.\n",
          "created_at": "2024-11-30T06:44:37Z"
        },
        {
          "author": "HumerousGorgon",
          "body": "> this feature is very useful for my applications. Ipex serve qwen32b-int8，too slow to use on 4 Arc 770 cards. I hope that I can follow your update， Testting and use it.\r\n\r\nWhat is your metric for 'too slow'? I run Qwen32b on 2 Arc A770's and I get around 22t/s on text generation and very very fast ",
          "created_at": "2024-11-30T06:54:17Z"
        },
        {
          "author": "thomas-hiddenpeak",
          "body": "4 Arc 770，2.4Tokens/s\nPlex8756 x16 pcie3.0\nQwen32B-int8\n\nmy target is qwen72-int4，but I can not run it with the ipex-serve docker.\n\nIt stop convert int4...，every time.😭\n\n",
          "created_at": "2024-11-30T08:58:15Z"
        }
      ]
    },
    {
      "issue_number": 12432,
      "title": "Unable to inference with Qwen2.5 GPTQ model",
      "body": "Hello, I'm trying the following script to use GPTQ model of Qwen 2.5:\r\n\r\n```python\r\nfrom ipex_llm.transformers import AutoModelForCausalLM\r\nfrom transformers import AutoTokenizer, pipeline\r\n\r\nmodel_path = \"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\"\r\n\r\n# Load model in 4 bit,\r\n# which convert the relevant layers in the model into INT4 format\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\n    model_path, load_in_4bit=True, optimize_model=True, trust_remote_code=True, use_cache=True\r\n)\r\n\r\n# Load tokenizer\r\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\r\n\r\ngen = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=\"xpu\")\r\ntext = gen(\"你好！\")\r\n\r\nprint(text)\r\n```\r\n\r\nAnd it results in:\r\n\r\n```\r\n2024-11-23 16:01:20,104 - INFO - PyTorch version 2.1.0.post3+cxx11.abi available.\r\n2024-11-23 16:01:20,470 - WARNING - CUDA extension not installed.\r\n2024-11-23 16:01:20,471 - WARNING - CUDA extension not installed.\r\n2024-11-23 16:01:22,072 - INFO - intel_extension_for_pytorch auto imported\r\n/var/home/pods/Projects/azarrot/venv/lib/python3.10/site-packages/transformers/quantizers/auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.However, loading attributes (e.g. ['use_cuda_fp16', 'use_exllama', 'max_input_length', 'exllama_config', 'disable_exllama']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\r\n  warnings.warn(warning_msg)\r\n/var/home/pods/Projects/azarrot/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:5006: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\r\n  warnings.warn(\r\n`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\r\nLoading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.94it/s]\r\n2024-11-23 16:01:26,093 - INFO - Converting the current model to asym_int4 format......\r\n/var/home/pods/Projects/azarrot/venv/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\r\n  warnings.warn(\r\npython: rope.cpp:100: void rotary_half_inplaced(torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor): Assertion `inv_freq.scalar_type() == query.scalar_type() && inv_freq.scalar_type() == key.scalar_type()' failed.\r\nAborted (core dumped)\r\n```\r\n\r\nNon-GPTQ version works. Any ideas?\r\n\r\nipex-llm[xpu] 2.2.0b20241122\r\ntorch 2.1.0.post3+cxx11.abi\r\nintel-extension-for-pytorch 2.1.40+xpu\r\nauto-gptq 0.7.1",
      "state": "closed",
      "author": "notsyncing",
      "author_type": "User",
      "created_at": "2024-11-23T08:05:09Z",
      "updated_at": "2025-03-10T11:33:47Z",
      "closed_at": "2024-11-24T02:39:04Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12432/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12432",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12432",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:09.404283",
      "comments": [
        {
          "author": "jason-dai",
          "body": "See https://github.com/intel-analytics/ipex-llm#save-and-load",
          "created_at": "2024-11-23T14:33:02Z"
        },
        {
          "author": "notsyncing",
          "body": "> See https://github.com/intel-analytics/ipex-llm#save-and-load\r\n\r\nCould you please be more specific? My code above is almost equivalent to the example in that link, and following that example with its default model `TheBloke/TinyLlama-1.1B-Chat-v1.0-GPTQ` results in:\r\n\r\n```\r\nImportError: /var/home/",
          "created_at": "2024-11-24T02:21:58Z"
        },
        {
          "author": "notsyncing",
          "body": "I read that example's code again, and now it works now with `torch_dtype=torch.float`. Thanks very much!",
          "created_at": "2024-11-24T02:39:04Z"
        }
      ]
    },
    {
      "issue_number": 12427,
      "title": "nf4 still unsupported?",
      "body": "In the example: example/CPU/QLoRA-FineTuning/qlora_finetuning_cpu.py\r\n\r\nIt mentions on a comment that nf4 is not supported on cpu yet but when I change the example from int4 -> nf4 it still runs without errors or warnings related to nf4.\r\n\r\nIs nf4 now supported?  Otherwise if it is defaulting back to int4 I think it's worth printing an error or warning.\r\n\r\n\r\n```\r\nbnb_config = BitsAndBytesConfig(\r\n        load_in_4bit=True,\r\n        bnb_4bit_use_double_quant=False,\r\n        bnb_4bit_quant_type=\"int4\",  # nf4 not supported on cpu yet\r\n        bnb_4bit_compute_dtype=torch.bfloat16\r\n    )\r\n```\r\n",
      "state": "closed",
      "author": "epage480",
      "author_type": "User",
      "created_at": "2024-11-21T14:49:08Z",
      "updated_at": "2025-03-10T11:33:47Z",
      "closed_at": "2024-12-16T08:09:16Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12427/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Uxito-Ada"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12427",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12427",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:09.649742",
      "comments": [
        {
          "author": "Uxito-Ada",
          "body": "Hi @epage480 ,\r\n\r\nThanks for your validation. Yes, NF4 is currently supported.\r\n\r\nThe CPU QLoRA example uses the quantization backend of bitsandbytes, which has already enabled NF4 on Intel 4th Gen Xeon (SPR) platform [as shown here](https://github.com/bitsandbytes-foundation/bitsandbytes/blob/main/",
          "created_at": "2024-11-22T02:29:58Z"
        }
      ]
    },
    {
      "issue_number": 12426,
      "title": "Disable XMX",
      "body": "Hello.\r\nI have an Intel ARC A380 and I'm using Ollama with IPEX-LLM using this script with Ubuntu:\r\n\r\n```\r\n#!/bin/bash\r\n\r\n# Activate conda environment\r\nsource /home/nikos/miniforge3/etc/profile.d/conda.sh  # Update this with the correct Conda path\r\nconda activate llm-cpp\r\n\r\n# Ensure init-ollama is in the PATH (adjust as needed)\r\nexport PATH=\"/home/nikos/llm_env/bin:$PATH\"\r\n\r\n# Initialize Ollama\r\ninit-ollama\r\n\r\n# Set environment variables\r\nexport OLLAMA_NUM_GPU=999\r\nexport no_proxy=localhost,127.0.0.1\r\nexport ZES_ENABLE_SYSMAN=1\r\nsource /opt/intel/oneapi/setvars.sh\r\nexport SYCL_CACHE_PERSISTENT=1\r\nexport OLLAMA_NUM_PARALLEL=1\r\n\r\n# Start Ollama\r\n./ollama serve\r\n```\r\n\r\nIt works fine.\r\n\r\n**For testing reasons I want to disable the use of XMX engine (DPAS)**\r\n\r\nI added these two environment variables at the end of script:\r\n```\r\nexport BIGDL_LLM_XMX_DISABLED=1\r\nexport SYCL_USE_XMX=0\r\n```\r\n\r\n```\r\n# Set environment variables\r\nexport OLLAMA_NUM_GPU=999\r\nexport no_proxy=localhost,127.0.0.1\r\nexport ZES_ENABLE_SYSMAN=1\r\nsource /opt/intel/oneapi/setvars.sh\r\nexport SYCL_CACHE_PERSISTENT=1\r\nexport OLLAMA_NUM_PARALLEL=1\r\nexport BIGDL_LLM_XMX_DISABLED=1\r\nexport SYCL_USE_XMX=0\r\n```\r\n\r\nUnfortunately, when I run Ollama with IPEX-LLM the runtime environment gives me this (at the server)\r\n\r\n```\r\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nggml_sycl_init: SYCL_USE_XMX: yes\r\nggml_sycl_init: found 1 SYCL devices:\r\nllm_load_tensors: ggml ctx size =    0.27 MiB\r\nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 33/33 layers to GPU\r\nllm_load_tensors:      SYCL0 buffer size =  3850.02 MiB\r\nllm_load_tensors:  SYCL_Host buffer size =    72.00 MiB\r\nllama_new_context_with_model: n_ctx      = 2048\r\nllama_new_context_with_model: n_batch    = 512\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 1000000.0\r\nllama_new_context_with_model: freq_scale = 1\r\n[SYCL] call ggml_check_sycl\r\nggml_check_sycl: GGML_SYCL_DEBUG: 0\r\nggml_check_sycl: GGML_SYCL_F16: no\r\nfound 1 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|                Intel Arc A380 Graphics|    1.3|    128|    1024|   32|  6064M|            1.3.29735|\r\nllama_kv_cache_init:      SYCL0 KV buffer size =   256.00 MiB\r\nllama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\r\nllama_new_context_with_model:  SYCL_Host  output buffer size =     0.14 MiB\r\nllama_new_context_with_model:      SYCL0 compute buffer size =    84.00 MiB\r\nllama_new_context_with_model:  SYCL_Host compute buffer size =    12.01 MiB\r\nllama_new_context_with_model: graph nodes  = 902\r\nllama_new_context_with_model: graph splits = 2\r\n[1732185101] warming up the model with an empty run\r\n```\r\nIt clearly says:\r\n`ggml_sycl_init: SYCL_USE_XMX: yes`\r\n\r\n**_Is it possible to disable the XMX engine ?_**\r\n\r\nThank you.\r\n",
      "state": "closed",
      "author": "NikosDi",
      "author_type": "User",
      "created_at": "2024-11-21T10:41:06Z",
      "updated_at": "2025-03-10T11:33:46Z",
      "closed_at": "2024-11-28T06:16:46Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12426/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12426",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12426",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:09.837500",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @NikosDi , `SYCL_USE_XMX` cannot be directly disabled. May I know the reason that you need to disable xmx?",
          "created_at": "2024-11-25T02:19:34Z"
        },
        {
          "author": "NikosDi",
          "body": "Hello.\r\nFor me, as I said above, it is for testing reasons.\r\n\r\nI want to know the impact of XMX in overall performance. \r\n\r\nBut someone who has a system with an iGPU with no XMX and no discrete card, how could he use IPEX-LLM ?\r\n\r\nCould XMX be disabled in other way ?",
          "created_at": "2024-11-25T04:01:55Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @NikosDi, XMX cannot be disabled through other methods. If your device does not have XMX, optimizations related to XMX will be disabled.",
          "created_at": "2024-11-26T01:46:40Z"
        },
        {
          "author": "NikosDi",
          "body": "Hello.\r\nThe parameter `BIGDL_LLM_XMX_DISABLED` was active in previous versions and you had to enable it for iGPUs in order to run BIGL-LLM.\r\n\r\nI think it would be useful considering a relevant parameter like that using IPEX-LLM - besides the automatic behavior you describe.",
          "created_at": "2024-11-26T04:38:25Z"
        },
        {
          "author": "sgwhat",
          "body": "`BIGDL_LLM_XMX_DISABLED ` is only applicable for ipex-llm transformers optimization.",
          "created_at": "2024-11-27T02:16:59Z"
        }
      ]
    },
    {
      "issue_number": 12403,
      "title": "Path of models using Ollama with IPEX-LLM (Windows)",
      "body": "Hello.\r\nI'm trying to figure out the path of the downloaded models when I run:\r\n\r\n`ollama pull <model name> `\r\n\r\nusing the version of Ollama with IPEX-LLM.\r\n\r\nIt's not `C:\\Users\\<username>\\.ollama\\models` (obviously)\r\n",
      "state": "closed",
      "author": "NikosDi",
      "author_type": "User",
      "created_at": "2024-11-14T11:16:33Z",
      "updated_at": "2025-03-10T11:33:46Z",
      "closed_at": "2024-11-17T09:50:03Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12403/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12403",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12403",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:10.012738",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Actually it should be `C:\\Users\\<username>\\.ollama\\models` as default.",
          "created_at": "2024-11-15T01:44:14Z"
        },
        {
          "author": "NikosDi",
          "body": "Well, no.\r\n\r\nI tried to search the model after downloading it and it should be easy due to the size (> 4GB) but it was invisible - search couldn't find it.\r\nSo, I had to monitor the download procedure in real-time and I discovered that the path actually is:\r\n\r\n`C:\\Windows\\System32\\config\\systemprofi",
          "created_at": "2024-11-15T03:55:36Z"
        },
        {
          "author": "sgwhat",
          "body": "You may set the environment variable `OLLAMA_MODELS`.",
          "created_at": "2024-11-15T09:04:32Z"
        },
        {
          "author": "NikosDi",
          "body": "It works like a charm.\r\n\r\nI had already seen the FAQ and suggestions of \"community ollama\" but somehow I think this \"Intel's ollama IPEX-LLM\" version is something different with its own settings.",
          "created_at": "2024-11-15T10:07:46Z"
        }
      ]
    },
    {
      "issue_number": 12371,
      "title": "ValueError: If `eos_token_id` is defined, make sure that `pad_token_id` is defined",
      "body": "Hello,\r\n\r\nI followed the instruction from the link below and got error in the last step when running  the demo.py. I have I-13700K with iGPU.\r\n\r\nCould you any one help with the error?\r\n\r\nthx\r\n\r\nhttps://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/install_linux_gpu.md",
      "state": "closed",
      "author": "fanlessfan",
      "author_type": "User",
      "created_at": "2024-11-08T18:54:55Z",
      "updated_at": "2025-03-10T11:33:45Z",
      "closed_at": "2024-11-10T13:31:19Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12371/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12371",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12371",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:10.294447",
      "comments": [
        {
          "author": "fanlessfan",
          "body": "not resolved, but Ollama works",
          "created_at": "2024-11-10T13:31:58Z"
        }
      ]
    },
    {
      "issue_number": 12732,
      "title": "deepseek",
      "body": null,
      "state": "closed",
      "author": "windedge",
      "author_type": "User",
      "created_at": "2025-01-22T06:12:51Z",
      "updated_at": "2025-03-10T11:32:53Z",
      "closed_at": "2025-01-22T06:13:22Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12732/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12732",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12732",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:10.472089",
      "comments": []
    },
    {
      "issue_number": 12719,
      "title": "Error Converting model for NPU LLM inference",
      "body": "I try to using the `convert.py` to convert the QWen2-1.5B-Instruct model to NPU form, and it resulted in Segmentation fault with the error similar to #12548 . \nDevice Information\n* NPU Driver version: 32.0.100.3104 (updated on Oct.25 2024)\n* Meteorlake\n* OS: Windows 11 23H2\n\nFull log:\n```shell\nhzk@DESKTOP-5L1BRNS MINGW64 ~/Documents/FYP/ipex-llm/python/llm/example/NPU/HF-Transformers-AutoModels/LLM/CPP_Examples (main)\n$ python convert.py --repo-id-or-model-path Qwen/Qwen2-1.5B-Instruct --save-directory \"C:\\Users\\hzk\\Documents\\FYP\\ipex-llm\\python\\llm\\example\\NPU\\HF-Transformers-AutoModels\\LLM\\CPP_Examples\\model\"\nC:\\Users\\hzk\\miniconda3\\envs\\llm\\Lib\\site-packages\\transformers\\deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n  warnings.warn(\nC:\\Users\\hzk\\miniconda3\\envs\\llm\\Lib\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n2025-01-17 23:36:01,931 - INFO - Converting model, it may takes up to several minutes ...\n2025-01-17 23:36:09,014 - INFO - Finish to convert model\ndecode start compiling\ndecode end compiling\nModel saved to C:\\Users\\hzk\\Documents\\FYP\\ipex-llm\\python\\llm\\example\\NPU\\HF-Transformers-AutoModels\\LLM\\CPP_Examples\\model\\decoder_layer_0.xml\nprefill start compiling\nprefill end compiling\nModel saved to C:\\Users\\hzk\\Documents\\FYP\\ipex-llm\\python\\llm\\example\\NPU\\HF-Transformers-AutoModels\\LLM\\CPP_Examples\\model\\decoder_layer_prefill.xml\nTraceback (most recent call last):\n  File \"C:\\Users\\hzk\\Documents\\FYP\\ipex-llm\\python\\llm\\example\\NPU\\HF-Transformers-AutoModels\\LLM\\CPP_Examples\\convert.py\", line 60, in <module>\n    model = AutoModelForCausalLM.from_pretrained(model_path,\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\hzk\\miniconda3\\envs\\llm\\Lib\\unittest\\mock.py\", line 1378, in patched\n    return func(*newargs, **newkeywargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\hzk\\miniconda3\\envs\\llm\\Lib\\site-packages\\ipex_llm\\transformers\\npu_model.py\", line 243, in from_pretrained\n    model = cls.optimize_npu_model(*args, **optimize_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\hzk\\miniconda3\\envs\\llm\\Lib\\site-packages\\ipex_llm\\transformers\\npu_model.py\", line 317, in optimize_npu_model\n    optimize_llm_single_process(\n  File \"C:\\Users\\hzk\\miniconda3\\envs\\llm\\Lib\\site-packages\\ipex_llm\\transformers\\npu_models\\convert.py\", line 458, in optimize_llm_single_process\n    convert_llm(model,\n  File \"C:\\Users\\hzk\\miniconda3\\envs\\llm\\Lib\\site-packages\\ipex_llm\\transformers\\npu_pipeline_model\\convert_pipeline.py\", line 215, in convert_llm\n    convert_llm_for_deploy(model,\n  File \"C:\\Users\\hzk\\miniconda3\\envs\\llm\\Lib\\site-packages\\ipex_llm\\transformers\\npu_pipeline_model\\convert_pipeline.py\", line 484, in convert_llm_for_deploy\n    convert_qwen_layer(model, 0, n_splits_linear, n_splits_down_proj,\n  File \"C:\\Users\\hzk\\miniconda3\\envs\\llm\\Lib\\site-packages\\ipex_llm\\transformers\\npu_pipeline_model\\qwen.py\", line 184, in convert_qwen_layer\n    rest_blob_path = update_names_of_IR_and_export_blob(single_decoder,\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\hzk\\miniconda3\\envs\\llm\\Lib\\site-packages\\ipex_llm\\transformers\\npu_pipeline_model\\common.py\", line 60, in update_names_of_IR_and_export_blob\n    compiledModel = core.compile_model(model, device_name=\"NPU\")\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\hzk\\miniconda3\\envs\\llm\\Lib\\site-packages\\intel_npu_acceleration_library\\backend\\..\\external\\openvino\\runtime\\ie_api.py\", line 543, in compile_model\n    super().compile_model(model, device_name, {} if config is None else config),\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Exception from src/inference/src/cpp/core.cpp:107:\nException from src/inference/src/dev/plugin.cpp:53:\nException from src/plugins/intel_npu/src/plugin/src/plugin.cpp:697:\nException from src/plugins/intel_npu/src/plugin/src/compiled_model.cpp:62:\nException from src/plugins/intel_npu/src/compiler/src/zero_compiler_in_driver.cpp:853:\nL0 pfnCreate2 result: ZE_RESULT_ERROR_INVALID_ARGUMENT, code 0x78000004\n\n\n\n\n\nSegmentation fault\n```",
      "state": "closed",
      "author": "Martin-HZK",
      "author_type": "User",
      "created_at": "2025-01-17T15:39:39Z",
      "updated_at": "2025-03-10T11:32:53Z",
      "closed_at": "2025-01-19T03:47:21Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12719/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12719",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12719",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:10.472110",
      "comments": [
        {
          "author": "Martin-HZK",
          "body": "The conversion failed with either convert.py or qwen.py provided.",
          "created_at": "2025-01-17T16:49:45Z"
        },
        {
          "author": "Martin-HZK",
          "body": "resolved",
          "created_at": "2025-01-19T03:47:22Z"
        }
      ]
    },
    {
      "issue_number": 12689,
      "title": "Install ollama failed? with Arc B580",
      "body": "Hello,\r\n\r\nI followed the instruction [ollama_quickstarts](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.zh-CN.md), but seems the GPU(B580) in my local machine is not detected.\r\n\r\n![1736446181653](https://github.com/user-attachments/assets/43da2a31-0e32-441c-b98a-141a45bc3e22)\r\n\r\nBesides, when I tried to create the model and also got failure.\r\n```\r\n(llm) PS E:\\ollama> .\\ollama.exe create example -f Modelfile\r\nError: something went wrong, please see the ollama server logs for details\r\n(llm) PS E:\\ollama>\r\n```\r\nAnd I couldn't find the server logs, any hint how to solve this?\r\n\r\nThanks,",
      "state": "closed",
      "author": "chhao01",
      "author_type": "User",
      "created_at": "2025-01-09T18:16:19Z",
      "updated_at": "2025-03-10T11:32:52Z",
      "closed_at": "2025-01-10T08:09:21Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12689/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12689",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12689",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:10.656208",
      "comments": [
        {
          "author": "jason-dai",
          "body": "See https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/bmg_quickstart.md",
          "created_at": "2025-01-10T00:33:36Z"
        },
        {
          "author": "chhao01",
          "body": "Thank you Jason. But it still doens't work.\r\n\r\n![1736475805781](https://github.com/user-attachments/assets/7787adc6-84e2-4142-8793-2d4dc0cc50a3)\r\n\r\nIt's supposed to output the GPU info right?\r\n\r\n\r\n",
          "created_at": "2025-01-10T02:28:56Z"
        },
        {
          "author": "chhao01",
          "body": "I did follow the instructions like:\r\nStep 1:\r\n```\r\nconda create -n llm2 python=3.11 libuv\r\nconda activate llm2\r\npip install --pre --upgrade ipex-llm[cpp] \r\nmkdir e:\\ollama\r\ne:\r\ncd ollama\r\n```\r\nStep 2:\r\n```\r\n(llm2) E:\\ollama>init-ollama.bat\r\n为 E:\\ollama\\ollama.exe <<===>> C:\\ProgramData\\anaconda3\\env",
          "created_at": "2025-01-10T02:48:48Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @chhao01, the `ollama server` is already running on the `ipex-llm runner`, and your `ollama server` output is correct. \r\nRegarding the issue with `ollama create example -f Modelfile`, I cannot reproduce this error, but I think you may test the model using `ollama run qwen2.5:3b-instruct-fp16`.",
          "created_at": "2025-01-10T03:28:54Z"
        },
        {
          "author": "chhao01",
          "body": "I used the open source llama.cpp script to convert the safetensors to gguf, does that cause the problem?",
          "created_at": "2025-01-10T03:35:04Z"
        }
      ]
    },
    {
      "issue_number": 12679,
      "title": "A typo leading to IPEX module load error",
      "body": "[This line](https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/src/ipex_llm/transformers/convert_ipex.py#L55) tries to import ipex optimized RMSNorm, however the name is misaligned with [the name in ipex](https://github.com/intel/intel-extension-for-pytorch/blob/main/intel_extension_for_pytorch/transformers/models/cpu/fusions/mha_fusion.py#L311) (the name should have a 'CPU' suffix).\r\nThe issue is initially reported at https://github.com/intel/intel-extension-for-pytorch/issues/766 .",
      "state": "closed",
      "author": "ZailiWang",
      "author_type": "User",
      "created_at": "2025-01-09T02:14:09Z",
      "updated_at": "2025-03-10T11:32:52Z",
      "closed_at": "2025-01-10T02:18:49Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12679/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12679",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12679",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:10.879644",
      "comments": []
    },
    {
      "issue_number": 12672,
      "title": "Cannot use B580",
      "body": "Hello,\r\n\r\nI'have followed the instructions at [Install WIndows GPU](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/install_windows_gpu.zh-CN.md)\r\n\r\nOS: Windows 11 Professional / 22621.3007\r\nPython: 4.11 \r\nCPU: 12th Gen Intel(R) Core(TM) i5-12490F   3.00 GHz\r\nDRAM: 32.0 GB\r\nGPU: B580\r\n\r\nAnd also validated the GPU and seems it works well.\r\n![1736316510327](https://github.com/user-attachments/assets/340725a9-62da-456a-9b62-49c51a63cc94)\r\n\r\nHowever, it failed and will exit the python intepreter directly without any logs.\r\n![1736317174192](https://github.com/user-attachments/assets/02e34b17-ea14-4282-87c5-76b335bc9a65)\r\n\r\nI don't have any clue to troubleshooting. Any suggestion will be great appricated. ",
      "state": "closed",
      "author": "chhao01",
      "author_type": "User",
      "created_at": "2025-01-08T06:25:01Z",
      "updated_at": "2025-03-10T11:32:51Z",
      "closed_at": "2025-01-08T07:31:50Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12672/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [
        "liu-shaojun"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12672",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12672",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:10.879665",
      "comments": [
        {
          "author": "liu-shaojun",
          "body": "Hi \r\n\r\nWe’ve recently updated our [guide](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/bmg_quickstart.md) for BMG. Please refer to this [guide](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/bmg_quickstart.md) for detailed instructions on",
          "created_at": "2025-01-08T07:08:56Z"
        },
        {
          "author": "chhao01",
          "body": "Many thanks!\r\n\r\nIt works now.",
          "created_at": "2025-01-08T07:31:51Z"
        }
      ]
    },
    {
      "issue_number": 12652,
      "title": "Unable to use Ollama on Intel Arc B580",
      "body": "I'm currently trying to get my B580 to work with ollama in Docker/Podman. I'm using the latest `intelanalytics/ipex-llm-inference-cpp-xpu` image on a Fedora 41 host (CPU: AMD Ryzen 5 5600, RAM: 32 GB).\r\n\r\n```\r\n$ uname -a\r\nLinux lab 6.12.7-200.fc41.x86_64 #1 SMP PREEMPT_DYNAMIC Fri Dec 27 17:05:33 UTC 2024 x86_64 GNU/Linux\r\n$ lspci -k\r\n0a:00.0 VGA compatible controller: Intel Corporation Battlemage G21 [Intel Graphics]\r\n\tSubsystem: Intel Corporation Device 1100\r\n\tKernel driver in use: xe\r\n\tKernel modules: xe\r\n```\r\n\r\nIf I set `OLLAMA_NUM_GPU=999` as documented (tested with mistral:7b and qwen2.5:14b) I get a SYCL error:\r\n\r\n```\r\nroot@5fb1794a72a1:/llm/ollama# ZES_ENABLE_SYSMAN=1 OLLAMA_NUM_GPU=999 ./ollama serve\r\n2025/01/05 23:38:16 routes.go:1197: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_H</details>OST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\r\ntime=2025-01-05T23:38:16.944+08:00 level=INFO source=images.go:753 msg=\"total blobs: 10\"\r\ntime=2025-01-05T23:38:16.944+08:00 level=INFO source=images.go:760 msg=\"total unused blobs removed: 0\"\r\n[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.\r\n\r\n[GIN-debug] [WARNING] Running in \"debug\" mode. Switch to \"release\" mode in production.\r\n - using env:\texport GIN_MODE=release\r\n - using code:\tgin.SetMode(gin.ReleaseMode)\r\n\r\n[GIN-debug] POST   /api/pull                 --> ollama/server.(*Server).PullHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/generate             --> ollama/server.(*Server).GenerateHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/chat                 --> ollama/server.(*Server).ChatHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/embed                --> ollama/server.(*Server).EmbedHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/embeddings           --> ollama/server.(*Server).EmbeddingsHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/create               --> ollama/server.(*Server).CreateHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/push                 --> ollama/server.(*Server).PushHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/copy                 --> ollama/server.(*Server).CopyHandler-fm (5 handlers)\r\n[GIN-debug] DELETE /api/delete               --> ollama/server.(*Server).DeleteHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/show                 --> ollama/server.(*Server).ShowHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/blobs/:digest        --> ollama/server.(*Server).CreateBlobHandler-fm (5 handlers)\r\n[GIN-debug] HEAD   /api/blobs/:digest        --> ollama/server.(*Server).HeadBlobHandler-fm (5 handlers)\r\n[GIN-debug] GET    /api/ps                   --> ollama/server.(*Server).PsHandler-fm (5 handlers)\r\n[GIN-debug] POST   /v1/chat/completions      --> ollama/server.(*Server).ChatHandler-fm (6 handlers)\r\n[GIN-debug] POST   /v1/completions           --> ollama/server.(*Server).GenerateHandler-fm (6 handlers)\r\n[GIN-debug] POST   /v1/embeddings            --> ollama/server.(*Server).EmbedHandler-fm (6 handlers)\r\n[GIN-debug] GET    /v1/models                --> ollama/server.(*Server).ListHandler-fm (6 handlers)\r\n[GIN-debug] GET    /v1/models/:model         --> ollama/server.(*Server).ShowHandler-fm (6 handlers)\r\n[GIN-debug] GET    /                         --> ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\r\n[GIN-debug] GET    /api/tags                 --> ollama/server.(*Server).ListHandler-fm (5 handlers)\r\n[GIN-debug] GET    /api/version              --> ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\r\n[GIN-debug] HEAD   /                         --> ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\r\n[GIN-debug] HEAD   /api/tags                 --> ollama/server.(*Server).ListHandler-fm (5 handlers)\r\n[GIN-debug] HEAD   /api/version              --> ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\r\ntime=2025-01-05T23:38:16.945+08:00 level=INFO source=routes.go:1248 msg=\"Listening on [::]:11434 (version 0.4.6-ipexllm-20250105)\"\r\ntime=2025-01-05T23:38:16.945+08:00 level=INFO source=common.go:135 msg=\"extracting embedded files\" dir=/tmp/ollama3587664585/runners\r\ntime=2025-01-05T23:38:16.985+08:00 level=INFO source=common.go:49 msg=\"Dynamic LLM libraries\" runners=[ipex_llm]\r\n[GIN] 2025/01/05 - 23:38:20 | 200 |      25.759µs |       127.0.0.1 | HEAD     \"/\"\r\n[GIN] 2025/01/05 - 23:38:20 | 200 |    4.516226ms |       127.0.0.1 | POST     \"/api/show\"\r\ntime=2025-01-05T23:38:20.501+08:00 level=INFO source=gpu.go:221 msg=\"looking for compatible GPUs\"\r\ntime=2025-01-05T23:38:20.501+08:00 level=WARN source=gpu.go:732 msg=\"unable to locate gpu dependency libraries\"\r\ntime=2025-01-05T23:38:20.502+08:00 level=WARN source=gpu.go:732 msg=\"unable to locate gpu dependency libraries\"\r\ntime=2025-01-05T23:38:20.502+08:00 level=WARN source=gpu.go:732 msg=\"unable to locate gpu dependency libraries\"\r\ntime=2025-01-05T23:38:20.504+08:00 level=WARN source=gpu.go:732 msg=\"unable to locate gpu dependency libraries\"\r\ntime=2025-01-05T23:38:20.513+08:00 level=INFO source=server.go:105 msg=\"system memory\" total=\"31.2 GiB\" free=\"24.5 GiB\" free_swap=\"8.0 GiB\"\r\ntime=2025-01-05T23:38:20.513+08:00 level=INFO source=memory.go:356 msg=\"offload to device\" layers.requested=-1 layers.model=33 layers.offload=0 layers.split=\"\" memory.available=\"[24.5 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"5.5 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"1.0 GiB\" memory.required.allocations=\"[5.5 GiB]\" memory.weights.total=\"4.7 GiB\" memory.weights.repeating=\"4.6 GiB\" memory.weights.nonrepeating=\"105.0 MiB\" memory.graph.full=\"560.0 MiB\" memory.graph.partial=\"585.0 MiB\"\r\ntime=2025-01-05T23:38:20.514+08:00 level=INFO source=server.go:401 msg=\"starting llama server\" cmd=\"/tmp/ollama3587664585/runners/ipex_llm/ollama_llama_server --model /models/blobs/sha256-ff82381e2bea77d91c1b824c7afb83f6fb73e9f7de9dda631bcdbca564aa5435 --ctx-size 8192 --batch-size 512 --n-gpu-layers 999 --threads 6 --no-mmap --parallel 4 --port 46313\"\r\ntime=2025-01-05T23:38:20.514+08:00 level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\r\ntime=2025-01-05T23:38:20.514+08:00 level=INFO source=server.go:580 msg=\"waiting for llama runner to start responding\"\r\ntime=2025-01-05T23:38:20.514+08:00 level=INFO source=server.go:614 msg=\"waiting for server to become available\" status=\"llm server error\"\r\ntime=2025-01-05T23:38:20.548+08:00 level=INFO source=runner.go:956 msg=\"starting go runner\"\r\ntime=2025-01-05T23:38:20.548+08:00 level=INFO source=runner.go:957 msg=system info=\"AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | cgo(gcc)\" threads=6\r\ntime=2025-01-05T23:38:20.549+08:00 level=INFO source=.:0 msg=\"Server listening on 127.0.0.1:46313\"\r\nllama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from /models/blobs/sha256-ff82381e2bea77d91c1b824c7afb83f6fb73e9f7de9dda631bcdbca564aa5435 (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3\r\nllama_model_loader: - kv   2:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   3:                       llama.context_length u32              = 32768\r\nllama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 2\r\nllama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768\r\nllama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default\r\nllama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\r\nllama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\r\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\r\nllama_model_loader: - kv  24:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q4_0:  225 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\r\nllm_load_vocab: special tokens cache size = 771\r\nllm_load_vocab: token to piece cache size = 0.1731 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32768\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 32768\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 32768\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q4_0\r\nllm_load_print_meta: model params     = 7.25 B\r\nllm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) \r\nllm_load_print_meta: general.name     = Mistral-7B-Instruct-v0.3\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 781 '<0x0A>'\r\nllm_load_print_meta: EOG token        = 2 '</s>'\r\nllm_load_print_meta: max token length = 48\r\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nggml_sycl_init: SYCL_USE_XMX: yes\r\nggml_sycl_init: found 1 SYCL devices:\r\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\r\nllm_load_tensors: ggml ctx size =    0.27 MiB\r\ntime=2025-01-05T23:38:20.766+08:00 level=INFO source=server.go:614 msg=\"waiting for server to become available\" status=\"llm server loading model\"\r\nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 33/33 layers to GPU\r\nllm_load_tensors:      SYCL0 buffer size =  3850.02 MiB\r\nllm_load_tensors:  SYCL_Host buffer size =    72.00 MiB\r\nllama_new_context_with_model: n_ctx      = 8192\r\nllama_new_context_with_model: n_batch    = 2048\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 1000000.0\r\nllama_new_context_with_model: freq_scale = 1\r\n[SYCL] call ggml_check_sycl\r\nggml_check_sycl: GGML_SYCL_DEBUG: 0\r\nggml_check_sycl: GGML_SYCL_F16: no\r\nfound 1 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|                Intel Graphics [0xe20b]|    1.6|    160|    1024|   32| 12168M|            1.3.31294|\r\nllama_kv_cache_init:      SYCL0 KV buffer size =  1024.00 MiB\r\nllama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\r\nllama_new_context_with_model:  SYCL_Host  output buffer size =     0.56 MiB\r\nllama_new_context_with_model:      SYCL0 compute buffer size =    96.00 MiB\r\nllama_new_context_with_model:  SYCL_Host compute buffer size =    24.01 MiB\r\nllama_new_context_with_model: graph nodes  = 902\r\nllama_new_context_with_model: graph splits = 2\r\ntime=2025-01-05T23:38:28.030+08:00 level=WARN source=runner.go:894 msg=\"%s: warming up the model with an empty run - please wait ... \" !BADKEY=loadModel\r\ncould not create a primitive descriptor for a matmul primitive\r\nException caught at file:/home/runner/_work/llm.cpp/llm.cpp/ollama-llama-cpp/ggml/src/ggml-sycl.cpp, line:3226, func:operator()\r\nSYCL error: CHECK_TRY_ERROR(op(ctx, src0, src1, dst, src0_dd_i, src1_ddf_i, src1_ddq_i, dst_dd_i, dev[i].row_low, dev[i].row_high, src1_ncols, src1_padded_col_size, stream)): Meet error in this line code!\r\n  in function ggml_sycl_op_mul_mat at /home/runner/_work/llm.cpp/llm.cpp/ollama-llama-cpp/ggml/src/ggml-sycl.cpp:3226\r\n/home/runner/_work/llm.cpp/llm.cpp/ollama-llama-cpp/ggml/src/ggml-sycl/common.hpp:107: SYCL error\r\nlibollama_ggml.so(+0x7d877)[0x7f3ca2a7d877]\r\nlibollama_ggml.so(ggml_abort+0xd8)[0x7f3ca2a7d808]\r\nlibollama_ggml.so(+0x200b98)[0x7f3ca2c00b98]\r\nlibollama_ggml.so(+0x237118)[0x7f3ca2c37118]\r\nlibollama_ggml.so(_Z25ggml_sycl_compute_forwardR25ggml_backend_sycl_contextP11ggml_tensor+0x5ef)[0x7f3ca2c036ff]\r\nlibollama_ggml.so(+0x24e12f)[0x7f3ca2c4e12f]\r\nlibollama_ggml.so(ggml_backend_sched_graph_compute_async+0x548)[0x7f3ca2aed698]\r\nlibollama_llama.so(llama_decode+0xb53)[0x7f3ca4a7dd53]\r\n/tmp/ollama3587664585/runners/ipex_llm/ollama_llama_server(_cgo_0deba22bda5f_Cfunc_llama_decode+0x4c)[0x55d72f10f88c]\r\n/tmp/ollama3587664585/runners/ipex_llm/ollama_llama_server(+0xf8b01)[0x55d72eef8b01]\r\nSIGABRT: abort\r\nPC=0x7f3ca22429fc m=4 sigcode=18446744073709551610\r\nsignal arrived during cgo execution\r\n\r\ngoroutine 6 gp=0xc000007dc0 m=4 mp=0xc00006d808 [syscall]:\r\nruntime.cgocall(0x55d72f10f840, 0xc0000e3c50)\r\n\truntime/cgocall.go:157 +0x4b fp=0xc0000e3c28 sp=0xc0000e3bf0 pc=0x55d72ee9046b\r\nollama/llama/llamafile._Cfunc_llama_decode(0x7f3c3eb6a0a0, {0x9, 0x7f3c3c126860, 0x0, 0x0, 0x7f3c3c126890, 0x7f3c3c1268c0, 0x7f3c3eaeec60, 0x7f3c3c00c6f0, 0x0, ...})\r\n\t_cgo_gotypes.go:548 +0x52 fp=0xc0000e3c50 sp=0xc0000e3c28 pc=0x55d72ef8d9f2\r\nollama/llama/llamafile.(*Context).Decode.func1(0x7f3c3c126890?, 0x7f3c3c1268c0?)\r\n\tollama/llama/llamafile/llama.go:121 +0xd8 fp=0xc0000e3d70 sp=0xc0000e3c50 pc=0x55d72ef900b8\r\nollama/llama/llamafile.(*Context).Decode(0x0?, 0x0?)\r\n\tollama/llama/llamafile/llama.go:121 +0x13 fp=0xc0000e3db8 sp=0xc0000e3d70 pc=0x55d72ef8ff53\r\nmain.(*Server).loadModel(0xc0000c0120, {0x3e7, 0x0, 0x0, 0x0, {0x0, 0x0, 0x0}, 0xc00003a1a0, 0x0}, ...)\r\n\tollama/llama/runner/runner.go:905 +0x3bd fp=0xc0000e3f10 sp=0xc0000e3db8 pc=0x55d72f10d25d\r\nmain.main.gowrap1()\r\n\tollama/llama/runner/runner.go:990 +0xda fp=0xc0000e3fe0 sp=0xc0000e3f10 pc=0x55d72f10e95a\r\nruntime.goexit({})\r\n\truntime/asm_amd64.s:1695 +0x1 fp=0xc0000e3fe8 sp=0xc0000e3fe0 pc=0x55d72eef8e81\r\ncreated by main.main in goroutine 1\r\n\tollama/llama/runner/runner.go:990 +0xc6c\r\n\r\ngoroutine 1 gp=0xc0000061c0 m=nil [IO wait]:\r\nruntime.gopark(0xc000046008?, 0x0?, 0xc0?, 0x61?, 0xc00003f898?)\r\n\truntime/proc.go:402 +0xce fp=0xc00003f860 sp=0xc00003f840 pc=0x55d72eec70ae\r\nruntime.netpollblock(0xc00003f8f8?, 0x2ee8fbc6?, 0xd7?)\r\n\truntime/netpoll.go:573 +0xf7 fp=0xc00003f898 sp=0xc00003f860 pc=0x55d72eebf2f7\r\ninternal/poll.runtime_pollWait(0x7f3ca5030020, 0x72)\r\n\truntime/netpoll.go:345 +0x85 fp=0xc00003f8b8 sp=0xc00003f898 pc=0x55d72eef3b45\r\ninternal/poll.(*pollDesc).wait(0x3?, 0x3fe?, 0x0)\r\n\tinternal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc00003f8e0 sp=0xc00003f8b8 pc=0x55d72ef43a67\r\ninternal/poll.(*pollDesc).waitRead(...)\r\n\tinternal/poll/fd_poll_runtime.go:89\r\ninternal/poll.(*FD).Accept(0xc0000ee080)\r\n\tinternal/poll/fd_unix.go:611 +0x2ac fp=0xc00003f988 sp=0xc00003f8e0 pc=0x55d72ef44f2c\r\nnet.(*netFD).accept(0xc0000ee080)\r\n\tnet/fd_unix.go:172 +0x29 fp=0xc00003fa40 sp=0xc00003f988 pc=0x55d72efb3b49\r\nnet.(*TCPListener).accept(0xc00007e1e0)\r\n\tnet/tcpsock_posix.go:159 +0x1e fp=0xc00003fa68 sp=0xc00003fa40 pc=0x55d72efc487e\r\nnet.(*TCPListener).Accept(0xc00007e1e0)\r\n\tnet/tcpsock.go:327 +0x30 fp=0xc00003fa98 sp=0xc00003fa68 pc=0x55d72efc3bd0\r\nnet/http.(*onceCloseListener).Accept(0xc0000c01b0?)\r\n\t<autogenerated>:1 +0x24 fp=0xc00003fab0 sp=0xc00003fa98 pc=0x55d72f0eade4\r\nnet/http.(*Server).Serve(0xc0000f4000, {0x55d72f416560, 0xc00007e1e0})\r\n\tnet/http/server.go:3260 +0x33e fp=0xc00003fbe0 sp=0xc00003fab0 pc=0x55d72f0e1bfe\r\nmain.main()\r\n\tollama/llama/runner/runner.go:1015 +0x10cd fp=0xc00003ff50 sp=0xc00003fbe0 pc=0x55d72f10e5cd\r\nruntime.main()\r\n\truntime/proc.go:271 +0x29d fp=0xc00003ffe0 sp=0xc00003ff50 pc=0x55d72eec6c7d\r\nruntime.goexit({})\r\n\truntime/asm_amd64.s:1695 +0x1 fp=0xc00003ffe8 sp=0xc00003ffe0 pc=0x55d72eef8e81\r\n\r\ngoroutine 2 gp=0xc000006c40 m=nil [force gc (idle)]:\r\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\r\n\truntime/proc.go:402 +0xce fp=0xc000066fa8 sp=0xc000066f88 pc=0x55d72eec70ae\r\nruntime.goparkunlock(...)\r\n\truntime/proc.go:408\r\nruntime.forcegchelper()\r\n\truntime/proc.go:326 +0xb8 fp=0xc000066fe0 sp=0xc000066fa8 pc=0x55d72eec6f38\r\nruntime.goexit({})\r\n\truntime/asm_amd64.s:1695 +0x1 fp=0xc000066fe8 sp=0xc000066fe0 pc=0x55d72eef8e81\r\ncreated by runtime.init.6 in goroutine 1\r\n\truntime/proc.go:314 +0x1a\r\n\r\ngoroutine 3 gp=0xc000007180 m=nil [GC sweep wait]:\r\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\r\n\truntime/proc.go:402 +0xce fp=0xc000067780 sp=0xc000067760 pc=0x55d72eec70ae\r\nruntime.goparkunlock(...)\r\n\truntime/proc.go:408\r\nruntime.bgsweep(0xc000024230)\r\n\truntime/mgcsweep.go:278 +0x94 fp=0xc0000677c8 sp=0xc000067780 pc=0x55d72eeb1bf4\r\nruntime.gcenable.gowrap1()\r\n\truntime/mgc.go:203 +0x25 fp=0xc0000677e0 sp=0xc0000677c8 pc=0x55d72eea6725\r\nruntime.goexit({})\r\n\truntime/asm_amd64.s:1695 +0x1 fp=0xc0000677e8 sp=0xc0000677e0 pc=0x55d72eef8e81\r\ncreated by runtime.gcenable in goroutine 1\r\n\truntime/mgc.go:203 +0x66\r\n\r\ngoroutine 4 gp=0xc000007340 m=nil [GC scavenge wait]:\r\nruntime.gopark(0xc000024230?, 0x55d72f18e208?, 0x1?, 0x0?, 0xc000007340?)\r\n\truntime/proc.go:402 +0xce fp=0xc000067f78 sp=0xc000067f58 pc=0x55d72eec70ae\r\nruntime.goparkunlock(...)\r\n\truntime/proc.go:408\r\nruntime.(*scavengerState).park(0x55d72f5e0680)\r\n\truntime/mgcscavenge.go:425 +0x49 fp=0xc000067fa8 sp=0xc000067f78 pc=0x55d72eeaf5e9\r\nruntime.bgscavenge(0xc000024230)\r\n\truntime/mgcscavenge.go:653 +0x3c fp=0xc000067fc8 sp=0xc000067fa8 pc=0x55d72eeafb7c\r\nruntime.gcenable.gowrap2()\r\n\truntime/mgc.go:204 +0x25 fp=0xc000067fe0 sp=0xc000067fc8 pc=0x55d72eea66c5\r\nruntime.goexit({})\r\n\truntime/asm_amd64.s:1695 +0x1 fp=0xc000067fe8 sp=0xc000067fe0 pc=0x55d72eef8e81\r\ncreated by runtime.gcenable in goroutine 1\r\n\truntime/mgc.go:204 +0xa5\r\n\r\ngoroutine 5 gp=0xc000007c00 m=nil [finalizer wait]:\r\nruntime.gopark(0xc000066648?, 0x55d72ee9a025?, 0xa8?, 0x1?, 0xc0000061c0?)\r\n\truntime/proc.go:402 +0xce fp=0xc000066620 sp=0xc000066600 pc=0x55d72eec70ae\r\nruntime.runfinq()\r\n\truntime/mfinal.go:194 +0x107 fp=0xc0000667e0 sp=0xc000066620 pc=0x55d72eea5767\r\nruntime.goexit({})\r\n\truntime/asm_amd64.s:1695 +0x1 fp=0xc0000667e8 sp=0xc0000667e0 pc=0x55d72eef8e81\r\ncreated by runtime.createfing in goroutine 1\r\n\truntime/mfinal.go:164 +0x3d\r\n\r\ngoroutine 7 gp=0xc0000f2000 m=nil [semacquire]:\r\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\r\n\truntime/proc.go:402 +0xce fp=0xc000068e08 sp=0xc000068de8 pc=0x55d72eec70ae\r\nruntime.goparkunlock(...)\r\n\truntime/proc.go:408\r\nruntime.semacquire1(0xc0000c0128, 0x0, 0x1, 0x0, 0x12)\r\n\truntime/sema.go:160 +0x22c fp=0xc000068e70 sp=0xc000068e08 pc=0x55d72eed94cc\r\nsync.runtime_Semacquire(0x0?)\r\n\truntime/sema.go:62 +0x25 fp=0xc000068ea8 sp=0xc000068e70 pc=0x55d72eef5305\r\nsync.(*WaitGroup).Wait(0x0?)\r\n\tsync/waitgroup.go:116 +0x48 fp=0xc000068ed0 sp=0xc000068ea8 pc=0x55d72ef13d88\r\nmain.(*Server).run(0xc0000c0120, {0x55d72f416ba0, 0xc00009a0a0})\r\n\tollama/llama/runner/runner.go:315 +0x47 fp=0xc000068fb8 sp=0xc000068ed0 pc=0x55d72f109627\r\nmain.main.gowrap2()\r\n\tollama/llama/runner/runner.go:995 +0x28 fp=0xc000068fe0 sp=0xc000068fb8 pc=0x55d72f10e848\r\nruntime.goexit({})\r\n\truntime/asm_amd64.s:1695 +0x1 fp=0xc000068fe8 sp=0xc000068fe0 pc=0x55d72eef8e81\r\ncreated by main.main in goroutine 1\r\n\tollama/llama/runner/runner.go:995 +0xd3e\r\n\r\ngoroutine 8 gp=0xc0000f21c0 m=nil [IO wait]:\r\nruntime.gopark(0x94?, 0xc0000e7958?, 0x40?, 0x79?, 0xb?)\r\n\truntime/proc.go:402 +0xce fp=0xc0000e7910 sp=0xc0000e78f0 pc=0x55d72eec70ae\r\nruntime.netpollblock(0x55d72ef2d5f8?, 0x2ee8fbc6?, 0xd7?)\r\n\truntime/netpoll.go:573 +0xf7 fp=0xc0000e7948 sp=0xc0000e7910 pc=0x55d72eebf2f7\r\ninternal/poll.runtime_pollWait(0x7f3ca502ff28, 0x72)\r\n\truntime/netpoll.go:345 +0x85 fp=0xc0000e7968 sp=0xc0000e7948 pc=0x55d72eef3b45\r\ninternal/poll.(*pollDesc).wait(0xc0000ee100?, 0xc0000f6000?, 0x0)\r\n\tinternal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc0000e7990 sp=0xc0000e7968 pc=0x55d72ef43a67\r\ninternal/poll.(*pollDesc).waitRead(...)\r\n\tinternal/poll/fd_poll_runtime.go:89\r\ninternal/poll.(*FD).Read(0xc0000ee100, {0xc0000f6000, 0x1000, 0x1000})\r\n\tinternal/poll/fd_unix.go:164 +0x27a fp=0xc0000e7a28 sp=0xc0000e7990 pc=0x55d72ef445ba\r\nnet.(*netFD).Read(0xc0000ee100, {0xc0000f6000?, 0xc0000e7a98?, 0x55d72ef43f25?})\r\n\tnet/fd_posix.go:55 +0x25 fp=0xc0000e7a70 sp=0xc0000e7a28 pc=0x55d72efb2a45\r\nnet.(*conn).Read(0xc00006a098, {0xc0000f6000?, 0x0?, 0xc0000a6ed8?})\r\n\tnet/net.go:185 +0x45 fp=0xc0000e7ab8 sp=0xc0000e7a70 pc=0x55d72efbcd05\r\nnet.(*TCPConn).Read(0xc0000a6ed0?, {0xc0000f6000?, 0xc0000ee100?, 0xc0000e7af0?})\r\n\t<autogenerated>:1 +0x25 fp=0xc0000e7ae8 sp=0xc0000e7ab8 pc=0x55d72efc86e5\r\nnet/http.(*connReader).Read(0xc0000a6ed0, {0xc0000f6000, 0x1000, 0x1000})\r\n\tnet/http/server.go:789 +0x14b fp=0xc0000e7b38 sp=0xc0000e7ae8 pc=0x55d72f0d7a0b\r\nbufio.(*Reader).fill(0xc000044480)\r\n\tbufio/bufio.go:110 +0x103 fp=0xc0000e7b70 sp=0xc0000e7b38 pc=0x55d72f094303\r\nbufio.(*Reader).Peek(0xc000044480, 0x4)\r\n\tbufio/bufio.go:148 +0x53 fp=0xc0000e7b90 sp=0xc0000e7b70 pc=0x55d72f094433\r\nnet/http.(*conn).serve(0xc0000c01b0, {0x55d72f416b68, 0xc0000a6db0})\r\n\tnet/http/server.go:2079 +0x749 fp=0xc0000e7fb8 sp=0xc0000e7b90 pc=0x55d72f0dd769\r\nnet/http.(*Server).Serve.gowrap3()\r\n\tnet/http/server.go:3290 +0x28 fp=0xc0000e7fe0 sp=0xc0000e7fb8 pc=0x55d72f0e1fe8\r\nruntime.goexit({})\r\n\truntime/asm_amd64.s:1695 +0x1 fp=0xc0000e7fe8 sp=0xc0000e7fe0 pc=0x55d72eef8e81\r\ncreated by net/http.(*Server).Serve in goroutine 1\r\n\tnet/http/server.go:3290 +0x4b4\r\n\r\nrax    0x0\r\nrbx    0x7f3c467fd640\r\nrcx    0x7f3ca22429fc\r\nrdx    0x6\r\nrdi    0x2bb\r\nrsi    0x2be\r\nrbp    0x2be\r\nrsp    0x7f3c467fa430\r\nr8     0x7f3c467fa500\r\nr9     0x5\r\nr10    0x8\r\nr11    0x246\r\nr12    0x6\r\nr13    0x16\r\nr14    0x7f3ca434e260\r\nr15    0x7f3ca23c6860\r\nrip    0x7f3ca22429fc\r\nrflags 0x246\r\ncs     0x33\r\nfs     0x0\r\ngs     0x0\r\ntime=2025-01-05T23:38:28.288+08:00 level=ERROR source=sched.go:455 msg=\"error loading llama server\" error=\"llama runner process has terminated: error:CHECK_TRY_ERROR(op(ctx, src0, src1, dst, src0_dd_i, src1_ddf_i, src1_ddq_i, dst_dd_i, dev[i].row_low, dev[i].row_high, src1_ncols, src1_padded_col_size, stream)): Meet error in this line code!\"\r\n```\r\n\r\n\r\nAs suggested [here](https://github.com/ollama/ollama/issues/1590#issuecomment-2468429913) I also tried `OLLAMA_NUM_GPU=1` which results in a `illegal instruction` error (although the CPU does support AVX/2):\r\n\r\n```\r\nroot@5fb1794a72a1:/llm/ollama# ZES_ENABLE_SYSMAN=1 OLLAMA_NUM_GPU=1 ./ollama serve\r\n2025/01/05 23:36:29 routes.go:1197: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\r\ntime=2025-01-05T23:36:29.576+08:00 level=INFO source=images.go:753 msg=\"total blobs: 10\"\r\ntime=2025-01-05T23:36:29.576+08:00 level=INFO source=images.go:760 msg=\"total unused blobs removed: 0\"\r\n[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.\r\n\r\n[GIN-debug] [WARNING] Running in \"debug\" mode. Switch to \"release\" mode in production.\r\n - using env:\texport GIN_MODE=release\r\n - using code:\tgin.SetMode(gin.ReleaseMode)\r\n\r\n[GIN-debug] POST   /api/pull                 --> ollama/server.(*Server).PullHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/generate             --> ollama/server.(*Server).GenerateHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/chat                 --> ollama/server.(*Server).ChatHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/embed                --> ollama/server.(*Server).EmbedHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/embeddings           --> ollama/server.(*Server).EmbeddingsHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/create               --> ollama/server.(*Server).CreateHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/push                 --> ollama/server.(*Server).PushHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/copy                 --> ollama/server.(*Server).CopyHandler-fm (5 handlers)\r\n[GIN-debug] DELETE /api/delete               --> ollama/server.(*Server).DeleteHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/show                 --> ollama/server.(*Server).ShowHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/blobs/:digest        --> ollama/server.(*Server).CreateBlobHandler-fm (5 handlers)\r\n[GIN-debug] HEAD   /api/blobs/:digest        --> ollama/server.(*Server).HeadBlobHandler-fm (5 handlers)\r\n[GIN-debug] GET    /api/ps                   --> ollama/server.(*Server).PsHandler-fm (5 handlers)\r\n[GIN-debug] POST   /v1/chat/completions      --> ollama/server.(*Server).ChatHandler-fm (6 handlers)\r\n[GIN-debug] POST   /v1/completions           --> ollama/server.(*Server).GenerateHandler-fm (6 handlers)\r\n[GIN-debug] POST   /v1/embeddings            --> ollama/server.(*Server).EmbedHandler-fm (6 handlers)\r\n[GIN-debug] GET    /v1/models                --> ollama/server.(*Server).ListHandler-fm (6 handlers)\r\n[GIN-debug] GET    /v1/models/:model         --> ollama/server.(*Server).ShowHandler-fm (6 handlers)\r\n[GIN-debug] GET    /                         --> ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\r\n[GIN-debug] GET    /api/tags                 --> ollama/server.(*Server).ListHandler-fm (5 handlers)\r\n[GIN-debug] GET    /api/version              --> ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\r\n[GIN-debug] HEAD   /                         --> ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\r\n[GIN-debug] HEAD   /api/tags                 --> ollama/server.(*Server).ListHandler-fm (5 handlers)\r\n[GIN-debug] HEAD   /api/version              --> ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\r\ntime=2025-01-05T23:36:29.577+08:00 level=INFO source=routes.go:1248 msg=\"Listening on [::]:11434 (version 0.4.6-ipexllm-20250105)\"\r\ntime=2025-01-05T23:36:29.577+08:00 level=INFO source=common.go:135 msg=\"extracting embedded files\" dir=/tmp/ollama357000223/runners\r\ntime=2025-01-05T23:36:29.617+08:00 level=INFO source=common.go:49 msg=\"Dynamic LLM libraries\" runners=[ipex_llm]\r\n[GIN] 2025/01/05 - 23:36:39 | 200 |       26.01µs |       127.0.0.1 | HEAD     \"/\"\r\n[GIN] 2025/01/05 - 23:36:39 | 200 |    4.596428ms |       127.0.0.1 | POST     \"/api/show\"\r\ntime=2025-01-05T23:36:39.040+08:00 level=INFO source=gpu.go:221 msg=\"looking for compatible GPUs\"\r\ntime=2025-01-05T23:36:39.041+08:00 level=WARN source=gpu.go:732 msg=\"unable to locate gpu dependency libraries\"\r\ntime=2025-01-05T23:36:39.041+08:00 level=WARN source=gpu.go:732 msg=\"unable to locate gpu dependency libraries\"\r\ntime=2025-01-05T23:36:39.041+08:00 level=WARN source=gpu.go:732 msg=\"unable to locate gpu dependency libraries\"\r\ntime=2025-01-05T23:36:39.043+08:00 level=WARN source=gpu.go:732 msg=\"unable to locate gpu dependency libraries\"\r\ntime=2025-01-05T23:36:39.052+08:00 level=INFO source=server.go:105 msg=\"system memory\" total=\"31.2 GiB\" free=\"24.8 GiB\" free_swap=\"8.0 GiB\"\r\ntime=2025-01-05T23:36:39.053+08:00 level=INFO source=memory.go:356 msg=\"offload to device\" layers.requested=-1 layers.model=33 layers.offload=0 layers.split=\"\" memory.available=\"[24.8 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"5.5 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"1.0 GiB\" memory.required.allocations=\"[5.5 GiB]\" memory.weights.total=\"4.7 GiB\" memory.weights.repeating=\"4.6 GiB\" memory.weights.nonrepeating=\"105.0 MiB\" memory.graph.full=\"560.0 MiB\" memory.graph.partial=\"585.0 MiB\"\r\ntime=2025-01-05T23:36:39.057+08:00 level=INFO source=server.go:401 msg=\"starting llama server\" cmd=\"/tmp/ollama357000223/runners/ipex_llm/ollama_llama_server --model /models/blobs/sha256-ff82381e2bea77d91c1b824c7afb83f6fb73e9f7de9dda631bcdbca564aa5435 --ctx-size 8192 --batch-size 512 --n-gpu-layers 1 --threads 6 --no-mmap --parallel 4 --port 35309\"\r\ntime=2025-01-05T23:36:39.057+08:00 level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\r\ntime=2025-01-05T23:36:39.057+08:00 level=INFO source=server.go:580 msg=\"waiting for llama runner to start responding\"\r\ntime=2025-01-05T23:36:39.058+08:00 level=INFO source=server.go:614 msg=\"waiting for server to become available\" status=\"llm server error\"\r\ntime=2025-01-05T23:36:39.092+08:00 level=INFO source=runner.go:956 msg=\"starting go runner\"\r\ntime=2025-01-05T23:36:39.092+08:00 level=INFO source=runner.go:957 msg=system info=\"AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | cgo(gcc)\" threads=6\r\ntime=2025-01-05T23:36:39.092+08:00 level=INFO source=.:0 msg=\"Server listening on 127.0.0.1:35309\"\r\nllama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from /models/blobs/sha256-ff82381e2bea77d91c1b824c7afb83f6fb73e9f7de9dda631bcdbca564aa5435 (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3\r\nllama_model_loader: - kv   2:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   3:                       llama.context_length u32              = 32768\r\nllama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 2\r\nllama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768\r\nllama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default\r\nllama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\r\nllama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\r\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\r\nllama_model_loader: - kv  24:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q4_0:  225 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\r\nllm_load_vocab: special tokens cache size = 771\r\nllm_load_vocab: token to piece cache size = 0.1731 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32768\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 32768\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 32768\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q4_0\r\nllm_load_print_meta: model params     = 7.25 B\r\nllm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) \r\nllm_load_print_meta: general.name     = Mistral-7B-Instruct-v0.3\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 781 '<0x0A>'\r\nllm_load_print_meta: EOG token        = 2 '</s>'\r\nllm_load_print_meta: max token length = 48\r\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nggml_sycl_init: SYCL_USE_XMX: yes\r\nggml_sycl_init: found 1 SYCL devices:\r\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\r\nllm_load_tensors: ggml ctx size =    0.27 MiB\r\ntime=2025-01-05T23:36:39.308+08:00 level=INFO source=server.go:614 msg=\"waiting for server to become available\" status=\"llm server loading model\"\r\nllm_load_tensors: offloading 1 repeating layers to GPU\r\nllm_load_tensors: offloaded 1/33 layers to GPU\r\nllm_load_tensors:      SYCL0 buffer size =   117.03 MiB\r\nllm_load_tensors:  SYCL_Host buffer size =  3804.98 MiB\r\nllama_new_context_with_model: n_ctx      = 8192\r\nllama_new_context_with_model: n_batch    = 2048\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 1000000.0\r\nllama_new_context_with_model: freq_scale = 1\r\n[SYCL] call ggml_check_sycl\r\nggml_check_sycl: GGML_SYCL_DEBUG: 0\r\nggml_check_sycl: GGML_SYCL_F16: no\r\nfound 1 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|                Intel Graphics [0xe20b]|    1.6|    160|    1024|   32| 12168M|            1.3.31294|\r\nllama_kv_cache_init:      SYCL0 KV buffer size =    32.00 MiB\r\nllama_kv_cache_init:  SYCL_Host KV buffer size =   992.00 MiB\r\nllama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\r\nllama_new_context_with_model:  SYCL_Host  output buffer size =     0.56 MiB\r\nllama_new_context_with_model:      SYCL0 compute buffer size =   560.00 MiB\r\nllama_new_context_with_model:  SYCL_Host compute buffer size =   552.01 MiB\r\nllama_new_context_with_model: graph nodes  = 1030\r\nllama_new_context_with_model: graph splits = 190\r\ntime=2025-01-05T23:36:41.274+08:00 level=WARN source=runner.go:894 msg=\"%s: warming up the model with an empty run - please wait ... \" !BADKEY=loadModel\r\ntime=2025-01-05T23:36:41.515+08:00 level=INFO source=server.go:614 msg=\"waiting for server to become available\" status=\"llm server not responding\"\r\ntime=2025-01-05T23:36:42.853+08:00 level=INFO source=server.go:614 msg=\"waiting for server to become available\" status=\"llm server error\"\r\ntime=2025-01-05T23:36:43.104+08:00 level=ERROR source=sched.go:455 msg=\"error loading llama server\" error=\"llama runner process has terminated: signal: illegal instruction (core dumped)\"\r\n```\r\n\r\nI've also tried different combinations of other parameters (e.g., `OLLAMA_NUM_PARALLEL=1`) but without any luck. I always get these two errors, depending on the value of `OLLAMA_NUM_GPU`.\r\n",
      "state": "closed",
      "author": "x1tan",
      "author_type": "User",
      "created_at": "2025-01-05T15:51:45Z",
      "updated_at": "2025-03-10T11:32:51Z",
      "closed_at": "2025-01-13T12:41:44Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12652/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "leonardozcm",
        "ACupofAir"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12652",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12652",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:11.074532",
      "comments": [
        {
          "author": "ACupofAir",
          "body": "We have updated the docker image and have verified that this problem has been solved. Please follow this command `docker pull intelanalytics/ipex-llm-inference-cpp-xpu:latest` to update the image and try again.",
          "created_at": "2025-01-10T02:15:14Z"
        }
      ]
    },
    {
      "issue_number": 12639,
      "title": "Can not run Ollama with Arc GPU(A770) on Ubuntu24.04",
      "body": "I can't run Ollama with Arc GPU(A770) on Ubuntu24.04\r\n\r\nIPEX-LLM was installed as follows\r\n~~~\r\npip install --pre --upgrade ipex-llm[cpp]\r\n~~~\r\n\r\nI got the following error when using Ollama\r\n~~~\r\nlibsvml.so: cannot open shared object file: No such file or directory\r\n~~~\r\n\r\nI suspect that there is a problem with the version of “bigdl-core-cpp”.\r\n\r\n\r\nThis error disappears when I do the following\r\n~~~\r\npip install --pre --upgrade ipex-llm[cpp]\r\npip install -U bigdl-core-cpp==2.5.0\r\n~~~",
      "state": "closed",
      "author": "dai-ichiro",
      "author_type": "User",
      "created_at": "2025-01-02T01:59:51Z",
      "updated_at": "2025-03-10T11:32:50Z",
      "closed_at": "2025-01-02T03:34:19Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12639/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12639",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12639",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:11.263327",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @dai-ichiro, \r\n\r\nPlease refer to our [documentation](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.md). Before running ollama, execute `source /opt/intel/oneapi/setvars.sh`.\r\n\r\nAnd also please only use `pip install --pre --upgrade ipex-llm[cpp]` to",
          "created_at": "2025-01-02T02:24:43Z"
        },
        {
          "author": "dai-ichiro",
          "body": "Thanks for your quick reply!\r\n\r\nBut I followed the manual faithfully when installation!\r\n\r\nI got bigdl-core-cpp==2.6.0b20250101 and ipex-llm==2.2.0b20250101.\r\n\r\nOllama does not work well with this combination. \r\n`./ollama serve` works fine. But when `./ollama run phi3` in another terminal, I got thi",
          "created_at": "2025-01-02T02:55:47Z"
        },
        {
          "author": "sgwhat",
          "body": "Please see the [ipex-llm ollama document](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.md), you need also execute `source /opt/intel/oneapi/setvars.sh` before running `./ollama run phi3`.",
          "created_at": "2025-01-02T03:16:27Z"
        },
        {
          "author": "dai-ichiro",
          "body": "Sorry. \r\n\r\nI missed the point you pointed out.\r\nI have to execute `source /opt/intel/oneapi/setvars.sh` in another terminal.\r\n\r\nEverything worked fine.\r\n\r\nThank you so much.",
          "created_at": "2025-01-02T03:34:19Z"
        }
      ]
    },
    {
      "issue_number": 12612,
      "title": "[LNL][llm-npu] Convert model failed",
      "body": "platform: LNL\r\nOS: win11\r\nNPU driver:  32.0.100.3104\r\n When convert model , it meets an error as below, but I check model file, it has model_type in config.json\r\n![image](https://github.com/user-attachments/assets/142ced5f-8e29-461e-b46c-93ebff8b07a7)\r\n\r\nconfig.json\r\n![image](https://github.com/user-attachments/assets/4b33474f-1dbb-47cb-833b-3819e39a587d)\r\n\r\npip list\r\n![image](https://github.com/user-attachments/assets/5cc3f047-c6be-4d73-b435-65b1a52abf9d)\r\n![image](https://github.com/user-attachments/assets/f2c2d5e8-e4c2-4b19-a8ca-4e4084615c34)\r\n\r\n\r\n\r\n",
      "state": "closed",
      "author": "johnysh",
      "author_type": "User",
      "created_at": "2024-12-25T08:50:12Z",
      "updated_at": "2025-03-10T11:32:50Z",
      "closed_at": "2024-12-27T02:08:40Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 14,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12612/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12612",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12612",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:11.462747",
      "comments": [
        {
          "author": "johnysh",
          "body": "PS :  model was downloaded from modelscope , not huggface.  ",
          "created_at": "2024-12-26T01:28:31Z"
        },
        {
          "author": "plusbang",
          "body": "Hi, @johnysh ,\r\n\r\nAs outlined in our documentation(https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/NPU/HF-Transformers-AutoModels/LLM#run), the argument `--save-directory` defines the path to save converted model.\r\n\r\nIn your case, you specifies `--save-directory` as an exist",
          "created_at": "2024-12-26T01:45:11Z"
        },
        {
          "author": "johnysh",
          "body": "Hi  @plusbang ,   as your suggestion, it can convert success , and output as sample. Further， I want to know how to use convert model?  I use \"python ./generate.py  --repo-id-or-path \"C:\\convert\"  \" , it alsp reports error  ,can you give me a sample cmd to  use it ?",
          "created_at": "2024-12-26T01:58:53Z"
        },
        {
          "author": "plusbang",
          "body": "> Hi @plusbang , as your suggestion, it can convert success , and output as sample. Further， I want to know how to use convert model? I use \"python ./generate.py --repo-id-or-path \"C:\\convert\" \" , it alsp reports error ,can you give me a sample cmd to use it ?\r\n\r\nIf you want to use the converted mod",
          "created_at": "2024-12-26T02:20:34Z"
        },
        {
          "author": "johnysh",
          "body": "Great,  it run successfully.\r\nThanks!\r\n\r\nFrom: binbin Deng ***@***.***>\r\nSent: Thursday, December 26, 2024 10:21 AM\r\nTo: intel-analytics/ipex-llm ***@***.***>\r\nCc: Shi, Junhan ***@***.***>; Mention ***@***.***>\r\nSubject: Re: [intel-analytics/ipex-llm] [LNL][llm-npu] Convert model failed (Issue #1261",
          "created_at": "2024-12-26T02:39:54Z"
        }
      ]
    },
    {
      "issue_number": 12601,
      "title": "RuntimeError: Build program log for 'Intel(R) Arc(TM) 140V GPU (16GB)': -11 (PI_ERROR_BUILD_PROGRAM_FAILURE)",
      "body": "Platform: LNL\r\nOS: Win11\r\nPip list : [lnl_piplist.txt](https://github.com/user-attachments/files/18236315/lnl_piplist.txt)\r\n\r\nWe upgraded ipex-llm using pip install --pre --upgrade ipex-llm[xpu_lnl] --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/lnl/cn/ to run Qwen1.5-1.8B models , but met runtime error as title. log as below:\r\n\r\n```\r\nC:\\Users\\zenbook\\.miniconda_dev_zone\\envs\\notebook-zone\\Lib\\site-packages\\transformers\\deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\r\n  warnings.warn(\r\nSYCL_ENABLE_DEFAULT_CONTEXTS=1\r\nSYCL_CACHE_PERSISTENT=1\r\nIPEX_LLM_LOW_MEM=1\r\n2024-12-24 09:17:26,969 - INFO - Converting the current model to sym_int4 format......\r\nC:\\Users\\zenbook\\.miniconda_dev_zone\\envs\\notebook-zone\\Lib\\site-packages\\torch\\nn\\init.py:452: UserWarning: Initializing zero-element tensors is a no-op\r\n  warnings.warn(\"Initializing zero-element tensors is a no-op\")\r\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\r\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\nCell In[1], line 39\r\n     37 input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to('xpu')\r\n     38 st = time.time()\r\n---> 39 output = model.generate(input_ids,\r\n     40                         max_new_tokens=n_predict)\r\n     41 torch.xpu.synchronize()\r\n     42 end = time.time()\r\n\r\nFile ~\\.miniconda_dev_zone\\envs\\notebook-zone\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115, in context_decorator.<locals>.decorate_context(*args, **kwargs)\r\n    112 @functools.wraps(func)\r\n    113 def decorate_context(*args, **kwargs):\r\n    114     with ctx_factory():\r\n--> 115         return func(*args, **kwargs)\r\n\r\nFile ~\\.miniconda_dev_zone\\envs\\notebook-zone\\Lib\\site-packages\\ipex_llm\\transformers\\lookup.py:125, in generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\r\n    113             logger.warning(\"Since you call the generate with lookahead parameter, \"\r\n    114                            f\"Speculative decoding parameters {spec_params} are \"\r\n    115                            \"removed in the generation.\")\r\n    116         return self.lookup_generate(inputs=inputs,\r\n    117                                     num_output_tokens=lookahead,\r\n    118                                     generation_config=generation_config,\r\n   (...)\r\n    122                                     prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\r\n    123                                     **kwargs)\r\n--> 125 return original_generate(self,\r\n    126                          inputs=inputs,\r\n    127                          generation_config=generation_config,\r\n    128                          logits_processor=logits_processor,\r\n    129                          stopping_criteria=stopping_criteria,\r\n    130                          prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\r\n    131                          synced_gpus=synced_gpus,\r\n    132                          assistant_model=assistant_model,\r\n    133                          streamer=streamer,\r\n    134                          **kwargs)\r\n\r\nFile ~\\.miniconda_dev_zone\\envs\\notebook-zone\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115, in context_decorator.<locals>.decorate_context(*args, **kwargs)\r\n    112 @functools.wraps(func)\r\n    113 def decorate_context(*args, **kwargs):\r\n    114     with ctx_factory():\r\n--> 115         return func(*args, **kwargs)\r\n\r\nFile ~\\.miniconda_dev_zone\\envs\\notebook-zone\\Lib\\site-packages\\ipex_llm\\transformers\\speculative.py:127, in generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\r\n    123 for var in ['max_step_draft', 'th_stop_draft', 'hf_adjust',\r\n    124             'auto_th_stop_draft', 'auto_parameters', 'min_step_draft',\r\n    125             'th_batch_num']:\r\n    126     kwargs.pop(var, None)\r\n--> 127 return original_generate(self,\r\n    128                          inputs=inputs,\r\n    129                          generation_config=generation_config,\r\n    130                          logits_processor=logits_processor,\r\n    131                          stopping_criteria=stopping_criteria,\r\n    132                          prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\r\n    133                          synced_gpus=synced_gpus,\r\n    134                          assistant_model=assistant_model,\r\n    135                          streamer=streamer,\r\n    136                          **kwargs)\r\n\r\nFile ~\\.miniconda_dev_zone\\envs\\notebook-zone\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115, in context_decorator.<locals>.decorate_context(*args, **kwargs)\r\n    112 @functools.wraps(func)\r\n    113 def decorate_context(*args, **kwargs):\r\n    114     with ctx_factory():\r\n--> 115         return func(*args, **kwargs)\r\n\r\nFile ~\\.miniconda_dev_zone\\envs\\notebook-zone\\Lib\\site-packages\\ipex_llm\\transformers\\pipeline_parallel.py:283, in generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\r\n    276         max_new_tokens = kwargs.pop(\"max_new_tokens\", None)\r\n    278     return self.pipeline_parallel_generate(inputs=inputs,\r\n    279                                            max_new_tokens=max_new_tokens,\r\n    280                                            generation_config=generation_config,\r\n    281                                            **kwargs)\r\n--> 283 return original_generate(self,\r\n    284                          inputs=inputs,\r\n    285                          generation_config=generation_config,\r\n    286                          logits_processor=logits_processor,\r\n    287                          stopping_criteria=stopping_criteria,\r\n    288                          prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\r\n    289                          synced_gpus=synced_gpus,\r\n    290                          assistant_model=assistant_model,\r\n    291                          streamer=streamer,\r\n    292                          negative_prompt_ids=negative_prompt_ids,\r\n    293                          negative_prompt_attention_mask=negative_prompt_attention_mask,\r\n    294                          **kwargs)\r\n\r\nFile ~\\.miniconda_dev_zone\\envs\\notebook-zone\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115, in context_decorator.<locals>.decorate_context(*args, **kwargs)\r\n    112 @functools.wraps(func)\r\n    113 def decorate_context(*args, **kwargs):\r\n    114     with ctx_factory():\r\n--> 115         return func(*args, **kwargs)\r\n\r\nFile ~\\.miniconda_dev_zone\\envs\\notebook-zone\\Lib\\site-packages\\transformers\\generation\\utils.py:1701, in GenerationMixin.generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\r\n   1698 batch_size = inputs_tensor.shape[0]\r\n   1700 device = inputs_tensor.device\r\n-> 1701 self._prepare_special_tokens(generation_config, kwargs_has_attention_mask, device=device)\r\n   1703 # decoder-only models must use left-padding for batched generation.\r\n   1704 if not self.config.is_encoder_decoder and not is_torchdynamo_compiling():\r\n   1705     # If `input_ids` was given, check if the last id in any sequence is `pad_token_id`\r\n   1706     # Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.\r\n\r\nFile ~\\.miniconda_dev_zone\\envs\\notebook-zone\\Lib\\site-packages\\transformers\\generation\\utils.py:1545, in GenerationMixin._prepare_special_tokens(self, generation_config, kwargs_has_attention_mask, device)\r\n   1542     logger.warning(f\"Setting `pad_token_id` to `eos_token_id`:{pad_token_tensor} for open-end generation.\")\r\n   1544 # we can't infer attn mask if pad token is set to be eos token in model's generation config\r\n-> 1545 if eos_token_tensor is not None and pad_token_tensor in eos_token_tensor:\r\n   1546     if kwargs_has_attention_mask is not None and not kwargs_has_attention_mask:\r\n   1547         logger.warning_once(\r\n   1548             \"The attention mask is not set and cannot be inferred from input because pad token is same as eos token.\"\r\n   1549             \"As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` \"\r\n   1550             \"to obtain reliable results.\"\r\n   1551         )\r\n\r\nFile ~\\.miniconda_dev_zone\\envs\\notebook-zone\\Lib\\site-packages\\torch\\_tensor.py:1116, in Tensor.__contains__(self, element)\r\n   1111     return handle_torch_function(Tensor.__contains__, (self,), self, element)\r\n   1112 if isinstance(\r\n   1113     element, (torch.Tensor, Number, torch.SymInt, torch.SymFloat, torch.SymBool)\r\n   1114 ):\r\n   1115     # type hint doesn't understand the __contains__ result array\r\n-> 1116     return (element == self).any().item()  # type: ignore[union-attr]\r\n   1118 raise RuntimeError(\r\n   1119     f\"Tensor.__contains__ only supports Tensor or scalar, but you passed in a {type(element)}.\"\r\n   1120 )\r\n\r\nRuntimeError: The program was built for 1 devices\r\nBuild program log for 'Intel(R) Arc(TM) 140V GPU (16GB)':\r\n-11 (PI_ERROR_BUILD_PROGRAM_FAILURE)\r\n ```",
      "state": "closed",
      "author": "johnysh",
      "author_type": "User",
      "created_at": "2024-12-24T05:55:42Z",
      "updated_at": "2025-03-10T11:32:49Z",
      "closed_at": "2024-12-27T02:07:34Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12601/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Oscilloscope98"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12601",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12601",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:11.667049",
      "comments": [
        {
          "author": "hkvision",
          "body": "Hi I suppose the error `\"Tensor.contains only supports Tensor or scalar, but you passed in a {type(element)}.\"` is an issue due to transformers versions. In your pip list, I find your transformers is 4.43.2, can you switch the default transformers version we support 4.37.0 to have another try?",
          "created_at": "2024-12-25T02:28:46Z"
        },
        {
          "author": "johnysh",
          "body": "Hi  , I tried transformers==4.37.0 , but still met the same error.\r\n\r\n***@***.***\r\n\r\nFrom: Kai Huang ***@***.***>\r\nSent: Wednesday, December 25, 2024 10:29 AM\r\nTo: intel-analytics/ipex-llm ***@***.***>\r\nCc: Shi, Junhan ***@***.***>; Author ***@***.***>\r\nSubject: Re: [intel-analytics/ipex-llm] Runtim",
          "created_at": "2024-12-25T06:39:38Z"
        },
        {
          "author": "Oscilloscope98",
          "body": "Hi @johnysh,\r\n\r\nWe did not reproduce this issue on our machine. Would you mind [upgrading your LNL iGPU driver to the latest](https://www.intel.com/content/www/us/en/download/785597/intel-arc-iris-xe-graphics-windows.html), and having a try again? :)",
          "created_at": "2024-12-26T06:18:51Z"
        },
        {
          "author": "johnysh",
          "body": "Thanks Yuwen， issue is fixed， pls close it",
          "created_at": "2024-12-26T08:30:28Z"
        }
      ]
    },
    {
      "issue_number": 12597,
      "title": "PI_ERROR_BUILD_PROGRAM_FAILURE error when running Ollama using ipex-llm on 12450H CPU",
      "body": "Hello,\r\n\r\nThe CPU is 12450H with driver version 32.0.101.6325.\r\nThe installed software is ipex-llm[cpp], and the Ollama version is 0.4.6.\r\n\r\nThe installation was successful, but an error occurred before inference while loading the model.\r\n\r\ntime=2024-12-23T23:18:56.511+08:00 level=INFO source=routes.go:1248 msg=\"Listening on 127.0.0.1:11434 (version 0.4.6-ipexllm-20241223)\"\r\ntime=2024-12-23T23:18:56.511+08:00 level=INFO source=common.go:49 msg=\"Dynamic LLM libraries\" runners=[ipex_llm]\r\n\r\n\r\ntime=2024-12-23T23:09:28.726+08:00 level=INFO source=server.go:619 msg=\"llama runner started in 3.77 seconds\"\r\nThe program was built for 1 devices\r\nBuild program log for 'Intel(R) UHD Graphics':\r\n -11 (PI_ERROR_BUILD_PROGRAM_FAILURE)Exception caught at file:D:/actions-runner/release-cpp-oneapi_2024_2/_work/llm.cpp/llm.cpp/llama-cpp-bigdl/ggml/src/ggml-sycl.cpp, line:3775\r\n\r\n\r\n![捕获](https://github.com/user-attachments/assets/05c91855-ddb0-45bf-8404-f36696eab522)\r\n",
      "state": "closed",
      "author": "qadzhang",
      "author_type": "User",
      "created_at": "2024-12-23T15:19:25Z",
      "updated_at": "2025-03-10T11:32:49Z",
      "closed_at": "2025-01-11T14:03:08Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 11,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12597/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "qiuxin2012",
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12597",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12597",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:11.862415",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "Which model are you using?",
          "created_at": "2024-12-24T06:44:06Z"
        },
        {
          "author": "qadzhang",
          "body": "use  qwen2.5:7b  \r\n\r\n\r\n@qiuxin2012 ",
          "created_at": "2024-12-24T08:46:09Z"
        },
        {
          "author": "qiuxin2012",
          "body": "similar issue: https://github.com/intel-analytics/ipex-llm/issues/12598, we are fixing it.",
          "created_at": "2024-12-26T02:01:25Z"
        },
        {
          "author": "qiuxin2012",
          "body": "@qadzhang You can try to update ipex-llm[cpp] to 2.2.0b20241226 tomorrow and try again.",
          "created_at": "2024-12-26T07:15:08Z"
        },
        {
          "author": "qadzhang",
          "body": "Thank you for your efforts.\r\n\r\nI upgraded the version and then tested qwen2.5:7b, qwen2.5:0.5b, qwen2:0.5b, bge-m3, and gemma2:9b.\r\n\r\nAmong them, qwen2:0.5b and gemma2:9b can run normally, while the other two report errors.\r\n--------------------\r\nWhen running qwen2.5:0.5b and qwen2.5:7b, the followi",
          "created_at": "2024-12-26T17:28:55Z"
        }
      ]
    },
    {
      "issue_number": 12595,
      "title": "main command, libmmd.dll / Svml_dispmd.dl / Sycl7.dll not found when try to run llama",
      "body": "My setup listed below, using windows 11 LNL device with iGPU.\r\n1. create virtual env named:llama-cpp-orig with python 3.11\r\n2. pip install --pre --upgrade ipex-llm[cpp]\r\n3. mkdir llama-cpp & cd llama-cpp\r\n4. use miniforge (installed by miniforge official website installer) with admin then run \"init-llama-cpp.bat\"\r\n5. set SYCL_CACHE_PERSISTENT=1\r\nrem under most circumstances, the following environment variable may improve performance, but sometimes this may also cause performance degradation\r\nset SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1\r\n\r\n\r\nFollow this link\r\nhttps://test-bigdl-llm.readthedocs.io/en/main/doc/LLM/Quickstart/llama3_llamacpp_ollama_quickstart.html\r\nRun llama3\r\nUnder your current directory, exceuting below command to do inference with Llama3:\r\n**main -m <model_dir>/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf -n 32 --prompt \"Once upon a time, there existed a little girl who liked to have adventures. She wanted to go to places and meet new people, and have fun doing something\" -e -ngl 33 --color --no-mmap**\r\n\r\noutput-\r\n(llama-cpp-orig) (base) C:\\Users\\Local_Admin\\Desktop\\llama-cpp>main -m C:\\Users\\Local_Admin\\Desktop\\models\\Meta-Llama-3-8B-Instruct-Q4_K_M.gguf -n 32 --prompt \"Once upon a time, there existed a little girl who liked to have adventures. She wanted to go to places and meet new people, and have fun doing something\" -e -ngl 33 --color --no-mmap\r\n_**'main' is not recognized as an internal or external command,\r\noperable program or batch file**_.\r\n\r\n\r\nFollow this link\r\nhttps://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/llama_cpp_quickstart.md\r\nTry to run **llama-cli -m model.gguf -n 32 --prompt \"Once upon a time, there existed a little girl who liked to have adventures. She wanted to go to places and meet new people, and have fun\" -c 1024 -t 8 -e -ngl 99 --color**\r\n\r\noutput\r\n![image](https://github.com/user-attachments/assets/9187bb82-c319-4c73-8002-b293857cb3ac)\r\nmissing below dlls..\r\n![image](https://github.com/user-attachments/assets/cd7a731b-62d3-42d0-98aa-0fcb170f34cf)\r\n",
      "state": "closed",
      "author": "georgiaWW",
      "author_type": "User",
      "created_at": "2024-12-23T06:18:18Z",
      "updated_at": "2025-03-10T11:32:48Z",
      "closed_at": "2024-12-24T03:15:57Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12595/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12595",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12595",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:12.129913",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "Could you show me your `pip list`?",
          "created_at": "2024-12-23T07:15:47Z"
        },
        {
          "author": "georgiaWW",
          "body": "> Could you show me your `pip list`?\r\n\r\n(llama-cpp-orig) (base) C:\\Users\\Local_Admin\\Desktop\\llama-cpp>pip list\r\nPackage                 Version\r\n----------------------- --------------\r\naccelerate              0.33.0\r\nbigdl-core-cpp          2.6.0b20241219\r\ncertifi                 2024.12.14\r\ncharse",
          "created_at": "2024-12-23T07:17:45Z"
        },
        {
          "author": "qiuxin2012",
          "body": "Your `pip list` looks fine. But please check your python visual env. `(llama-cpp-orig) (base) ` shows you may have two different conda managers?\r\nHere is my result, I have only one `(xin-cpp)`\r\n![image](https://github.com/user-attachments/assets/9b97c2b6-cc17-4eef-bcee-b6bab538ad79)\r\nIf I dedactivat",
          "created_at": "2024-12-23T07:31:32Z"
        },
        {
          "author": "georgiaWW",
          "body": "> Your `pip list` looks fine. But please check your python visual env. `(llama-cpp-orig) (base) ` shows you may have two different conda managers? Here is my result, I have only one `(xin-cpp)` ![image](https://private-user-images.githubusercontent.com/4495653/398084531-9b97c2b6-cc17-4eef-bcee-b6bab",
          "created_at": "2024-12-23T07:47:35Z"
        },
        {
          "author": "jason-dai",
          "body": "You may use Miniforge instead of conda: https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/install_windows_gpu.md#setup-python-environment",
          "created_at": "2024-12-23T07:54:15Z"
        }
      ]
    },
    {
      "issue_number": 12593,
      "title": "Exception 0xc0000005 0x0 0x40 0x7ff86e2a27cf after simple request",
      "body": "This query leads to an exception:\r\n\r\n```\r\ncurl http://localhost:11434/api/generate -d '{\r\n  \"model\": \"llama3\",\r\n  \"prompt\": \"Why is the sky blue?\"\r\n}'\r\n{\"error\":\"POST predict: Post \\\"http://127.0.0.1:56983/completion\\\": read tcp 127.0.0.1:56985-\\u003e127.0.0.1:56983: wsarecv: An existing connection was forcibly closed by the remote host.\"}\r\n```\r\n\r\nLog:\r\n```\r\ntime=2024-12-22T19:51:14.534+01:00 level=INFO source=server.go:105 msg=\"system memory\" total=\"31.6 GiB\" free=\"17.5 GiB\" free_swap=\"16.3 GiB\"\r\ntime=2024-12-22T19:51:14.535+01:00 level=INFO source=memory.go:356 msg=\"offload to device\" layers.requested=-1 layers.model=33 layers.offload=0 layers.split=\"\" memory.available=\"[17.5 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"4.7 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"256.0 MiB\" memory.required.allocations=\"[4.7 GiB]\" memory.weights.total=\"3.9 GiB\" memory.weights.repeating=\"3.5 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"258.5 MiB\" memory.graph.partial=\"677.5 MiB\"\r\ntime=2024-12-22T19:51:14.538+01:00 level=INFO source=server.go:401 msg=\"starting llama server\" cmd=\"D:\\\\devtools\\\\llama-cpp\\\\dist\\\\windows-amd64\\\\lib\\\\ollama\\\\runners\\\\ipex_llm\\\\ollama_llama_server.exe --model C:\\\\Users\\\\Bastian\\\\.ollama\\\\models\\\\blobs\\\\sha256-00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29 --ctx-size 2048 --batch-size 512 --n-gpu-layers 999 --threads 6 --no-mmap --parallel 1 --port 56983\"\r\ntime=2024-12-22T19:51:14.543+01:00 level=INFO source=sched.go:449 msg=\"loaded runners\" count=2\r\ntime=2024-12-22T19:51:14.543+01:00 level=INFO source=server.go:580 msg=\"waiting for llama runner to start responding\"\r\ntime=2024-12-22T19:51:14.543+01:00 level=INFO source=server.go:614 msg=\"waiting for server to become available\" status=\"llm server error\"\r\ntime=2024-12-22T19:51:14.579+01:00 level=INFO source=runner.go:941 msg=\"starting go runner\"\r\ntime=2024-12-22T19:51:14.593+01:00 level=INFO source=runner.go:942 msg=system info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | cgo(clang)\" threads=6\r\ntime=2024-12-22T19:51:14.594+01:00 level=INFO source=.:0 msg=\"Server listening on 127.0.0.1:56983\"\r\nllama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from C:\\Users\\Bastian\\.ollama\\models\\blobs\\sha256-00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29 (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct\r\nllama_model_loader: - kv   2:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   3:                       llama.context_length u32              = 8192\r\nllama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 2\r\nllama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\r\nllama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\r\nllama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000\r\nllama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001\r\nllama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\r\nllama_model_loader: - kv  20:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q4_0:  225 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\ntime=2024-12-22T19:51:14.795+01:00 level=INFO source=server.go:614 msg=\"waiting for server to become available\" status=\"llm server loading model\"\r\nllm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'\r\nllm_load_vocab: control-looking token: '<|eot_id|>' was not control-type; this is probably a bug in the model. its type will be overridden\r\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\r\nllm_load_vocab: special tokens cache size = 256\r\nllm_load_vocab: token to piece cache size = 0.8000 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 128256\r\nllm_load_print_meta: n_merges         = 280147\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 8192\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 500000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 8192\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = 8B\r\nllm_load_print_meta: model ftype      = Q4_0\r\nllm_load_print_meta: model params     = 8.03 B\r\nllm_load_print_meta: model size       = 4.33 GiB (4.64 BPW)\r\nllm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct\r\nllm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\r\nllm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\r\nllm_load_print_meta: LF token         = 128 'Ä'\r\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: EOG token        = 128001 '<|end_of_text|>'\r\nllm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: max token length = 256\r\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nggml_sycl_init: SYCL_USE_XMX: yes\r\nggml_sycl_init: found 2 SYCL devices:\r\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\r\nllm_load_tensors: ggml ctx size =    0.41 MiB\r\nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 33/33 layers to GPU\r\nllm_load_tensors:      SYCL0 buffer size =  1872.50 MiB\r\nllm_load_tensors:      SYCL1 buffer size =  2283.49 MiB\r\nllm_load_tensors:  SYCL_Host buffer size =   281.81 MiB\r\nllama_new_context_with_model: n_ctx      = 2048\r\nllama_new_context_with_model: n_batch    = 512\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 500000.0\r\nllama_new_context_with_model: freq_scale = 1\r\n[SYCL] call ggml_check_sycl\r\nggml_check_sycl: GGML_SYCL_DEBUG: 0\r\nggml_check_sycl: GGML_SYCL_F16: no\r\nfound 2 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|                 Intel Iris Xe Graphics|    1.6|     96|     512|   32| 15485M|            1.3.31441|\r\n| 1| [level_zero:gpu:1]|               Intel Arc A770M Graphics|    1.6|    512|    1024|   32| 16704M|            1.3.31441|\r\nllama_kv_cache_init:      SYCL0 KV buffer size =   128.00 MiB\r\nllama_kv_cache_init:      SYCL1 KV buffer size =   128.00 MiB\r\nllama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\r\nllama_new_context_with_model:  SYCL_Host  output buffer size =     0.50 MiB\r\nllama_new_context_with_model:      SYCL0 compute buffer size =    84.00 MiB\r\nllama_new_context_with_model:      SYCL1 compute buffer size =   258.50 MiB\r\nllama_new_context_with_model:  SYCL_Host compute buffer size =    12.01 MiB\r\nllama_new_context_with_model: graph nodes  = 902\r\nllama_new_context_with_model: graph splits = 3\r\ntime=2024-12-22T19:51:21.806+01:00 level=INFO source=server.go:619 msg=\"llama runner started in 7.26 seconds\"\r\nException 0xc0000005 0x0 0x40 0x7ff86e2a27cf\r\nPC=0x7ff86e2a27cf\r\nsignal arrived during external code execution\r\n\r\nruntime.cgocall(0x7ff6f1eb0850, 0xc0001a7b50)\r\n        runtime/cgocall.go:167 +0x3e fp=0xc0001a7b28 sp=0xc0001a7ac0 pc=0x7ff6f1c65c1e\r\nollama/llama/llamafile._Cfunc_llama_decode(0x299fee7edf0, {0x10, 0x299f41bf7c0, 0x0, 0x0, 0x299f41de080, 0x299ae0be050, 0x299fe705a40, 0x299f9bde8a0, 0x0, ...})\r\n        _cgo_gotypes.go:540 +0x58 fp=0xc0001a7b50 sp=0xc0001a7b28 pc=0x7ff6f1d12618\r\nollama/llama/llamafile.(*Context).Decode.func1(0x7ff6f1eac28b?, 0x299fee7edf0?)\r\n        ollama/llama/llamafile/llama.go:121 +0xd8 fp=0xc0001a7c70 sp=0xc0001a7b50 pc=0x7ff6f1d14f58\r\nollama/llama/llamafile.(*Context).Decode(0xc0001a7d60?, 0x0?)\r\n        ollama/llama/llamafile/llama.go:121 +0x13 fp=0xc0001a7cb8 sp=0xc0001a7c70 pc=0x7ff6f1d14df3\r\nmain.(*Server).processBatch(0xc0001541b0, 0xc00023c000, 0xc0001a7f10)\r\n        ollama/llama/runner/runner.go:434 +0x23f fp=0xc0001a7ed0 sp=0xc0001a7cb8 pc=0x7ff6f1eaaf5f\r\nmain.(*Server).run(0xc0001541b0, {0x7ff6f1fe50a8, 0xc00019e050})\r\n        ollama/llama/runner/runner.go:342 +0x1d5 fp=0xc0001a7fb8 sp=0xc0001a7ed0 pc=0x7ff6f1eaa9d5\r\nmain.main.gowrap2()\r\n        ollama/llama/runner/runner.go:980 +0x28 fp=0xc0001a7fe0 sp=0xc0001a7fb8 pc=0x7ff6f1eaf808\r\nruntime.goexit({})\r\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc0001a7fe8 sp=0xc0001a7fe0 pc=0x7ff6f1c73a01\r\ncreated by main.main in goroutine 1\r\n        ollama/llama/runner/runner.go:980 +0xd3e\r\n\r\ngoroutine 1 gp=0xc000076000 m=nil [IO wait]:\r\nruntime.gopark(0xc00016b708?, 0x7ff6f21cc4a0?, 0x20?, 0x65?, 0xc0001265cc?)\r\n        runtime/proc.go:424 +0xce fp=0xc00016b6f0 sp=0xc00016b6d0 pc=0x7ff6f1c6ba6e\r\nruntime.netpollblock(0x28c?, 0xf1c05366?, 0xf6?)\r\n        runtime/netpoll.go:575 +0xf7 fp=0xc00016b728 sp=0xc00016b6f0 pc=0x7ff6f1c314d7\r\ninternal/poll.runtime_pollWait(0x299f38affa8, 0x72)\r\n        runtime/netpoll.go:351 +0x85 fp=0xc00016b748 sp=0xc00016b728 pc=0x7ff6f1c6ace5\r\ninternal/poll.(*pollDesc).wait(0x7ff6f1cc4975?, 0x7ff6f1c66cbd?, 0x0)\r\n        internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc00016b770 sp=0xc00016b748 pc=0x7ff6f1cc5bc7\r\ninternal/poll.execIO(0xc000126520, 0xc00016b818)\r\n        internal/poll/fd_windows.go:177 +0x105 fp=0xc00016b7e8 sp=0xc00016b770 pc=0x7ff6f1cc6465\r\ninternal/poll.(*FD).acceptOne(0xc000126508, 0x2a8, {0xc0001ac0f0?, 0xc00016b878?, 0x7ff6f1cc9265?}, 0xc00016b8ac?)\r\n        internal/poll/fd_windows.go:946 +0x65 fp=0xc00016b848 sp=0xc00016b7e8 pc=0x7ff6f1cc8485\r\ninternal/poll.(*FD).Accept(0xc000126508, 0xc00016b9f8)\r\n        internal/poll/fd_windows.go:980 +0x1b6 fp=0xc00016b900 sp=0xc00016b848 pc=0x7ff6f1cc87b6\r\nnet.(*netFD).accept(0xc000126508)\r\n        net/fd_windows.go:182 +0x4b fp=0xc00016ba18 sp=0xc00016b900 pc=0x7ff6f1d3b3ab\r\nnet.(*TCPListener).accept(0xc0001247c0)\r\n        net/tcpsock_posix.go:159 +0x1e fp=0xc00016ba68 sp=0xc00016ba18 pc=0x7ff6f1d4c5be\r\nnet.(*TCPListener).Accept(0xc0001247c0)\r\n        net/tcpsock.go:372 +0x30 fp=0xc00016ba98 sp=0xc00016ba68 pc=0x7ff6f1d4b7d0\r\nnet/http.(*onceCloseListener).Accept(0xc000154240?)\r\n        <autogenerated>:1 +0x24 fp=0xc00016bab0 sp=0xc00016ba98 pc=0x7ff6f1e88024\r\nnet/http.(*Server).Serve(0xc00019a4b0, {0x7ff6f1fe4b30, 0xc0001247c0})\r\n        net/http/server.go:3330 +0x30c fp=0xc00016bbe0 sp=0xc00016bab0 pc=0x7ff6f1e79d6c\r\nmain.main()\r\n        ollama/llama/runner/runner.go:1000 +0x10ae fp=0xc00016bf50 sp=0xc00016bbe0 pc=0x7ff6f1eaf58e\r\nruntime.main()\r\n        runtime/proc.go:272 +0x27d fp=0xc00016bfe0 sp=0xc00016bf50 pc=0x7ff6f1c3a41d\r\nruntime.goexit({})\r\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00016bfe8 sp=0xc00016bfe0 pc=0x7ff6f1c73a01\r\n\r\ngoroutine 2 gp=0xc000076700 m=nil [force gc (idle)]:\r\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\r\n        runtime/proc.go:424 +0xce fp=0xc000079fa8 sp=0xc000079f88 pc=0x7ff6f1c6ba6e\r\nruntime.goparkunlock(...)\r\n        runtime/proc.go:430\r\nruntime.forcegchelper()\r\n        runtime/proc.go:337 +0xb8 fp=0xc000079fe0 sp=0xc000079fa8 pc=0x7ff6f1c3a738\r\nruntime.goexit({})\r\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000079fe8 sp=0xc000079fe0 pc=0x7ff6f1c73a01\r\ncreated by runtime.init.7 in goroutine 1\r\n        runtime/proc.go:325 +0x1a\r\n\r\ngoroutine 3 gp=0xc000076a80 m=nil [GC sweep wait]:\r\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\r\n        runtime/proc.go:424 +0xce fp=0xc00007bf80 sp=0xc00007bf60 pc=0x7ff6f1c6ba6e\r\nruntime.goparkunlock(...)\r\n        runtime/proc.go:430\r\nruntime.bgsweep(0xc000088000)\r\n        runtime/mgcsweep.go:277 +0x94 fp=0xc00007bfc8 sp=0xc00007bf80 pc=0x7ff6f1c236f4\r\nruntime.gcenable.gowrap1()\r\n        runtime/mgc.go:203 +0x25 fp=0xc00007bfe0 sp=0xc00007bfc8 pc=0x7ff6f1c17fc5\r\nruntime.goexit({})\r\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00007bfe8 sp=0xc00007bfe0 pc=0x7ff6f1c73a01\r\ncreated by runtime.gcenable in goroutine 1\r\n        runtime/mgc.go:203 +0x66\r\n\r\ngoroutine 4 gp=0xc000076c40 m=nil [GC scavenge wait]:\r\nruntime.gopark(0xc000088000?, 0x7ff6f1fdea60?, 0x1?, 0x0?, 0xc000076c40?)\r\n        runtime/proc.go:424 +0xce fp=0xc00008ff78 sp=0xc00008ff58 pc=0x7ff6f1c6ba6e\r\nruntime.goparkunlock(...)\r\n        runtime/proc.go:430\r\nruntime.(*scavengerState).park(0x7ff6f21d8f80)\r\n        runtime/mgcscavenge.go:425 +0x49 fp=0xc00008ffa8 sp=0xc00008ff78 pc=0x7ff6f1c21129\r\nruntime.bgscavenge(0xc000088000)\r\n        runtime/mgcscavenge.go:653 +0x3c fp=0xc00008ffc8 sp=0xc00008ffa8 pc=0x7ff6f1c2169c\r\nruntime.gcenable.gowrap2()\r\n        runtime/mgc.go:204 +0x25 fp=0xc00008ffe0 sp=0xc00008ffc8 pc=0x7ff6f1c17f65\r\nruntime.goexit({})\r\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00008ffe8 sp=0xc00008ffe0 pc=0x7ff6f1c73a01\r\ncreated by runtime.gcenable in goroutine 1\r\n        runtime/mgc.go:204 +0xa5\r\n\r\ngoroutine 18 gp=0xc000106380 m=nil [finalizer wait]:\r\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\r\n        runtime/proc.go:424 +0xce fp=0xc00008be20 sp=0xc00008be00 pc=0x7ff6f1c6ba6e\r\nruntime.runfinq()\r\n        runtime/mfinal.go:193 +0x107 fp=0xc00008bfe0 sp=0xc00008be20 pc=0x7ff6f1c17087\r\nruntime.goexit({})\r\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00008bfe8 sp=0xc00008bfe0 pc=0x7ff6f1c73a01\r\ncreated by runtime.createfing in goroutine 1\r\n        runtime/mfinal.go:163 +0x3d\r\n\r\ngoroutine 19 gp=0xc000106540 m=nil [chan receive]:\r\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\r\n        runtime/proc.go:424 +0xce fp=0xc00008df18 sp=0xc00008def8 pc=0x7ff6f1c6ba6e\r\nruntime.chanrecv(0xc000108150, 0x0, 0x1)\r\n        runtime/chan.go:639 +0x41e fp=0xc00008df90 sp=0xc00008df18 pc=0x7ff6f1c07c7e\r\nruntime.chanrecv1(0x0?, 0x0?)\r\n        runtime/chan.go:489 +0x12 fp=0xc00008dfb8 sp=0xc00008df90 pc=0x7ff6f1c07852\r\nruntime.unique_runtime_registerUniqueMapCleanup.func1(...)\r\n        runtime/mgc.go:1732\r\nruntime.unique_runtime_registerUniqueMapCleanup.gowrap1()\r\n        runtime/mgc.go:1735 +0x2f fp=0xc00008dfe0 sp=0xc00008dfb8 pc=0x7ff6f1c1ae6f\r\nruntime.goexit({})\r\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00008dfe8 sp=0xc00008dfe0 pc=0x7ff6f1c73a01\r\ncreated by unique.runtime_registerUniqueMapCleanup in goroutine 1\r\n        runtime/mgc.go:1730 +0x96\r\n\r\ngoroutine 47 gp=0xc000106700 m=nil [IO wait]:\r\nruntime.gopark(0x0?, 0xc0001267a0?, 0x48?, 0x68?, 0xc00012684c?)\r\n        runtime/proc.go:424 +0xce fp=0xc00007dd20 sp=0xc00007dd00 pc=0x7ff6f1c6ba6e\r\nruntime.netpollblock(0x298?, 0xf1c05366?, 0xf6?)\r\n        runtime/netpoll.go:575 +0xf7 fp=0xc00007dd58 sp=0xc00007dd20 pc=0x7ff6f1c314d7\r\ninternal/poll.runtime_pollWait(0x299f38afea0, 0x72)\r\n        runtime/netpoll.go:351 +0x85 fp=0xc00007dd78 sp=0xc00007dd58 pc=0x7ff6f1c6ace5\r\ninternal/poll.(*pollDesc).wait(0xc00007ddd8?, 0x7ff6f1c12a65?, 0x0)\r\n        internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc00007dda0 sp=0xc00007dd78 pc=0x7ff6f1cc5bc7\r\ninternal/poll.execIO(0xc0001267a0, 0x7ff6f1f8d888)\r\n        internal/poll/fd_windows.go:177 +0x105 fp=0xc00007de18 sp=0xc00007dda0 pc=0x7ff6f1cc6465\r\ninternal/poll.(*FD).Read(0xc000126788, {0xc000202041, 0x1, 0x1})\r\n        internal/poll/fd_windows.go:438 +0x2a7 fp=0xc00007dec0 sp=0xc00007de18 pc=0x7ff6f1cc7167\r\nnet.(*netFD).Read(0xc000126788, {0xc000202041?, 0xc00007df48?, 0x7ff6f1c6cf10?})\r\n        net/fd_posix.go:55 +0x25 fp=0xc00007df08 sp=0xc00007dec0 pc=0x7ff6f1d3a2c5\r\nnet.(*conn).Read(0xc0001140e8, {0xc000202041?, 0x0?, 0x7ff6f221e980?})\r\n        net/net.go:189 +0x45 fp=0xc00007df50 sp=0xc00007df08 pc=0x7ff6f1d45785\r\nnet.(*TCPConn).Read(0x7ff6f21c7950?, {0xc000202041?, 0x0?, 0x0?})\r\n        <autogenerated>:1 +0x25 fp=0xc00007df80 sp=0xc00007df50 pc=0x7ff6f1d508a5\r\nnet/http.(*connReader).backgroundRead(0xc000202030)\r\n        net/http/server.go:690 +0x37 fp=0xc00007dfc8 sp=0xc00007df80 pc=0x7ff6f1e701d7\r\nnet/http.(*connReader).startBackgroundRead.gowrap2()\r\n        net/http/server.go:686 +0x25 fp=0xc00007dfe0 sp=0xc00007dfc8 pc=0x7ff6f1e70105\r\nruntime.goexit({})\r\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00007dfe8 sp=0xc00007dfe0 pc=0x7ff6f1c73a01\r\ncreated by net/http.(*connReader).startBackgroundRead in goroutine 22\r\n        net/http/server.go:686 +0xb6\r\n\r\ngoroutine 22 gp=0xc000106c40 m=nil [select]:\r\nruntime.gopark(0xc000035a68?, 0x2?, 0xee?, 0x70?, 0xc000035834?)\r\n        runtime/proc.go:424 +0xce fp=0xc0000356a0 sp=0xc000035680 pc=0x7ff6f1c6ba6e\r\nruntime.selectgo(0xc000035a68, 0xc000035830, 0x10?, 0x0, 0x1?, 0x1)\r\n        runtime/select.go:335 +0x7a5 fp=0xc0000357c8 sp=0xc0000356a0 pc=0x7ff6f1c4b225\r\nmain.(*Server).completion(0xc0001541b0, {0x7ff6f1fe4c80, 0xc000226b60}, 0xc000211040)\r\n        ollama/llama/runner/runner.go:698 +0xa86 fp=0xc000035ac0 sp=0xc0000357c8 pc=0x7ff6f1eacda6\r\nmain.(*Server).completion-fm({0x7ff6f1fe4c80?, 0xc000226b60?}, 0x7ff6f1e7e067?)\r\n        <autogenerated>:1 +0x36 fp=0xc000035af0 sp=0xc000035ac0 pc=0x7ff6f1eb0036\r\nnet/http.HandlerFunc.ServeHTTP(0xc00017a0e0?, {0x7ff6f1fe4c80?, 0xc000226b60?}, 0x67685fa9?)\r\n        net/http/server.go:2220 +0x29 fp=0xc000035b18 sp=0xc000035af0 pc=0x7ff6f1e76c29\r\nnet/http.(*ServeMux).ServeHTTP(0x7ff6f1c0e565?, {0x7ff6f1fe4c80, 0xc000226b60}, 0xc000211040)\r\n        net/http/server.go:2747 +0x1ca fp=0xc000035b68 sp=0xc000035b18 pc=0x7ff6f1e78aca\r\nnet/http.serverHandler.ServeHTTP({0x7ff6f1fe3d28?}, {0x7ff6f1fe4c80?, 0xc000226b60?}, 0x6?)\r\n        net/http/server.go:3210 +0x8e fp=0xc000035b98 sp=0xc000035b68 pc=0x7ff6f1e7f9ce\r\nnet/http.(*conn).serve(0xc000154240, {0x7ff6f1fe5070, 0xc00010b140})\r\n        net/http/server.go:2092 +0x5d0 fp=0xc000035fb8 sp=0xc000035b98 pc=0x7ff6f1e75850\r\nnet/http.(*Server).Serve.gowrap3()\r\n        net/http/server.go:3360 +0x28 fp=0xc000035fe0 sp=0xc000035fb8 pc=0x7ff6f1e7a168\r\nruntime.goexit({})\r\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000035fe8 sp=0xc000035fe0 pc=0x7ff6f1c73a01\r\ncreated by net/http.(*Server).Serve in goroutine 1\r\n        net/http/server.go:3360 +0x485\r\nrax     0x63c\r\nrbx     0x40\r\nrcx     0x40\r\nrdx     0x0\r\nrdi     0x0\r\nrsi     0x0\r\nrbp     0x9f538fcc40\r\nrsp     0x9f538fcb90\r\nr8      0x299fe841d10\r\nr9      0x299f5eb79c8\r\nr10     0x0\r\nr11     0x9f538fcda0\r\nr12     0x0\r\nr13     0x9f538fccc0\r\nr14     0x9f538fd0c0\r\nr15     0x299f409a030\r\nrip     0x7ff86e2a27cf\r\nrflags  0x10206\r\ncs      0x33\r\nfs      0x53\r\ngs      0x2b\r\n[GIN] 2024/12/22 - 19:51:22 | 200 |    8.3196671s |       127.0.0.1 | POST     \"/api/generate\"\r\n```\r\n\r\nI am using a fresh install on Win11 with a Intel Arc A770.",
      "state": "closed",
      "author": "cherub-i",
      "author_type": "User",
      "created_at": "2024-12-22T18:58:09Z",
      "updated_at": "2025-03-10T11:32:48Z",
      "closed_at": "2024-12-28T23:21:16Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12593/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12593",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12593",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:12.328906",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @cherub-i , \r\nThis error occurs because ollama is running on Intel Iris Xe Graphics. \r\nPlease set `ONEAPI_DEVICE_SELECTOR=level_zero:1` (before you starting `ollama serve`) to specify that ollama should run on the A770.",
          "created_at": "2024-12-23T01:38:21Z"
        },
        {
          "author": "cherub-i",
          "body": "Thank you very much for the quick reply!\r\nI only could try it out today. I am on the correct graphics card now, but I still get an exception:\r\n\r\nLog \r\n```text\r\n2024/12/26 18:18:25 routes.go:1197: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_",
          "created_at": "2024-12-26T17:25:47Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @cherub-i,\r\n\r\nI cannot reproduce your issue, the ipex-llm ollama is running normally in my windows arc770 desktop. You may check as follows:\r\n\r\n1. Check your environment. We recommend creating a new conda environment to **only** install the latest version of `ipex-llm[cpp]` via `pip install --pre",
          "created_at": "2024-12-27T16:49:39Z"
        },
        {
          "author": "cherub-i",
          "body": "Hi @sgwhat,\r\nI just pulled llama3 again, as you described in 2. - and it fixed the problem!\r\nI am really happy, to have LLM now running on my ARC.\r\nThank you so much for your kind help!",
          "created_at": "2024-12-28T23:20:58Z"
        },
        {
          "author": "cherub-i",
          "body": "Using `ollama pull llama3` to get the model fixed my problem.",
          "created_at": "2024-12-28T23:21:48Z"
        }
      ]
    },
    {
      "issue_number": 12571,
      "title": "Question:  What's the recommended way to finetune newish models?",
      "body": "I want to finetune Qwen 2.5 3B with Intel GPU.\r\n\r\nCan't do it with the IPEX axolotl.\r\n\r\nWhat's currently the best way to do this?",
      "state": "closed",
      "author": "electroglyph",
      "author_type": "User",
      "created_at": "2024-12-18T06:53:10Z",
      "updated_at": "2025-03-10T11:32:47Z",
      "closed_at": "2025-01-06T05:12:25Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12571/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "qiyuangong"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12571",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12571",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:12.532599",
      "comments": [
        {
          "author": "qiyuangong",
          "body": "> I want to finetune Qwen 2.5 3B with Intel GPU.\r\n> \r\n> Can't do it with the IPEX axolotl.\r\n> \r\n> What's currently the best way to do this?\r\n\r\nIt seems axolotl begins to support Qwen 2 after [V0.5.0](https://github.com/axolotl-ai-cloud/axolotl/tree/v0.5.0/examples).  However, ipex-llm only supports ",
          "created_at": "2024-12-19T01:21:09Z"
        },
        {
          "author": "electroglyph",
          "body": "it doesn't have to be axolotl, i want to know what will work right now.\r\n\r\nwill torchtune work? or transformers trainer with newish transformers version?",
          "created_at": "2024-12-19T12:18:42Z"
        },
        {
          "author": "qiyuangong",
          "body": "> it doesn't have to be axolotl, i want to know what will work right now.\r\n> \r\n> will torchtune work? or transformers trainer with newish transformers version?\r\n\r\nTorchtune is not supported yet. Also transformers trainer is not recommended. You can use Peft for finetuning.\r\n\r\nYou can check [Running ",
          "created_at": "2024-12-19T13:37:02Z"
        },
        {
          "author": "electroglyph",
          "body": "sorry for late response, closing this now.  Thanks @qiyuangong for the advice, i didn't know that folder was in the repo",
          "created_at": "2025-01-06T05:12:25Z"
        }
      ]
    },
    {
      "issue_number": 12568,
      "title": "Megrez-3B-Omni模型支持",
      "body": "模型支持报错：\r\nhttps://huggingface.co/Infinigence/Megrez-3B-Omni\r\n\r\nTransformers： 4.45.0 LNL platform\r\nC:\\Users\\test\\miniforge3\\envs\\ipex-llm\\lib\\site-packages\\transformers\\models\\auto\\image_processing_auto.py:517: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\r\n  warnings.warn(\r\nTraceback (most recent call last):\r\n  File \"D:\\Mergrez-3B-Omni\\generate.py\", line 42, in <module>\r\n    response = model.chat(\r\n  File \"C:\\Users\\test\\miniforge3\\envs\\ipex-llm\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\test\\.cache\\huggingface\\modules\\transformers_modules\\Megrez-3B-Omni\\modeling_megrezo.py\", line 330, in chat\r\n    output_ids = self.generate(**data, **generation_config)\r\n  File \"C:\\Users\\test\\.cache\\huggingface\\modules\\transformers_modules\\Megrez-3B-Omni\\modeling_megrezo.py\", line 288, in generate\r\n    input_ids, input_embeds, position_ids = self.compose_embeddings(data)\r\n  File \"C:\\Users\\test\\.cache\\huggingface\\modules\\transformers_modules\\Megrez-3B-Omni\\modeling_megrezo.py\", line 230, in compose_embeddings\r\n    embeddings_image = self.vision(pixel_values, tgt_sizes, patch_attention_mask=patch_attention_mask)\r\n  File \"C:\\Users\\test\\miniforge3\\envs\\ipex-llm\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"C:\\Users\\test\\miniforge3\\envs\\ipex-llm\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"C:\\Users\\test\\.cache\\huggingface\\modules\\transformers_modules\\Megrez-3B-Omni\\modeling_megrezo.py\", line 173, in forward\r\n    embedding = self.resampler(embedding, tgt_sizes)\r\n  File \"C:\\Users\\test\\miniforge3\\envs\\ipex-llm\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"C:\\Users\\test\\miniforge3\\envs\\ipex-llm\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"C:\\Users\\test\\.cache\\huggingface\\modules\\transformers_modules\\Megrez-3B-Omni\\resampler.py\", line 174, in forward\r\n    out = self.attn(\r\n  File \"C:\\Users\\test\\miniforge3\\envs\\ipex-llm\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"C:\\Users\\test\\miniforge3\\envs\\ipex-llm\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"C:\\Users\\test\\miniforge3\\envs\\ipex-llm\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1266, in forward\r\n    attn_output, attn_output_weights = F.multi_head_attention_forward(\r\n  File \"C:\\Users\\test\\miniforge3\\envs\\ipex-llm\\lib\\site-packages\\torch\\nn\\functional.py\", line 5477, in multi_head_attention_forward\r\n    attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\r\nRuntimeError: self and mat2 must have the same dtype, but got Half and Byte\r\n",
      "state": "closed",
      "author": "juan-OY",
      "author_type": "User",
      "created_at": "2024-12-18T06:03:22Z",
      "updated_at": "2025-03-10T11:32:47Z",
      "closed_at": "2024-12-20T09:29:09Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12568/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "MeouSker77"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12568",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12568",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:12.753134",
      "comments": [
        {
          "author": "MeouSker77",
          "body": "supported in <https://github.com/intel-analytics/ipex-llm/pull/12582>, try latest ipex-llm: `pip install --pre --upgrade ipex-llm`",
          "created_at": "2024-12-20T01:43:42Z"
        },
        {
          "author": "juan-OY",
          "body": "问题已经解决，thanks",
          "created_at": "2024-12-20T09:29:09Z"
        }
      ]
    },
    {
      "issue_number": 12557,
      "title": "Can I select which GPU to run the LLM workload on ipex-llm[cpp]",
      "body": "Assume I have a ARL-S + Intel B580, then there are both Intel iGpu and dGpu on my PC. Is there any API or setting so that I can choose which gpu to run the LLM workload?",
      "state": "closed",
      "author": "jianjungu",
      "author_type": "User",
      "created_at": "2024-12-17T02:02:28Z",
      "updated_at": "2025-03-10T11:32:46Z",
      "closed_at": "2024-12-20T01:39:46Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12557/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12557",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12557",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:13.044285",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "You can use ONEAPI_DEVICE_SELECTOR, see https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Overview/KeyFeatures/multi_gpus_selection.md#2-oneapi-device-selector.",
          "created_at": "2024-12-17T02:11:24Z"
        },
        {
          "author": "jianjungu",
          "body": "can I switch the gpu per request? for example, this request to use iGPU and next request to use dGPU?",
          "created_at": "2024-12-17T04:12:54Z"
        },
        {
          "author": "qiuxin2012",
          "body": "> can I switch the gpu per request? for example, this request to use iGPU and next request to use dGPU?\r\n\r\nRequest? I'm not clear about your question. What service are you using, ollama, llama-server or llama-cli? ",
          "created_at": "2024-12-18T02:25:56Z"
        },
        {
          "author": "jianjungu",
          "body": "I want to use \"ollama serve\" to provide a unified LLL infer service to all apps running on windows. But I noticed that different app required different ollama settings, for example: game assistant want to run the infer on iGfx; some app need set OLLAMA_NUM_PARALLEL, OLLAMA_MAX_QUEUE to a large value",
          "created_at": "2024-12-20T01:39:47Z"
        }
      ]
    },
    {
      "issue_number": 12548,
      "title": "Got ZE_RESULT_ERROR_INVALID_ARGUMENT error during running Qwen2.5-Coder-3B-Instruct on npu",
      "body": "I am trying to run Qwen2.5-Coder-3B-Instruct on npu\r\nThe running command is `python qwen.py --repo-id-or-model-path \"Qwen/Qwen2.5-Coder-3B-Instruct\" --save-directory E:\\llm\\models\\Qwen2.5-Coder-3B-Instruct-low-bit`\r\n\r\nBut I got an error `ZE_RESULT_ERROR_INVALID_ARGUMENT`\r\nThe full log:\r\n```\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:07<00:00,  3.63s/it]\r\n2024-12-14 04:31:56,001 - INFO - Converting model, it may takes up to several minutes ...\r\n2024-12-14 04:32:12,330 - INFO - Finish to convert model\r\ndecode start compiling\r\ndecode end compiling\r\nModel saved to E:\\llm\\models\\Qwen2.5-Coder-3B-Instruct-low-bit\\decoder_layer_0.xml\r\ndecode start compiling\r\ndecode end compiling\r\nModel saved to E:\\llm\\models\\Qwen2.5-Coder-3B-Instruct-low-bit\\decoder_layer_1.xml\r\nprefill start compiling\r\nprefill end compiling\r\nModel saved to E:\\llm\\models\\Qwen2.5-Coder-3B-Instruct-low-bit\\decoder_layer_prefill.xml\r\nTraceback (most recent call last):\r\n  File \"E:\\llm\\ipex-llm\\python\\llm\\example\\NPU\\HF-Transformers-AutoModels\\LLM\\qwen.py\", line 61, in <module>\r\n    model = AutoModelForCausalLM.from_pretrained(\r\n  File \"C:\\Users\\31092\\AppData\\Local\\Programs\\Python\\Python310\\lib\\unittest\\mock.py\", line 1379, in patched\r\n    return func(*newargs, **newkeywargs)\r\n  File \"C:\\Users\\31092\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipex_llm\\transformers\\npu_model.py\", line 238, in from_pretrained\r\n    model = cls.optimize_npu_model(*args, **optimize_kwargs)\r\n  File \"C:\\Users\\31092\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipex_llm\\transformers\\npu_model.py\", line 308, in optimize_npu_model\r\n    optimize_llm_single_process(\r\n  File \"C:\\Users\\31092\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipex_llm\\transformers\\npu_models\\convert.py\", line 457, in optimize_llm_single_process\r\n    convert_llm(model,\r\n  File \"C:\\Users\\31092\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipex_llm\\transformers\\npu_pipeline_model\\convert_pipeline.py\", line 213, in convert_llm\r\n    convert_llm_for_deploy(model,\r\n  File \"C:\\Users\\31092\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipex_llm\\transformers\\npu_pipeline_model\\convert_pipeline.py\", line 481, in convert_llm_for_deploy\r\n    convert_qwen_layer(model, 0, n_splits_linear, n_splits_down_proj,\r\n  File \"C:\\Users\\31092\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipex_llm\\transformers\\npu_pipeline_model\\qwen.py\", line 200, in convert_qwen_layer\r\n    rest_blob_path = update_names_of_IR_and_export_blob(single_decoder,\r\n  File \"C:\\Users\\31092\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipex_llm\\transformers\\npu_pipeline_model\\common.py\", line 59, in update_names_of_IR_and_export_blob\r\n    compiledModel = core.compile_model(model, device_name=\"NPU\")\r\n  File \"C:\\Users\\31092\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\intel_npu_acceleration_library\\backend\\..\\external\\openvino\\runtime\\ie_api.py\", line 543, in compile_model\r\n    super().compile_model(model, device_name, {} if config is None else config),\r\nRuntimeError: Exception from src/inference/src/cpp/core.cpp:107:\r\nException from src/inference/src/dev/plugin.cpp:53:\r\nException from src/plugins/intel_npu/src/plugin/src/plugin.cpp:697:\r\nException from src/plugins/intel_npu/src/plugin/src/compiled_model.cpp:62:\r\nException from src/plugins/intel_npu/src/compiler/src/zero_compiler_in_driver.cpp:853:\r\nL0 pfnCreate2 result: ZE_RESULT_ERROR_INVALID_ARGUMENT, code 0x78000004\r\n```\r\n",
      "state": "closed",
      "author": "rsj123",
      "author_type": "User",
      "created_at": "2024-12-13T20:43:12Z",
      "updated_at": "2025-03-10T11:32:46Z",
      "closed_at": "2024-12-21T13:31:55Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12548/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12548",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12548",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:13.246300",
      "comments": [
        {
          "author": "plusbang",
          "body": "Hi, @rsj123 \r\n\r\nCould you please provide more details about the hardware (e.g., MTL or LNL, specific CPU model), NPU driver version, and dependencies?",
          "created_at": "2024-12-16T01:58:42Z"
        },
        {
          "author": "shichang00",
          "body": "> Hi, @rsj123\r\n> \r\n> Could you please provide more details about the hardware (e.g., MTL or LNL, specific CPU model), NPU driver version, and dependencies?\r\n\r\nThe same issue is occurred with 226V in my side, but when i use 258V to retry, the issue does not occur",
          "created_at": "2024-12-16T06:50:48Z"
        },
        {
          "author": "plusbang",
          "body": "> The same issue is occurred with 226V in my side, but when i use 258V to retry, the issue does not occur\r\n\r\nHi, to avoid the issue on 226V, please try to `set IPEX_LLM_NPU_DISABLE_COMPILE_OPT=1`.\r\n",
          "created_at": "2024-12-16T07:35:23Z"
        },
        {
          "author": "rsj123",
          "body": "> Hi, @rsj123\r\n> \r\n> Could you please provide more details about the hardware (e.g., MTL or LNL, specific CPU model), NPU driver version, and dependencies?\r\n\r\nHi, I am using ultra7 155H on laptop, npu driver version is 32.0.100.2540",
          "created_at": "2024-12-19T16:48:14Z"
        },
        {
          "author": "plusbang",
          "body": "> \r\n> Hi, I am using ultra7 155H on laptop, npu driver version is 32.0.100.2540\r\n\r\nPlease refer to our doc (https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/npu_quickstart.md) for details.\r\n\r\nNeed to update NPU driver to `32.0.100.3104`, and runtime configuration `set IPE",
          "created_at": "2024-12-20T01:55:52Z"
        }
      ]
    },
    {
      "issue_number": 12428,
      "title": "Error loading for file torch\\lib\\backend_with_compiler.dll",
      "body": "After installed IPEX-LLM with MTL AOT+PyTorch 2.3, there is Error loading for file torch\\lib\\backend_with_compiler.dll.\r\n\r\n1. create python 3.11 conda environment with ipex-llm:\r\n\r\ninstall ipex & ipex-llm with command:pip install -pre --upgrade ipex-llm[xpu_lnl]==2.2.0b2  -extra-index-url https://pytorch-extension.intel.com/release-whl/stable/mtl/cn/\r\npip uninstall torch torchvision intel_extension_for_pytorch -y\r\n\r\npip install torch==2.3.1.post0+cxx11.abi torchvision==0.18.1.post0+cxx11.abi intel-extension-for-pytorch==2.3.110.post0+xpu --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/mtl/cn/\r\n\r\n2. start jupyter and execute command in cell:(eval.py will import torch)\r\n!python eval.py --model_path ./models/qwen-lora --eval_type validation\r\n\r\n3. Reported following error:\r\nThe system cannot find the file specified.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Local_Admin\\AppData\\Local\\Programs\\IntelAIDevZone\\notebooks\\aigc_apps\\eval.py\", line 18, in <module>\r\n    import torch\r\n  File \"C:\\Users\\Local_Admin\\.miniconda_dev_zone\\envs\\notebook-zone\\Lib\\site-packages\\torch_init_.py\", line 132, in <module>\r\n    raise err\r\nOSError: [WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"C:\\Users\\Local_Admin\\.miniconda_dev_zone\\envs\\notebook-zone\\Lib\\site-packages\\torch\\lib\\backend_with_compiler.dll\" or one of its dependencies.\r\n\r\n4. If import torch in cell directly there is no error.",
      "state": "closed",
      "author": "LiangtaoJin",
      "author_type": "User",
      "created_at": "2024-11-22T03:16:17Z",
      "updated_at": "2025-03-10T11:32:45Z",
      "closed_at": "2024-11-22T03:39:58Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12428/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12428",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12428",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:13.474486",
      "comments": [
        {
          "author": "LiangtaoJin",
          "body": "Will close the issue. The root cause is double imported pandas.\r\nimport pandas as pd\r\nimport torch\r\nimport json\r\nimport csv\r\nimport pathlib\r\nimport pandas as pd",
          "created_at": "2024-11-22T03:39:58Z"
        },
        {
          "author": "rkilchmn",
          "body": "encountering same issue but not sure where the double imported pandas need to be fixed?",
          "created_at": "2025-01-16T03:05:54Z"
        }
      ]
    },
    {
      "issue_number": 11959,
      "title": "how to use ipex-llm to run both the original and quantized versions of the Baichuan2 model on an Intel GPU",
      "body": "model_path = \"/home/test/models/LLM/baichuan2-7b/pytorch\"\r\n\r\n# Load and optimize the INT4 model with IPEX\r\nlow_bit = \"sym_int4\"\r\nmodel_int4 = BigdlForCausalLM.from_pretrained(model_path, load_in_low_bit=low_bit, optimize_model=True,\r\n                                              trust_remote_code=True, use_cache=True).eval()\r\n\r\n# Load the FP32 model and tokenizer\r\nmodel = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)\r\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\r\n\r\n# Move models and data to XPU\r\ndevice = 'xpu'\r\nmodel = model.to(device)\r\nmodel_int4 = model_int4.to(device)\r\n\r\n# Ensure data is also moved to XPU\r\ndef load_data_and_to_device(dataset, isplit, dataset_field, device):\r\n    prompts = load_prompts(dataset, isplit, dataset_field)\r\n    if prompts is not None:\r\n        prompts[\"questions\"] = [torch.tensor(item).to(device) for item in prompts[\"questions\"]]\r\n    return prompts\r\n\r\ndataset = \"squad\"\r\nisplit = \"validation[:32]\"\r\ndataset_field = \"question\"\r\n\r\nprompts = load_data_and_to_device(dataset, isplit, dataset_field, device)\r\n\r\n# Create the evaluator with the models and tokenizer\r\nevaluator = whowhatbench.Evaluator(base_model=model, tokenizer=tokenizer, test_data=prompts)\r\n\r\n# Score the model and get metrics\r\nall_metrics_per_question, all_metrics = evaluator.score(model_int4)\r\n\r\nprint(all_metrics_per_question)\r\nprint(all_metrics)",
      "state": "open",
      "author": "tao-ov",
      "author_type": "User",
      "created_at": "2024-08-29T07:35:40Z",
      "updated_at": "2025-03-10T11:31:22Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11959/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11959",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11959",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:13.657710",
      "comments": [
        {
          "author": "tao-ov",
          "body": "  return self.fget.__get__(instance, owner)()\r\n2024-08-29 15:39:14,061 - INFO - Converting the current model to sym_int4 format......\r\nTraceback (most recent call last):\r\n  File \"/home/test/ipexllm_whowhat/ipex-llm/python/llm/example/GPU/HuggingFace/LLM/baichuan2/who_what_benchmark/examples/ipex-llm",
          "created_at": "2024-08-29T07:40:46Z"
        }
      ]
    },
    {
      "issue_number": 11796,
      "title": "`Qwen/Qwen2-7B-Instruct` gives garbled outputs in LongBench with `load_in_low_bit=\"fp16\"` and `optimize_model=False`",
      "body": "**device:** Intel(R) Data Center GPU Max 1100\r\n**ipex-llm:** 2.1.0b20240813\r\n**transformers:**  4.37.0\r\n**model:** Qwen/Qwen2-7B-Instruct\r\n\r\n---\r\n\r\nIt's confirmed : \r\n- set optimize_model=True can prevent qwen2 from outputting garbled words.\r\n- This also happened in Arc GPU and precision=`sym_int4`\r\n\r\n---\r\n\r\nHere is a minimal reproducible case, which use dataset `multi_news` as an example, run this example:\r\n```python linenums=\"1\"\r\nfrom transformers import AutoTokenizer\r\nfrom ipex_llm.transformers import AutoModelForCausalLM\r\nfrom datasets import load_dataset\r\nimport json\r\nfrom tqdm import tqdm\r\nimport numpy as np\r\nimport random\r\nimport torch\r\n\r\n\r\ndef seed_everything(seed):\r\n    torch.manual_seed(seed)\r\n    torch.cuda.manual_seed(seed)\r\n    np.random.seed(seed)\r\n    random.seed(seed)\r\n    torch.backends.cudnn.benchmark = False\r\n    torch.backends.cudnn.deterministic = True\r\n    torch.cuda.manual_seed_all(seed)\r\n\r\n\r\n@torch.inference_mode()\r\ndef get_pred_single_gpu(data, max_gen, \r\n                        prompt_format, model_path, out_path):\r\n    \r\n\r\n    model = AutoModelForCausalLM.from_pretrained(\r\n                model_path,\r\n                optimize_model=False,\r\n                load_in_low_bit=\"fp16\",\r\n                use_cache=True,\r\n                torch_dtype = torch.float16,\r\n    ).to(\"xpu\").eval()\r\n    tokenizer = AutoTokenizer.from_pretrained(\r\n            model_path,\r\n            padding_side=\"right\",\r\n            use_fast=False,\r\n            trust_remote_code=True,\r\n    )\r\n\r\n    device = model.device\r\n    print(f\"model_device: {model.device}\")\r\n    \r\n\r\n    for json_obj in tqdm(data):\r\n        \r\n        \r\n        prompt = prompt_format.format(**json_obj)\r\n        \r\n        input = tokenizer(prompt, truncation=False, return_tensors=\"pt\").to(device)\r\n        \r\n        context_length = input.input_ids.shape[-1]\r\n        print(f'context_length = {context_length}')\r\n    \r\n        output = model.generate(\r\n                **input,\r\n                max_new_tokens=max_gen,\r\n                num_beams=1,\r\n                do_sample=False,\r\n                temperature=1.0,\r\n                min_length=context_length+1,\r\n            )[0]\r\n        \r\n        pred = tokenizer.decode(output[context_length:], skip_special_tokens=True)\r\n\r\n        with open(out_path, \"a\", encoding=\"utf-8\") as f:\r\n            json.dump({\r\n                        \"prompt\": prompt,\r\n                        \"pred\": pred, \r\n                        \"answers\": json_obj[\"answers\"], \r\n                        \"all_classes\": json_obj[\"all_classes\"], \r\n                        \"length\": json_obj[\"length\"]\r\n                       }, \r\n                       f, ensure_ascii=False, indent=4)\r\n            f.write('\\n')\r\n\r\n\r\nif __name__ == '__main__':\r\n    seed_everything(42)\r\n\r\n    model_name = \"qwen-2\"\r\n    model_path = \"Qwen/Qwen2-7B-Instruct\"\r\n\r\n    dataset = \"multi_news\"\r\n \r\n    # prompt format in config/dataset2prompt.json\r\n    prompt_format = \"You are given several news passages. Write a one-page summary of all news. \\n\\nNews:\\n{context}\\n\\nNow, write a one-page summary of all the news.\\n\\nSummary:\"\r\n    max_gen = 512    # 512 in config/dataset2maxlen.json\r\n\r\n    data = load_dataset('THUDM/LongBench', dataset, split='test')\r\n    data_all = [data_sample for data_sample in data][0:4]    \r\n    \r\n    out_path = \"./qwen-check-out.jsonl\"\r\n    \r\n    get_pred_single_gpu(data_all, max_gen, prompt_format, model_path, out_path)\r\n\r\n```\r\n\r\nHere is some of the Qwen's garbled output:\r\n![image](https://github.com/user-attachments/assets/78c4ac83-f15b-48c5-baa0-f6e0115dce83)\r\n",
      "state": "open",
      "author": "ATMxsp01",
      "author_type": "User",
      "created_at": "2024-08-14T08:16:24Z",
      "updated_at": "2025-03-10T11:31:21Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11796/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11796",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11796",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:13.872771",
      "comments": []
    },
    {
      "issue_number": 12117,
      "title": "Issue with Windows Quickstart Guide.",
      "body": "Not sure where else to mention this -- [the windows quickstart guide](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/install_windows_gpu.md) worked a dream up until actually running the `demo.py` for both hf and ModelScope. The issue I continually ran into was:\r\n\r\n```\"ImportError: This modeling file requires the following packages that were not found in your environment: transformers_stream_generator. Run `pip install transformers_stream_generator`\"```\r\n\r\nI did a bit of googling which recommended that I downgrade transformers to anything pre 4.37. I did so (ending up with 4.36.2), and that seemed to resolve the issue which let the sample code run. Hooray!\r\n\r\nNot sure at which step I ended up loading an incompatible version of transformers, but wanted to mention it here since a search of existing issues didn't seem to cover it already.",
      "state": "open",
      "author": "pmusser",
      "author_type": "User",
      "created_at": "2024-09-24T19:17:27Z",
      "updated_at": "2025-03-10T11:30:59Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12117/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "ch1y0q"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12117",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12117",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:13.872791",
      "comments": [
        {
          "author": "ch1y0q",
          "body": "Hi @pmusser , we confirmed that this issue can be reproduced. It is due to a change in `transformers` 4.37.0. When importing `transformers_stream_generator`, `from transformers.generation.utils import GenerateOutput, SampleOutput, logger` requires `SampleOutput`, however `SampleOutput` is removed in",
          "created_at": "2024-09-25T03:46:00Z"
        }
      ]
    },
    {
      "issue_number": 12153,
      "title": "models",
      "body": "Supported models are old\r\nWhy is there no support for modern models?\r\n\r\ncomfui\r\nvlux\r\nphi3.5\r\n",
      "state": "open",
      "author": "ayttop",
      "author_type": "User",
      "created_at": "2024-09-30T20:48:08Z",
      "updated_at": "2025-03-10T11:30:41Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12153/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12153",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12153",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:14.059607",
      "comments": [
        {
          "author": "glorysdj",
          "body": "Sorry, we don't have any plan to support Confyui and flux yet. \r\nBTW, phi-3 is supported, please refer to https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/GPU/HuggingFace/Multimodal/phi-3-vision",
          "created_at": "2024-10-08T02:55:34Z"
        }
      ]
    },
    {
      "issue_number": 12819,
      "title": "Error: unsupported architecture for ollama create model file",
      "body": "I am trying to run ds r1 on ARC B580(windows) with following steps:\n\n1. git clone the DeepSeek-R1-Distill-Qwen-1.5B repo from HF.\n2. create a file name `Modelfile` with single line: `FROM .` under the folder \"DeepSeek-R1-Distill-Qwen-1.5B\"\n3. command line below:\n```shell\n(llm2) PS E:\\ollama\\DeepSeek-R1-Distill-Qwen-1.5B>ollama.exe --version\nollama version is 0.5.1-ipexllm-20250107\n\n(llm2) PS E:\\ollama\\DeepSeek-R1-Distill-Qwen-1.5B>ollama.exe create 15b\ntransferring model data 100%\nconverting model\nError: unsupported architecture\n```\nIs it a known issue?",
      "state": "closed",
      "author": "chhao01",
      "author_type": "User",
      "created_at": "2025-02-13T02:14:12Z",
      "updated_at": "2025-03-10T11:30:21Z",
      "closed_at": "2025-02-13T03:24:22Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12819/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12819",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12819",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:14.308179",
      "comments": [
        {
          "author": "chhao01",
          "body": "I guess it's not an issue for ipex-llm, but for ollama itself. see https://github.com/ollama/ollama/issues/6231\n\nBtw, I can run it with .gguf model file now, which converted by the other tool.",
          "created_at": "2025-02-13T03:24:22Z"
        }
      ]
    },
    {
      "issue_number": 12812,
      "title": "Unable to run on Intel CometLake-U GT2 [UHD Graphics]",
      "body": "Hello,\nI am trying to run ollama using ipex-llm on Intel CometLake-U GT2.\nI have followed the Linux tutorial and passedthrough my GPU into my Proxmox container which is running Ubuntu 23.04.\nSadly I can't get any model to work.\n\nI am using\n- `oneAPI 2024`\n- `ipex-llm[cpp]==2.2.0b20250105`.\n\nAnd the following variables : \n`\nexport OLLAMA_NUM_GPU=999\nexport no_proxy=localhost,127.0.0.1\nexport ZES_ENABLE_SYSMAN=1\nsource /opt/intel/oneapi/setvars.sh\nexport SYCL_CACHE_PERSISTENT=1\nexport SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1\nexport ONEAPI_DEVICE_SELECTOR=level_zero:0\nexport OLLAMA_NUM_PARALLEL=1\n`\n\nYou'll find below my hardware configuration and the errors I'm getting.\n\n`\n00:02.0 VGA compatible controller: Intel Corporation CometLake-U GT2 [UHD Graphics] (rev 02) (prog-if 00 [VGA controller])\n        DeviceName:  GPU\n        Subsystem: Intel Corporation UHD Graphics\n        Flags: bus master, fast devsel, latency 0, IRQ 159, IOMMU group 1\n        Memory at 6022000000 (64-bit, non-prefetchable) [size=16M]\n        Memory at 4000000000 (64-bit, prefetchable) [size=2G]\n        I/O ports at 3000 [size=64]\n        Expansion ROM at 000c0000 [virtual] [disabled] [size=128K]\n        Capabilities: [40] Vendor Specific Information: Len=0c <?>\n        Capabilities: [70] Express Root Complex Integrated Endpoint, MSI 00\n        Capabilities: [ac] MSI: Enable+ Count=1/1 Maskable- 64bit-\n        Capabilities: [d0] Power Management version 2\n        Capabilities: [100] Process Address Space ID (PASID)\n        Capabilities: [200] Address Translation Service (ATS)\n        Capabilities: [300] Page Request Interface (PRI)\n        Kernel driver in use: i915\n        Kernel modules: i915\n`\n\n`\nvendor_id       : GenuineIntel\ncpu family      : 6\nmodel           : 142\nmodel name      : Intel(R) Core(TM) i5-10210U CPU @ 1.60GHz\nstepping        : 12\nmicrocode       : 0xfa\ncpu MHz         : 2100.000\ncache size      : 6144 KB\nphysical id     : 0\nsiblings        : 8\ncore id         : 3\ncpu cores       : 4\napicid          : 7\ninitial apicid  : 7\nfpu             : yes\nfpu_exception   : yes\ncpuid level     : 22\nwp              : yes\nflags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust sgx bmi1 avx2 smep bmi2 erms invpcid mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm arat pln pts hwp hwp_notify hwp_act_window hwp_epp md_clear flush_l1d arch_capabilities\nvmx flags       : vnmi preemption_timer invvpid ept_x_only ept_ad ept_1gb flexpriority tsc_offset vtpr mtf vapic ept vpid unrestricted_guest ple pml ept_mode_based_exec\nbugs            : spectre_v1 spectre_v2 spec_store_bypass swapgs itlb_multihit srbds mmio_stale_data retbleed eibrs_pbrsb gds bhi\nbogomips        : 4199.88\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 39 bits physical, 48 bits virtual\n`\n\nOllama log while trying to run : mistral:v0.1 [error.log](https://github.com/user-attachments/files/18757722/error.log)\nIs this supposed to work on hardware this old ?\nI also tried running a newer version of ipex-llm with oneAPI 2025 but I am getting an illegal instruction error.\n\nThanks",
      "state": "closed",
      "author": "arths31",
      "author_type": "User",
      "created_at": "2025-02-11T19:28:22Z",
      "updated_at": "2025-03-10T11:30:21Z",
      "closed_at": "2025-02-12T07:45:07Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12812/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12812",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12812",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:14.559886",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "Sorry, we don't support CometLake's iGPU, we only support CPUs newer than 11th Gen.",
          "created_at": "2025-02-12T02:18:34Z"
        },
        {
          "author": "arths31",
          "body": "Okay thank you !\nHave a nice day",
          "created_at": "2025-02-12T07:45:07Z"
        }
      ]
    },
    {
      "issue_number": 12803,
      "title": "how to use our ipex framework to inference deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
      "body": "please help me",
      "state": "closed",
      "author": "K-Alex13",
      "author_type": "User",
      "created_at": "2025-02-10T08:54:01Z",
      "updated_at": "2025-03-10T11:30:20Z",
      "closed_at": "2025-02-10T09:52:16Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12803/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12803",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12803",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:14.744673",
      "comments": [
        {
          "author": "qiyuangong",
          "body": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B is the same with Qwen-2.5-Math-1.5B. It's already supported. :)",
          "created_at": "2025-02-10T10:06:15Z"
        }
      ]
    },
    {
      "issue_number": 12784,
      "title": "ollama missed some libraries in the latest version",
      "body": "I did below steps\n```\n1. conda create -n llm python=3.11 libuv\n2. conda activate llm\n3. pip install --pre --upgrade ipex-llm[cpp]\n4. mkdir E:\\ollama\n5. cd E:\\ollama\n6. init-ollama.bat \n7. set OLLAMA_NUM_GPU=999\n8. set SYCL_CACHE_PERSISTENT=1\n9. ollama list\n```\nThen it reports miss library **sycl8.dll, mkl_sycl_blas.5.dll, ur_win_proxy_loader.dll**\n\nI checked my conda envs, only found sycl7.dll, mkl_sycl_blas.4.dll under Library\\bin folder. It looks like some modules mismatched.\n\nDevice: Thinkpad with Intel i185H\nOS: Win11\npython envs\n```\n(llm) E:\\ollama>pip list\nPackage                 Version\n----------------------- --------------\naccelerate              0.33.0\nbigdl-core-cpp          2.6.0b20250206\ncertifi                 2025.1.31\ncharset-normalizer      3.4.1\ncolorama                0.4.6\ndpcpp-cpp-rt            2024.2.1\nfilelock                3.17.0\nfsspec                  2025.2.0\ngguf                    0.14.0\nhuggingface-hub         0.28.1\nidna                    3.10\nintel-cmplr-lib-rt      2024.2.1\nintel-cmplr-lib-ur      2024.2.1\nintel-cmplr-lic-rt      2024.2.1\nintel-opencl-rt         2024.2.1\nintel-openmp            2024.2.1\nintel-sycl-rt           2024.2.1\nipex-llm                2.2.0b20250206\nJinja2                  3.1.5\nMarkupSafe              3.0.2\nmkl                     2024.2.1\nmkl-dpcpp               2024.2.1\nmpmath                  1.3.0\nnetworkx                3.4.2\nnumpy                   1.26.4\nonednn                  2024.2.1\nonednn-devel            2024.2.1\nonemkl-sycl-blas        2024.2.1\nonemkl-sycl-datafitting 2024.2.1\nonemkl-sycl-dft         2024.2.1\nonemkl-sycl-lapack      2024.2.1\nonemkl-sycl-rng         2024.2.1\nonemkl-sycl-sparse      2024.2.1\nonemkl-sycl-stats       2024.2.1\nonemkl-sycl-vm          2024.2.1\npackaging               24.2\npip                     25.0\nprotobuf                4.25.6\npsutil                  6.1.1\nPyYAML                  6.0.2\nregex                   2024.11.6\nrequests                2.32.3\nsafetensors             0.5.2\nsentencepiece           0.1.99\nsetuptools              75.8.0\nsympy                   1.13.3\ntbb                     2021.13.1\ntokenizers              0.19.1\ntorch                   2.2.0\ntqdm                    4.67.1\ntransformers            4.44.2\ntyping_extensions       4.12.2\nurllib3                 2.3.0\nwheel                   0.45.1\n```\n\n",
      "state": "closed",
      "author": "jeffreygu",
      "author_type": "User",
      "created_at": "2025-02-07T03:05:11Z",
      "updated_at": "2025-03-10T11:30:20Z",
      "closed_at": "2025-02-07T08:17:30Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12784/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12784",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12784",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:14.900258",
      "comments": [
        {
          "author": "NewBeeFly",
          "body": " using **pip install ipex-llm[cpp]==2.2.0b20250204**  instead of step 3.",
          "created_at": "2025-02-07T07:08:04Z"
        },
        {
          "author": "jeffreygu",
          "body": "@NewBeeFly Thanks, it looks like a duplication of #12781 ",
          "created_at": "2025-02-07T08:17:30Z"
        }
      ]
    },
    {
      "issue_number": 12752,
      "title": "Inexplicable Windows / Arc Ollama Crash",
      "body": "OS: Windows 11 24H2, 23H2, others\nHW: Tested with various Intel CPUs and Arc A770 16GB cards\nSW: Arc Driver 32.0.101.6458, oneAPI base Toolkit 2025.0.1.47\n\n```\nfound 1 SYCL devices:\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\n| 0| [level_zero:gpu:0]|                Intel Arc A770 Graphics|    1.6|    512|    1024|   32| 16704M|            1.3.31441|\n```\n\nJust setting up a new environment, as per the Arc guide for Ollama with llama.cpp here https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.md#1-install-ipex-llm-for-ollama:\n\nAttempting with latest [cpu] as usual - init-ollama runs fine, but  crashes on inference immediately after loading the model (any model, full trace attached):\n[OLLAMA_LOG.txt](https://github.com/user-attachments/files/18545036/OLLAMA_LOG.txt)\n\n```\ntime=2025-01-24T18:01:43.075+02:00 level=INFO source=server.go:619 msg=\"llama runner started in 5.51 seconds\"\n[GIN] 2025/01/24 - 18:01:43 | 200 |    5.5799221s |       127.0.0.1 | POST     \"/api/generate\"\nException 0xc0000005 0x0 0x40 0x7ffb044b281f\nPC=0x7ffb044b281f\nsignal arrived during external code execution\n\nruntime.cgocall(0x7ff63f370850, 0xc000089b50)\n        runtime/cgocall.go:167 +0x3e fp=0xc000089b28 sp=0xc000089ac0 pc=0x7ff63f125c1e\nollama/llama/llamafile._Cfunc_llama_decode(0x1e9e9a4f390, {0xa, 0x1e9e995c010, 0x0, 0x0, 0x1e9e9962040, 0x1e9e995e020, 0x1e99ccce9b0, 0x1e9e26c84e0, 0x0, ...})\n...\n```\n\nAny ideas would be greatly appreciated - tried older ipex-llm versions, different build tools, different drivers - to no avail.\nGPU seems to be working fine otherwise.\n\nEven tried reinstalling Windows, same results. Tried on entirely different machine - again - same error...\n\nAttached install steps.\n[setup.txt](https://github.com/user-attachments/files/18545037/setup.txt)\n\nOn another 14-series system that I use for dev this install order seems to be working fine, but was done more than a month ago, so there might be other variables I guess...",
      "state": "closed",
      "author": "vladislavdonchev",
      "author_type": "User",
      "created_at": "2025-01-25T06:13:17Z",
      "updated_at": "2025-03-10T11:30:20Z",
      "closed_at": "2025-02-02T14:29:33Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12752/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12752",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12752",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:15.088569",
      "comments": [
        {
          "author": "vladislavdonchev",
          "body": "Update: Tried swapping out GPUs, older builds of Windows, even copy-pasted the entire environment from the working machine... Nothing. Still the same error.\n\nUpdate: OK, tested on a third machine, and this still persists, regardless of the ipex-llm version. Going just a tiiiiiiny bit mental here.",
          "created_at": "2025-01-25T07:24:51Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @vladislavdonchev, as you may see in our [document](https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.md), you don't need to call `\"C:\\Program Files (x86)\\Intel\\oneAPI\\setvars.bat\"` on Windows, and IPEX-LLM Ollama currently doesn't support oneAPI 2025 but instea",
          "created_at": "2025-01-25T17:05:15Z"
        },
        {
          "author": "vladislavdonchev",
          "body": "> Hi [@vladislavdonchev](https://github.com/vladislavdonchev), as you may see in our [document](https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.md), you don't need to call `\"C:\\Program Files (x86)\\Intel\\oneAPI\\setvars.bat\"` on Windows, and IPEX-LLM Ollama current",
          "created_at": "2025-01-25T18:03:43Z"
        },
        {
          "author": "vladislavdonchev",
          "body": "Update: I am re-opening this issue because I am unable to find older versions of the oneAPI Base Toolkit. If that is the only supported version, is it possible to download anywhere?",
          "created_at": "2025-01-28T19:00:27Z"
        },
        {
          "author": "jason-dai",
          "body": "> Update: I am re-opening this issue because I am unable to find older versions of the oneAPI Base Toolkit. If that is the only supported version, is it possible to download anywhere?\n\nOn Windows, just install `ipex-llm[cpp]` (https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/llama_",
          "created_at": "2025-01-31T03:47:42Z"
        }
      ]
    },
    {
      "issue_number": 12704,
      "title": "[ipex-llm[cpp]][ollama][MTL iGfx] The GLM4:9b model produced erroneous output starting from the second inference",
      "body": "Sent 3 prompts sequentially to GLM4:9b model with ollama 0.5.1-ipex-llm-20250112 on intel MTL 155H & LNL 258V.\r\n\r\nI got the wrong answers from the second prompt. And the output contents on LNL are correct.\r\n\r\nThe prompts are:\r\n        \"我的内容是：{飞机模型}.\\n\\n你是一个文档助手，请按照下面的要求完成任务\\n\\n要求：\\n1、根据提炼的内容进行输出返回markdown格式。\\n2、根据提炼的内容进行输出要求4-8个章节，每个章节下面有5-9个子章节，每个子章节下面有至少两个小标题序号及对应小标题。\\n3、针对我的小标题进行丰富内容，要求保留原内容的原义。要求适当润色丰富原内容， 要求每个小标题要输出500字内容。\",\r\n        \"我的内容是：{柬埔寨}.\\n\\n你是一个文档助手，请按照下面的要求完成任务\\n\\n要求：\\n1、根据提炼的内容进行输出返回markdown格式。\\n2、根据提炼的内容进行输出要求4-8个章节，每个章节下面有5-9个子章节，每个子章节下面有至少两个小标题序号及对应小标题。\\n3、针对我的小标题进行丰富内容，要求保留原内容的原义。要求适当润色丰富原内容， 要求每个小标题要输出500字内容。\",\r\n        \"我的内容是：{墨西哥}.\\n\\n你是一个文档助手，请按照下面的要求完成任务\\n\\n要求：\\n1、根据提炼的内容进行输出返回markdown格式。\\n2、根据提炼的内容进行输出要求4-8个章节，每个章节下面有5-9个子章节，每个子章节下面有至少两个小标题序号及对应小标题。\\n3、针对我的小标题进行丰富内容，要求保留原内容的原义。要求适当润色丰富原内容， 要求每个小标题要输出500字内容。\",\r\n\r\nThe output contents on MTL are wrong from the 2nd question\r\n\r\n```\r\n\r\n==================== 测试提示词 ====================\r\n提示词: 我的内容是：{飞机模型}.\r\n\r\n你是一个文档助手，请按照下面的要求完成任务\r\n\r\n要求：\r\n1、根据提炼的内容进行输出返回markdown格式。\r\n2、根据提炼的内容进行输出要求4-8个章节，每个章节下面有5-9个子章节，每个子章节下面有至少两个小标题序号及对应小标题。\r\n3、针对我的小标题进行丰富内容，要求保留原内容的原义。要求适当润色丰富原内容， 要求每个小标题要输出500字内容。\r\n\r\n=== 测试 #1 ===\r\n(首次运行，包含模型加载时间)\r\n\r\n响应内容:\r\n# 飞机模型\r\n\r\n## 第一章：飞机模型概述\r\n\r\n\r\n### 1.1 飞机模型的定义\r\n飞机模型是指模仿真实飞机的缩小版模型，它们可以是纸质的、塑料的或金属的。这些模型不仅在外观上与真实飞机相似，而且在飞行原理上也尽量接近。\r\n\r\n### 1.2 飞机模型的历史发展\r\n飞机模型的历史可以追溯到20世纪初，当时人们为了验证飞行理论而制作了最早的飞行器模型。随着航空科技的进步，飞机模型也逐渐成为了一种流行的爱好和收藏品。\r\n\r\n### 1.3 飞机模型的分类\r\n按照材质可以分为纸质模型、塑料模型和金属模型；按照用途可以分为教育模型、比赛模型和娱乐模型；按照飞行方式可以分为遥控模型和自由飞模型。\r\n\r\n### 1.4 飞机模型的制作工艺\r\n飞机模型的制作工艺包括裁剪、折叠、粘贴、上色等多个环节。随着科技的发展，一些新型材料如碳纤维也被应用于飞机模型的制作中。\r\n### 1.5 飞机模型的飞行原理\r\n飞机模型的飞行原理与真实飞机类似，都是依靠升力来克服重力的作用。飞机模型在飞行过程中通过调整翼面形状和角度来实现起降、俯仰、滚转等动作。\r\n\r\n## 第二章：纸质飞机模型制作\r\n\r\n### 2.1 材料准备\r\n\r\n....\r\n\r\n==================== 测试提示词 ====================\r\n提示词: 我的内容是：{柬埔寨}.\r\n\r\n你是一个文档助手，请按照下面的要求完成任务\r\n\r\n要求：\r\n1、根据提炼的内容进行输出返回markdown格式。\r\n2、根据提炼的内容进行输出要求4-8个章节，每个章节下面有5-9个子章节，每个子章节下面有至少两个小标题序号及对应小标题。\r\n3、针对我的小标题进行丰富内容，要求保留原内容的原义。要求适当润色丰富原内容， 要求每个小标题要输出500字内容。\r\n\r\n=== 测试 #1 ===\r\n(首次运行，包含模型加载时间)\r\n\r\n响应内容:\r\n# 主题：个人成长与自我提升\r\n\r\n## 第一章：自我认知\r\n\r\n### 子章节1：了解自己的优点\r\n- **小标题1.1：认识自己的长处**\r\n  - 在追求个人成长的过程中，第一步是认识到自己的优点。这有助于我们自信地面对挑战并发挥潜力。通过反思和分析，我们可以发现自己在某些领域的独特才能和优势。\r\n- **小标题1.2：识别自己的不足**\r\n  - 同样重要的是识别自己的不足之处。这样可以帮助我们意识到需要改进的地方，从而有针对性地进行学习和实践。\r\n\r\n### 子章节2：兴趣与激情\r\n\r\n#### 小标题2.1：培养广泛兴趣\r\n- 拥有广泛的兴趣爱好可以丰富我们的生活，拓展我们的视野。通过尝试不同的活动，我们可以找到自己真正热爱的事物。\r\n#### 小标题2.2：激发内在激情\r\n- 追求激情和热情是个人成长的关键。当我们对某个领域或事物充满热情时，我们会更加积极主动地投入到学习和实践中。\r\n\r\n## 第二章：知识储备\r\n\r\n....\r\n\r\n\r\n==================== 测试提示词 ====================\r\n提示词: 我的内容是：{墨西哥}.\r\n\r\n你是一个文档助手，请按照下面的要求完成任务\r\n\r\n要求：\r\n1、根据提炼的内容进行输出返回markdown格式。\r\n2、根据提炼的内容进行输出要求4-8个章节，每个章节下面有5-9个子章节，每个子章节下面有至少两个小标题序号及对应小标题。\r\n3、针对我的小标题进行丰富内容，要求保留原内容的原义。要求适当润色丰富原内容， 要求每个小标题要输出500字内容。\r\n\r\n=== 测试 #1 ===\r\n(首次运行，包含模型加载时间)\r\n\r\n响应内容:\r\n# 内容概述\r\n\r\n以下是您提供的内容的提炼和扩展：\r\n\r\n## 第一章：核心主题一\r\n### 1.1 子章节一\r\n- **子标题一**：[丰富内容1](#)\r\n- **子标题二**：[丰富内容2](#)\r\n\r\n### 1.2 子章节二\r\n- **子标题一**：[丰富内容3](#)\r\n- **子标题二**：[丰富内容4](#)\r\n\r\n### 1.3 子章节三\r\n- **子标题一**：[丰富内容5](#)\r\n- **子标题二**：[丰富内容6](#)\r\n```\r\n\r\nAnd \r\nmy reproduce python code is here\r\n[glm4_eva_prompt.zip](https://github.com/user-attachments/files/18405365/glm4_eva_prompt.zip)\r\n\r\nthe full output :\r\n[prompt_error_ollama_log.txt](https://github.com/user-attachments/files/18405389/prompt_error_ollama_log.txt)\r\n\r\nollama serve log: \r\n[prompt_error_ollama_log.txt](https://github.com/user-attachments/files/18405360/prompt_error_ollama_log.txt)\r\n\r\n",
      "state": "closed",
      "author": "jianjungu",
      "author_type": "User",
      "created_at": "2025-01-14T04:41:13Z",
      "updated_at": "2025-03-10T11:30:19Z",
      "closed_at": "2025-02-05T02:29:01Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12704/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "qiuxin2012",
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12704",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12704",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:15.341935",
      "comments": [
        {
          "author": "sgwhat",
          "body": "This issue has been reproduced, working on fixing it.",
          "created_at": "2025-01-16T01:48:19Z"
        },
        {
          "author": "qiuxin2012",
          "body": "We have fixed this issue, please try 20250121 tomorrow.",
          "created_at": "2025-01-21T06:20:41Z"
        },
        {
          "author": "jianjungu",
          "body": "verified with 20250122, close the issue",
          "created_at": "2025-02-05T02:29:01Z"
        }
      ]
    },
    {
      "issue_number": 12696,
      "title": "the reference results are blank with deepseek model and our generate example code ",
      "body": "Following the guild of https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/GPU/HuggingFace/LLM/deepseek\r\nand change the model to deepseek-ai/deepseek-coder-1.3b-instruct\r\nfollowing is the results \r\n![img_v3_02id_66a4783d-b64d-4bd4-80c2-edee438dbc1g](https://github.com/user-attachments/assets/fd13763f-bbb8-4b9e-9a2b-5542f0d1f83e)\r\nplease help me",
      "state": "closed",
      "author": "K-Alex13",
      "author_type": "User",
      "created_at": "2025-01-10T03:24:16Z",
      "updated_at": "2025-03-10T11:30:19Z",
      "closed_at": "2025-02-10T08:52:52Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12696/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Oscilloscope98"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12696",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12696",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:15.593898",
      "comments": [
        {
          "author": "K-Alex13",
          "body": "![img_v3_02id_f2123c93-47d6-4c35-a542-9f91964032bg](https://github.com/user-attachments/assets/d860352a-6b7a-4dc4-9192-63e7d5e59098)\r\neven try to use original 6.7B model, a new problem come up",
          "created_at": "2025-01-10T08:33:13Z"
        },
        {
          "author": "Oscilloscope98",
          "body": "Hi @K-Alex13,\r\n\r\nWe are reproducing this issue, and will let you know for any updates :)",
          "created_at": "2025-01-13T02:08:21Z"
        },
        {
          "author": "Oscilloscope98",
          "body": "Hi @K-Alex13,\r\n\r\nWe have reproduced this issue and currently fixing it. We will update here for any progress :)",
          "created_at": "2025-01-14T10:29:02Z"
        },
        {
          "author": "Oscilloscope98",
          "body": "Hi @K-Alex13,\r\n\r\nWe have fixed this blank output issue for deepseek-coder models, you could upgrade to `ipex-llm>=2.2.0b20250115` and have a try again with the [updated deepseek GPU example](https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/GPU/HuggingFace/LLM/deepseek).\r\n\r\nFo",
          "created_at": "2025-01-16T06:39:49Z"
        },
        {
          "author": "K-Alex13",
          "body": "thankyou ",
          "created_at": "2025-02-10T08:52:52Z"
        }
      ]
    },
    {
      "issue_number": 12669,
      "title": "QWEN2.5 inference issue on vLLM 0.6.2",
      "body": "I install 0.6.2 branche on https://github.com/analytics-zoo/vllm\r\nrun a QWEN2.5 based Fine-tuning model use this command\r\n```\r\npython -m ipex_llm.vllm.xpu.entrypoints.openai.api_server --served-model-name QWEN2_5 --model Model_Path --device xpu --dtype float16 --enforce-eager --max-model-len 8192 --load-in-low-bit fp8\r\n```\r\nThrows the following error\r\n```\r\n2025-01-07 13:28:06,477 - INFO - Converting the current model to fp8_e5m2 format......\r\n2025-01-07 13:28:06,479 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\r\n2025-01-07 13:28:24,947 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\r\n2025-01-07 13:28:30,761 - INFO - Loading model weights took 15.4058 GB\r\nProcess SpawnProcess-21:\r\nTraceback (most recent call last):\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 145, in run_mp_engine\r\n    engine = IPEXLLMMQLLMEngine.from_engine_args(engine_args=engine_args,\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 133, in from_engine_args\r\n    return super().from_engine_args(engine_args, usage_context, ipc_path)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/vllm-0.6.2+xpu-py3.11-linux-x86_64.egg/vllm/engine/multiprocessing/engine.py\", line 138, in from_engine_args\r\n    return cls(\r\n           ^^^^\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/vllm-0.6.2+xpu-py3.11-linux-x86_64.egg/vllm/engine/multiprocessing/engine.py\", line 78, in __init__\r\n    self.engine = LLMEngine(*args,\r\n                  ^^^^^^^^^^^^^^^^\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/vllm-0.6.2+xpu-py3.11-linux-x86_64.egg/vllm/engine/llm_engine.py\", line 339, in __init__\r\n    self._initialize_kv_caches()\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/vllm-0.6.2+xpu-py3.11-linux-x86_64.egg/vllm/engine/llm_engine.py\", line 474, in _initialize_kv_caches\r\n    self.model_executor.determine_num_available_blocks())\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/vllm-0.6.2+xpu-py3.11-linux-x86_64.egg/vllm/executor/gpu_executor.py\", line 114, in determine_num_available_blocks\r\n    return self.driver_worker.determine_num_available_blocks()\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/vllm-0.6.2+xpu-py3.11-linux-x86_64.egg/vllm/worker/xpu_worker.py\", line 128, in determine_num_available_blocks\r\n    self.model_runner.profile_run()\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/vllm-0.6.2+xpu-py3.11-linux-x86_64.egg/vllm/worker/xpu_model_runner.py\", line 538, in profile_run\r\n    self.execute_model(model_input, kv_caches, intermediate_tensors)\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/vllm-0.6.2+xpu-py3.11-linux-x86_64.egg/vllm/worker/xpu_model_runner.py\", line 643, in execute_model\r\n    hidden_or_intermediate_states = model_executable(\r\n                                    ^^^^^^^^^^^^^^^^^\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/vllm-0.6.2+xpu-py3.11-linux-x86_64.egg/vllm/model_executor/models/qwen2.py\", line 369, in forward\r\n    hidden_states = self.model(input_ids, positions, kv_caches,\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/vllm-0.6.2+xpu-py3.11-linux-x86_64.egg/vllm/model_executor/models/qwen2.py\", line 285, in forward\r\n    hidden_states, residual = layer(\r\n                              ^^^^^^\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/vllm-0.6.2+xpu-py3.11-linux-x86_64.egg/vllm/model_executor/models/qwen2.py\", line 210, in forward\r\n    hidden_states = self.self_attn(\r\n                    ^^^^^^^^^^^^^^^\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/vllm-0.6.2+xpu-py3.11-linux-x86_64.egg/vllm/model_executor/models/qwen2.py\", line 157, in forward\r\n    attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/vllm-0.6.2+xpu-py3.11-linux-x86_64.egg/vllm/attention/layer.py\", line 98, in forward\r\n    return self.impl.forward(query,\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/vllm-0.6.2+xpu-py3.11-linux-x86_64.egg/vllm/attention/backends/ipex_attn.py\", line 340, in forward\r\n    sub_out = xe_addons.sdp_causal(\r\n              ^^^^^^^^^^^^^^^^^^^^^\r\nTypeError: sdp_causal(): incompatible function arguments. The following argument types are supported:\r\n    1. (arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: Optional[torch.Tensor], arg4: float) -> torch.Tensor\r\n\r\nInvoked with: tensor([[[[-3.2773, -1.8691,  2.1016,  ..., -0.1118, -0.0577, -0.0062],\r\n          [ 1.2305,  0.7646,  2.6621,  ..., -0.1118, -0.0577, -0.0062],\r\n          [ 4.6094,  2.9277,  2.1426,  ..., -0.1118, -0.0577, -0.0062],\r\n          ...,\r\n          [ 0.0844, -2.4434,  2.0723,  ..., -0.1119, -0.0577, -0.0062],\r\n          [-4.0312, -3.4102,  2.6621,  ..., -0.1119, -0.0577, -0.0062],\r\n          [-4.4375, -2.2793,  2.1680,  ..., -0.1119, -0.0577, -0.0062]],\r\n\r\n         [[ 0.1819,  0.0770,  0.0330,  ...,  1.1670,  0.7852,  1.4102],\r\n          [-0.1307,  0.0313,  0.0271,  ...,  1.1670,  0.7852,  1.4102],\r\n          [-0.3232, -0.0336,  0.0102,  ...,  1.1670,  0.7852,  1.4102],\r\n          ...,\r\n          [ 0.0446,  0.0152,  0.0329,  ...,  1.1670,  0.7852,  1.4102],\r\n          [ 0.2969,  0.0693,  0.0274,  ...,  1.1670,  0.7852,  1.4102],\r\n          [ 0.2764,  0.0807,  0.0107,  ...,  1.1670,  0.7852,  1.4102]],\r\n\r\n         [[-3.6953, -1.9141, -1.3545,  ...,  0.1860,  0.9736, -0.1448],\r\n          [-2.0488, -2.2266, -0.5103,  ...,  0.1860,  0.9736, -0.1448],\r\n          [ 1.4834, -1.1709,  0.5415,  ...,  0.1860,  0.9736, -0.1448],\r\n          ...,\r\n          [ 2.8047,  1.5928, -1.3711,  ...,  0.1860,  0.9736, -0.1448],\r\n          [-0.5103, -0.0794, -0.5376,  ...,  0.1860,  0.9736, -0.1448],\r\n          [-3.3555, -1.7041,  0.5146,  ...,  0.1860,  0.9736, -0.1448]],\r\n\r\n         ...,\r\n\r\n         [[-0.0356, -0.1814, -0.2642,  ...,  0.1151, -0.1958,  0.0391],\r\n          [ 0.3279, -0.3879, -0.5889,  ...,  0.1151, -0.1958,  0.0391],\r\n          [ 0.3901, -0.3564, -0.6738,  ...,  0.1151, -0.1958,  0.0391],\r\n          ...,\r\n          [-0.2471,  0.3918, -0.2532,  ...,  0.1151, -0.1958,  0.0391],\r\n          [-0.4131,  0.1932, -0.5830,  ...,  0.1151, -0.1958,  0.0391],\r\n          [-0.1993, -0.1244, -0.6753,  ...,  0.1151, -0.1958,  0.0391]],\r\n\r\n         [[ 0.4756, -0.2690, -0.3613,  ..., -0.5610, -0.1216,  0.2231],\r\n          [-1.2764,  1.2744,  0.6343,  ..., -0.5610, -0.1216,  0.2231],\r\n          [-1.8555,  2.0352,  1.3730,  ..., -0.5610, -0.1216,  0.2231],\r\n          ...,\r\n          [ 0.8530, -1.9365, -0.3879,  ..., -0.5610, -0.1216,  0.2231],\r\n          [ 1.8730, -1.8115,  0.6099,  ..., -0.5610, -0.1216,  0.2231],\r\n          [ 1.1709, -0.5718,  1.3584,  ..., -0.5610, -0.1216,  0.2231]],\r\n\r\n         [[ 0.2749, -0.4829, -0.2566,  ...,  0.4026, -2.5898, -1.5234],\r\n          [ 0.3611, -0.2015, -0.1865,  ...,  0.4026, -2.5898, -1.5234],\r\n          [ 0.1151,  0.2037, -0.0405,  ...,  0.4026, -2.5898, -1.5234],\r\n          ...,\r\n          [-0.3730, -0.0883, -0.2571,  ...,  0.4023, -2.5898, -1.5234],\r\n          [-0.2070, -0.4285, -0.1897,  ...,  0.4023, -2.5898, -1.5234],\r\n          [ 0.1494, -0.5054, -0.0449,  ...,  0.4023, -2.5898, -1.5234]]]],\r\n       device='xpu:0', dtype=torch.float16), tensor([[[[-0.1953, -1.5000,  0.3149,  ...,  0.4949,  0.5200,  1.7520],\r\n          [-0.5244, -0.7461,  0.2632,  ...,  0.4949,  0.5200,  1.7520],\r\n          [-0.3716,  0.4666,  0.1044,  ...,  0.4949,  0.5200,  1.7520],\r\n          ...,\r\n          [ 0.4766, -0.1113,  0.3147,  ...,  0.4946,  0.5200,  1.7520],\r\n          [ 0.4619, -1.1953,  0.2664,  ...,  0.4946,  0.5200,  1.7520],\r\n          [ 0.0227, -1.5449,  0.1096,  ...,  0.4946,  0.5200,  1.7520]],\r\n\r\n         [[-0.1953, -1.5000,  0.3149,  ...,  0.4949,  0.5200,  1.7520],\r\n          [-0.5244, -0.7461,  0.2632,  ...,  0.4949,  0.5200,  1.7520],\r\n          [-0.3716,  0.4666,  0.1044,  ...,  0.4949,  0.5200,  1.7520],\r\n          ...,\r\n          [ 0.4766, -0.1113,  0.3147,  ...,  0.4946,  0.5200,  1.7520],\r\n          [ 0.4619, -1.1953,  0.2664,  ...,  0.4946,  0.5200,  1.7520],\r\n          [ 0.0227, -1.5449,  0.1096,  ...,  0.4946,  0.5200,  1.7520]],\r\n\r\n         [[-0.1953, -1.5000,  0.3149,  ...,  0.4949,  0.5200,  1.7520],\r\n          [-0.5244, -0.7461,  0.2632,  ...,  0.4949,  0.5200,  1.7520],\r\n          [-0.3716,  0.4666,  0.1044,  ...,  0.4949,  0.5200,  1.7520],\r\n          ...,\r\n          [ 0.4766, -0.1113,  0.3147,  ...,  0.4946,  0.5200,  1.7520],\r\n          [ 0.4619, -1.1953,  0.2664,  ...,  0.4946,  0.5200,  1.7520],\r\n          [ 0.0227, -1.5449,  0.1096,  ...,  0.4946,  0.5200,  1.7520]],\r\n\r\n         ...,\r\n\r\n         [[ 1.1270, -0.2896, -1.4219,  ..., -1.1260,  0.5679, -3.2266],\r\n          [ 0.6606,  0.3298, -0.2344,  ..., -1.1260,  0.5679, -3.2266],\r\n          [-0.4136,  0.7466,  1.0488,  ..., -1.1260,  0.5679, -3.2266],\r\n          ...,\r\n          [-0.8833, -0.6665, -1.4482,  ..., -1.1260,  0.5679, -3.2266],\r\n          [ 0.1136, -0.7681, -0.2703,  ..., -1.1260,  0.5679, -3.2266],\r\n          [ 1.0059, -0.3972,  1.0176,  ..., -1.1260,  0.5679, -3.2266]],\r\n\r\n         [[ 1.1270, -0.2896, -1.4219,  ..., -1.1260,  0.5679, -3.2266],\r\n          [ 0.6606,  0.3298, -0.2344,  ..., -1.1260,  0.5679, -3.2266],\r\n          [-0.4136,  0.7466,  1.0488,  ..., -1.1260,  0.5679, -3.2266],\r\n          ...,\r\n          [-0.8833, -0.6665, -1.4482,  ..., -1.1260,  0.5679, -3.2266],\r\n          [ 0.1136, -0.7681, -0.2703,  ..., -1.1260,  0.5679, -3.2266],\r\n          [ 1.0059, -0.3972,  1.0176,  ..., -1.1260,  0.5679, -3.2266]],\r\n\r\n         [[ 1.1270, -0.2896, -1.4219,  ..., -1.1260,  0.5679, -3.2266],\r\n          [ 0.6606,  0.3298, -0.2344,  ..., -1.1260,  0.5679, -3.2266],\r\n          [-0.4136,  0.7466,  1.0488,  ..., -1.1260,  0.5679, -3.2266],\r\n          ...,\r\n          [-0.8833, -0.6665, -1.4482,  ..., -1.1260,  0.5679, -3.2266],\r\n          [ 0.1136, -0.7681, -0.2703,  ..., -1.1260,  0.5679, -3.2266],\r\n          [ 1.0059, -0.3972,  1.0176,  ..., -1.1260,  0.5679, -3.2266]]]],\r\n       device='xpu:0', dtype=torch.float16), tensor([[[[-0.0333,  0.1555, -0.0426,  ...,  0.0063, -0.0494,  0.0855],\r\n          [-0.0333,  0.1555, -0.0426,  ...,  0.0063, -0.0494,  0.0855],\r\n          [-0.0333,  0.1555, -0.0426,  ...,  0.0063, -0.0494,  0.0855],\r\n          ...,\r\n          [-0.0333,  0.1555, -0.0426,  ...,  0.0063, -0.0494,  0.0855],\r\n          [-0.0333,  0.1555, -0.0426,  ...,  0.0063, -0.0494,  0.0855],\r\n          [-0.0333,  0.1555, -0.0426,  ...,  0.0063, -0.0494,  0.0855]],\r\n\r\n         [[-0.0333,  0.1555, -0.0426,  ...,  0.0063, -0.0494,  0.0855],\r\n          [-0.0333,  0.1555, -0.0426,  ...,  0.0063, -0.0494,  0.0855],\r\n          [-0.0333,  0.1555, -0.0426,  ...,  0.0063, -0.0494,  0.0855],\r\n          ...,\r\n          [-0.0333,  0.1555, -0.0426,  ...,  0.0063, -0.0494,  0.0855],\r\n          [-0.0333,  0.1555, -0.0426,  ...,  0.0063, -0.0494,  0.0855],\r\n          [-0.0333,  0.1555, -0.0426,  ...,  0.0063, -0.0494,  0.0855]],\r\n\r\n         [[-0.0333,  0.1555, -0.0426,  ...,  0.0063, -0.0494,  0.0855],\r\n          [-0.0333,  0.1555, -0.0426,  ...,  0.0063, -0.0494,  0.0855],\r\n          [-0.0333,  0.1555, -0.0426,  ...,  0.0063, -0.0494,  0.0855],\r\n          ...,\r\n          [-0.0333,  0.1555, -0.0426,  ...,  0.0063, -0.0494,  0.0855],\r\n          [-0.0333,  0.1555, -0.0426,  ...,  0.0063, -0.0494,  0.0855],\r\n          [-0.0333,  0.1555, -0.0426,  ...,  0.0063, -0.0494,  0.0855]],\r\n\r\n         ...,\r\n\r\n         [[-0.0724, -0.0535, -0.0873,  ..., -0.0245, -0.0409,  0.0598],\r\n          [-0.0724, -0.0535, -0.0873,  ..., -0.0245, -0.0409,  0.0598],\r\n          [-0.0724, -0.0535, -0.0873,  ..., -0.0245, -0.0409,  0.0598],\r\n          ...,\r\n          [-0.0724, -0.0535, -0.0873,  ..., -0.0245, -0.0409,  0.0598],\r\n          [-0.0724, -0.0535, -0.0873,  ..., -0.0245, -0.0409,  0.0598],\r\n          [-0.0724, -0.0535, -0.0873,  ..., -0.0245, -0.0409,  0.0598]],\r\n\r\n         [[-0.0724, -0.0535, -0.0873,  ..., -0.0245, -0.0409,  0.0598],\r\n          [-0.0724, -0.0535, -0.0873,  ..., -0.0245, -0.0409,  0.0598],\r\n          [-0.0724, -0.0535, -0.0873,  ..., -0.0245, -0.0409,  0.0598],\r\n          ...,\r\n          [-0.0724, -0.0535, -0.0873,  ..., -0.0245, -0.0409,  0.0598],\r\n          [-0.0724, -0.0535, -0.0873,  ..., -0.0245, -0.0409,  0.0598],\r\n          [-0.0724, -0.0535, -0.0873,  ..., -0.0245, -0.0409,  0.0598]],\r\n\r\n         [[-0.0724, -0.0535, -0.0873,  ..., -0.0245, -0.0409,  0.0598],\r\n          [-0.0724, -0.0535, -0.0873,  ..., -0.0245, -0.0409,  0.0598],\r\n          [-0.0724, -0.0535, -0.0873,  ..., -0.0245, -0.0409,  0.0598],\r\n          ...,\r\n          [-0.0724, -0.0535, -0.0873,  ..., -0.0245, -0.0409,  0.0598],\r\n          [-0.0724, -0.0535, -0.0873,  ..., -0.0245, -0.0409,  0.0598],\r\n          [-0.0724, -0.0535, -0.0873,  ..., -0.0245, -0.0409,  0.0598]]]],\r\n       device='xpu:0', dtype=torch.float16), None\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/ipex_llm/vllm/xpu/entrypoints/openai/api_server.py\", line 574, in <module>\r\n    uvloop.run(run_server(args))\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/uvloop/__init__.py\", line 105, in run\r\n    return runner.run(wrapper())\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/asyncio/runners.py\", line 118, in run\r\n    return self._loop.run_until_complete(task)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/uvloop/__init__.py\", line 61, in wrapper\r\n    return await main\r\n           ^^^^^^^^^^\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/ipex_llm/vllm/xpu/entrypoints/openai/api_server.py\", line 541, in run_server\r\n    async with build_async_engine_client(args) as engine_client:\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/contextlib.py\", line 210, in __aenter__\r\n    return await anext(self.gen)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/ipex_llm/vllm/xpu/entrypoints/openai/api_server.py\", line 105, in build_async_engine_client\r\n    async with build_async_engine_client_from_engine_args(\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/contextlib.py\", line 210, in __aenter__\r\n    return await anext(self.gen)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/ipex_llm/vllm/xpu/entrypoints/openai/api_server.py\", line 195, in build_async_engine_client_from_engine_args\r\n    raise RuntimeError(\r\nRuntimeError: Engine process failed to start\r\n```",
      "state": "closed",
      "author": "kunger97",
      "author_type": "User",
      "created_at": "2025-01-07T13:32:36Z",
      "updated_at": "2025-03-10T11:30:18Z",
      "closed_at": "2025-02-12T02:46:05Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12669/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "gc-fu"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12669",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12669",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:15.791188",
      "comments": [
        {
          "author": "gc-fu",
          "body": "Hi, Can you please post the result of the following commands:\r\n\r\n```bash\r\npip list | grep bigdl\r\n```\r\n\r\nand \r\n```bash\r\ncd /llm/vllm\r\ngit log\r\n```",
          "created_at": "2025-01-08T02:06:23Z"
        },
        {
          "author": "kunger97",
          "body": "```\r\nbigdl-core-xe-21                  2.6.0b20250106\r\nbigdl-core-xe-addons-21           2.6.0b20250106\r\nbigdl-core-xe-batch-21            2.6.0b20250106\r\n```\r\n\r\n```\r\ncommit 8fb3efa86344ca90d014dbf17ffc8e810b766e15 (HEAD -> 0.6.2, origin/0.6.2)\r\nAuthor: Wang, Jian4 <61138589+hzjane@users.noreply.git",
          "created_at": "2025-01-08T02:29:26Z"
        },
        {
          "author": "gc-fu",
          "body": "The vLLM installed seems not the latest version.  Can you try to reinstall the vLLM?\r\n\r\n\r\nThe error here `\"/home/ua55abec29206204c6df28b1eb1b8906/.conda/envs/ipex_pip/lib/python3.11/site-packages/vllm-0.6.2+xpu-py3.11-linux-x86_64.egg/vllm/attention/backends/ipex_attn.py\", line 340, in forward` indi",
          "created_at": "2025-01-08T05:40:49Z"
        },
        {
          "author": "Sanhajio",
          "body": "@kunger97  This is the tag of the latest version: 867ca0d1\n\n\nYou should be able to `git checkout 867ca0d1` and run `VLLM_TARGET_DEVICE=xpu python setup.py install`\n\nI don't know about:\n\n```\nbigdl-core-xe-21                  2.6.0b20250106\nbigdl-core-xe-addons-21           2.6.0b20250106\nbigdl-core-x",
          "created_at": "2025-02-12T00:43:27Z"
        },
        {
          "author": "gc-fu",
          "body": "Glad to know that you have solved the problem ~",
          "created_at": "2025-02-12T02:46:05Z"
        }
      ]
    },
    {
      "issue_number": 12594,
      "title": "Latest ipex-llm[cpp] depends on older sycl and blas?",
      "body": "Hello, i'm on Ubuntu 24.04 and just recreated my conda environment and have a problem with the latest version of ipex-llm[cpp].  It looks like llama and ollama are linked partially against both 2025 OneAPI libs and 2024 OneAPI Libs.\r\n\r\nSteps I used to reproduce:\r\n\r\n> conda create -n llm-cpp python=3.11\r\n>pip install --pre --upgrade ipex-llm[cpp]\r\n>mkdir llama-cpp\r\n>cd llama-cpp\r\n>init-llama-cpp\r\n>init-ollama\r\n\r\nWhen I run llama or ollama, I get this:\r\n`(llm-cpp) bmaddox@sdf1:~/src/llama-cpp$ ./llama-cli\r\n./llama-cli: error while loading shared libraries: libsycl.so.7: cannot open shared object file: No such file or directory`\r\n\r\nand:\r\n\r\n`(llm-cpp) bmaddox@sdf1:~/src/llama-cpp$ ./ollama\r\n./ollama: error while loading shared libraries: libsycl.so.7: cannot open shared object file: No such file or directory`\r\n\r\nSo I thought I'd check what libraries they're linked against.\r\n\r\n> (llm-cpp) bmaddox@sdf1:~/src/llama-cpp$ ldd ./llama-cli\r\n> \tlinux-vdso.so.1 (0x00007ffc9c0c4000)\r\n> \tlibllama.so (0x000079d82c000000)\r\n> \tlibggml.so (0x000079d829e00000)\r\n> \tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x000079d829a00000)\r\n> \tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x000079d82c53d000)\r\n> \tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x000079d82c50f000)\r\n> \tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x000079d82c50a000)\r\n> \tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x000079d82c505000)\r\n> \tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x000079d829600000)\r\n> \tlibsvml.so => /opt/intel/oneapi/compiler/2025.0/lib/libsvml.so (0x000079d827e00000)\r\n> \tlibirng.so => /opt/intel/oneapi/compiler/2025.0/lib/libirng.so (0x000079d82c40a000)\r\n> \tlibimf.so => /opt/intel/oneapi/compiler/2025.0/lib/libimf.so (0x000079d827800000)\r\n> \tlibintlc.so.5 => /opt/intel/oneapi/compiler/2025.0/lib/libintlc.so.5 (0x000079d82bf9e000)\r\n> \t/lib64/ld-linux-x86-64.so.2 (0x000079d82c656000)\r\n> \tlibdnnl.so.3 => /opt/intel/oneapi/dnnl/2025.0/lib/libdnnl.so.3 (0x000079d822000000)\r\n> \tlibsycl.so.7 => not found\r\n> \tlibOpenCL.so.1 => /opt/intel/oneapi/compiler/2025.0/lib/libOpenCL.so.1 (0x000079d82c3f8000)\r\n> \tlibmkl_core.so.2 => /opt/intel/oneapi/mkl/2025.0/lib/libmkl_core.so.2 (0x000079d81e000000)\r\n> \tlibmkl_sycl_blas.so.4 => not found\r\n> \tlibmkl_intel_ilp64.so.2 => /opt/intel/oneapi/mkl/2025.0/lib/libmkl_intel_ilp64.so.2 (0x000079d81d200000)\r\n> \tlibmkl_tbb_thread.so.2 => /opt/intel/oneapi/mkl/2025.0/lib/libmkl_tbb_thread.so.2 (0x000079d81b400000)\r\n> \tlibiomp5.so => /opt/intel/oneapi/compiler/2025.0/lib/libiomp5.so (0x000079d81ae00000)\r\n> \tlibtbb.so.12 => /opt/intel/oneapi/tbb/2022.0/env/../lib/intel64/gcc4.8/libtbb.so.12 (0x000079d81a800000)\r\n> \tlibsycl.so.8 => /opt/intel/oneapi/compiler/2025.0/lib/libsycl.so.8 (0x000079d81a400000)\r\n> \tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x000079d82c3f1000)\r\n> \tlibur_loader.so.0 => /opt/intel/oneapi/compiler/2025.0/lib/libur_loader.so.0 (0x000079d819800000)\r\n> \tlibz.so.1 => /lib/x86_64-linux-gnu/libz.so.1 (0x000079d82c3d3000)\r\n\r\nIt's a similar thing with ollama\r\n\r\n> (llm-cpp) bmaddox@sdf1:~/src/llama-cpp$ ldd ./ollama\r\n> \tlinux-vdso.so.1 (0x00007fff089ca000)\r\n> \tlibresolv.so.2 => /lib/x86_64-linux-gnu/libresolv.so.2 (0x000073a6513be000)\r\n> \tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x000073a6513b9000)\r\n> \tlibmllama.so => /home/bmaddox/src/llama-cpp/./libmllama.so (0x000073a650e00000)\r\n> \tlibsample.so => /home/bmaddox/src/llama-cpp/./libsample.so (0x000073a650800000)\r\n> \tlibllama.so => /home/bmaddox/src/llama-cpp/./libllama.so (0x000073a650200000)\r\n> \tlibggml.so => /home/bmaddox/src/llama-cpp/./libggml.so (0x000073a64e000000)\r\n> \tlibllava_shared.so => /home/bmaddox/src/llama-cpp/./libllava_shared.so (0x000073a64da00000)\r\n> \tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x000073a64d600000)\r\n> \tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x000073a6513b2000)\r\n> \tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x000073a6513ab000)\r\n> \tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x000073a6512c2000)\r\n> \tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x000073a64d200000)\r\n> \tlibsvml.so => /opt/intel/oneapi/compiler/2025.0/lib/libsvml.so (0x000073a64ba00000)\r\n> \tlibirng.so => /opt/intel/oneapi/compiler/2025.0/lib/libirng.so (0x000073a6511c9000)\r\n> \tlibimf.so => /opt/intel/oneapi/compiler/2025.0/lib/libimf.so (0x000073a64b400000)\r\n> \tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x000073a651199000)\r\n> \tlibintlc.so.5 => /opt/intel/oneapi/compiler/2025.0/lib/libintlc.so.5 (0x000073a651137000)\r\n> \t/lib64/ld-linux-x86-64.so.2 (0x000073a6513ff000)\r\n> \tlibdnnl.so.3 => /opt/intel/oneapi/dnnl/2025.0/lib/libdnnl.so.3 (0x000073a645c00000)\r\n> \tlibsycl.so.7 => not found\r\n> \tlibOpenCL.so.1 => /opt/intel/oneapi/compiler/2025.0/lib/libOpenCL.so.1 (0x000073a651127000)\r\n> \tlibmkl_core.so.2 => /opt/intel/oneapi/mkl/2025.0/lib/libmkl_core.so.2 (0x000073a641c00000)\r\n> \tlibmkl_sycl_blas.so.4 => not found\r\n> \tlibmkl_intel_ilp64.so.2 => /opt/intel/oneapi/mkl/2025.0/lib/libmkl_intel_ilp64.so.2 (0x000073a640e00000)\r\n> \tlibmkl_tbb_thread.so.2 => /opt/intel/oneapi/mkl/2025.0/lib/libmkl_tbb_thread.so.2 (0x000073a63f000000)\r\n> \tlibiomp5.so => /opt/intel/oneapi/compiler/2025.0/lib/libiomp5.so (0x000073a63ea00000)\r\n> \tlibtbb.so.12 => /opt/intel/oneapi/tbb/2022.0/env/../lib/intel64/gcc4.8/libtbb.so.12 (0x000073a63e400000)\r\n> \tlibsycl.so.8 => /opt/intel/oneapi/compiler/2025.0/lib/libsycl.so.8 (0x000073a63e000000)\r\n> \tlibur_loader.so.0 => /opt/intel/oneapi/compiler/2025.0/lib/libur_loader.so.0 (0x000073a63d400000)\r\n> \tlibz.so.1 => /lib/x86_64-linux-gnu/libz.so.1 (0x000073a651107000)\r\n\r\nSo for libsycl, it looks like it's linked against both the 2025 version of intel-oneapi-compiler-dpcpp-cpp-runtime-2025.0 and ther older .7 version which I think is from the 2024 versions (which I can't download any more)?  It also looks like it's linked against a 2024 version of \r\nlibmkl_sycl_blas.so.4 while /opt/intel/oneapi/mkl/2025.0/lib/libmkl_sycl_blas.so.5 exists and is from the intel-oneapi-mkl-sycl-blas-2025.0 package.  \r\n\r\nI'm assuming the libsycl.so.7 link dependency is due to the older libmkl_sycl_blas.so.4 being used?  Anyone have any ideas?\r\n\r\n\r\nHere are the versions of everything:\r\n\r\n> ipex-llm==2.2.0b20241222\r\n> intel-oneapi-mkl-sycl 2025.0.1-14\r\n> intel-oneapi-mkl-sycl-2025.0 2025.0.1-14\r\n> intel-oneapi-mkl-sycl-blas 2025.0.1-14\r\n> intel-oneapi-mkl-sycl-blas-2025.0 2025.0.1-14\r\n> intel-oneapi-mkl-sycl-data-fitting-2025.0 2025.0.1-14\r\n> intel-oneapi-mkl-sycl-devel 2025.0.1-14\r\n> intel-oneapi-mkl-sycl-devel-2025.0 2025.0.1-14\r\n> intel-oneapi-mkl-sycl-dft-2025.0 2025.0.1-14\r\n> intel-oneapi-mkl-sycl-include-2025.0 2025.0.1-14\r\n> intel-oneapi-mkl-sycl-lapack-2025.0  2025.0.1-14\r\n> intel-oneapi-mkl-sycl-rng-2025.0 2025.0.1-14\r\n> intel-oneapi-mkl-sycl-sparse-2025.0 2025.0.1-14 \r\n> intel-oneapi-mkl-sycl-stats-2025.0 2025.0.1-14\r\n> intel-oneapi-mkl-sycl-vm 2025.0.1-14\r\n> intel-oneapi-mkl-sycl-vm-2025.0 2025.0.1-14\r\n> intel-oneapi-runtime-dpcpp-sycl-core 2025.0.4-1519\r\n> intel-oneapi-runtime-dpcpp-sycl-opencl-cpu 2025.0.4-1519",
      "state": "closed",
      "author": "briangmaddox",
      "author_type": "User",
      "created_at": "2024-12-22T19:43:44Z",
      "updated_at": "2025-03-10T11:30:18Z",
      "closed_at": "2024-12-27T16:35:09Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12594/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12594",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12594",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:15.978760",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "For linux user, ipex-llm[cpp] won't install oneapi to your conda env.\r\nPlease follow this quickstart https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/llama_cpp_quickstart.md#linux to install oneapi with apt. Then, create a new conda env and try again. ",
          "created_at": "2024-12-23T01:03:01Z"
        },
        {
          "author": "briangmaddox",
          "body": "Actually I did all of that before just didn’t want to have a ticket that was 20 pages long :). This was a clean install where I removed all of the previous ones oneapi packages.  As you can see several of the 2025 version libs are found but I’m assuming the missing older ones are from the 2024 versi",
          "created_at": "2024-12-23T01:29:38Z"
        },
        {
          "author": "qiuxin2012",
          "body": "> Actually I did all of that before just didn’t want to have a ticket that was 20 pages long :). This was a clean install where I removed all of the previous ones oneapi packages.  As you can see several of the 2025 version libs are found but I’m assuming the missing older ones are from the 2024 ver",
          "created_at": "2024-12-23T02:29:31Z"
        },
        {
          "author": "briangmaddox",
          "body": "Actually no I started on a clean setup.  I ran \r\n\r\n> sudo apt-get install intel-oneapi-runtime-dpcpp-cpp intel-oneapi-runtime-mkl intel-oneapi-common-oneapi-vars intel-oneapi-common-licensing intel-oneapi-common-vars intel-oneapi-dpcpp-ct intel-oneapi-mkl-cluster intel-oneapi-dal intel-oneapi-mkl-sy",
          "created_at": "2024-12-23T14:57:09Z"
        },
        {
          "author": "qiuxin2012",
          "body": "Our llama-cli is build with oneapi 2024.0, so you should install the same version. \r\n\r\nhttps://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/install_linux_gpu.md#install-oneapi shows you how to install:\r\n```\r\nsudo apt install intel-oneapi-common-vars=2024.0.0-49406 \\\r\n  intel-",
          "created_at": "2024-12-24T02:26:01Z"
        }
      ]
    },
    {
      "issue_number": 12471,
      "title": "Error: llama runner process has terminated: exit status 127",
      "body": "Hello.\r\nVery often lately I get this message on both WIndows and Ubuntu installations of IPEX-LLM using Ollama.\r\n\r\nEverything seems to work fine, the scripts are running without issues, but when you actually try to run a model you get this error:\r\n\r\n```\r\n(base) nikos@PC-9700:~$ ./ollama -v\r\nollama version is 0.3.6-ipexllm-20241116\r\n(base) nikos@PC-9700:~$ ./ollama run mistral\r\nError: llama runner process has terminated: exit status 127\r\n```\r\n\r\nSometimes I re-install the Ollama IPEX-LLM for Windows and Linux and it works, but now I'm not so lucky.\r\n\r\nAny reason why is this keep happening and a possible workaround ?\r\n\r\n",
      "state": "closed",
      "author": "NikosDi",
      "author_type": "User",
      "created_at": "2024-11-30T05:35:40Z",
      "updated_at": "2025-03-10T11:30:17Z",
      "closed_at": "2025-02-05T02:29:06Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12471/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12471",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12471",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:16.210263",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @NikosDi , could you please provide more detailed logs?",
          "created_at": "2024-12-02T01:42:28Z"
        },
        {
          "author": "NikosDi",
          "body": "Unfortunately there are no detailed logs.\r\n\r\nThis thing is just happens and when it's happening, I just get this message while everything looks find in the background.\r\n\r\nI have noticed that using Linux, it can happen after a system update.\r\n\r\nAnyway, I re-installed oneAPI and IPEX-LLM for Linux and",
          "created_at": "2024-12-02T03:10:07Z"
        },
        {
          "author": "Wuqiyang312",
          "body": "13:37\n[GIN] 2025/02/03 - 13:37:02 | 500 | 289.519868ms | 127.0.0.1 | POST \"/api/generate\"\nstart_ollama.sh\n13:37\ntime=2025-02-03T13:37:02.911+08:00 level=ERROR source=sched.go:455 msg=\"error loading llama server\" error=\"llama runner process has terminated: exit status 127\"\nstart_ollama.sh\n13:37\n**/tm",
          "created_at": "2025-02-03T05:40:42Z"
        },
        {
          "author": "Wuqiyang312",
          "body": "My problem has been fixed\n```\nexport LD_LIBRARY_PATH=$HOME/ollama:$LD_LIBRARY_PATH\n```",
          "created_at": "2025-02-03T06:22:30Z"
        }
      ]
    },
    {
      "issue_number": 12519,
      "title": "Using NPU inference model, how to set up a single process ",
      "body": "When using NPU to inference the MiniCPM_2_6 model, it was found that it divided python into four parallel processes for inferencing, which took up a lot of memory. \r\n![40d75b5789744ceb4f11a44ba4aef83d](https://github.com/user-attachments/assets/d600359f-5cdd-4efb-8cec-c12f635746c6)\r\n\r\nThe use of GPU inference is single-process inference,\r\n![1733810498139_D90AB50F-2BF5-4fc2-8A36-4723B9D72249](https://github.com/user-attachments/assets/cd71552e-eaaf-44c6-af93-56ee2bda7b96)\r\n\r\n in the demo coding did not find the relevant setting of single-process code, and how to set it?",
      "state": "open",
      "author": "Bofuser",
      "author_type": "User",
      "created_at": "2024-12-10T06:24:52Z",
      "updated_at": "2025-03-10T11:29:23Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12519/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Oscilloscope98"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12519",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12519",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:16.406809",
      "comments": [
        {
          "author": "Oscilloscope98",
          "body": "Hi @Bofuser,\r\n\r\nFor MiniCPM-V_2_6, we currently only supports running on NPU with multiple processes. We will update here for any updates if we further support this model running on a single process :)",
          "created_at": "2024-12-11T02:32:35Z"
        },
        {
          "author": "Bofuser",
          "body": "Got it, thanks for your reply!\r\n\r\n",
          "created_at": "2024-12-11T02:48:12Z"
        }
      ]
    },
    {
      "issue_number": 12448,
      "title": "Request to upgrade \"Langchain-Chatchat\" based on the latest version in github.",
      "body": "Given the extent to which \"Langchain-Chatchat\" has been upgraded to support more features, such as db, txt2img and multimodes etc., in this link - https://github.com/chatchat-space/Langchain-Chatchat\r\nHowever, it is not currently supported for operation on the Intel platform with IPEX-LLM. \r\nCould you please update the Intel version to the latest ChatChat release? \r\nCurrent Intel version： https://github.com/intel-analytics/Langchain-Chatchat ",
      "state": "open",
      "author": "liang1wang",
      "author_type": "User",
      "created_at": "2024-11-26T06:51:32Z",
      "updated_at": "2025-03-10T11:29:23Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12448/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Oscilloscope98"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12448",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12448",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:16.578841",
      "comments": [
        {
          "author": "Oscilloscope98",
          "body": "Hi @liang1wang,\r\n\r\nWe currently have no plans for an upgrade. We will let you know if we proceed with an upgrade in the future :)",
          "created_at": "2024-11-27T03:25:06Z"
        },
        {
          "author": "liang1wang",
          "body": "Got it, thanks!",
          "created_at": "2024-11-27T05:19:13Z"
        }
      ]
    },
    {
      "issue_number": 12440,
      "title": " Inference is exceptionally slow on the L20 GPU",
      "body": "<img width=\"1850\" alt=\"截屏2024-11-25 15 35 43\" src=\"https://github.com/user-attachments/assets/1e3cafe8-29b0-4302-be92-dc420b1418be\">\r\nspeed is 0.08tokens/sec\r\nand the gpu usage is extremely low:\r\n<img width=\"1609\" alt=\"截屏2024-11-25 14 24 49\" src=\"https://github.com/user-attachments/assets/a28334a5-f7cc-4d80-a85d-95c4f97d6b85\">\r\n\r\nsystem info:\r\ngpu: L20\r\ncuda: 12.2\r\npytorch: 2.5.1\r\ngraphics card driver version: 535.161.08\r\nvllm version: 0.6.4.post1\r\n\r\ninference script:\r\n```python\r\nfrom transformers import AutoTokenizer\r\nfrom vllm import LLM, SamplingParams\r\n\r\n# Initialize the tokenizer\r\ntokenizer = AutoTokenizer.from_pretrained(\"../Qwen2-Math-7B-Instruct\")\r\n\r\n# Pass the default decoding hyperparameters of Qwen2.5-7B-Instruct\r\n# max_tokens is for the maximum length for generation.\r\nsampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=512)\r\n\r\n# Input the model name or path. Can be GPTQ or AWQ models.\r\nllm = LLM(model=\"../Qwen2-Math-7B-Instruct\", enforce_eager=True)\r\n\r\n# Prepare your prompts\r\nprompt = \"Tell me something about large language models.\"\r\nmessages = [\r\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\r\n    {\"role\": \"user\", \"content\": prompt}\r\n]\r\ntext = tokenizer.apply_chat_template(\r\n    messages,\r\n    tokenize=False,\r\n    add_generation_prompt=True\r\n)\r\n\r\n# generate outputs\r\noutputs = llm.generate([text], sampling_params)\r\n\r\n# Print the outputs.\r\nfor output in outputs:\r\n    prompt = output.prompt\r\n    generated_text = output.outputs[0].text\r\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\r\n",
      "state": "open",
      "author": "joey9503",
      "author_type": "User",
      "created_at": "2024-11-25T08:03:48Z",
      "updated_at": "2025-03-10T11:29:22Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12440/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12440",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12440",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:16.803433",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "Thanks for your question. We don't support nvidia GPUs.",
          "created_at": "2024-11-26T02:18:32Z"
        }
      ]
    },
    {
      "issue_number": 12435,
      "title": "Kernel NULL pointer dereference in i915 driver",
      "body": "A kernel NULL pointer dereference has been observed in the i915 driver, causing a kernel oops. Details are as follows:\r\n\r\n- Kernel version: 6.5.0-35-generic 35~22.04.1-Ubuntu\r\n- Hardware: INTEL LC2580, BIOS 5.19 05/30/2024\r\n\r\nError message:\r\nNov 24 09:15:40 jammy kernel: BUG: kernel NULL pointer dereference, address: 00000000000000c8\r\nNov 24 09:15:40 jammy kernel: #PF: supervisor read access in kernel mode\r\nNov 24 09:15:40 jammy kernel: #PF: error_code(0x0000) - not-present page\r\n\r\ncall stack:\r\n[2024-11-23-2024-11-25.txt](https://github.com/user-attachments/files/17896315/2024-11-23-2024-11-25.txt)\r\n\r\ncpu info:\r\n```\r\nvendor_id       : GenuineIntel\r\ncpu family      : 6\r\nmodel           : 140\r\nmodel name      : 11th Gen Intel(R) Core(TM) i5-1155G7 @ 2.50GHz\r\nstepping        : 2\r\nmicrocode       : 0x38\r\ncpu MHz         : 400.000\r\ncache size      : 8192 KB\r\nphysical id     : 0\r\nsiblings        : 8\r\ncore id         : 3\r\ncpu cores       : 4\r\napicid          : 7\r\ninitial apicid  : 7\r\nfpu             : yes\r\nfpu_exception   : yes\r\ncpuid level     : 27\r\nwp              : yes\r\nflags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l2 invpcid_single cdp_l2 ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves split_lock_detect dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid movdiri movdir64b fsrm avx512_vp2intersect md_clear ibt flush_l1d arch_capabilities\r\nvmx flags       : vnmi preemption_timer posted_intr invvpid ept_x_only ept_ad ept_1gb flexpriority apicv tsc_offset vtpr mtf vapic ept vpid unrestricted_guest vapic_reg vid ple pml ept_mode_based_exec tsc_scaling\r\nbugs            : apic_c1e spectre_v1 spectre_v2 spec_store_bypass swapgs eibrs_pbrsb gds bhi\r\nbogomips        : 4992.00\r\nclflush size    : 64\r\ncache_alignment : 64\r\naddress sizes   : 39 bits physical, 48 bits virtual\r\npower management:\r\n\r\n```\r\nuname -a:\r\n```\r\nLinux jammy 6.5.0-35-generic #35~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue May  7 09:00:52 UTC 2 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n\r\nos release:\r\n```\r\nPRETTY_NAME=\"Ubuntu 22.04.5 LTS\"\r\nNAME=\"Ubuntu\"\r\nVERSION_ID=\"22.04\"\r\nVERSION=\"22.04.5 LTS (Jammy Jellyfish)\"\r\nVERSION_CODENAME=jammy\r\nID=ubuntu\r\nID_LIKE=debian\r\nHOME_URL=\"https://www.ubuntu.com/\"\r\nSUPPORT_URL=\"https://help.ubuntu.com/\"\r\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\r\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\r\nUBUNTU_CODENAME=jammy\r\n```\r\ndpkg --list\r\n[dpkg-list.txt](https://github.com/user-attachments/files/17896588/dpkg-list.txt)\r\n\r\nfree -h\r\n```\r\nfree -h\r\n               total        used        free      shared  buff/cache   available\r\nMem:            15Gi       810Mi       6.3Gi       1.0Mi       8.3Gi        14Gi\r\nSwap:          4.0Gi       0.0Ki       4.0Gi\r\n```\r\n\r\nconda list \r\n```\r\n_libgcc_mutex             0.1                 conda_forge    conda-forge\r\n_openmp_mutex             4.5                       2_gnu    conda-forge\r\naccelerate                0.23.0                   pypi_0    pypi\r\naiofiles                  23.2.1                   pypi_0    pypi\r\naiohappyeyeballs          2.4.3                    pypi_0    pypi\r\naiohttp                   3.11.7                   pypi_0    pypi\r\naiosignal                 1.3.1                    pypi_0    pypi\r\nannotated-types           0.7.0                    pypi_0    pypi\r\nanyio                     4.6.2.post1              pypi_0    pypi\r\narpeggio                  2.0.2                    pypi_0    pypi\r\nattrs                     24.2.0                   pypi_0    pypi\r\nbigdl-core-xe-21          2.1.0b2                  pypi_0    pypi\r\nbigdl-core-xe-addons-21   2.1.0b2                  pypi_0    pypi\r\nbigdl-core-xe-batch-21    2.1.0b2                  pypi_0    pypi\r\nbzip2                     1.0.8                h4bc722e_7    conda-forge\r\nca-certificates           2024.8.30            hbcca054_0    conda-forge\r\ncaliper-reader            0.4.1                    pypi_0    pypi\r\ncertifi                   2024.8.30                pypi_0    pypi\r\ncharset-normalizer        3.4.0                    pypi_0    pypi\r\nclick                     8.1.7                    pypi_0    pypi\r\ncloudpickle               3.1.0                    pypi_0    pypi\r\ncmake                     3.31.1                   pypi_0    pypi\r\ncontourpy                 1.3.1                    pypi_0    pypi\r\ncycler                    0.12.1                   pypi_0    pypi\r\ndatasets                  3.1.0                    pypi_0    pypi\r\ndill                      0.3.8                    pypi_0    pypi\r\ndiskcache                 5.6.3                    pypi_0    pypi\r\ndistro                    1.9.0                    pypi_0    pypi\r\nfastapi                   0.112.4                  pypi_0    pypi\r\nffmpy                     0.4.0                    pypi_0    pypi\r\nfilelock                  3.16.1                   pypi_0    pypi\r\nfonttools                 4.55.0                   pypi_0    pypi\r\nfrozenlist                1.5.0                    pypi_0    pypi\r\nfsspec                    2024.9.0                 pypi_0    pypi\r\ngradio                    4.43.0                   pypi_0    pypi\r\ngradio-client             1.3.0                    pypi_0    pypi\r\nh11                       0.14.0                   pypi_0    pypi\r\nhttpcore                  1.0.7                    pypi_0    pypi\r\nhttptools                 0.6.4                    pypi_0    pypi\r\nhttpx                     0.27.2                   pypi_0    pypi\r\nhuggingface-hub           0.26.2                   pypi_0    pypi\r\nidna                      3.10                     pypi_0    pypi\r\nimportlib-resources       6.4.5                    pypi_0    pypi\r\nintel-cmplr-lib-ur        2025.0.2                 pypi_0    pypi\r\nintel-extension-for-pytorch 2.1.10+xpu               pypi_0    pypi\r\nintel-openmp              2025.0.2                 pypi_0    pypi\r\ninteregular               0.3.3                    pypi_0    pypi\r\nipex-llm                  2.1.0b2                  pypi_0    pypi\r\njinja2                    3.1.4                    pypi_0    pypi\r\njiter                     0.7.1                    pypi_0    pypi\r\njsonschema                4.23.0                   pypi_0    pypi\r\njsonschema-specifications 2024.10.1                pypi_0    pypi\r\nkiwisolver                1.4.7                    pypi_0    pypi\r\nlark                      1.2.2                    pypi_0    pypi\r\nld_impl_linux-64          2.43                 h712a8e2_2    conda-forge\r\nlibexpat                  2.6.4                h5888daf_0    conda-forge\r\nlibffi                    3.4.2                h7f98852_5    conda-forge\r\nlibgcc                    14.2.0               h77fa898_1    conda-forge\r\nlibgcc-ng                 14.2.0               h69a702a_1    conda-forge\r\nlibgomp                   14.2.0               h77fa898_1    conda-forge\r\nlibnsl                    2.0.1                hd590300_0    conda-forge\r\nlibsqlite                 3.47.0               hadc24fc_1    conda-forge\r\nlibuuid                   2.38.1               h0b41bf4_0    conda-forge\r\nlibxcrypt                 4.4.36               hd590300_1    conda-forge\r\nlibzlib                   1.3.1                hb9d3cd8_2    conda-forge\r\nllnl-hatchet              2024.1.3                 pypi_0    pypi\r\nllvmlite                  0.43.0                   pypi_0    pypi\r\nlm-format-enforcer        0.10.3                   pypi_0    pypi\r\nmarkdown-it-py            3.0.0                    pypi_0    pypi\r\nmarkupsafe                2.1.5                    pypi_0    pypi\r\nmatplotlib                3.9.2                    pypi_0    pypi\r\nmdurl                     0.1.2                    pypi_0    pypi\r\nmpi4py                    4.0.1                    pypi_0    pypi\r\nmpmath                    1.3.0                    pypi_0    pypi\r\nmsgpack                   1.1.0                    pypi_0    pypi\r\nmultidict                 6.1.0                    pypi_0    pypi\r\nmultiprocess              0.70.16                  pypi_0    pypi\r\nncurses                   6.5                  he02047a_1    conda-forge\r\nnest-asyncio              1.6.0                    pypi_0    pypi\r\nnetworkx                  3.4.2                    pypi_0    pypi\r\nninja                     1.11.1.2                 pypi_0    pypi\r\nnumba                     0.60.0                   pypi_0    pypi\r\nnumpy                     1.26.4                   pypi_0    pypi\r\noneccl-bind-pt            2.1.300+xpu              pypi_0    pypi\r\nopenai                    1.55.0                   pypi_0    pypi\r\nopenssl                   3.4.0                hb9d3cd8_0    conda-forge\r\norjson                    3.10.12                  pypi_0    pypi\r\noutlines                  0.0.46                   pypi_0    pypi\r\npackaging                 24.2                     pypi_0    pypi\r\npandas                    2.2.3                    pypi_0    pypi\r\npillow                    10.4.0                   pypi_0    pypi\r\npip                       24.3.1             pyh8b19718_0    conda-forge\r\nprometheus-client         0.21.0                   pypi_0    pypi\r\nprometheus-fastapi-instrumentator 7.0.0                    pypi_0    pypi\r\npropcache                 0.2.0                    pypi_0    pypi\r\nprotobuf                  5.29.0rc3                pypi_0    pypi\r\npsutil                    6.1.0                    pypi_0    pypi\r\npy-cpuinfo                9.0.0                    pypi_0    pypi\r\npyairports                2.1.1                    pypi_0    pypi\r\npyarrow                   18.0.0                   pypi_0    pypi\r\npycountry                 24.6.1                   pypi_0    pypi\r\npydantic                  2.10.1                   pypi_0    pypi\r\npydantic-core             2.27.1                   pypi_0    pypi\r\npydot                     3.0.2                    pypi_0    pypi\r\npydub                     0.25.1                   pypi_0    pypi\r\npygments                  2.18.0                   pypi_0    pypi\r\npyparsing                 3.2.0                    pypi_0    pypi\r\npython                    3.11.10         hc5c86c4_3_cpython    conda-forge\r\npython-dateutil           2.9.0.post0              pypi_0    pypi\r\npython-dotenv             1.0.1                    pypi_0    pypi\r\npython-multipart          0.0.17                   pypi_0    pypi\r\npytz                      2024.2                   pypi_0    pypi\r\npyyaml                    6.0.2                    pypi_0    pypi\r\npyzmq                     26.2.0                   pypi_0    pypi\r\nray                       2.39.0                   pypi_0    pypi\r\nreadline                  8.2                  h8228510_1    conda-forge\r\nreferencing               0.35.1                   pypi_0    pypi\r\nregex                     2024.11.6                pypi_0    pypi\r\nrequests                  2.32.3                   pypi_0    pypi\r\nrich                      13.9.4                   pypi_0    pypi\r\nrpds-py                   0.21.0                   pypi_0    pypi\r\nruff                      0.8.0                    pypi_0    pypi\r\nsafetensors               0.4.6.dev0               pypi_0    pypi\r\nsemantic-version          2.10.0                   pypi_0    pypi\r\nsentencepiece             0.2.0                    pypi_0    pypi\r\nsetuptools                69.5.1                   pypi_0    pypi\r\nshellingham               1.5.4                    pypi_0    pypi\r\nsix                       1.16.0                   pypi_0    pypi\r\nsniffio                   1.3.1                    pypi_0    pypi\r\nstarlette                 0.38.6                   pypi_0    pypi\r\nsympy                     1.13.3                   pypi_0    pypi\r\ntabulate                  0.9.0                    pypi_0    pypi\r\ntcmlib                    1.2.0                    pypi_0    pypi\r\ntextx                     4.1.0                    pypi_0    pypi\r\ntiktoken                  0.8.0                    pypi_0    pypi\r\ntk                        8.6.13          noxft_h4845f30_101    conda-forge\r\ntokenizers                0.20.3                   pypi_0    pypi\r\ntomlkit                   0.12.0                   pypi_0    pypi\r\ntorch                     2.1.0a0+cxx11.abi          pypi_0    pypi\r\ntorchvision               0.16.0a0+cxx11.abi          pypi_0    pypi\r\ntqdm                      4.67.0                   pypi_0    pypi\r\ntransformers              4.46.3                   pypi_0    pypi\r\ntriton-xpu                3.0.0b2                  pypi_0    pypi\r\ntyper                     0.13.1                   pypi_0    pypi\r\ntyping-extensions         4.12.2                   pypi_0    pypi\r\ntzdata                    2024.2                   pypi_0    pypi\r\numf                       0.9.1                    pypi_0    pypi\r\nurllib3                   2.2.3                    pypi_0    pypi\r\nuvicorn                   0.32.1                   pypi_0    pypi\r\nuvloop                    0.21.0                   pypi_0    pypi\r\nvllm                      0.5.4+xpu                pypi_0    pypi\r\nwatchfiles                0.24.0                   pypi_0    pypi\r\nwebsockets                12.0                     pypi_0    pypi\r\nwheel                     0.45.1             pyhd8ed1ab_0    conda-forge\r\nxxhash                    3.5.0                    pypi_0    pypi\r\nxz                        5.2.6                h166bdaf_0    conda-forge\r\nyarl                      1.18.0                   pypi_0    pypi\r\n\r\n```\r\n\r\noneAPI env:\r\n```\r\nsource /opt/intel/oneapi/setvars.sh\r\nexport SYCL_CACHE_PERSISTENT=1\r\n \r\n:: initializing oneAPI environment ...\r\n   -bash: BASH_VERSION = 5.1.16(1)-release\r\n   args: Using \"$@\" for setvars.sh arguments: \r\n:: ccl -- latest\r\n:: compiler -- latest\r\n:: dal -- latest\r\n:: debugger -- latest\r\n:: dev-utilities -- latest\r\n:: dnnl -- latest\r\n:: dpcpp-ct -- latest\r\n:: dpl -- latest\r\n:: ipp -- latest\r\n:: ippcp -- latest\r\n:: mkl -- latest\r\n:: mpi -- latest\r\n:: tbb -- latest\r\n:: oneAPI environment initialized ::\r\n\r\n```",
      "state": "open",
      "author": "luhuaei",
      "author_type": "User",
      "created_at": "2024-11-25T02:30:36Z",
      "updated_at": "2025-03-10T11:29:22Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12435/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12435",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12435",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:17.022018",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "\r\nCould give me your output of `clinfo | grep \"Device Name\"`?\r\nSee https://github.com/intel-analytics/ipex-llm/blob/0e23bd779f043145710f46b400555a3beff07a04/docs/mddocs/Quickstart/install_linux_gpu.md#5-configure-permmision-and-verify-gpu-driver-setup for details.",
          "created_at": "2024-11-26T02:22:02Z"
        },
        {
          "author": "luhuaei",
          "body": "clinfo | grep \"Device Name\"\r\n```\r\n  Device Name                                     Intel(R) Iris(R) Xe Graphics\r\n    Device Name                                   Intel(R) Iris(R) Xe Graphics\r\n    Device Name                                   Intel(R) Iris(R) Xe Graphics\r\n    Device Name           ",
          "created_at": "2024-11-28T03:15:23Z"
        },
        {
          "author": "qiuxin2012",
          "body": "I find below errors in your [2024-11-23-2024-11-25.txt](https://github.com/user-attachments/files/17896315/2024-11-23-2024-11-25.txt)\r\n```\r\nNov 23 17:48:49 jammy kernel: i915_compat: loading out-of-tree module taints kernel.\r\nNov 23 17:48:49 jammy kernel: i915_compat: module verification failed: sig",
          "created_at": "2024-12-02T00:36:40Z"
        }
      ]
    },
    {
      "issue_number": 12707,
      "title": "Error of running llama3.2-vision with the ipex-llm's built-in ollama.",
      "body": "I followed the steps in #https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.md to set up ollama.  I can successfully run llama3.2 and qwen. However, when I use `ollama run llama3.2-vision`, I get an error:\r\nGGML_ASSERT(ggml_nelements(a) == ne0*ne1*ne2) failed\r\ntime=2025-01-15T10:43:34.636+08:00 level=INFO source=server.go:614 msg=\"waiting for server to become available\" status=\"llm server error\"\r\ntime=2025-01-15T10:43:34.887+08:00 level=ERROR source=sched.go:455 msg=\"error loading llama server\" error=\"llama runner process has terminated: GGML_ASSERT(ggml_nelements(a) == ne0*ne1*ne2) failed\"",
      "state": "open",
      "author": "1ngram433",
      "author_type": "User",
      "created_at": "2025-01-15T02:55:33Z",
      "updated_at": "2025-03-10T11:28:57Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12707/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12707",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12707",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:17.244460",
      "comments": [
        {
          "author": "1ngram433",
          "body": "The following is the complete and detailed error message:\r\n```\r\ntime=2025-01-15T10:43:29.059+08:00 level=INFO source=gpu.go:221 msg=\"looking for compatible GPUs\"\r\ntime=2025-01-15T10:43:29.059+08:00 level=INFO source=gpu_windows.go:167 msg=packages count=1\r\ntime=2025-01-15T10:43:29.059+08:00 level=IN",
          "created_at": "2025-01-15T02:56:23Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @1ngram433,  what's your ollama version? And could you pls provide the specific information of your device?",
          "created_at": "2025-01-16T01:58:41Z"
        },
        {
          "author": "1ngram433",
          "body": "Thank you, my ollama version is 0.5.1-ipexllm-20250114, and ipex-llm version is 2.2.0b20250114\r\n\r\nAMD Ryzen 5 5500\r\nGPU Intel Arc 750 Graphics\r\nRAM 32GB",
          "created_at": "2025-01-16T02:06:55Z"
        },
        {
          "author": "sgwhat",
          "body": "Well, I cannot reproduce your issue. I suspect it is related to the use of an AMD CPU on your device.",
          "created_at": "2025-01-16T06:47:49Z"
        }
      ]
    },
    {
      "issue_number": 12702,
      "title": "[bug] ollama returns garbage for longer texts",
      "body": "I have installed ollama on a system with an intel arc a770 and loaded llama3.2:3b.\r\nThe initial loading of the model takes a long time, but it works.\r\nInitial requests are successfully answered with ~1000t/s. As the chat continues, things get a bit weird. In the middle of a story, the text turned into javascript and then into pure garbage.\r\n<details>\r\n<summary>screenshot</summary>\r\n\r\n![grafik](https://github.com/user-attachments/assets/5edd3338-fc83-4c5b-a110-ade090a167da)\r\n\r\n</details>\r\n\r\n\r\nThats the deployment I used.\r\n```yaml\r\n---\r\napiVersion: v1\r\nkind: ConfigMap\r\nmetadata:\r\n  name: open-webui-config\r\n  namespace: ollama\r\ndata:\r\n  OLLAMA_BASE_URL: \"http://ollama:11434\"\r\n---\r\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: ollama\r\n  namespace: ollama\r\nspec:\r\n  replicas: 1\r\n  selector:\r\n    matchLabels:\r\n      app: ollama\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: ollama\r\n    spec:\r\n      containers:\r\n        - name: ollama\r\n          image: intelanalytics/ipex-llm-inference-cpp-xpu:2.2.0-SNAPSHOT\r\n          env:\r\n            - name: OLLAMA_HOST\r\n              value: \"0.0.0.0:11434\"\r\n            - name: ZES_ENABLE_SYSMAN\r\n              value: \"1\"\r\n            - name: OLLAMA_INTEL_GPU\r\n              value: \"true\"\r\n          command:\r\n            - /bin/sh\r\n            - -c\r\n            - |\r\n              mkdir -p /llm/ollama\r\n              cd /llm/ollama\r\n              init-ollama\r\n              ./ollama serve\r\n          ports:\r\n            - containerPort: 11434\r\n          securityContext:\r\n            privileged: true\r\n          volumeMounts:\r\n            - mountPath: /root/.ollama\r\n              name: ollama-data\r\n          resources:\r\n            requests:\r\n              memory: \"4096Mi\"\r\n              cpu: \"1\"\r\n            limits:\r\n              cpu: \"4\"\r\n              memory: \"8192Mi\"\r\n      volumes:\r\n        - name: ollama-data\r\n          persistentVolumeClaim:\r\n            claimName: ollama-data\r\n```\r\n\r\nLogs:\r\n```yaml\r\nfound 1 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|                Intel Arc A770 Graphics|    1.6|    512|    1024|   32| 16225M|            1.3.31294|\r\nllama_kv_cache_init:      SYCL0 KV buffer size =   896.00 MiB\r\nllama_new_context_with_model: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB\r\nllama_new_context_with_model:  SYCL_Host  output buffer size =     2.00 MiB\r\nllama_new_context_with_model:      SYCL0 compute buffer size =   256.50 MiB\r\nllama_new_context_with_model:  SYCL_Host compute buffer size =    22.01 MiB\r\nllama_new_context_with_model: graph nodes  = 790\r\nllama_new_context_with_model: graph splits = 2\r\ntime=2025-01-12T19:31:36.457+08:00 level=WARN source=runner.go:894 msg=\"%s: warming up the model with an empty run - please wait ... \" !BADKEY=loadModel\r\ntime=2025-01-12T19:31:45.794+08:00 level=INFO source=server.go:619 msg=\"llama runner started in 11.28 seconds\"\r\n```\r\n\r\nLinking:\r\n- https://github.com/ollama/ollama/issues/1590",
      "state": "open",
      "author": "frzifus",
      "author_type": "User",
      "created_at": "2025-01-12T11:49:40Z",
      "updated_at": "2025-03-10T11:28:56Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12702/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat",
        "ACupofAir"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12702",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12702",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:17.486145",
      "comments": [
        {
          "author": "ACupofAir",
          "body": "The problem cannot be reproduced, and the output is still normal after trying multiple rounds of sessions. ",
          "created_at": "2025-01-14T01:22:06Z"
        },
        {
          "author": "frzifus",
          "body": "mh.. Let me try again and come back to you.",
          "created_at": "2025-01-14T09:23:39Z"
        },
        {
          "author": "frzifus",
          "body": "~It worked until this log line occured:~\n```\ntime=2025-01-17T09:34:37.836+08:00 level=WARN source=runner.go:129 msg=\"truncating input prompt\" limit=2048 prompt=2175 keep=5 new=2048\n```\n\n**Update**\n\n~It seems to have nothing to do with the previously listed log line.~\n~The next test did not show anyt",
          "created_at": "2025-01-17T01:41:44Z"
        }
      ]
    },
    {
      "issue_number": 12701,
      "title": " Error: No device of requested type available",
      "body": "run:  ./ollama run qwen2.5-coder:0.5b\r\n\r\nError: llama runner process has terminated: error loading model: No device of requested type available. Please check https://software.intel.com/content/www/us/en/develop/articles/intel-oneapi-dpcpp-system-requirements.html -1 (PI_ERROR_DEVICE_NOT_FOUND)\r\n\r\n- ollma version:\r\nollama version is 0.4.6-ipexllm-20250105\r\n- ipex-llm[cpp] version\r\n```shell\r\nPackage                 Version\r\n----------------------- --------------\r\naccelerate              0.33.0\r\nbigdl-core-cpp          2.6.0b20250105\r\ncertifi                 2024.12.14\r\ncharset-normalizer      3.4.1\r\ncolorama                0.4.6\r\ndpcpp-cpp-rt            2024.2.1\r\nfilelock                3.16.1\r\nfsspec                  2024.12.0\r\ngguf                    0.14.0\r\nhuggingface-hub         0.27.1\r\nidna                    3.10\r\nimpi-rt                 2021.14.1\r\nintel-cmplr-lib-rt      2024.2.1\r\nintel-cmplr-lib-ur      2024.2.1\r\nintel-cmplr-lic-rt      2024.2.1\r\nintel-opencl-rt         2024.2.1\r\nintel-openmp            2024.2.1\r\nintel-sycl-rt           2024.2.1\r\nipex-llm                2.2.0b20250105\r\nJinja2                  3.1.5\r\nMarkupSafe              3.0.2\r\nmkl                     2024.2.1\r\nmkl-dpcpp               2024.2.1\r\nmpmath                  1.3.0\r\nnetworkx                3.4.2\r\nnumpy                   1.26.4\r\nonednn                  2024.2.1\r\nonednn-devel            2024.2.1\r\nonemkl-sycl-blas        2024.2.1\r\nonemkl-sycl-datafitting 2024.2.1\r\nonemkl-sycl-dft         2024.2.1\r\nonemkl-sycl-lapack      2024.2.1\r\nonemkl-sycl-rng         2024.2.1\r\nonemkl-sycl-sparse      2024.2.1\r\nonemkl-sycl-stats       2024.2.1\r\nonemkl-sycl-vm          2024.2.1\r\npackaging               24.2\r\npip                     24.2\r\nprotobuf                4.25.5\r\npsutil                  6.1.1\r\nPyYAML                  6.0.2\r\nregex                   2024.11.6\r\nrequests                2.32.3\r\nsafetensors             0.5.2\r\nsentencepiece           0.1.99\r\nsetuptools              75.1.0\r\nsympy                   1.13.3\r\ntbb                     2021.13.1\r\ntcmlib                  1.2.0\r\ntokenizers              0.19.1\r\ntorch                   2.2.0\r\ntqdm                    4.67.1\r\ntransformers            4.44.2\r\ntyping_extensions       4.12.2\r\numf                     0.9.1\r\nurllib3                 2.3.0\r\nwheel                   0.44.0\r\n```\r\nsystem info:\r\n```plain text\r\nwindows\r\n设备名称\tDESKTOP-E86TOAU\r\n处理器\tIntel(R) Core(TM) Ultra 5 125H   3.60 GHz\r\n机带 RAM\t16.0 GB (15.6 GB 可用)\r\n设备 ID\t6CC66654-D557-4672-8A6D-1B4B12C6109E\r\n产品 ID\t00326-70000-00001-AA517\r\n系统类型\t64 位操作系统, 基于 x64 的处理器\r\n```\r\ngpu info:\r\n- Intel(R) Arc(TM) Graphics\r\n- GPU驱动程序版本: 32.0.101.6449\r\n\r\nIntel° oneAPl Base Toolkit Info\r\n- C:\\Program Files (x86)\\Intel\\oneAPI\\2024.2\r\n- C:\\Program Files (x86)\\Intel\\oneAPI\\2025.0",
      "state": "open",
      "author": "Wait1997",
      "author_type": "User",
      "created_at": "2025-01-12T06:58:41Z",
      "updated_at": "2025-03-10T11:28:56Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12701/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12701",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12701",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:17.718415",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @Wait1997, I believe Ollama is unable to run on your device due to multiple versions (2025.0) of oneAPI being installed simultaneously. Please refer to our [ollama quickstart](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.md) and [llama.cpp quickst",
          "created_at": "2025-01-13T02:24:39Z"
        },
        {
          "author": "Wait1997",
          "body": "Hi @sgwhat，I uninstalled versions (2025.0) of oneAPI as you said and use 2024.02 versions, but it seems I still encounter the same problem\r\n> Error: llama runner process has terminated: error loading model: No device of requested type available. Please check https://software.intel.com/content/www/us",
          "created_at": "2025-01-13T06:36:56Z"
        },
        {
          "author": "sgwhat",
          "body": "Please ensure that you have only one ollama serve process running. You can run `set OLLAMA_HOST=0.0.0.0` in the current directory.",
          "created_at": "2025-01-13T06:41:15Z"
        },
        {
          "author": "Wait1997",
          "body": "Thanks, but it still doesn't seem to work.",
          "created_at": "2025-01-13T07:01:47Z"
        },
        {
          "author": "sgwhat",
          "body": "I am a bit confused 😂, could you please provide more detailed logs?",
          "created_at": "2025-01-13T07:07:04Z"
        }
      ]
    },
    {
      "issue_number": 12690,
      "title": "[NPU] BUG - Failed to run glm-edge-1.5b-chat & glm-edge-4b-chat",
      "body": "Not able to run glm.py following the example in NPU directory when loading the model directly through repo-id\r\n![glm-edge-error](https://github.com/user-attachments/assets/2218219a-7fb6-4f75-959c-0a62da3b50d7)\r\n\r\n\r\n",
      "state": "open",
      "author": "climh",
      "author_type": "User",
      "created_at": "2025-01-10T00:38:38Z",
      "updated_at": "2025-03-10T11:28:55Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12690/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "plusbang"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12690",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12690",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:17.954884",
      "comments": [
        {
          "author": "plusbang",
          "body": "We are working on reproducing the issue and fix it. If there is any progress, will update here to let you know.\r\n\r\nAnd maybe you could try to load the model **from local path** to run the example.",
          "created_at": "2025-01-10T02:18:22Z"
        },
        {
          "author": "plusbang",
          "body": "Hi, @climh \r\nThis issue is fixed in https://github.com/intel-analytics/ipex-llm/pull/12698. Please try `ipex-llm` later than `2.2.0b20250110` to load glm-edge model directly through repo-id.",
          "created_at": "2025-01-10T08:11:22Z"
        }
      ]
    },
    {
      "issue_number": 12675,
      "title": "[ipex-llm[cpp]][ollama] low performance and gpu usage when running minicpm3-4B model",
      "body": "I'm trying to running miniCPM3-4B:Q4_K_M model with ollama 0.5.1-ipex-llm-20250107 on intel MTL iGpu and Arc A770. \r\n\r\n- When running the model on MTL igpu, there is no GPU compute usage with very high VRAM  occupation\r\n\r\nbefore load the model into VRAM, the VRAM usage is 2.4GB\r\n![Taskmgr_7pIqPjiV6W](https://github.com/user-attachments/assets/f312e870-bd5e-44ac-bce4-c10753ffa0e6)\r\n\r\nwhen running the model, the compute usage is almost 0 and VRAM usage is 8.2GB(this 4B model is using VRAM higher than GLM4:9B)\r\n![explorer_2f7Lurdzar](https://github.com/user-attachments/assets/e1c5595b-4556-4427-813a-f2d72c7dd647)\r\n\r\n- Runing the model on Arc A770\r\n\r\nbefore load the model , the VRAM usage is 1.1GB\r\n\r\nwhen runing the modek ,the compute usage is around 43% and VRAM usage is 9.5GB\r\nrunning with \"ollama run gfunsai/minicpm3-4b:q4_k_m\" command\r\n<img width=\"1715\" alt=\"explorer_lMZUq1Spam\" src=\"https://github.com/user-attachments/assets/3321dd65-931c-4b3f-91c3-e6b4af86d91c\" />\r\n\r\nrunning with curl command\r\n<img width=\"1712\" alt=\"explorer_YWjFlah2Zy\" src=\"https://github.com/user-attachments/assets/500d9a02-4213-4462-98a9-1bb4cbeda341\" />\r\n\r\n\r\nThe output token per second performance is significant dropped when doing inference on both GPUs.\r\n\r\nPlease help fix this issue :)",
      "state": "open",
      "author": "jianjungu",
      "author_type": "User",
      "created_at": "2025-01-08T11:02:24Z",
      "updated_at": "2025-03-10T11:28:55Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12675/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12675",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12675",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:18.252868",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @jianjungu, we have reproduced this issue and are working on fixing it :)",
          "created_at": "2025-01-09T06:31:33Z"
        }
      ]
    },
    {
      "issue_number": 12662,
      "title": "Are there some plane to support Qwen2-VL for lnl's npu?",
      "body": null,
      "state": "open",
      "author": "shichang00",
      "author_type": "User",
      "created_at": "2025-01-07T07:03:05Z",
      "updated_at": "2025-03-10T11:28:54Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12662/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12662",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12662",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:18.451790",
      "comments": [
        {
          "author": "jdjs20210918",
          "body": "and deepseek v3?",
          "created_at": "2025-01-07T08:00:31Z"
        }
      ]
    },
    {
      "issue_number": 12651,
      "title": "Why i cannot use my gpu",
      "body": "i created conda env llm-cpp\r\nand then\r\nconda activate llm-cpp \r\ninit-ollama.bat\r\n$env:no_proxy = \"localhost,127.0.0.1\"; $env:ZES_ENABLE_SYSMAN = \"1\"; $env:OLLAMA_NUM_GPU = \"999\" ;$env:SYCL_CACHE_PERSISTENT = \"1\"; $env:SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS = \"1\" \r\nand then \r\n.\\ollama.exe serve\r\n\r\n![image](https://github.com/user-attachments/assets/834c923e-aa30-4870-a8e8-c20b6141e4da)\r\n\r\nas we can see, it doesn't match the situation in the link\r\n\r\nCPU: intel ultra7 258v\r\nGPU: intergated Arc 140v \r\n\r\nlink:\r\nhttps://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.md",
      "state": "open",
      "author": "jackphj",
      "author_type": "User",
      "created_at": "2025-01-04T09:36:04Z",
      "updated_at": "2025-03-10T11:28:54Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12651/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12651",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12651",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:18.674687",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @jackphj, \r\n\r\nMay I ask why you think ollama isn't running on a GPU. Have you tested running a model with ollama? If you have more questions, please provide the complete log from ollama server side.",
          "created_at": "2025-01-06T01:27:20Z"
        },
        {
          "author": "jackphj",
          "body": "> Hi @jackphj,\r\n> \r\n> May I ask why you think ollama isn't running on a GPU. Have you tested running a model with ollama? If you have more questions, please provide the complete log from ollama server side.请问您为什么认为 ollama 不在 GPU 上运行。您是否测试过使用 ollama 运行模型？如果您还有其他问题，请提供来自 ollama 服务器端的完整日志。\r\n\r\nBecause I",
          "created_at": "2025-01-06T02:41:13Z"
        },
        {
          "author": "sgwhat",
          "body": "> Because I found that the log in the picture above says OLLAMA_INTEL_GPU:FALSE.\r\n\r\nSorry that's a confusing and useless log, it's meaningless, I will delete it later. You may run your model via `ollama run <models>` to check.\r\n",
          "created_at": "2025-01-06T08:54:37Z"
        }
      ]
    },
    {
      "issue_number": 12646,
      "title": "[LNL][Cogagent] RuntimeError: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 0",
      "body": "Model：https://huggingface.co/THUDM/cogagent-9b-20241220\r\nCogAgent-9B-20241220 model is based on [GLM-4V-9B](https://huggingface.co/THUDM/glm-4v-9b)， but i fail  to run this CogAgent-9B-20241220\r\n\r\nSetup guide follows： https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/GPU/HuggingFace/Multimodal/glm-4v\r\nenv: \r\nipex-llm                    2.2.0b20250102\r\ntransformers             tried both   4.42.4 & 4.47.1\r\n\r\nFailure as below:\r\nTraceback (most recent call last):\r\n  File \"D:\\cogagent\\generate.py\", line 75, in <module>\r\n    inputs = tokenizer.apply_chat_template([{\"role\": \"user\", \"image\": image, \"content\": query}],\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\test\\.cache\\huggingface\\modules\\transformers_modules\\cogagent-9b-20241220\\tokenization_chatglm.py\", line 232, in apply_chat_template\r\n    result = handle_single_conversation(conversation)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\test\\.cache\\huggingface\\modules\\transformers_modules\\cogagent-9b-20241220\\tokenization_chatglm.py\", line 200, in handle_single_conversation\r\n    input_image = transform(item[\"image\"])\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\test\\miniforge3\\envs\\cogagent\\Lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 95, in __call__\r\n    img = t(img)\r\n          ^^^^^^\r\n  File \"C:\\Users\\test\\miniforge3\\envs\\cogagent\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\test\\miniforge3\\envs\\cogagent\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\test\\miniforge3\\envs\\cogagent\\Lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 277, in forward\r\n    return F.normalize(tensor, self.mean, self.std, self.inplace)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\test\\miniforge3\\envs\\cogagent\\Lib\\site-packages\\torchvision\\transforms\\functional.py\", line 350, in normalize\r\n    return F_t.normalize(tensor, mean=mean, std=std, inplace=inplace)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\test\\miniforge3\\envs\\cogagent\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py\", line 926, in normalize\r\n    return tensor.sub_(mean).div_(std)\r\n           ^^^^^^^^^^^^^^^^^\r\nRuntimeError: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 0\r\n\r\n\r\n",
      "state": "open",
      "author": "juan-OY",
      "author_type": "User",
      "created_at": "2025-01-03T07:12:58Z",
      "updated_at": "2025-03-10T11:28:53Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12646/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "lzivan"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12646",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12646",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:18.862129",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "The cogagent's prompt concatenation has strict limits, our glm-4v-9B's example can't meet their requirement. Have you changed generate.py followed by [cogagent's requirements](https://huggingface.co/THUDM/cogagent-9b-20241220#run-the-model)? \r\nYou can also reference their example https://github.com/",
          "created_at": "2025-01-06T00:55:23Z"
        },
        {
          "author": "juan-OY",
          "body": "It is not with the format, it also fails with running https://github.com/THUDM/CogAgent/blob/main/inference/cli_demo.py or web_demo.py, it fails with the vision part.",
          "created_at": "2025-01-06T01:11:41Z"
        },
        {
          "author": "juan-OY",
          "body": "It reports error as (web_demo.py):\r\n\r\n  File \"C:\\Users\\test\\.cache\\huggingface\\modules\\transformers_modules\\cogagent-9b-20241220\\visual.py\", line 193, in forward\r\n    x = x.view(b, grid_size, grid_size, h).permute(0, 3, 1, 2)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: shape '[1, 80, ",
          "created_at": "2025-01-06T01:34:58Z"
        }
      ]
    },
    {
      "issue_number": 12608,
      "title": "loading model from glm-4v crashed",
      "body": "(glm-4v) PS D:\\dev\\models> python D:\\dev\\models\\vllm-ipex-server\\glm-4v.py\r\nC:\\ProgramData\\miniconda3\\envs\\glm-4v\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\ProgramData\\miniconda3\\envs\\glm-4v\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n  warn(\r\n2024-12-25 12:12:09,139 - INFO - intel_extension_for_pytorch auto imported\r\n2024-12-25 12:12:11,063 - modelscope - INFO - PyTorch version 2.1.0a0+cxx11.abi Found.\r\n2024-12-25 12:12:11,064 - modelscope - INFO - Loading ast index from C:\\Users\\sisu\\.cache\\modelscope\\ast_indexer\r\n2024-12-25 12:12:11,223 - modelscope - INFO - Loading done! Current index file version is 1.11.0, with md5 061b1804e8e732cc7bbff4121137b87c and a total number of 953 components indexed\r\nLoading checkpoint shards:  67%|███████████████████████████████████████████████████████████▎                             | 10/15 [00:01<00:00,  7.34it/s]\r\n(glm-4v) PS D:\\dev\\models> \r\nplatform: win11 dGPU: A770",
      "state": "open",
      "author": "zalsay",
      "author_type": "User",
      "created_at": "2024-12-25T04:15:18Z",
      "updated_at": "2025-03-10T11:28:53Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12608/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "glorysdj",
        "hzjane"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12608",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12608",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:19.051347",
      "comments": [
        {
          "author": "zalsay",
          "body": "code:\r\nmodel_path = \"ZhipuAI/glm-4v-9b\"\r\nmodel = AutoModel.from_pretrained(model_path, \r\n                                load_in_4bit=True,\r\n                                optimize_model=True,\r\n                                trust_remote_code=True,\r\n                                use_cache=True,\r",
          "created_at": "2024-12-25T04:17:14Z"
        },
        {
          "author": "zalsay",
          "body": "in the model's config.json, I added \"image_size\": 1120,",
          "created_at": "2024-12-25T04:22:36Z"
        },
        {
          "author": "hzjane",
          "body": "Hi, You can follow [this guide](https://github.com/intel-analytics/ipex-llm/tree/9e895f04ecbed45858e87a76323e513b1d231005/python/llm/example/GPU/HuggingFace/Multimodal/glm-4v) to check whether your environment and code are correct.",
          "created_at": "2024-12-26T02:09:06Z"
        }
      ]
    },
    {
      "issue_number": 12598,
      "title": "RuntimeError: Unable to run llama3.2 on ipex-llm[cpp]",
      "body": "I'm trying to run `ollama` on an integrated GPU of `Intel i5-1240P` processor. I followed this [doc](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.md).\r\n\r\nEverything is installed okay, however, when I try to run the model, it crashes at runtime.\r\n\r\nAttaching the error details:\r\n<details><summary>Details</summary>\r\n<p>\r\n\r\n```bash\r\n[GIN] 2024/12/23 - 19:55:38 | 200 |      19.579µs |       127.0.0.1 | HEAD     \"/\"\r\n[GIN] 2024/12/23 - 19:55:38 | 200 |   13.552991ms |       127.0.0.1 | POST     \"/api/show\"\r\ntime=2024-12-23T19:55:38.186Z level=INFO source=server.go:105 msg=\"system memory\" total=\"16.0 GiB\" free=\"15.2 GiB\" free_swap=\"8.0 GiB\"\r\ntime=2024-12-23T19:55:38.186Z level=INFO source=memory.go:356 msg=\"offload to device\" layers.requested=-1 layers.model=29 layers.offload=0 layers.split=\"\" memory.available=\"[15.2 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"3.3 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"896.0 MiB\" memory.required.allocations=\"[3.3 GiB]\" memory.weights.total=\"2.4 GiB\" memory.weights.repeating=\"2.1 GiB\" memory.weights.nonrepeating=\"308.2 MiB\" memory.graph.full=\"424.0 MiB\" memory.graph.partial=\"570.7 MiB\"\r\ntime=2024-12-23T19:55:38.187Z level=INFO source=server.go:401 msg=\"starting llama server\" cmd=\"/tmp/ollama1778822854/runners/ipex_llm/ollama_llama_server --model /root/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 8192 --batch-size 512 --n-gpu-layers 999 --threads 4 --no-mmap --parallel 4 --port 46365\"\r\ntime=2024-12-23T19:55:38.191Z level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\r\ntime=2024-12-23T19:55:38.191Z level=INFO source=server.go:580 msg=\"waiting for llama runner to start responding\"\r\ntime=2024-12-23T19:55:38.191Z level=INFO source=server.go:614 msg=\"waiting for server to become available\" status=\"llm server error\"\r\ntime=2024-12-23T19:55:38.221Z level=INFO source=runner.go:941 msg=\"starting go runner\"\r\ntime=2024-12-23T19:55:38.221Z level=INFO source=runner.go:942 msg=system info=\"AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | cgo(gcc)\" threads=4\r\ntime=2024-12-23T19:55:38.222Z level=INFO source=.:0 msg=\"Server listening on 127.0.0.1:46365\"\r\nllama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /root/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\r\nllama_model_loader: - kv   3:                           general.finetune str              = Instruct\r\nllama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\r\nllama_model_loader: - kv   5:                         general.size_label str              = 3B\r\nllama_model_loader: - kv   6:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\r\nllama_model_loader: - kv   7:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\r\nllama_model_loader: - kv   8:                          llama.block_count u32              = 28\r\nllama_model_loader: - kv   9:                       llama.context_length u32              = 131072\r\nllama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072\r\nllama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192\r\nllama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24\r\nllama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000\r\nllama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\r\nllama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\r\nllama_model_loader: - kv  18:                          general.file_type u32              = 15\r\nllama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\r\nllama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\r\nllama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\r\nllama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\r\nllama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009\r\nllama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\r\nllama_model_loader: - kv  29:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   58 tensors\r\nllama_model_loader: - type q4_K:  168 tensors\r\nllama_model_loader: - type q6_K:   29 tensors\r\ntime=2024-12-23T19:55:38.442Z level=INFO source=server.go:614 msg=\"waiting for server to become available\" status=\"llm server loading model\"\r\nllm_load_vocab: special tokens cache size = 256\r\nllm_load_vocab: token to piece cache size = 0.7999 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 128256\r\nllm_load_print_meta: n_merges         = 280147\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 131072\r\nllm_load_print_meta: n_embd           = 3072\r\nllm_load_print_meta: n_layer          = 28\r\nllm_load_print_meta: n_head           = 24\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 3\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 8192\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 500000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 131072\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = 3B\r\nllm_load_print_meta: model ftype      = Q4_K - Medium\r\nllm_load_print_meta: model params     = 3.21 B\r\nllm_load_print_meta: model size       = 1.87 GiB (5.01 BPW)\r\nllm_load_print_meta: general.name     = Llama 3.2 3B Instruct\r\nllm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\r\nllm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: LF token         = 128 'Ä'\r\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\r\nllm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\r\nllm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: max token length = 256\r\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nggml_sycl_init: SYCL_USE_XMX: yes\r\nggml_sycl_init: found 1 SYCL devices:\r\nllm_load_tensors: ggml ctx size =    0.24 MiB\r\nllm_load_tensors: offloading 28 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 29/29 layers to GPU\r\nllm_load_tensors:      SYCL0 buffer size =  1918.36 MiB\r\nllm_load_tensors:  SYCL_Host buffer size =   308.23 MiB\r\nllama_new_context_with_model: n_ctx      = 8192\r\nllama_new_context_with_model: n_batch    = 2048\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 500000.0\r\nllama_new_context_with_model: freq_scale = 1\r\n[SYCL] call ggml_check_sycl\r\nggml_check_sycl: GGML_SYCL_DEBUG: 0\r\nggml_check_sycl: GGML_SYCL_F16: no\r\nfound 1 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|                     Intel UHD Graphics|    1.3|     80|     512|   32| 30747M|            1.3.29735|\r\nllama_kv_cache_init:      SYCL0 KV buffer size =   896.00 MiB\r\nllama_new_context_with_model: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB\r\nllama_new_context_with_model:  SYCL_Host  output buffer size =     2.00 MiB\r\nllama_new_context_with_model:      SYCL0 compute buffer size =   256.50 MiB\r\nllama_new_context_with_model:  SYCL_Host compute buffer size =    22.01 MiB\r\nllama_new_context_with_model: graph nodes  = 790\r\nllama_new_context_with_model: graph splits = 2\r\ntime=2024-12-23T19:55:40.702Z level=INFO source=server.go:619 msg=\"llama runner started in 2.51 seconds\"\r\n[GIN] 2024/12/23 - 19:55:40 | 200 |  2.554739911s |       127.0.0.1 | POST     \"/api/generate\"\r\nollama_llama_server: /home/runner/_work/llm.cpp/llm.cpp/llm.cpp/bigdl-core-xe/llama_backend/sdp_xmx_kernel.cpp:439: auto ggml_sycl_op_sdp_xmx_casual(fp16 *, fp16 *, fp16 *, fp16 *, fp16 *, float *, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, float *, float, sycl::queue &)::(anonymous class)::operator()() const: Assertion `false' failed.\r\nSIGABRT: abort\r\nPC=0x7d574ae969fc m=3 sigcode=18446744073709551610\r\nsignal arrived during cgo execution\r\n\r\ngoroutine 7 gp=0xc0000e2000 m=3 mp=0xc00005b008 [syscall]:\r\nruntime.cgocall(0x5f36e170f540, 0xc000069b48)\r\n\truntime/cgocall.go:157 +0x4b fp=0xc000069b20 sp=0xc000069ae8 pc=0x5f36e149042b\r\nollama/llama/llamafile._Cfunc_llama_decode(0x7d56b2384880, {0x21, 0x7d56b238cac0, 0x0, 0x0, 0x7d56d8027bc0, 0x7d56d80233e0, 0x7d56d800d350, 0x7d56d807cb70, 0x0, ...})\r\n\t_cgo_gotypes.go:548 +0x52 fp=0xc000069b48 sp=0xc000069b20 pc=0x5f36e158d9b2\r\nollama/llama/llamafile.(*Context).Decode.func1(0x5f36e170afab?, 0x7d56b2384880?)\r\n\tollama/llama/llamafile/llama.go:121 +0xd8 fp=0xc000069c68 sp=0xc000069b48 pc=0x5f36e158ffd8\r\nollama/llama/llamafile.(*Context).Decode(0xc000069d58?, 0x0?)\r\n\tollama/llama/llamafile/llama.go:121 +0x13 fp=0xc000069cb0 sp=0xc000069c68 pc=0x5f36e158fe73\r\nmain.(*Server).processBatch(0xc0000b2120, 0xc000136000, 0xc000069f10)\r\n\tollama/llama/runner/runner.go:434 +0x24d fp=0xc000069ed0 sp=0xc000069cb0 pc=0x5f36e1709c6d\r\nmain.(*Server).run(0xc0000b2120, {0x5f36e1a15e00, 0xc0000880a0})\r\n\tollama/llama/runner/runner.go:342 +0x1e5 fp=0xc000069fb8 sp=0xc000069ed0 pc=0x5f36e17096e5\r\nmain.main.gowrap2()\r\n\tollama/llama/runner/runner.go:980 +0x28 fp=0xc000069fe0 sp=0xc000069fb8 pc=0x5f36e170e548\r\nruntime.goexit({})\r\n\truntime/asm_amd64.s:1695 +0x1 fp=0xc000069fe8 sp=0xc000069fe0 pc=0x5f36e14f8e41\r\ncreated by main.main in goroutine 1\r\n\tollama/llama/runner/runner.go:980 +0xd3e\r\n\r\ngoroutine 1 gp=0xc0000061c0 m=nil [IO wait]:\r\nruntime.gopark(0xc000042008?, 0x0?, 0xc0?, 0x61?, 0xc00003b898?)\r\n\truntime/proc.go:402 +0xce fp=0xc00003b860 sp=0xc00003b840 pc=0x5f36e14c706e\r\nruntime.netpollblock(0xc00003b8f8?, 0xe148fb86?, 0x36?)\r\n\truntime/netpoll.go:573 +0xf7 fp=0xc00003b898 sp=0xc00003b860 pc=0x5f36e14bf2b7\r\ninternal/poll.runtime_pollWait(0x7d574ec49020, 0x72)\r\n\truntime/netpoll.go:345 +0x85 fp=0xc00003b8b8 sp=0xc00003b898 pc=0x5f36e14f3b05\r\ninternal/poll.(*pollDesc).wait(0x3?, 0x3fe?, 0x0)\r\n\tinternal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc00003b8e0 sp=0xc00003b8b8 pc=0x5f36e1543a27\r\ninternal/poll.(*pollDesc).waitRead(...)\r\n\tinternal/poll/fd_poll_runtime.go:89\r\ninternal/poll.(*FD).Accept(0xc0000de080)\r\n\tinternal/poll/fd_unix.go:611 +0x2ac fp=0xc00003b988 sp=0xc00003b8e0 pc=0x5f36e1544eec\r\nnet.(*netFD).accept(0xc0000de080)\r\n\tnet/fd_unix.go:172 +0x29 fp=0xc00003ba40 sp=0xc00003b988 pc=0x5f36e15b3a69\r\nnet.(*TCPListener).accept(0xc000040220)\r\n\tnet/tcpsock_posix.go:159 +0x1e fp=0xc00003ba68 sp=0xc00003ba40 pc=0x5f36e15c479e\r\nnet.(*TCPListener).Accept(0xc000040220)\r\n\tnet/tcpsock.go:327 +0x30 fp=0xc00003ba98 sp=0xc00003ba68 pc=0x5f36e15c3af0\r\nnet/http.(*onceCloseListener).Accept(0xc0000b21b0?)\r\n\t<autogenerated>:1 +0x24 fp=0xc00003bab0 sp=0xc00003ba98 pc=0x5f36e16ead04\r\nnet/http.(*Server).Serve(0xc0000f4000, {0x5f36e1a157c0, 0xc000040220})\r\n\tnet/http/server.go:3260 +0x33e fp=0xc00003bbe0 sp=0xc00003bab0 pc=0x5f36e16e1b1e\r\nmain.main()\r\n\tollama/llama/runner/runner.go:1000 +0x10cd fp=0xc00003bf50 sp=0xc00003bbe0 pc=0x5f36e170e2cd\r\nruntime.main()\r\n\truntime/proc.go:271 +0x29d fp=0xc00003bfe0 sp=0xc00003bf50 pc=0x5f36e14c6c3d\r\nruntime.goexit({})\r\n\truntime/asm_amd64.s:1695 +0x1 fp=0xc00003bfe8 sp=0xc00003bfe0 pc=0x5f36e14f8e41\r\n\r\ngoroutine 2 gp=0xc000006c40 m=nil [force gc (idle)]:\r\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\r\n\truntime/proc.go:402 +0xce fp=0xc000054fa8 sp=0xc000054f88 pc=0x5f36e14c706e\r\nruntime.goparkunlock(...)\r\n\truntime/proc.go:408\r\nruntime.forcegchelper()\r\n\truntime/proc.go:326 +0xb8 fp=0xc000054fe0 sp=0xc000054fa8 pc=0x5f36e14c6ef8\r\nruntime.goexit({})\r\n\truntime/asm_amd64.s:1695 +0x1 fp=0xc000054fe8 sp=0xc000054fe0 pc=0x5f36e14f8e41\r\ncreated by runtime.init.6 in goroutine 1\r\n\truntime/proc.go:314 +0x1a\r\n\r\ngoroutine 3 gp=0xc000007180 m=nil [GC sweep wait]:\r\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\r\n\truntime/proc.go:402 +0xce fp=0xc000055780 sp=0xc000055760 pc=0x5f36e14c706e\r\nruntime.goparkunlock(...)\r\n\truntime/proc.go:408\r\nruntime.bgsweep(0xc000024150)\r\n\truntime/mgcsweep.go:278 +0x94 fp=0xc0000557c8 sp=0xc000055780 pc=0x5f36e14b1bb4\r\nruntime.gcenable.gowrap1()\r\n\truntime/mgc.go:203 +0x25 fp=0xc0000557e0 sp=0xc0000557c8 pc=0x5f36e14a66e5\r\nruntime.goexit({})\r\n\truntime/asm_amd64.s:1695 +0x1 fp=0xc0000557e8 sp=0xc0000557e0 pc=0x5f36e14f8e41\r\ncreated by runtime.gcenable in goroutine 1\r\n\truntime/mgc.go:203 +0x66\r\n\r\ngoroutine 4 gp=0xc000007340 m=nil [GC scavenge wait]:\r\nruntime.gopark(0xc000024150?, 0x5f36e178de60?, 0x1?, 0x0?, 0xc000007340?)\r\n\truntime/proc.go:402 +0xce fp=0xc000055f78 sp=0xc000055f58 pc=0x5f36e14c706e\r\nruntime.goparkunlock(...)\r\n\truntime/proc.go:408\r\nruntime.(*scavengerState).park(0x5f36e1bdf660)\r\n\truntime/mgcscavenge.go:425 +0x49 fp=0xc000055fa8 sp=0xc000055f78 pc=0x5f36e14af5a9\r\nruntime.bgscavenge(0xc000024150)\r\n\truntime/mgcscavenge.go:653 +0x3c fp=0xc000055fc8 sp=0xc000055fa8 pc=0x5f36e14afb3c\r\nruntime.gcenable.gowrap2()\r\n\truntime/mgc.go:204 +0x25 fp=0xc000055fe0 sp=0xc000055fc8 pc=0x5f36e14a6685\r\nruntime.goexit({})\r\n\truntime/asm_amd64.s:1695 +0x1 fp=0xc000055fe8 sp=0xc000055fe0 pc=0x5f36e14f8e41\r\ncreated by runtime.gcenable in goroutine 1\r\n\truntime/mgc.go:204 +0xa5\r\n\r\ngoroutine 5 gp=0xc000007c00 m=nil [finalizer wait]:\r\nruntime.gopark(0xc000054648?, 0x5f36e1499fe5?, 0xa8?, 0x1?, 0xc0000061c0?)\r\n\truntime/proc.go:402 +0xce fp=0xc000054620 sp=0xc000054600 pc=0x5f36e14c706e\r\nruntime.runfinq()\r\n\truntime/mfinal.go:194 +0x107 fp=0xc0000547e0 sp=0xc000054620 pc=0x5f36e14a5727\r\nruntime.goexit({})\r\n\truntime/asm_amd64.s:1695 +0x1 fp=0xc0000547e8 sp=0xc0000547e0 pc=0x5f36e14f8e41\r\ncreated by runtime.createfing in goroutine 1\r\n\truntime/mfinal.go:164 +0x3d\r\n\r\ngoroutine 37 gp=0xc000007dc0 m=nil [IO wait]:\r\nruntime.gopark(0x10?, 0x10?, 0xf0?, 0x6d?, 0xb?)\r\n\truntime/proc.go:402 +0xce fp=0xc000056da8 sp=0xc000056d88 pc=0x5f36e14c706e\r\nruntime.netpollblock(0x5f36e152d5b8?, 0xe148fb86?, 0x36?)\r\n\truntime/netpoll.go:573 +0xf7 fp=0xc000056de0 sp=0xc000056da8 pc=0x5f36e14bf2b7\r\ninternal/poll.runtime_pollWait(0x7d574ec48f28, 0x72)\r\n\truntime/netpoll.go:345 +0x85 fp=0xc000056e00 sp=0xc000056de0 pc=0x5f36e14f3b05\r\ninternal/poll.(*pollDesc).wait(0xc0000de100?, 0xc000182041?, 0x0)\r\n\tinternal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc000056e28 sp=0xc000056e00 pc=0x5f36e1543a27\r\ninternal/poll.(*pollDesc).waitRead(...)\r\n\tinternal/poll/fd_poll_runtime.go:89\r\ninternal/poll.(*FD).Read(0xc0000de100, {0xc000182041, 0x1, 0x1})\r\n\tinternal/poll/fd_unix.go:164 +0x27a fp=0xc000056ec0 sp=0xc000056e28 pc=0x5f36e154457a\r\nnet.(*netFD).Read(0xc0000de100, {0xc000182041?, 0xc000056f48?, 0x5f36e14f5730?})\r\n\tnet/fd_posix.go:55 +0x25 fp=0xc000056f08 sp=0xc000056ec0 pc=0x5f36e15b2965\r\nnet.(*conn).Read(0xc000058098, {0xc000182041?, 0x0?, 0x5f36e1c3f9a0?})\r\n\tnet/net.go:185 +0x45 fp=0xc000056f50 sp=0xc000056f08 pc=0x5f36e15bcc25\r\nnet.(*TCPConn).Read(0x5f36e1ba2040?, {0xc000182041?, 0x0?, 0x0?})\r\n\t<autogenerated>:1 +0x25 fp=0xc000056f80 sp=0xc000056f50 pc=0x5f36e15c8605\r\nnet/http.(*connReader).backgroundRead(0xc000182030)\r\n\tnet/http/server.go:681 +0x37 fp=0xc000056fc8 sp=0xc000056f80 pc=0x5f36e16d7497\r\nnet/http.(*connReader).startBackgroundRead.gowrap2()\r\n\tnet/http/server.go:677 +0x25 fp=0xc000056fe0 sp=0xc000056fc8 pc=0x5f36e16d73c5\r\nruntime.goexit({})\r\n\truntime/asm_amd64.s:1695 +0x1 fp=0xc000056fe8 sp=0xc000056fe0 pc=0x5f36e14f8e41\r\ncreated by net/http.(*connReader).startBackgroundRead in goroutine 8\r\n\tnet/http/server.go:677 +0xba\r\n\r\ngoroutine 8 gp=0xc0000e21c0 m=nil [select]:\r\nruntime.gopark(0xc00015ba28?, 0x2?, 0x50?, 0x61?, 0xc00015b7ec?)\r\n\truntime/proc.go:402 +0xce fp=0xc00015b660 sp=0xc00015b640 pc=0x5f36e14c706e\r\nruntime.selectgo(0xc00015ba28, 0xc00015b7e8, 0x21?, 0x0, 0x1?, 0x1)\r\n\truntime/select.go:327 +0x725 fp=0xc00015b780 sp=0xc00015b660 pc=0x5f36e14d8445\r\nmain.(*Server).completion(0xc0000b2120, {0x5f36e1a15970, 0xc00012c2a0}, 0xc00011c360)\r\n\tollama/llama/runner/runner.go:698 +0xa86 fp=0xc00015bab8 sp=0xc00015b780 pc=0x5f36e170bac6\r\nmain.(*Server).completion-fm({0x5f36e1a15970?, 0xc00012c2a0?}, 0x5f36e16e5e4d?)\r\n\t<autogenerated>:1 +0x36 fp=0xc00015bae8 sp=0xc00015bab8 pc=0x5f36e170ed76\r\nnet/http.HandlerFunc.ServeHTTP(0xc000098ea0?, {0x5f36e1a15970?, 0xc00012c2a0?}, 0x10?)\r\n\tnet/http/server.go:2171 +0x29 fp=0xc00015bb10 sp=0xc00015bae8 pc=0x5f36e16de8e9\r\nnet/http.(*ServeMux).ServeHTTP(0x5f36e1499fe5?, {0x5f36e1a15970, 0xc00012c2a0}, 0xc00011c360)\r\n\tnet/http/server.go:2688 +0x1ad fp=0xc00015bb60 sp=0xc00015bb10 pc=0x5f36e16e076d\r\nnet/http.serverHandler.ServeHTTP({0x5f36e1a14cc0?}, {0x5f36e1a15970?, 0xc00012c2a0?}, 0x6?)\r\n\tnet/http/server.go:3142 +0x8e fp=0xc00015bb90 sp=0xc00015bb60 pc=0x5f36e16e178e\r\nnet/http.(*conn).serve(0xc0000b21b0, {0x5f36e1a15dc8, 0xc000096db0})\r\n\tnet/http/server.go:2044 +0x5e8 fp=0xc00015bfb8 sp=0xc00015bb90 pc=0x5f36e16dd528\r\nnet/http.(*Server).Serve.gowrap3()\r\n\tnet/http/server.go:3290 +0x28 fp=0xc00015bfe0 sp=0xc00015bfb8 pc=0x5f36e16e1f08\r\nruntime.goexit({})\r\n\truntime/asm_amd64.s:1695 +0x1 fp=0xc00015bfe8 sp=0xc00015bfe0 pc=0x5f36e14f8e41\r\ncreated by net/http.(*Server).Serve in goroutine 1\r\n\tnet/http/server.go:3290 +0x4b4\r\n\r\nrax    0x0\r\nrbx    0x7d56ee400640\r\nrcx    0x7d574ae969fc\r\nrdx    0x6\r\nrdi    0x145a\r\nrsi    0x145c\r\nrbp    0x145c\r\nrsp    0x7d56ee3fefa0\r\nr8     0x7d56ee3ff070\r\nr9     0x0\r\nr10    0x8\r\nr11    0x246\r\nr12    0x6\r\nr13    0x16\r\nr14    0x7d574d53eb0c\r\nr15    0xffffaaae15800000\r\nrip    0x7d574ae969fc\r\nrflags 0x246\r\ncs     0x33\r\nfs     0x0\r\ngs     0x0\r\n[GIN] 2024/12/23 - 19:55:43 | 200 |  194.012636ms |       127.0.0.1 | POST     \"/api/chat\"\r\n```\r\n\r\n</p>\r\n</details> \r\n\r\nThe CGO call fails with\r\n```\r\nollama_llama_server: /home/runner/_work/llm.cpp/llm.cpp/llm.cpp/bigdl-core-xe/llama_backend/sdp_xmx_kernel.cpp:439: auto ggml_sycl_op_sdp_xmx_casual(fp16 *, fp16 *, fp16 *, fp16 *, fp16 *, float *, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, float *, float, sycl::queue &)::(anonymous class)::operator()() const: Assertion `false' failed.\r\nSIGABRT: abort\r\n```\r\n\r\nInstalled versions:\r\n<details><summary>Details</summary>\r\n<p>\r\n\r\n```bash\r\n./ollama --version\r\nollama version is 0.4.6-ipexllm-20241223\r\n```\r\n\r\n```bash\r\nintel-oneapi-ccl-2021.11/all,now 2021.11.2-5 amd64 [installed,automatic]\r\nintel-oneapi-ccl-devel-2021.11/all,now 2021.11.2-5 amd64 [installed,automatic]\r\nintel-oneapi-ccl-devel/all,now 2021.11.2-5 amd64 [installed,upgradable to: 2021.14.0-505]\r\nintel-oneapi-ccl/all,now 2021.11.2-5 amd64 [installed,upgradable to: 2021.14.0-505]\r\nintel-oneapi-common-licensing-2024.0/all,now 2024.0.0-49406 all [installed,automatic]\r\nintel-oneapi-common-oneapi-vars-2024.0/all,now 2024.0.0-49406 all [installed,automatic]\r\nintel-oneapi-common-oneapi-vars/all,now 2024.0.0-49406 all [installed,upgradable to: 2025.0.1-15]\r\nintel-oneapi-common-vars/all,now 2024.0.0-49406 all [installed,upgradable to: 2025.0.1-15]\r\nintel-oneapi-compiler-cpp-eclipse-cfg-2024.0/all,now 2024.0.2-49895 all [installed,automatic]\r\nintel-oneapi-compiler-dpcpp-cpp-2024.0/all,now 2024.0.2-49895 amd64 [installed,automatic]\r\nintel-oneapi-compiler-dpcpp-cpp-common-2024.0/all,now 2024.0.2-49895 all [installed,automatic]\r\nintel-oneapi-compiler-dpcpp-cpp-runtime-2024.0/all,now 2024.0.2-49895 amd64 [installed,automatic]\r\nintel-oneapi-compiler-dpcpp-cpp/all,now 2024.0.2-49895 amd64 [installed,upgradable to: 2025.0.4-1519]\r\nintel-oneapi-compiler-dpcpp-eclipse-cfg-2024.0/all,now 2024.0.2-49895 all [installed,automatic]\r\nintel-oneapi-compiler-shared-2024.0/all,now 2024.0.2-49895 amd64 [installed,automatic]\r\nintel-oneapi-compiler-shared-common-2024.0/all,now 2024.0.2-49895 all [installed,automatic]\r\nintel-oneapi-compiler-shared-runtime-2024.0/all,now 2024.0.2-49895 amd64 [installed,automatic]\r\nintel-oneapi-dal-2024.0/all,now 2024.0.1-25 amd64 [installed,automatic]\r\nintel-oneapi-dal-common-2024.0/all,now 2024.0.1-25 all [installed,automatic]\r\nintel-oneapi-dal-common-devel-2024.0/all,now 2024.0.1-25 all [installed,automatic]\r\nintel-oneapi-dal-devel-2024.0/all,now 2024.0.1-25 amd64 [installed,automatic]\r\nintel-oneapi-dal-devel/all,now 2024.0.1-25 amd64 [installed,upgradable to: 2025.0.1-9]\r\nintel-oneapi-dal/all,now 2024.0.1-25 amd64 [installed,upgradable to: 2025.0.1-9]\r\nintel-oneapi-dev-utilities-2024.0/all,now 2024.0.0-49320 amd64 [installed,automatic]\r\nintel-oneapi-dev-utilities-eclipse-cfg-2024.0/all,now 2024.0.0-49320 all [installed,automatic]\r\nintel-oneapi-diagnostics-utility-2024.0/all,now 2024.0.0-49093 amd64 [installed,automatic]\r\nintel-oneapi-diagnostics-utility/all,now 2024.0.0-49093 amd64 [installed,upgradable to: 2024.2.1-13]\r\nintel-oneapi-dnnl-2024.0/all,now 2024.0.0-49521 amd64 [installed,automatic]\r\nintel-oneapi-dnnl-devel-2024.0/all,now 2024.0.0-49521 amd64 [installed,automatic]\r\nintel-oneapi-dnnl-devel/all,now 2024.0.0-49521 amd64 [installed,upgradable to: 2025.0.1-6]\r\nintel-oneapi-dnnl/all,now 2024.0.0-49521 amd64 [installed,upgradable to: 2025.0.1-6]\r\nintel-oneapi-dpcpp-cpp-2024.0/all,now 2024.0.2-49895 amd64 [installed,automatic]\r\nintel-oneapi-dpcpp-ct-2024.0/all,now 2024.0.0-49381 amd64 [installed,automatic]\r\nintel-oneapi-dpcpp-ct-eclipse-cfg-2024.0/all,now 2024.0.0-49381 all [installed,automatic]\r\nintel-oneapi-dpcpp-ct/all,now 2024.0.0-49381 amd64 [installed,upgradable to: 2025.0.1-17]\r\nintel-oneapi-dpcpp-debugger-2024.0/all,now 2024.0.1-6 amd64 [installed,automatic]\r\nintel-oneapi-icc-eclipse-plugin-cpp-2024.0/all,now 2024.0.2-49895 all [installed,automatic]\r\nintel-oneapi-ipp-2021.10/all,now 2021.10.1-13 amd64 [installed,automatic]\r\nintel-oneapi-ipp-common-2021.10/all,now 2021.10.1-13 all [installed,automatic]\r\nintel-oneapi-ipp-common-devel-2021.10/all,now 2021.10.1-13 all [installed,automatic]\r\nintel-oneapi-ipp-devel-2021.10/all,now 2021.10.1-13 amd64 [installed,automatic]\r\nintel-oneapi-ipp-devel/all,now 2021.10.1-13 amd64 [installed,upgradable to: 2022.0.0-808]\r\nintel-oneapi-ipp/all,now 2021.10.1-13 amd64 [installed,upgradable to: 2022.0.0-808]\r\nintel-oneapi-ippcp-2021.9/all,now 2021.9.1-5 amd64 [installed,automatic]\r\nintel-oneapi-ippcp-common-2021.9/all,now 2021.9.1-5 all [installed,automatic]\r\nintel-oneapi-ippcp-common-devel-2021.9/all,now 2021.9.1-5 all [installed,automatic]\r\nintel-oneapi-ippcp-devel-2021.9/all,now 2021.9.1-5 amd64 [installed,automatic]\r\nintel-oneapi-ippcp-devel/all,now 2021.9.1-5 amd64 [installed,upgradable to: 2025.0.0-615]\r\nintel-oneapi-ippcp/all,now 2021.9.1-5 amd64 [installed,upgradable to: 2025.0.0-615]\r\nintel-oneapi-libdpstd-devel-2022.3/all,now 2022.3.0-49369 amd64 [installed,automatic]\r\nintel-oneapi-mkl-2024.0/all,now 2024.0.0-49656 amd64 [installed,automatic]\r\nintel-oneapi-mkl-common-2024.0/all,now 2024.0.0-49656 all [installed,automatic]\r\nintel-oneapi-mkl-common-devel-2024.0/all,now 2024.0.0-49656 all [installed,automatic]\r\nintel-oneapi-mkl-devel-2024.0/all,now 2024.0.0-49656 amd64 [installed,automatic]\r\nintel-oneapi-mkl-devel/all,now 2024.0.0-49656 amd64 [installed,upgradable to: 2025.0.1-14]\r\nintel-oneapi-mkl/all,now 2024.0.0-49656 amd64 [installed,upgradable to: 2025.0.1-14]\r\nintel-oneapi-mpi-2021.11/all,now 2021.11.0-49493 amd64 [installed,automatic]\r\nintel-oneapi-mpi-devel-2021.11/all,now 2021.11.0-49493 amd64 [installed,automatic]\r\nintel-oneapi-mpi-devel/all,now 2021.11.0-49493 amd64 [installed,upgradable to: 2021.14.1-5]\r\nintel-oneapi-mpi/all,now 2021.11.0-49493 amd64 [installed,upgradable to: 2021.14.1-5]\r\nintel-oneapi-openmp-2024.0/all,now 2024.0.2-49895 amd64 [installed,automatic]\r\nintel-oneapi-openmp-common-2024.0/all,now 2024.0.2-49895 all [installed,automatic]\r\nintel-oneapi-tbb-2021.11/all,now 2021.11.0-49513 amd64 [installed,automatic]\r\nintel-oneapi-tbb-common-2021.11/all,now 2021.11.0-49513 all [installed,automatic]\r\nintel-oneapi-tbb-common-devel-2021.11/all,now 2021.11.0-49513 all [installed,automatic]\r\nintel-oneapi-tbb-devel-2021.11/all,now 2021.11.0-49513 amd64 [installed,automatic]\r\nintel-oneapi-tcm-1.0/all,now 1.0.0-435 amd64 [installed,upgradable to: 1.0.1-175]\r\nintel-oneapi-tlt-2024.0/all,now 2024.0.0-352 amd64 [installed,automatic]\r\nintel-oneapi-tlt/all,now 2024.0.0-352 amd64 [installed,upgradable to: 2025.0.0-550]\r\n```\r\n\r\n</p>\r\n</details> \r\n\r\nIs this a known issue? I didn't see anywhere that this GPU is supported, but I went ahead and gave it a try anyway.",
      "state": "open",
      "author": "ajatprabha",
      "author_type": "User",
      "created_at": "2024-12-23T20:05:01Z",
      "updated_at": "2025-03-10T11:28:52Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 16,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12598/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "leonardozcm"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12598",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12598",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:19.260381",
      "comments": [
        {
          "author": "ajatprabha",
          "body": "I tried `llama3.1:8b` and that worked well. It is also able to use the GPU\r\n<details><summary>Details</summary>\r\n<p>\r\n\r\n```bash\r\nintel-gpu-top: 8086:4626 @ /dev/dri/card0 - 1298/1298 MHz;   0% RC6; 13.50/35.05 W\r\n         71 irqs/s\r\n\r\n         ENGINES     BUSY                                        ",
          "created_at": "2024-12-23T20:20:13Z"
        },
        {
          "author": "qiuxin2012",
          "body": "Llama 3.2 works fine on our i9 13900H, also Iris Graphics.\r\nCan you check your GPU in task manager? i5-1240P should be Iris Graphics\r\n![image](https://github.com/user-attachments/assets/ed4aaee8-3f23-44ca-96d6-7639613ab8d8)\r\nBut your log shows it's a UHD graphics?\r\n```\r\nfound 1 SYCL devices:\r\n|  |  ",
          "created_at": "2024-12-24T07:58:35Z"
        },
        {
          "author": "ajatprabha",
          "body": "I'm on a linux machine, the difference in Device name can be because of RAM installed.\r\n\r\nIntel specs [say](https://www.intel.com/content/www/us/en/products/sku/132221/intel-core-i51240p-processor-12m-cache-up-to-4-40-ghz/specifications.html):\r\n```\r\nIntel® Iris® Xe Graphics only: to use the Intel® I",
          "created_at": "2024-12-24T08:07:21Z"
        },
        {
          "author": "ajatprabha",
          "body": "The error is intermittent! I have been able to run both models every now and then, but most of the time it fails to run with assertion failure.",
          "created_at": "2024-12-24T08:09:08Z"
        },
        {
          "author": "ajatprabha",
          "body": "I tried `lspci -k` to check the video device details. It could be a driver issue as well\r\n```\r\n00:02.0 VGA compatible controller: Intel Corporation Alder Lake-P Integrated Graphics Controller (rev 0c)\r\n        DeviceName: Onboard - Video\r\n        Subsystem: Intel Corporation Alder Lake-P Integrated ",
          "created_at": "2024-12-24T08:25:38Z"
        }
      ]
    },
    {
      "issue_number": 12592,
      "title": "WSL / Docker ipex-llm-inference-cpp-xpu:latest SIGSEGV on model load",
      "body": "Hello, \r\n\r\nBelow is my Alder Lake A770 WSL / Docker setup configuration (2 GPUs):\r\n```\r\nWindows 11 24H2 (also tested with 23H2)\r\nLatest WHQL 32.0.101.6325\r\n<2 A770 cards confirmed working correctly on host Windows and host WSL Ubuntu 22.04>\r\n\r\n$ wsl --version\r\nWSL version: 2.3.26.0\r\nKernel version: 5.15.167.4-1\r\nWSLg version: 1.0.65\r\nMSRDC version: 1.2.5620\r\nDirect3D version: 1.611.1-81528511\r\nDXCore version: 10.0.26100.1-240331-1435.ge-release\r\nWindows version: 10.0.26100.2605\r\n```\r\n\r\nFrom the IPEX-LLM container:\r\n```\r\n# sycl-ls\r\nWarning: ONEAPI_DEVICE_SELECTOR environment variable is set to level_zero:*.\r\nTo see the correct device id, please unset ONEAPI_DEVICE_SELECTOR.\r\n\r\n[ext_oneapi_level_zero:gpu:0] Intel(R) Level-Zero, Intel(R) Graphics [0x56a0] 1.6 [1.3.31294]\r\n[ext_oneapi_level_zero:gpu:1] Intel(R) Level-Zero, Intel(R) Graphics [0x56a0] 1.6 [1.3.31294]\r\n\r\nAfter unset ONEAPI_DEVICE_SELECTOR:\r\n\r\n# sycl-ls\r\n[opencl:acc:0] Intel(R) FPGA Emulation Platform for OpenCL(TM), Intel(R) FPGA Emulation Device OpenCL 1.2  [2023.16.12.0.12_195853.xmain-hotfix]\r\n[opencl:cpu:1] Intel(R) OpenCL, Intel(R) Core(TM) i9-14900F OpenCL 3.0 (Build 0) [2023.16.12.0.12_195853.xmain-hotfix]\r\n[opencl:gpu:2] Intel(R) OpenCL Graphics, Intel(R) Graphics [0x56a0] OpenCL 3.0 NEO  [24.39.31294.12]\r\n[opencl:gpu:3] Intel(R) OpenCL Graphics, Intel(R) Graphics [0x56a0] OpenCL 3.0 NEO  [24.39.31294.12]\r\n[ext_oneapi_level_zero:gpu:0] Intel(R) Level-Zero, Intel(R) Graphics [0x56a0] 1.6 [1.3.31294]\r\n[ext_oneapi_level_zero:gpu:1] Intel(R) Level-Zero, Intel(R) Graphics [0x56a0] 1.6 [1.3.31294]\r\n\r\n# clinfo\r\n...\r\nPlatform Name                                   Intel(R) OpenCL Graphics\r\nNumber of devices                                 2\r\n  Device Name                                     Intel(R) Graphics [0x56a0]\r\n  Device Vendor                                   Intel(R) Corporation\r\n  Device Vendor ID                                0x8086\r\n  Device Version                                  OpenCL 3.0 NEO\r\n  ...\r\n```\r\n\r\nThis is the Docker command used for IPEX-LLM:\r\n```\r\ndocker run -d --restart=always --net=bridge --device=/dev/dxg --device=/dev/dri\r\n--name=ipex-llm \r\n-p 11434:11434 \r\n-v /usr/lib/wsl:/usr/lib/wsl\r\n-v ~/.ollama/models:/root/.ollama/models \r\n-e PATH=/llm/ollama:<OMITTED FOR BREVITY> \r\n-e OLLAMA_HOST=0.0.0.0 \r\n-e no_proxy=localhost,127.0.0.1 \r\n-e ZES_ENABLE_SYSMAN=1 \r\n-e ENABLE_GPU=1 \r\n-e OLLAMA_INTEL_GPU=true \r\n-e ONEAPI_DEVICE_SELECTOR=level_zero:*\r\n-e DEVICE=Arc \r\n--shm-size=\"16g\" \r\n--memory=\"32G\"\r\nintelanalytics/ipex-llm-inference-cpp-xpu:latest\r\nbash -c \"cd /llm/scripts/ && source ipex-llm-init --gpu --device Arc && bash start-ollama.sh && tail -f /llm/ollama/ollama.log\"\r\n```\r\nSwitching the device selection between level_zero:0 / 1 / * doesn't change the below observed behaviour.\r\n\r\nPulling a model with ollama works just fine, but trying to run it results in the following:\r\n```\r\n[GIN] 2024/12/21 - 14:52:52 | 200 |      18.746µs |       127.0.0.1 | HEAD     \"/\"\r\n[GIN] 2024/12/21 - 14:52:52 | 200 |  715.446465ms |       127.0.0.1 | POST     \"/api/pull\"\r\ntime=2024-12-21T14:52:59.385+08:00 level=INFO source=gpu.go:221 msg=\"looking for compatible GPUs\"\r\ntime=2024-12-21T14:52:59.385+08:00 level=WARN source=gpu.go:732 msg=\"unable to locate gpu dependency libraries\"\r\ntime=2024-12-21T14:52:59.385+08:00 level=WARN source=gpu.go:732 msg=\"unable to locate gpu dependency libraries\"\r\ntime=2024-12-21T14:52:59.385+08:00 level=WARN source=gpu.go:732 msg=\"unable to locate gpu dependency libraries\"\r\n[GIN] 2024/12/21 - 14:52:59 | 200 |      10.148µs |       127.0.0.1 | HEAD     \"/\"\r\n[GIN] 2024/12/21 - 14:52:59 | 200 |    2.748599ms |       127.0.0.1 | POST     \"/api/show\"\r\nSIGSEGV: segmentation violation\r\nPC=0x7f866495324d m=9 sigcode=1 addr=0x31f\r\nsignal arrived during cgo execution\r\n```\r\n\r\n[docker_logs.txt](https://github.com/user-attachments/files/18217616/docker_logs.txt)\r\nFull log attached. Any hints / ideas on what I might be going wrong are welcome as it's my 3rd day battling this (rookie numbers, I know, but still).\r\n\r\n\r\nUpdate:\r\n\r\nConfirmed with WSL kernels 6.6.36.6-microsoft-standard-WSL2+ and 5.15.167.4",
      "state": "open",
      "author": "vladislavdonchev",
      "author_type": "User",
      "created_at": "2024-12-21T06:56:39Z",
      "updated_at": "2025-03-10T11:28:52Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12592/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12592",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12592",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:19.521159",
      "comments": [
        {
          "author": "vladislavdonchev",
          "body": "Just noticed the following in WSL Ubuntu host dmesg:\r\n```\r\n# dmesg | grep \"dxg\"\r\n[    0.448782] hv_vmbus: registering driver dxgkrnl\r\n[    1.515553] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22\r\n[    1.516804] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22\r\n[    1.51707",
          "created_at": "2024-12-21T12:09:44Z"
        },
        {
          "author": "hzjane",
          "body": "@vladislavdonchev  You can follow [this guide](https://github.com/intel-analytics/ipex-llm/tree/098eb335b2d60c3b922ac311e92c428c3682411c/docker/llm/inference-cpp#start-docker-container) to start docker container on windows wsl and run it again. Maybe some environment setting on your script crash the",
          "created_at": "2024-12-23T02:06:23Z"
        }
      ]
    },
    {
      "issue_number": 12535,
      "title": "Assertion error when running EAGLE GPU example",
      "body": "Hi Team!\r\n\r\nI followed this docs to run EAGLE, but encountered assertion error as shown in the picture.\r\nhttps://github.com/intel-analytics/ipex-llm/blob/509bdb4661d4c45e518698bbc85abd984a5ada23/python/llm/example/GPU/Speculative-Decoding/EAGLE/README.md?plain=1#L1\r\n\r\n![image](https://github.com/user-attachments/assets/60140c85-9834-4cc5-9955-17651bb858dd)\r\n\r\n\r\nI can confirmed that error dumped at the ipdb breakpoint: function `layer_outputs = decoder_layer(...)`.\r\n\r\nThe execute script is \r\n```\r\nUSE_XETLA=OFF SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1 SYCL_CACHE_PERSISTENT=1 python -m evaluation.gen_ea_answer_llama2chat_e2_ipex_optimize \\ \r\n--ea-model-path yuhuili/EAGLE-llama2-chat-7B \\\r\n--base-model-path meta-llama/Llama-2-7b-chat-hf \\\r\n--model-id e2-llama-2-7b-chat --enable-ipex-llm\r\n```\r\n\r\nMy conda env is as below, also tried ipex-llm==2.1.0 which does not work:\r\n```\r\nbigdl-core-xe                    2.5.0b20231210\r\nbigdl-core-xe-21                 2.6.0b20241212\r\nbigdl-core-xe-addons-21          2.6.0b20241212\r\nbigdl-core-xe-batch-21           2.6.0b20241212\r\nbigdl-core-xe-esimd              2.5.0b20231210\r\nbigdl-llm                        2.5.0b20231210\r\nbitsandbytes                     0.41.2.post2\r\neagle-llm                        1.2.1                /data1/kyyx/github/LLM/speculative_decoding/EAGLE\r\nhuggingface-hub                  0.23.4\r\nintel-cmplr-lib-ur               2024.2.0\r\nintel-extension-for-pytorch      2.1.10+xpu\r\nintel-extension-for-transformers 1.2.1\r\nintel-openmp                     2024.2.0\r\nipex-llm                         2.2.0b20241212\r\ntorch                            2.1.0a0+cxx11.abi\r\ntorch-grammar                    0.3.3\r\ntorchvision                      0.16.0a0+cxx11.abi\r\n```\r\nAs for eagle-llm package, I've tried install branch main or v1, both of them raise the same error.",
      "state": "open",
      "author": "kyang-06",
      "author_type": "User",
      "created_at": "2024-12-12T16:25:38Z",
      "updated_at": "2025-03-10T11:28:51Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12535/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "jenniew"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12535",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12535",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:19.694280",
      "comments": [
        {
          "author": "jenniew",
          "body": "@kyang-06 Can you try earlier version of Eagle2 because recently there are some changes may affect the example code. You can try the older version like Jun 30 commit of main branch to see if there is any issue.",
          "created_at": "2024-12-13T07:27:15Z"
        },
        {
          "author": "kyang-06",
          "body": "Jun 30 commit `bffca4358a6615f9717ce82114d9a69ca9416091` still threw the error. VRAM uses 4.5GB, far away from OOM.",
          "created_at": "2024-12-16T03:24:03Z"
        },
        {
          "author": "kyang-06",
          "body": "@jenniew Any update?",
          "created_at": "2024-12-25T03:03:43Z"
        }
      ]
    },
    {
      "issue_number": 12949,
      "title": "ipex-llm Ollama 3.2:1b model low computer size",
      "body": "I am trying to get ollama to run on my computer's iGPU and CPU which is working but only half the speed then on just the CPU based on the computer size. I'm still a beginner on using the chatbot with ipex-llm but still haven't figured out the performance drop so some advice would be great.\n\nSYCL0 - iGPU\nSYCL_Host - CPU\n\nFirst image is using ipex-llm on CMD in a conda virtual environment the ollama serve\nSecond image is just ollama running on the background with the server log\n\n![Image](https://github.com/user-attachments/assets/d707fc4c-c4d6-4625-8d85-cce657ed1ed7)\n\n![Image](https://github.com/user-attachments/assets/5d55553b-9aa2-4e64-8938-6bd6cd0e3ff4)",
      "state": "closed",
      "author": "dta-kurai",
      "author_type": "User",
      "created_at": "2025-03-06T21:02:52Z",
      "updated_at": "2025-03-10T11:28:17Z",
      "closed_at": "2025-03-08T19:54:42Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12949/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12949",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12949",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:19.885517",
      "comments": [
        {
          "author": "dta-kurai",
          "body": "The speeds I'm getting is related to the compute size",
          "created_at": "2025-03-06T21:03:33Z"
        }
      ]
    },
    {
      "issue_number": 12944,
      "title": "Performance issue @ MTL 155h 16G DDR5",
      "body": "[ds benchmark.xlsx](https://github.com/user-attachments/files/19100626/ds.benchmark.xlsx)\nI have followed the guideline run benchmark for ds-distill-Qwen7b and 1.5b, but the results data seems a little bit incorrect (especially for tps and peak memory), could you help to check about it, please? Thanks.",
      "state": "closed",
      "author": "Waying13",
      "author_type": "User",
      "created_at": "2025-03-06T03:11:30Z",
      "updated_at": "2025-03-10T11:28:16Z",
      "closed_at": "2025-03-10T01:50:05Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12944/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12944",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12944",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:20.132503",
      "comments": [
        {
          "author": "Waying13",
          "body": "Have done with the following inspect:\nPS C:\\Windows\\system32> winsat mem\nWindows 系统评估工具\n> 正在运行: 功能枚举 ''\n> 运行时间 00:00:00.00\n> 正在运行: 系统内存性能评估 ''\n> 运行时间 00:00:06.44\n> 内存性能                                         46357.78 MB/s\n> Dshow 视频编码时间                                 0.00000 s\n> Dshow 视频解码时间      ",
          "created_at": "2025-03-10T01:50:00Z"
        }
      ]
    },
    {
      "issue_number": 12943,
      "title": "Performance issue @ MTL 155h 16G DDR5",
      "body": "I followed the guideline https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/benchmark_quickstart.md#run-on-windows to run benchmark for ds-distill-Qwen-7b and 1.5b, but the tps data and peak memory data seems a little bit incorrect. \nCould you help to confirm the result, or any other comments to rectify the resutls.",
      "state": "closed",
      "author": "Waying13",
      "author_type": "User",
      "created_at": "2025-03-06T03:08:05Z",
      "updated_at": "2025-03-10T11:28:16Z",
      "closed_at": "2025-03-07T02:04:37Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12943/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12943",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12943",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:20.358440",
      "comments": [
        {
          "author": "Waying13",
          "body": "close, duplicated",
          "created_at": "2025-03-06T03:14:34Z"
        }
      ]
    },
    {
      "issue_number": 12942,
      "title": "Performance issue @ MTL 155h 16G DDR5",
      "body": "I followed the guideline: https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/benchmark_quickstart.md#run-on-windows , finally execute python run.py on MTL 155h 16G DDR5, use deepseek-ai/DeepSeek-R1-Distill-Qwen-7B and 1.5B model for inference.  But the TPS only got 9 tokens/second\nGot the following data: \n\n模型列表\tprecision\tinputsize\tinput token length\treal input\toutput token length\tbatch\t1st token latency(ms)\tRest token latency(ms)\tTPS\tOverall Time (ms)  \nDS-R1-Distill-Qwen-7B\tint4\t1024\t512\t1025\t512\t1\t5891.07\t109.22\t9.155832265\t61702.49\nDS-R1-Distill-Qwen-7B\tint4\t2048\t512\t2049\t512\t1\t7812.89\t112.04\t8.925383792\t65065.33\n\niGPU Memory最大占用值(G)\n6.810547\n6.941406\n\nBesides, how about the peak Memory value, are those data trustful?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "state": "closed",
      "author": "Waying13",
      "author_type": "User",
      "created_at": "2025-03-06T02:20:44Z",
      "updated_at": "2025-03-10T11:28:16Z",
      "closed_at": "2025-03-07T02:04:34Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12942/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12942",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12942",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:20.556365",
      "comments": [
        {
          "author": "Waying13",
          "body": "duplicated",
          "created_at": "2025-03-06T03:13:33Z"
        }
      ]
    },
    {
      "issue_number": 12933,
      "title": "在GPU上运行无法达到最佳的tokens",
      "body": "机器配置：14代Core i9；\n内存：64GB；\n显卡：双intelA770 ，16G显存x2；\nSSD：500GB\n\n问题描述：单一账户做交互的时候，GPU处理数据，最大的tokens只有15/S左右，日志如下：\n \n[outputlog.txt](https://github.com/user-attachments/files/19081156/outputlog.txt)",
      "state": "closed",
      "author": "cyskdlx",
      "author_type": "User",
      "created_at": "2025-03-05T03:01:59Z",
      "updated_at": "2025-03-10T11:28:15Z",
      "closed_at": "2025-03-06T02:11:04Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12933/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12933",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12933",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:20.735452",
      "comments": [
        {
          "author": "biyuehuang",
          "body": "vllm在两张A770 16G独显 运行deepseek-r1-distill-qwen-32b，单一账户做交互推理的时候，理论上tokens有30token/S左右",
          "created_at": "2025-03-05T03:22:30Z"
        },
        {
          "author": "glorysdj",
          "body": "Which Docker image are you using? ",
          "created_at": "2025-03-05T05:28:59Z"
        },
        {
          "author": "biyuehuang",
          "body": "Closed\n在镜像外面运行 \nsudo xpu-smi config -d x -t 0 --frequencyrange 2400,2400\n然后再进入镜像运行模型推理",
          "created_at": "2025-03-05T07:58:07Z"
        },
        {
          "author": "liu-shaojun",
          "body": "Thanks~ \n\nWe added CPU and GPU Frequency Locking Instructions into QuickStart: https://github.com/intel/ipex-llm/blob/main/docs/mddocs/DockerGuides/vllm_docker_quickstart.md#running-vllm-serving-with-ipex-llm-on-intel-gpu-in-docker",
          "created_at": "2025-03-07T02:08:14Z"
        },
        {
          "author": "cyskdlx",
          "body": "Thanks发自我的手机-------- 原始邮件 --------发件人： Shaojun Liu ***@***.***>日期： 2025年3月7日周五 10:08收件人： intel/ipex-llm ***@***.***>抄送： cyskdlx ***@***.***>, State change ***@***.***>主    题： Re: [intel/ipex-llm] 在GPU上运行无法达到最佳的tokens (Issue #12933)\r\n  Thanks~\r\nWe added CPU and GPU Frequency Locking Instructions into",
          "created_at": "2025-03-07T12:46:01Z"
        }
      ]
    },
    {
      "issue_number": 12931,
      "title": "Support ipex 2.5.x, 2.1.0a0 is broken.",
      "body": "Latest version of ipex is 2.5.10 with pytorch 2.5 and the new version doesn't require the extra oneapi packages which simplifies installation.  Furthermore, current instrauctions use torch==2.1.0a0 which no longer exists in the repository[1] so installation using the instructions[2] is broken.\n\n\n[1] - https://pytorch-extension.intel.com/release-whl/stable/xpu/us/\n[2] - https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/install_linux_gpu.md#install-ipex-llm",
      "state": "closed",
      "author": "tripzero",
      "author_type": "User",
      "created_at": "2025-03-04T18:10:08Z",
      "updated_at": "2025-03-10T11:28:15Z",
      "closed_at": "2025-03-06T19:28:37Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12931/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Oscilloscope98"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12931",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12931",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:20.957510",
      "comments": [
        {
          "author": "zcwang",
          "body": "ipex **2.5.10+xpu** works well but unable to import ipex-llm **v2.2.0b20250303** correctly.\n\n```\n(ipex-vllm) intel@mydevice:~$ python -c \"import torch; import intel_extension_for_pytorch as ipex; print(torch.__version__); print(ipex.__version__); [print(f'[{i}]: {torch.xpu.get_device_properties(i)}'",
          "created_at": "2025-03-05T07:31:38Z"
        },
        {
          "author": "Oscilloscope98",
          "body": "Hi @tripzero,\n\nWe did not reproduce the PyTorch 2.1 broken issue. `pip install --pre --upgrade ipex-llm[xpu] --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/cn/` or `pip install --pre --upgrade ipex-llm[xpu] --extra-index-url https://pytorch-extension.intel.com/release-w",
          "created_at": "2025-03-05T11:05:11Z"
        },
        {
          "author": "tripzero",
          "body": "> ipex **2.5.10+xpu** works well but unable to import ipex-llm **v2.2.0b20250303** correctly.\n> \n> ```\n> (ipex-vllm) intel@mydevice:~$ python -c \"import torch; import intel_extension_for_pytorch as ipex; print(torch.__version__); print(ipex.__version__); [print(f'[{i}]: {torch.xpu.get_device_propert",
          "created_at": "2025-03-05T17:33:13Z"
        },
        {
          "author": "tripzero",
          "body": "> Hi [@tripzero](https://github.com/tripzero),\n> \n> We did not reproduce the PyTorch 2.1 broken issue. `pip install --pre --upgrade ipex-llm[xpu] --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/cn/` or `pip install --pre --upgrade ipex-llm[xpu] --extra-index-url https://",
          "created_at": "2025-03-05T17:50:54Z"
        },
        {
          "author": "tripzero",
          "body": "Recreated my environment and installed oneapi and ipex-llm via the instructions and it worked this time.",
          "created_at": "2025-03-05T22:59:17Z"
        }
      ]
    },
    {
      "issue_number": 12913,
      "title": "FileNotFoundError: Could not find module 'D:\\anaconda\\envs\\ipex_npu\\Lib\\site-packages\\bigdl-core-npu\\npu_llm.dll'",
      "body": "When I used npu_quickstart to run llama3.1-8b and qwen2.5-7b, I encountered the following detailed error: FileNotFoundError: Could not find module 'D:\\anaconda\\envs\\ipex_npu\\Lib\\site-packages\\bigdl-core-npu\\npu_llm.dll' (or one of its dependencies). Try using the full path with constructor syntax. \nMy ipex-llm version is 2.2.0b20250227.",
      "state": "closed",
      "author": "jzw02",
      "author_type": "User",
      "created_at": "2025-02-28T10:55:44Z",
      "updated_at": "2025-03-10T11:28:14Z",
      "closed_at": "2025-02-28T12:02:15Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12913/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12913",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12913",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:21.199555",
      "comments": [
        {
          "author": "jzw02",
          "body": "The above issue can be resolved by rolling back ipex-llm to version 2.2.0b20250120. \nHowever, I have encountered a new problem: L0 pfnCreate2 result: ZE_RESULT_ERROR_INVALID_ARGUMENT, code 0x78000004.\n I have updated the driver to 32.0.100.3104 and set IPEX_LLM_NPU_MTL=1, but the problem persists.\n ",
          "created_at": "2025-02-28T11:37:06Z"
        },
        {
          "author": "jzw02",
          "body": "In PowerShell,should set $env:IPEX_LLM_NPU_MTL = 1 instead of set IPEX_LLM_NPU_MTL = 1.\nA very stupid mistake for me.",
          "created_at": "2025-02-28T12:02:16Z"
        }
      ]
    },
    {
      "issue_number": 12912,
      "title": "Ollama Portable Zip on Windows with Intel ARC B580 and nomic-embed-text 100% gpu loop",
      "body": "Using Ollama Portable Zip on Windows 11 with Intel ARC B580 and nomic-embed-text 100% gpu loop. The embedding never finishes and stays at this stage:\n\n[GIN] 2025/02/27 - 20:42:52 | 500 |   14.1118255s |  172.27.100.205 | POST     \"/api/embed\"\n[GIN] 2025/02/28 - 11:09:01 | 200 |      1.0139ms |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/02/28 - 11:09:01 | 200 |    203.5074ms |       127.0.0.1 | GET      \"/api/tags\"\ntime=2025-02-28T11:14:32.401+01:00 level=INFO source=server.go:104 msg=\"system memory\" total=\"31.9 GiB\" free=\"15.1 GiB\" free_swap=\"14.7 GiB\"\ntime=2025-02-28T11:14:32.402+01:00 level=INFO source=memory.go:356 msg=\"offload to device\" layers.requested=-1 layers.model=13 layers.offload=0 layers.split=\"\" memory.available=\"[15.1 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"352.9 MiB\" memory.required.partial=\"0 B\" memory.required.kv=\"24.0 MiB\" memory.required.allocations=\"[352.9 MiB]\" memory.weights.total=\"240.1 MiB\" memory.weights.repeating=\"195.4 MiB\" memory.weights.nonrepeating=\"44.7 MiB\" memory.graph.full=\"48.0 MiB\" memory.graph.partial=\"48.0 MiB\"\ntime=2025-02-28T11:14:32.411+01:00 level=INFO source=server.go:392 msg=\"starting llama server\" cmd=\"C:\\\\Users\\\\Admin\\\\ollama-0.5.4-ipex-llm\\\\ollama-lib.exe runner --model C:\\\\Users\\\\Admin\\\\.ollama\\\\models\\\\blobs\\\\sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 999 --threads 4 --no-mmap --parallel 1 --port 55922\"\ntime=2025-02-28T11:14:32.423+01:00 level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\ntime=2025-02-28T11:14:32.426+01:00 level=INFO source=server.go:571 msg=\"waiting for llama runner to start responding\"\ntime=2025-02-28T11:14:32.427+01:00 level=INFO source=server.go:605 msg=\"waiting for server to become available\" status=\"llm server error\"\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\nggml_sycl_init: SYCL_USE_XMX: yes\nggml_sycl_init: found 1 SYCL devices:\ntime=2025-02-28T11:14:33.071+01:00 level=INFO source=runner.go:967 msg=\"starting go runner\"\ntime=2025-02-28T11:14:33.151+01:00 level=INFO source=runner.go:968 msg=system info=\"CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | CPU: SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | cgo(clang)\" threads=4\ntime=2025-02-28T11:14:33.153+01:00 level=INFO source=runner.go:1026 msg=\"Server listening on 127.0.0.1:55922\"\ntime=2025-02-28T11:14:33.191+01:00 level=INFO source=server.go:605 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nllama_load_model_from_file: using device SYCL0 (Intel(R) Arc(TM) B580 Graphics) - 11633 MiB free\nllama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from C:\\Users\\Admin\\.ollama\\models\\blobs\\sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = nomic-bert\nllama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5\nllama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12\nllama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048\nllama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768\nllama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072\nllama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12\nllama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000\nllama_model_loader: - kv   8:                          general.file_type u32              = 1\nllama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false\nllama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1\nllama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000\nllama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2\nllama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101\nllama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102\nllama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert\nllama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = [\"[PAD]\", \"[unused0]\", \"[unused1]\", \"...\nllama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100\nllama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101\nllama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103\nllama_model_loader: - type  f32:   51 tensors\nllama_model_loader: - type  f16:   61 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 5\nllm_load_vocab: token to piece cache size = 0.2032 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = nomic-bert\nllm_load_print_meta: vocab type       = WPM\nllm_load_print_meta: n_vocab          = 30522\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 2048\nllm_load_print_meta: n_embd           = 768\nllm_load_print_meta: n_layer          = 12\nllm_load_print_meta: n_head           = 12\nllm_load_print_meta: n_head_kv        = 12\nllm_load_print_meta: n_rot            = 64\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 64\nllm_load_print_meta: n_embd_head_v    = 64\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 768\nllm_load_print_meta: n_embd_v_gqa     = 768\nllm_load_print_meta: f_norm_eps       = 1.0e-12\nllm_load_print_meta: f_norm_rms_eps   = 0.0e+00\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 3072\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 0\nllm_load_print_meta: pooling type     = 1\nllm_load_print_meta: rope type        = 2\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 1000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 2048\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 137M\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 136.73 M\nllm_load_print_meta: model size       = 260.86 MiB (16.00 BPW)\nllm_load_print_meta: general.name     = nomic-embed-text-v1.5\nllm_load_print_meta: BOS token        = 101 '[CLS]'\nllm_load_print_meta: EOS token        = 102 '[SEP]'\nllm_load_print_meta: UNK token        = 100 '[UNK]'\nllm_load_print_meta: SEP token        = 102 '[SEP]'\nllm_load_print_meta: PAD token        = 0 '[PAD]'\nllm_load_print_meta: CLS token        = 101 '[CLS]'\nllm_load_print_meta: MASK token       = 103 '[MASK]'\nllm_load_print_meta: LF token         = 0 '[PAD]'\nllm_load_print_meta: EOG token        = 102 '[SEP]'\nllm_load_print_meta: max token length = 21\nllm_load_tensors: offloading 12 repeating layers to GPU\nllm_load_tensors: offloading output layer to GPU\nllm_load_tensors: offloaded 13/13 layers to GPU\nllm_load_tensors:        SYCL0 model buffer size =   216.14 MiB\nllm_load_tensors:    SYCL_Host model buffer size =    44.72 MiB\nllama_new_context_with_model: n_seq_max     = 1\nllama_new_context_with_model: n_ctx         = 8192\nllama_new_context_with_model: n_ctx_per_seq = 8192\nllama_new_context_with_model: n_batch       = 512\nllama_new_context_with_model: n_ubatch      = 512\nllama_new_context_with_model: flash_attn    = 0\nllama_new_context_with_model: freq_base     = 1000.0\nllama_new_context_with_model: freq_scale    = 1\nllama_new_context_with_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow\n[SYCL] call ggml_check_sycl\nggml_check_sycl: GGML_SYCL_DEBUG: 0\nggml_check_sycl: GGML_SYCL_F16: no\nFound 1 SYCL devices:\n|  |                   |                                       |       |Max    |        |Max  |Global |\n    |\n|  |                   |                                       |       |compute|Max work|sub  |mem    |\n    |\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\n| 0| [level_zero:gpu:0]|                Intel Arc B580 Graphics|   20.1|    160|    1024|   32| 12508M|            1.6.31896|\nllama_kv_cache_init:      SYCL0 KV buffer size =   144.00 MiB\nllama_new_context_with_model: KV self size  =  144.00 MiB, K (i8):   72.00 MiB, V (i8):   72.00 MiB\nllama_new_context_with_model:  SYCL_Host  output buffer size =     0.00 MiB\nllama_new_context_with_model:      SYCL0 compute buffer size =    17.50 MiB\nllama_new_context_with_model:  SYCL_Host compute buffer size =     3.50 MiB\nllama_new_context_with_model: graph nodes  = 429\nllama_new_context_with_model: graph splits = 4 (with bs=512), 2 (with bs=1)\nllama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from C:\\Users\\Admin\\.ollama\\models\\blobs\\sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))25-02-28T11:14:36.453+01:00 level=INFO source=server.go:610 msg=\"llama runner started in 4.03 seconds\"\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = nomic-bert\nllama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5\nllama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12\nllama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048\nllama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768\nllama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072\nllama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12\nllama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000\nllama_model_loader: - kv   8:                          general.file_type u32              = 1\nllama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false\nllama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1\nllama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000\nllama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2\nllama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101\nllama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102\nllama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert\nllama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = [\"[PAD]\", \"[unused0]\", \"[unused1]\", \"...\nllama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100\nllama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101\nllama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103\nllama_model_loader: - type  f32:   51 tensors\nllama_model_loader: - type  f16:   61 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 5\nllm_load_vocab: token to piece cache size = 0.2032 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = nomic-bert\nllm_load_print_meta: vocab type       = WPM\nllm_load_print_meta: n_vocab          = 30522\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 1\nllm_load_print_meta: model type       = ?B\nllm_load_print_meta: model ftype      = all F32\nllm_load_print_meta: model params     = 136.73 M\nllm_load_print_meta: model size       = 260.86 MiB (16.00 BPW)\nllm_load_print_meta: general.name     = nomic-embed-text-v1.5\nllm_load_print_meta: BOS token        = 101 '[CLS]'\nllm_load_print_meta: EOS token        = 102 '[SEP]'\nllm_load_print_meta: UNK token        = 100 '[UNK]'\nllm_load_print_meta: SEP token        = 102 '[SEP]'\nllm_load_print_meta: PAD token        = 0 '[PAD]'\nllm_load_print_meta: CLS token        = 101 '[CLS]'\nllm_load_print_meta: MASK token       = 103 '[MASK]'\nllm_load_print_meta: LF token         = 0 '[PAD]'\nllm_load_print_meta: EOG token        = 102 '[SEP]'\nllm_load_print_meta: max token length = 21\nllama_model_load: vocab only - skipping tensors",
      "state": "closed",
      "author": "DediCATeD88",
      "author_type": "User",
      "created_at": "2025-02-28T10:22:55Z",
      "updated_at": "2025-03-10T11:28:14Z",
      "closed_at": "2025-02-28T16:25:26Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12912/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12912",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12912",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:21.393622",
      "comments": []
    },
    {
      "issue_number": 12829,
      "title": "deepseek-v2:16b fails to load due to missing tensor 'blk.1.exp_probs_b.bias'",
      "body": "Title. ",
      "state": "closed",
      "author": "stereomato",
      "author_type": "User",
      "created_at": "2025-02-14T19:56:52Z",
      "updated_at": "2025-03-10T11:28:13Z",
      "closed_at": "2025-02-17T18:29:57Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12829/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12829",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12829",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:21.393644",
      "comments": [
        {
          "author": "qiyuangong",
          "body": "Can you share more details about the hardware (GPU/NPU or CPU) and the code/example used? So we can reproduce this error and fix it.",
          "created_at": "2025-02-16T02:34:15Z"
        },
        {
          "author": "stereomato",
          "body": "using this container, running on NixOS https://github.com/mattcurf/ollama-intel-gpu\n\npodman build -t \"ollama-intel-gpu\" .\n\npodman run --rm -p 127.0.0.1:11434:11434 -v /home/stereomato/models:/mnt -v ollama-volume:/root/.ollama -e OLLAMA_NUM_PARALLEL=1 -e OLLAMA_MAX_LOADED_MODELS=1 -e OLLAMA_FLASH_AT",
          "created_at": "2025-02-16T16:21:32Z"
        },
        {
          "author": "qiuxin2012",
          "body": "Your RAM is 24GB, so iGPU can only use half of them. 12GB is very hard to run a 16b model due to memory limitation.",
          "created_at": "2025-02-17T01:27:41Z"
        },
        {
          "author": "stereomato",
          "body": "7b has the same issue\n\nSent from Proton Mail Android\n\n-------- Original Message --------\nOn 2/16/25 8:28 PM, Xin Qiu  wrote:\n\n> Your RAM is 24GB, so iGPU can only use half of them. 12GB is very hard to run a 16b model due to memory limitation.\n>\n> —\n> Reply to this email directly, [view it on GitHub",
          "created_at": "2025-02-17T01:33:14Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @stereomato , we have reproduced and fixed this issue, pls reinstall our latest version of ipex-llm ollama tmr :)",
          "created_at": "2025-02-17T08:36:07Z"
        }
      ]
    },
    {
      "issue_number": 12809,
      "title": "[The current device architecture is not supported by sycl_ext_oneapi_device_architecture]",
      "body": "Hello,\n\nI am using intelanalytics/ipex-llm-serving-xpu docker image  to run VLLM on the  Battlemage card B580\n\ni got this error : \n\nterminate called after throwing an instance of 'sycl::_V1::exception'\n  what():  The current device architecture is not supported by sycl_ext_oneapi_device_architecture.\nAborted (core dumped)\n\ncommand line : \n\n\npython3 ./benchmark_vllm_throughput.py     --backend vllm     --dataset ./ShareGPT_V3_unfiltered_cleaned_split.json     --model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B      --num-prompts 100     --seed 42     --trust-remote-code     --enforce-eager     --dtype float16     --device xpu\n\nThank you in advance \nAmine mrabet\n\n",
      "state": "closed",
      "author": "ammrabet",
      "author_type": "User",
      "created_at": "2025-02-11T10:09:09Z",
      "updated_at": "2025-03-10T11:28:12Z",
      "closed_at": "2025-02-17T15:31:39Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12809/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "gc-fu"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12809",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12809",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:21.583062",
      "comments": [
        {
          "author": "gc-fu",
          "body": "Hi, we will investigate the issue.",
          "created_at": "2025-02-12T02:29:50Z"
        },
        {
          "author": "gc-fu",
          "body": "Hi, we have not encountered this problem after trying with our newest image: `intelanalytics/ipex-llm-serving-xpu:2.2.0-b13`.\n\nTry pull this image and see if this problem persists~\n\nThe command:\n```bash\npython3 ./benchmark_vllm_throughput.py --backend vllm --dataset ./ShareGPT_V3_unfiltered_cleaned_",
          "created_at": "2025-02-13T07:16:30Z"
        },
        {
          "author": "ammrabet",
          "body": "Hi, it works very well with the new image.\n\nthank you very much ",
          "created_at": "2025-02-17T11:07:17Z"
        }
      ]
    },
    {
      "issue_number": 12792,
      "title": "[Security] Removing remaining privileged container flags",
      "body": "Following the successful removal of privileged containers in #8432, we noticed some privileged usage still exists in the codebase ([search results](https://github.com/search?q=repo%3Aintel%2Fipex-llm%20privileged%20&type=code)). \n\nWould you consider:\n1. Comprehensive audit to remove all `privileged` flags\n2. Using granular capabilities instead where needed\n\nThis would align with container security best practices and build on the great work already done in #8432.",
      "state": "closed",
      "author": "charliez0",
      "author_type": "User",
      "created_at": "2025-02-09T15:14:32Z",
      "updated_at": "2025-03-10T11:28:12Z",
      "closed_at": "2025-03-03T15:30:16Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12792/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "liu-shaojun"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12792",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12792",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:21.766088",
      "comments": [
        {
          "author": "liu-shaojun",
          "body": "Thanks for bringing this up! We'll go through the codebase to identify any unnecessary `--privileged` usage and remove it. Where needed, we'll replace it with more granular capabilities to align with container security best practices. Appreciate the suggestion!",
          "created_at": "2025-02-10T02:15:00Z"
        },
        {
          "author": "charliez0",
          "body": "any progresses?",
          "created_at": "2025-03-02T13:46:30Z"
        },
        {
          "author": "liu-shaojun",
          "body": "Hi @charliez0 \n\nWe've submitted a [PR](https://github.com/intel/ipex-llm/pull/12920) to remove the unnecessary `--privileged` flag on Linux. However, in the **Windows + WSL** scenario, `--privileged` is still required for GPU access, so we've kept it for now.  \n\nLet us know if you have any further c",
          "created_at": "2025-03-03T01:25:38Z"
        }
      ]
    },
    {
      "issue_number": 12085,
      "title": "Intel Arc A770, Ubuntu 24.04: RuntimeError: could not create an engine",
      "body": "I'm trying to see if I got the install process right, and I'm trying to follow the \"A quick example\" section found [here](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/install_linux_gpu.md). \r\n\r\nHere's what my demo.py file looks like:\r\n```\r\nfrom ipex_llm.transformers import AutoModelForCausalLM\r\nfrom transformers import AutoTokenizer, GenerationConfig\r\ngeneration_config = GenerationConfig(use_cache = True)\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\", trust_remote_code=True)\r\n# load Model using ipex-llm and load it to GPU\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\n    \"tiiuae/falcon-7b\", load_in_4bit=True, cpu_embedding=False, trust_remote_code=True)\r\nmodel = model.to('xpu')\r\n\r\n# Format the prompt\r\nquestion = \"What is AI?\"\r\nprompt = \" Question:{prompt}\\n\\n Answer:\".format(prompt=question)\r\n# Generate predicted tokens\r\nwith torch.inference_mode():\r\n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to('xpu')\r\n    # warm up one more time before the actual generation task for the first run, see details in `Tips & Troubleshooting`\r\n    # output = model.generate(input_ids, do_sample=False, max_new_tokens=32, generation_config = generation_config)\r\n    output = model.generate(input_ids, do_sample=False, max_new_tokens=32, generation_config = generation_config).cpu()\r\n    output_str = tokenizer.decode(output[0], skip_special_tokens=True)\r\n    print(output_str)\r\n```\r\n\r\nHere's what I get in the terminal when I try to run the demo file:\r\n\r\n\r\n```\r\n(llm) cbytes@cbytes-ubuntu:~$ python demo.py\r\n/home/cbytes/miniforge3/envs/llm/lib/python3.11/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\r\n  warnings.warn(\r\n/home/cbytes/miniforge3/envs/llm/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n  warn(\r\n2024-09-15 00:38:22,224 - INFO - intel_extension_for_pytorch auto imported\r\n/home/cbytes/miniforge3/envs/llm/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\n2024-09-15 00:38:22,556 - WARNING - \r\nWARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\r\n\r\nLoading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]/home/cbytes/miniforge3/envs/llm/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\nLoading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.87s/it]\r\n2024-09-15 00:38:28,447 - INFO - Converting the current model to sym_int4 format......\r\nTraceback (most recent call last):\r\n  File \"/home/cbytes/demo.py\", line 21, in <module>\r\n    output = model.generate(input_ids, do_sample=False, max_new_tokens=32, generation_config = generation_config).cpu()\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/cbytes/miniforge3/envs/llm/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/cbytes/miniforge3/envs/llm/lib/python3.11/site-packages/ipex_llm/transformers/lookup.py\", line 123, in generate\r\n    return original_generate(self,\r\n           ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/cbytes/miniforge3/envs/llm/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/cbytes/miniforge3/envs/llm/lib/python3.11/site-packages/ipex_llm/transformers/speculative.py\", line 109, in generate\r\n    return original_generate(self,\r\n           ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/cbytes/miniforge3/envs/llm/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/cbytes/miniforge3/envs/llm/lib/python3.11/site-packages/ipex_llm/transformers/pipeline_parallel.py\", line 281, in generate\r\n    return original_generate(self,\r\n           ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/cbytes/miniforge3/envs/llm/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/cbytes/miniforge3/envs/llm/lib/python3.11/site-packages/transformers/generation/utils.py\", line 1474, in generate\r\n    return self.greedy_search(\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/cbytes/miniforge3/envs/llm/lib/python3.11/site-packages/transformers/generation/utils.py\", line 2335, in greedy_search\r\n    outputs = self(\r\n              ^^^^^\r\n  File \"/home/cbytes/miniforge3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/cbytes/miniforge3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/cbytes/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/898df1396f35e447d5fe44e0a3ccaaaa69f30d36/modeling_falcon.py\", line 900, in forward\r\n    transformer_outputs = self.transformer(\r\n                          ^^^^^^^^^^^^^^^^^\r\n  File \"/home/cbytes/miniforge3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/cbytes/miniforge3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/cbytes/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/898df1396f35e447d5fe44e0a3ccaaaa69f30d36/modeling_falcon.py\", line 797, in forward\r\n    outputs = block(\r\n              ^^^^^^\r\n  File \"/home/cbytes/miniforge3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/cbytes/miniforge3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/cbytes/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/898df1396f35e447d5fe44e0a3ccaaaa69f30d36/modeling_falcon.py\", line 453, in forward\r\n    attn_outputs = self.self_attention(\r\n                   ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/cbytes/miniforge3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/cbytes/miniforge3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/cbytes/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/898df1396f35e447d5fe44e0a3ccaaaa69f30d36/modeling_falcon.py\", line 341, in forward\r\n    attn_output = F.scaled_dot_product_attention(\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: could not create an engine\r\n```\r\n\r\n\r\n\r\n",
      "state": "closed",
      "author": "compellingbytes",
      "author_type": "User",
      "created_at": "2024-09-15T04:45:32Z",
      "updated_at": "2025-03-10T11:28:11Z",
      "closed_at": "2024-09-15T05:00:51Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12085/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12085",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12085",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:21.958929",
      "comments": [
        {
          "author": "caijimin",
          "body": "I met the same problem. Could you help to tell how to resolve? Suppose it's environment problem.",
          "created_at": "2025-02-28T02:42:55Z"
        },
        {
          "author": "caijimin",
          "body": "Should \"unset OCL_ICD_VENDORS\".\nhttps://github.com/intel/ipex-llm/blob/8d94752c4beb9ff8aa7e3912747cdbc215d85c3f/docs/mddocs/Quickstart/bmg_quickstart.md#41-runtimeerror-could-not-create-an-engine",
          "created_at": "2025-02-28T07:36:09Z"
        }
      ]
    },
    {
      "issue_number": 12818,
      "title": "Multivariate forecasting using Chronos",
      "body": "Hi,\n\nI am trying to do multivariate forecasting using Chronos but unable to find any documentation for it. I found a link of this repo for an example on how to do it but it seems to be broken.\n\nhttps://github.com/intel/ipex-llm/blob/main/python/chronos/use-case/network_traffic/network_traffic_multivariate_multistep_tcnforecaster.ipynb\n\nCan anyone please point me in the right direction?",
      "state": "open",
      "author": "asfand-khan-98",
      "author_type": "User",
      "created_at": "2025-02-12T15:16:17Z",
      "updated_at": "2025-03-10T11:27:40Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12818/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12818",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12818",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:22.186054",
      "comments": [
        {
          "author": "hkvision",
          "body": "Hi, the link has been migrated to: https://github.com/intel/bigdl/blob/main/python/chronos/use-case/network_traffic/network_traffic_multivariate_multistep_tcnforecaster.ipynb\nPlease check it :)",
          "created_at": "2025-02-13T02:10:52Z"
        }
      ]
    },
    {
      "issue_number": 12806,
      "title": "The latest driver 32.0.101.6559 broke inference speed",
      "body": "Intel Arc B580\nDriver: 32.0.101.6559\nOS: Windows\nModel: llama3.2:1B\nPrompt: 'write js code hello world'\nInference speed dropped from 150 tokens per second to 20.06 tokens per second\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "state": "open",
      "author": "xyang2013",
      "author_type": "User",
      "created_at": "2025-02-11T07:16:03Z",
      "updated_at": "2025-03-10T11:27:40Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12806/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12806",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12806",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:22.379891",
      "comments": [
        {
          "author": "Oscilloscope98",
          "body": "Hi @xyang2013, \n\nWe have reproduced this issue. Would you mind unsetting the environment variable `SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS` through e.g. `set SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=` and having a try again?",
          "created_at": "2025-02-11T10:40:08Z"
        },
        {
          "author": "xyang2013",
          "body": "Hi @Oscilloscope98,\n\nThank you! I was able to return to the expected speed (between 155 to 160 tokens per second).\n\nHowever, the documentation states that setting the following should improve performance:\nset SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1\n\nIs your suggestion a temporary fix?\n",
          "created_at": "2025-02-11T13:19:38Z"
        },
        {
          "author": "xyang2013",
          "body": "By the way, is it possible to communicate with Microsoft and ask them to stop rolling back the graphics driver to a version that is several months old? It creates a lot of unnecessary work constantly. As you know, the performance can be dependant of on the graphic driver.\n\nhttps://x.com/XYang2023/st",
          "created_at": "2025-02-12T00:19:43Z"
        },
        {
          "author": "Oscilloscope98",
          "body": "> Hi [@Oscilloscope98](https://github.com/Oscilloscope98),\n> \n> Thank you! I was able to return to the expected speed (between 155 to 160 tokens per second).\n> \n> However, the documentation states that setting the following should improve performance: set SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLIST",
          "created_at": "2025-02-12T06:21:28Z"
        },
        {
          "author": "xyang2013",
          "body": "Hi @Oscilloscope98,\n\nThank you for suggesting a way to prevent driver rollbacks. I just did what you suggested.\n\nI mentioned the SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS flag because, with the previous driver, it provided a 5–10% performance boost. However, with the new driver, I can’t match th",
          "created_at": "2025-02-12T07:21:40Z"
        }
      ]
    },
    {
      "issue_number": 12782,
      "title": "Unable to run llama.cpp on A770, missing .so file",
      "body": "Hello,\n\nI have a fresh install of Ubuntu Server 24.04 on my machine. After following the guides at\n\nhttps://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/install_linux_gpu.md\nhttps://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/llama_cpp_quickstart.md\n\nI cannot get llama.cpp running, I get an error for a missing .so file:\n\n```\n./llama-cli: error while loading shared libraries: libggml-cpu.so: cannot open shared object file: No such file or directory\n```\n\nThese are the steps I took to install the drivers:\n\n```\n# Adding apt repo\nsudo apt-get install -y gpg-agent wget\nwget -qO - https://repositories.intel.com/gpu/intel-graphics.key | \\\n  sudo gpg --dearmor --output /usr/share/keyrings/intel-graphics.gpg\necho \"deb [arch=amd64,i386 signed-by=/usr/share/keyrings/intel-graphics.gpg] https://repositories.intel.com/gpu/ubuntu jammy client\" | \\\n  sudo tee /etc/apt/sources.list.d/intel-gpu-jammy.list\nsudo apt-get update\n\n# Installing Mesa\nsudo apt-get install -y udev \\\n  intel-opencl-icd intel-level-zero-gpu level-zero \\\n  intel-media-va-driver-non-free libmfx1 libmfxgen1 libvpl2 \\\n  libegl-mesa0 libegl1-mesa libegl1-mesa-dev libgbm1 libgl1-mesa-dev libgl1-mesa-dri \\\n  libglapi-mesa libgles2-mesa-dev libglx-mesa0 libigdgmm12 libxatracker2 mesa-va-drivers \\\n  mesa-vdpau-drivers mesa-vulkan-drivers va-driver-all vainfo\n\nsudo reboot\n```\n\nInstalling oneAPI:\n\n```\n# Creating group\nsudo gpasswd -a ${USER} render\nnewgrp render\n\n# Adding apt repo for oneAPI\nwget -O- https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB | \\\n  gpg --dearmor | sudo tee /usr/share/keyrings/oneapi-archive-keyring.gpg > /dev/null\n  \necho \"deb [signed-by=/usr/share/keyrings/oneapi-archive-keyring.gpg] https://apt.repos.intel.com/oneapi all main\" | \\\n  sudo tee /etc/apt/sources.list.d/oneAPI.list\n  \nsudo apt update\n\n# Installing oneAPI\nsudo apt install intel-oneapi-common-vars=2024.0.0-49406 \\\n  intel-oneapi-common-oneapi-vars=2024.0.0-49406 \\\n  intel-oneapi-diagnostics-utility=2024.0.0-49093 \\\n  intel-oneapi-compiler-dpcpp-cpp=2024.0.2-49895 \\\n  intel-oneapi-dpcpp-ct=2024.0.0-49381 \\\n  intel-oneapi-mkl=2024.0.0-49656 \\\n  intel-oneapi-mkl-devel=2024.0.0-49656 \\\n  intel-oneapi-mpi=2021.11.0-49493 \\\n  intel-oneapi-mpi-devel=2021.11.0-49493 \\\n  intel-oneapi-dal=2024.0.1-25 \\\n  intel-oneapi-dal-devel=2024.0.1-25 \\\n  intel-oneapi-ippcp=2021.9.1-5 \\\n  intel-oneapi-ippcp-devel=2021.9.1-5 \\\n  intel-oneapi-ipp=2021.10.1-13 \\\n  intel-oneapi-ipp-devel=2021.10.1-13 \\\n  intel-oneapi-tlt=2024.0.0-352 \\\n  intel-oneapi-ccl=2021.11.2-5 \\\n  intel-oneapi-ccl-devel=2021.11.2-5 \\\n  intel-oneapi-dnnl-devel=2024.0.0-49521 \\\n  intel-oneapi-dnnl=2024.0.0-49521 \\\n  intel-oneapi-tcm-1.0=1.0.0-435\n\nsudo reboot\n```\n\nWith the above setup, I was able to run a modified version of the demo (it seems the demo listed on the page is broken?)\n\nInstalling llama.cpp:\n```\nconda create -n llm-cpp python=3.11\nconda activate llm-cpp\npip install --pre --upgrade ipex-llm[cpp]\n\nmkdir llama-cpp\ncd llama-cpp\n\ninit-llama-cpp\n\nwget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf\n\n```\n\nAttempting to run the llama.cpp demo:\n```\nsource /opt/intel/oneapi/setvars.sh\nexport SYCL_CACHE_PERSISTENT=1\n./llama-cli -m mistral-7b-instruct-v0.1.Q4_K_M.gguf -n 32 --prompt \"Once upon a time, there existed a little girl who liked to have adventures. She wanted to go to places and meet new people, and have fun\" -c 1024 -t 8 -e -ngl 99 --color\n./llama-cli: error while loading shared libraries: libggml-cpu.so: cannot open shared object file: No such file or directory\n```\n\nI'm somewhat confused as to what could've gone wrong. Did I miss a step, or do I have the wrong version of some package? I also get a similar missing .so file error when trying to run ollama.",
      "state": "open",
      "author": "KczBen",
      "author_type": "User",
      "created_at": "2025-02-07T00:06:06Z",
      "updated_at": "2025-03-10T11:27:39Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12782/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12782",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12782",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:22.576569",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @KczBen,  You may install ollama through `pip install ipex-llm[cpp]==2.2.0b20250204`.\n\nWe will fix this issue today, and you may also try the latest version tmr.",
          "created_at": "2025-02-07T02:11:25Z"
        },
        {
          "author": "KczBen",
          "body": "Thank you, that works! Now I have Ollama running, though I'm also encountering the same issue as #12761 with `deepseek-r1:14b`",
          "created_at": "2025-02-07T12:47:16Z"
        },
        {
          "author": "sgwhat",
          "body": "> Thank you, that works! Now I have Ollama running, though I'm also encountering the same issue as [#12761](https://github.com/intel/ipex-llm/issues/12761) with `deepseek-r1:14b`\n\nCould you pls provide some detailed logs? And in which round of conversation did the error begin to occur?",
          "created_at": "2025-02-10T04:05:25Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @KczBen , for deepseek-r1 garbage output issue, you may refer to my response in https://github.com/intel/ipex-llm/issues/12761#issuecomment-2647175453.",
          "created_at": "2025-02-10T08:05:53Z"
        },
        {
          "author": "KczBen",
          "body": "I always got the issue on the second response from r1:14b. Using today's ollama with IPEX-LLM and oneAPI 202**5**, I have gotten 5 coherent messages so far in the same chat without specifying the longer context. I'm using the same prompts I did when I was getting the garbage outputs before.\n\nI have ",
          "created_at": "2025-02-10T14:32:19Z"
        }
      ]
    },
    {
      "issue_number": 12781,
      "title": "A770 use ollama serve err",
      "body": "![Image](https://github.com/user-attachments/assets/66ec2404-a7dc-40d1-979f-6db67d78460f)",
      "state": "open",
      "author": "NewBeeFly",
      "author_type": "User",
      "created_at": "2025-02-06T15:18:25Z",
      "updated_at": "2025-03-10T11:27:39Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12781/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12781",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12781",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:24.559679",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @NewBeeFly , we are releasing oneapi-2025 ipex-llm ollama. You may install ollama through `pip install ipex-llm[cpp]==2.2.0b20250204`.\n\nWe will fix this issue today, and you may also try the latest version tmr.",
          "created_at": "2025-02-07T01:55:51Z"
        },
        {
          "author": "NewBeeFly",
          "body": "thank you.  it works now.\n\n> Hi [@NewBeeFly](https://github.com/NewBeeFly) , we are releasing oneapi-2025 ipex-llm ollama. You may install ollama through `pip install ipex-llm[cpp]==2.2.0b20250204`.\n> \n> We will fix this issue today, and you may also try the latest version tmr.\n\n",
          "created_at": "2025-02-07T06:57:53Z"
        }
      ]
    },
    {
      "issue_number": 12717,
      "title": "Does ipex-llm support  for Intel® Movidius™ Vision Processing Unit (VPU)",
      "body": "if no, any plane to support it?",
      "state": "open",
      "author": "shichang00",
      "author_type": "User",
      "created_at": "2025-01-17T07:06:12Z",
      "updated_at": "2025-03-10T11:27:38Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12717/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12717",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12717",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:24.741455",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "No plan to support VPU.",
          "created_at": "2025-01-20T03:21:53Z"
        }
      ]
    },
    {
      "issue_number": 12660,
      "title": "performance downgrade on dGPU Arc770 after loading more than one LLM model",
      "body": "1) Test env is setup as follow: \r\nhttps://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/install_linux_gpu.md\r\nhttps://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/GPU/HuggingFace/LLM/chatglm3\r\n2) OS env \r\n- dGPU Arc770\r\n- Linux kernel = 6.1.59\r\n- Ubuntu = 22.04\r\n3) python test.py\r\nif load more than one models, the inference latency increase:\r\n- if only chatglm3 model loaded: llm infer 0.90 s\r\n- if only whisper-small model loaded: wsp infer 0.68 s\r\n- if both whisper-small + chatglm3 loaded: \r\n llm infer 1.22 s\r\n wsp infer 1.01 s\r\n- if 4 models loaded: \r\n llm infer 2.07 s\r\n cpm infer 2.97 s\r\n sd  infer 0.74 s\r\n wsp infer 1.93 s",
      "state": "open",
      "author": "qing-xu-intel",
      "author_type": "User",
      "created_at": "2025-01-07T05:38:24Z",
      "updated_at": "2025-03-10T11:27:37Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 11,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12660/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "ACupofAir"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12660",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12660",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:24.942744",
      "comments": [
        {
          "author": "qing-xu-intel",
          "body": "import os\r\nimport time\r\nimport torch\r\nimport requests\r\nimport librosa\r\nfrom diffusers import DiffusionPipeline\r\nfrom PIL import Image\r\nfrom ipex_llm.transformers import AutoModel\r\nfrom ipex_llm.transformers import AutoModelForCausalLM\r\nfrom ipex_llm.optimize import low_memory_init, load_low_bit\r\nfro",
          "created_at": "2025-01-07T05:39:55Z"
        },
        {
          "author": "qing-xu-intel",
          "body": "Please use below value to run the test cases (if want to run the case, set it to True): \r\n    llm_test = True\r\n    sd_test = False\r\n    minicpm_test = False\r\n    wp_test = False",
          "created_at": "2025-01-07T05:46:11Z"
        },
        {
          "author": "ACupofAir",
          "body": "> import os import time import torch import requests import librosa from diffusers import DiffusionPipeline from PIL import Image from ipex_llm.transformers import AutoModel from ipex_llm.transformers import AutoModelForCausalLM from ipex_llm.optimize import low_memory_init, load_low_bit from ipex_l",
          "created_at": "2025-01-08T00:33:36Z"
        },
        {
          "author": "qing-xu-intel",
          "body": "[test.txt](https://github.com/user-attachments/files/18342211/test.txt)\r\n\r\nplease modify attached test.txt to test.py",
          "created_at": "2025-01-08T05:29:32Z"
        },
        {
          "author": "qing-xu-intel",
          "body": "Do you have any progress on this issue? Is there any other information that you need from me?",
          "created_at": "2025-01-10T04:47:40Z"
        }
      ]
    },
    {
      "issue_number": 12955,
      "title": "ollama-0.5.4-ipex-llm-2.2.0b20250220-win Socket address issue",
      "body": "![Image](https://github.com/user-attachments/assets/caefba9f-3712-480a-84ef-52ecfc957b9c)\n\nWhen I run start-ollama.bat\n\nNew popup appears but It stops due to socket address issue.\n\n",
      "state": "open",
      "author": "respwill",
      "author_type": "User",
      "created_at": "2025-03-08T06:54:24Z",
      "updated_at": "2025-03-10T11:27:05Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12955/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12955",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12955",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:25.172424",
      "comments": [
        {
          "author": "Oscilloscope98",
          "body": "Hi @respwill,\n\nIt seems that there is another Ollama serve already running on your machine (maybe the official Ollama). Please make sure to stop it first before running `start-ollama.bat` :)\n\n",
          "created_at": "2025-03-10T02:21:12Z"
        }
      ]
    },
    {
      "issue_number": 12953,
      "title": "vllm镜像无法启动",
      "body": "由于vllm直接访问huggingface.co\n在启动镜像的过程中，huggingface.co无法访问，导致容器无法启动\n\n\n\n2025-03-07 18:32:47   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n2025-03-07 18:32:47     self._target(*self._args, **self._kwargs)\n2025-03-07 18:32:47   File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 242, in run_mp_engine\n2025-03-07 18:32:47     raise e  # noqa\n2025-03-07 18:32:47     ^^^^^^^\n2025-03-07 18:32:47   File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 234, in run_mp_engine\n2025-03-07 18:32:47     engine = IPEXLLMMQLLMEngine.from_engine_args(engine_args=engine_args,\n2025-03-07 18:32:47              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-03-07 18:32:47   File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 221, in from_engine_args\n2025-03-07 18:32:47     return super().from_engine_args(engine_args, usage_context, ipc_path)\n2025-03-07 18:32:47            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-03-07 18:32:47   File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/multiprocessing/engine.py\", line 114, in from_engine_args\n2025-03-07 18:32:47     engine_config = engine_args.create_engine_config(usage_context)\n2025-03-07 18:32:47                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-03-07 18:32:47   File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/arg_utils.py\", line 1066, in create_engine_config\n2025-03-07 18:32:47     model_config = self.create_model_config()\n2025-03-07 18:32:47                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-03-07 18:32:47   File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/arg_utils.py\", line 983, in create_model_config\n2025-03-07 18:32:47     return ModelConfig(\n2025-03-07 18:32:47            ^^^^^^^^^^^^\n2025-03-07 18:32:47   File \"/usr/local/lib/python3.11/dist-packages/vllm/config.py\", line 286, in __init__\n2025-03-07 18:32:47     hf_config = get_config(self.model, trust_remote_code, revision,\n2025-03-07 18:32:47                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-03-07 18:32:47   File \"/usr/local/lib/python3.11/dist-packages/vllm/transformers_utils/config.py\", line 180, in get_config\n2025-03-07 18:32:47     if is_gguf or file_or_path_exists(\n2025-03-07 18:32:47                   ^^^^^^^^^^^^^^^^^^^^\n2025-03-07 18:32:47   File \"/usr/local/lib/python3.11/dist-packages/vllm/transformers_utils/config.py\", line 99, in file_or_path_exists\n2025-03-07 18:32:47     return file_exists(model,\n2025-03-07 18:32:47            ^^^^^^^^^^^^^^^^^^\n2025-03-07 18:32:47   File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n2025-03-07 18:32:47     return fn(*args, **kwargs)\n2025-03-07 18:32:47            ^^^^^^^^^^^^^^^^^^^\n2025-03-07 18:32:47   File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py\", line 2885, in file_exists\n2025-03-07 18:32:47     get_hf_file_metadata(url, token=token)\n2025-03-07 18:32:47   File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n2025-03-07 18:32:47     return fn(*args, **kwargs)\n2025-03-07 18:32:47            ^^^^^^^^^^^^^^^^^^^\n2025-03-07 18:32:47   File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 1296, in get_hf_file_metadata\n2025-03-07 18:32:47     r = _request_wrapper(\n2025-03-07 18:32:47         ^^^^^^^^^^^^^^^^^\n2025-03-07 18:32:47   File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 280, in _request_wrapper\n2025-03-07 18:32:47     response = _request_wrapper(\n2025-03-07 18:32:47                ^^^^^^^^^^^^^^^^^\n2025-03-07 18:32:47   File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 303, in _request_wrapper\n2025-03-07 18:32:47     response = get_session().request(method=method, url=url, **params)\n2025-03-07 18:32:47                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-03-07 18:32:47   File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 589, in request\n2025-03-07 18:32:47     resp = self.send(prep, **send_kwargs)\n2025-03-07 18:32:47            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-03-07 18:32:47   File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 703, in send\n2025-03-07 18:32:47     r = adapter.send(request, **kwargs)\n2025-03-07 18:32:47         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-03-07 18:32:47   File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\", line 96, in send\n2025-03-07 18:32:47     return super().send(request, *args, **kwargs)\n2025-03-07 18:32:47            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-03-07 18:32:47   File \"/usr/local/lib/python3.11/dist-packages/requests/adapters.py\", line 713, in send\n2025-03-07 18:32:47     raise ReadTimeout(e, request=request)\n2025-03-07 18:32:47 requests.exceptions.ReadTimeout: (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 7bee7530-d2d2-4544-a13d-4e79cf7d19a8)')",
      "state": "open",
      "author": "maxsnow",
      "author_type": "User",
      "created_at": "2025-03-07T10:35:34Z",
      "updated_at": "2025-03-10T11:27:04Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12953/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12953",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12953",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:25.405946",
      "comments": [
        {
          "author": "Airren",
          "body": "encountered the same problem today",
          "created_at": "2025-03-07T12:59:26Z"
        },
        {
          "author": "liu-shaojun",
          "body": "Hi\n\nThanks for reaching out!\n\nWe've recently added an `ENTRYPOINT` to our Dockerfile to automatically launch the vLLM service when the container starts. This might be the reason for the issue you're experiencing.\n\nIf you'd like to start the container **and manually launch the service**, you can over",
          "created_at": "2025-03-10T02:16:27Z"
        }
      ]
    },
    {
      "issue_number": 12948,
      "title": "Please provide support for Microsoft's Phi4",
      "body": "Please provide support for Microsoft's Phi4",
      "state": "open",
      "author": "JamasChuang94",
      "author_type": "User",
      "created_at": "2025-03-06T15:07:02Z",
      "updated_at": "2025-03-10T11:27:03Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12948/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12948",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12948",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:25.619558",
      "comments": [
        {
          "author": "Oscilloscope98",
          "body": "Hi @JamasChuang94,\n\nYou could try Phi4 with [IPEX-LLM Ollama](https://github.com/intel/ipex-llm/releases/tag/v2.2.0-nightly), or [IPEX-LLM llama.cpp](https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/llamacpp_portable_zip_gpu_quickstart.md) :)",
          "created_at": "2025-03-07T02:16:06Z"
        },
        {
          "author": "JamasChuang94",
          "body": "Microsoft's Phi4 does not work in IPEX-LLM llama.cpp, and this error occurs: llama.cpp:10896: GGML_ASSERT(hparams.n_swa > 0) failed",
          "created_at": "2025-03-07T08:20:30Z"
        }
      ]
    },
    {
      "issue_number": 12922,
      "title": "Illegal instruction     (core dumped)",
      "body": "##Running Scripts：\n#!/bin/bash\nmodel=\"/llm/models/DeepSeek-R1-Distill-Qwen-7B\"\nserved_model_name=\"deepseek-7b\"\n \n\npython -m ipex_llm.vllm.cpu.entrypoints.openai.api_server \\\n  --served-model-name $served_model_name \\\n  --port 8001 \\\n  --model $model \\\n  --trust-remote-code \\\n  --device cpu \\\n  --dtype float16 \\\n  --enforce-eager \\\n  --load-in-low-bit \"none\" \\\n  --max-model-len 4096 \\\n  --max-num-batched-tokens 10240 \\\n  --max-num-seqs 12 \\\n  --tensor-parallel-size 1\n\n##error message\nstart-vllm-service.sh: line 18:  1669 Illegal instruction     (core dumped) python -m ipex_llm.vllm.cpu.entrypoints.openai.api_server --served-model-name $served_model_name --port 8001 --model $model --trust-remote-code --device cpu --dtype float16 --enforce-eager --load-in-low-bit \"none\" --max-model-len 4096 --max-num-batched-tokens 10240 --max-num-seqs 12 --tensor-parallel-size 1\n\n##cpu message\nArchitecture:          x86_64\nCPU op-mode(s):        32-bit, 64-bit\nByte Order:            Little Endian\nCPU(s):                80\nOn-line CPU(s) list:   0-79\nThread(s) per core:    2\nCore(s) per socket:    10\nSocket(s):             4\nNUMA node(s):          4\nVendor ID:             GenuineIntel\nCPU family:            6\nModel:                 62\nModel name:            Intel(R) Xeon(R) CPU E7-4830 v2 @ 2.20GHz\nStepping:              7\nCPU MHz:               1203.662\nCPU max MHz:           2200.0000\nCPU min MHz:           1200.0000\nBogoMIPS:              4389.39\nVirtualization:        VT-x\nL1d cache:             32K\nL1i cache:             32K\nL2 cache:              256K\nL3 cache:              20480K\nNUMA node0 CPU(s):     0-9,40-49\nNUMA node1 CPU(s):     10-19,50-59\nNUMA node2 CPU(s):     20-29,60-69\nNUMA node3 CPU(s):     30-39,70-79\nFlags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm epb intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt dtherm arat pln pts md_clear spec_ctrl intel_stibp flush_l1d\n",
      "state": "open",
      "author": "leoneyar",
      "author_type": "User",
      "created_at": "2025-03-03T12:30:05Z",
      "updated_at": "2025-03-10T11:27:00Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12922/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "xiangyuT"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12922",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12922",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:25.798077",
      "comments": [
        {
          "author": "xiangyuT",
          "body": "Hi @leoneyar,\n\nIt seems that the `none` setting for `load-in-low-bit` in your scripts is invalid for this command. I suggest trying the settings we provide:\n\n```bash\npython -m ipex_llm.vllm.cpu.entrypoints.openai.api_server \\\n  --served-model-name $served_model_name \\\n  --port 8000 \\\n  --model $mode",
          "created_at": "2025-03-04T02:20:58Z"
        },
        {
          "author": "leoneyar",
          "body": "\n\n\n\n> Hi [@leoneyar](https://github.com/leoneyar),\n> \n> It seems that the `none` setting for `load-in-low-bit` in your scripts is invalid for this command. I suggest trying the settings we provide:\n> \n> python -m ipex_llm.vllm.cpu.entrypoints.openai.api_server \\\n>   --served-model-name $served_model",
          "created_at": "2025-03-06T08:38:12Z"
        },
        {
          "author": "xiangyuT",
          "body": "> Hi [@xiangyuT](https://github.com/xiangyuT) ，I changed it and it's still the same error\n\nThe CPU (`E7-4830 v2`) does not support the `AVX512` or even the `AVX 2` Instruction Set Architecture. You may need to test this in a more modern environment.\n\nIf you are using our docker image (`intelanalytic",
          "created_at": "2025-03-06T09:07:41Z"
        }
      ]
    },
    {
      "issue_number": 12889,
      "title": "Support for Transformers 4.48+ to Address Security Vulnerabilities",
      "body": "The current implementation of IPEX-LLM relies on transformers python package up to version 4.45 and earlier. Unfortunately, these versions have known security vulnerabilities:\nhttps://nvd.nist.gov/vuln/detail/CVE-2024-11394\n\nThe issues have been addressed starting with version 4.48 of transformers.\n\nIt would be highly beneficial for security and stability if IPEX-LLM could be updated to support transformers 4.48 and later. ",
      "state": "open",
      "author": "hkarray",
      "author_type": "User",
      "created_at": "2025-02-24T18:57:38Z",
      "updated_at": "2025-03-10T11:25:22Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12889/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "liu-shaojun"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12889",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12889",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:26.194042",
      "comments": [
        {
          "author": "qiyuangong",
          "body": "> The current implementation of IPEX-LLM relies on transformers python package up to version 4.45 and earlier. Unfortunately, these versions have known security vulnerabilities: https://nvd.nist.gov/vuln/detail/CVE-2024-11394\n> \n> The issues have been addressed starting with version 4.48 of transfor",
          "created_at": "2025-03-01T09:41:35Z"
        }
      ]
    },
    {
      "issue_number": 12841,
      "title": "To many memory using when using utlra 200V's npu",
      "body": "Before calling load_model_from_file(save_directory)\nCPU memory Usage:4GB\nNPU memory Usage:0GB\n\nAfter  calling load_model_from_file(save_directory)\nCPU memory Usage:12.5GB\nNPU memory Usage:8.9GB\n\nIt seems that both CPU and NPU allocated the same memory for model.\nIs this an issue, or was it designed for any reason?\n\nBy the way, does any plan to open source of bigdl-core-npu/npu-llm-cpp?",
      "state": "open",
      "author": "shichang00",
      "author_type": "User",
      "created_at": "2025-02-18T03:00:56Z",
      "updated_at": "2025-03-10T11:25:19Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12841/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12841",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12841",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:26.400760",
      "comments": [
        {
          "author": "plusbang",
          "body": "Hi , could you please provide more information about the memory usage (e.g. which model and configuration of `max-context-len` / `max-prompt-len` is used). And some CPU memory usage is expected because embedding and kv cache.\n\nI'm afraid that we have no plan to open source recently.",
          "created_at": "2025-02-19T03:28:58Z"
        },
        {
          "author": "shichang00",
          "body": "> Hi [@dockerg](https://github.com/dockerg) , could you please provide more information about the memory usage (e.g. which model and configuration of `max-context-len` / `max-prompt-len` is used). And some CPU memory usage is expected because embedding and kv cache.\n> \n> I'm afraid that we have no p",
          "created_at": "2025-02-19T03:49:48Z"
        },
        {
          "author": "plusbang",
          "body": "> \n> max-context-len:1024 max-prompt-len:960\n> \n> I check this issue at two device; With Ultra 258V, CPU memory usage is just 4GB when the first time to run with AutoModel.from_pretrained(), but 9-10GB when using AutoModel.load_low_bit();\n> \n\nWhich model is used? We will try to reproduce it first : ",
          "created_at": "2025-02-19T05:24:56Z"
        },
        {
          "author": "shichang00",
          "body": "> > max-context-len:1024 max-prompt-len:960\n> > I check this issue at two device; With Ultra 258V, CPU memory usage is just 4GB when the first time to run with AutoModel.from_pretrained(), but 9-10GB when using AutoModel.load_low_bit();\n> \n> Which model is used? We will try to reproduce it first : )",
          "created_at": "2025-02-19T05:35:35Z"
        },
        {
          "author": "plusbang",
          "body": "> qwen/qwen2.5-7B, and --low-bit set sym_int4\n\nHi, I failed to reproduce this on Ultra 7 258V with [qwen2 example](https://github.com/intel/ipex-llm/blob/main/python/llm/example/NPU/HF-Transformers-AutoModels/LLM/qwen.py).\n\nWith `32.0.100.3104` NPU driver and `ipex-llm==2.2.0b20250218` (no runtime c",
          "created_at": "2025-02-19T08:59:57Z"
        }
      ]
    },
    {
      "issue_number": 12654,
      "title": "Unable to use ollama in the ipex-llm docker container",
      "body": "On the Host, i could use ollama and ipex with a Arc750 GPU but,\r\nIn the container, i got a fail , the step is :\r\n0.start the container\r\n```\r\n#/bin/bash\r\nexport DOCKER_IMAGE=intelanalytics/ipex-llm-inference-cpp-xpu:latest\r\nexport CONTAINER_NAME=ipex-llm-inference-cpp-xpu-container\r\nsudo docker run -itd \\\r\n                --net=host \\\r\n                --device=/dev/dri \\\r\n                -v ~/.ollama/models:/root/models \\\r\n                -e no_proxy=localhost,127.0.0.1 \\\r\n                --memory=\"32G\" \\\r\n                --name=$CONTAINER_NAME \\\r\n                -e bench_model=\"mistral-7b-v0.1.Q4_0.gguf\" \\\r\n                -e DEVICE=Arc \\\r\n                --shm-size=\"16g\" \\\r\n                $DOCKER_IMAGE\r\n```\r\n1. Verify the device map\r\n```\r\nsycl-ls\r\n\r\nroot@calico-B450M-HDV-R4-0:/llm/scripts# sycl-ls\r\n[opencl:acc:0] Intel(R) FPGA Emulation Platform for OpenCL(TM), Intel(R) FPGA Emulation Device OpenCL 1.2  [2023.16.12.0.12_195853.xmain-hotfix]\r\n[opencl:cpu:1] Intel(R) OpenCL, AMD Ryzen 5 5500                                OpenCL 3.0 (Build 0) [2023.16.12.0.12_195853.xmain-hotfix]\r\n[opencl:gpu:2] Intel(R) OpenCL Graphics, Intel(R) Arc(TM) A750 Graphics OpenCL 3.0 NEO  [24.39.31294.12]\r\n[ext_oneapi_level_zero:gpu:0] Intel(R) Level-Zero, Intel(R) Arc(TM) A750 Graphics 1.6 [1.3.31294]\r\n```\r\n2. start ollama\r\n```\r\ncd /llm/scripts/\r\n# set the recommended Env\r\nsource ipex-llm-init --gpu --device $DEVICE\r\nbash start-ollama.sh # ctrl+c to exit, and the ollama serve will run on the background\r\n```\r\noutput:\r\n```\r\nroot@calico-B450M-HDV-R4-0:/llm/scripts# source ipex-llm-init --gpu --device $DEVICE\r\nfound oneapi in /opt/intel/oneapi/setvars.sh\r\n\r\n:: initializing oneAPI environment ...\r\n   bash: BASH_VERSION = 5.1.16(1)-release\r\n   args: Using \"$@\" for setvars.sh arguments: --force\r\n:: advisor -- latest\r\n:: ccl -- latest\r\n:: compiler -- latest\r\n:: dal -- latest\r\n:: debugger -- latest\r\n:: dev-utilities -- latest\r\n:: dnnl -- latest\r\n:: dpcpp-ct -- latest\r\n:: dpl -- latest\r\n:: ipp -- latest\r\n:: ippcp -- latest\r\n:: mkl -- latest\r\n:: mpi -- latest\r\n:: tbb -- latest\r\n:: vtune -- latest\r\n:: oneAPI environment initialized ::\r\n\r\n/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\r\n  _torch_pytree._register_pytree_node(\r\n/usr/local/lib/python3.11/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\r\n  warnings.warn(\r\n/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\r\n  _torch_pytree._register_pytree_node(\r\n/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\r\n  _torch_pytree._register_pytree_node(\r\n+++++ Env Variables +++++\r\nInternal:\r\n    ENABLE_IOMP     = 1\r\n    ENABLE_GPU      = 1\r\n    ENABLE_JEMALLOC = 0\r\n    ENABLE_TCMALLOC = 0\r\n    LIB_DIR    = /usr/local/lib\r\n    BIN_DIR    = bin64\r\n    LLM_DIR    = /usr/local/lib/python3.11/dist-packages/ipex_llm\r\n\r\nExported:\r\n    LD_PRELOAD             =\r\n    OMP_NUM_THREADS        =\r\n    MALLOC_CONF            =\r\n    USE_XETLA              = OFF\r\n    ENABLE_SDP_FUSION      =\r\n    SYCL_CACHE_PERSISTENT  = 1\r\n    BIGDL_LLM_XMX_DISABLED =\r\n    SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS = 1\r\n+++++++++++++++++++++++++\r\nComplete.\r\nroot@calico-B450M-HDV-R4-0:/llm/scripts# bash start-ollama.sh\r\nroot@calico-B450M-HDV-R4-0:/llm/scripts# 2025/01/06 13:59:18 routes.go:1197: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:localhost,127.0.0.1]\"\r\ntime=2025-01-06T13:59:18.275+08:00 level=INFO source=images.go:753 msg=\"total blobs: 28\"\r\ntime=2025-01-06T13:59:18.276+08:00 level=INFO source=images.go:760 msg=\"total unused blobs removed: 0\"\r\ntime=2025-01-06T13:59:18.276+08:00 level=INFO source=routes.go:1248 msg=\"Listening on 127.0.0.1:11434 (version 0.4.6-ipexllm-20250104)\"\r\ntime=2025-01-06T13:59:18.276+08:00 level=INFO source=common.go:135 msg=\"extracting embedded files\" dir=/tmp/ollama4065154349/runners\r\ntime=2025-01-06T13:59:18.313+08:00 level=INFO source=common.go:49 msg=\"Dynamic LLM libraries\" runners=[ipex_llm]\r\n```\r\n\r\n4. invoke a http request to ollama and got error\r\n```\r\nroot@calico-B450M-HDV-R4-0:/llm# curl http://localhost:11434/api/generate -d '\r\n{\r\n   \"model\": \"qwen2.5\",\r\n   \"prompt\": \"What is AI?\",\r\n   \"stream\": false\r\n}'\r\n{\"error\":\"llama runner process has terminated: exit status 2\"}\r\n```\r\n5. then the crack log of ollama is :\r\n```\r\nroot@calico-B450M-HDV-R4-0:/llm/scripts# 2025/01/06 13:59:18 routes.go:1197: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:localhost,127.0.0.1]\"\r\ntime=2025-01-06T13:59:18.275+08:00 level=INFO source=images.go:753 msg=\"total blobs: 28\"\r\ntime=2025-01-06T13:59:18.276+08:00 level=INFO source=images.go:760 msg=\"total unused blobs removed: 0\"\r\ntime=2025-01-06T13:59:18.276+08:00 level=INFO source=routes.go:1248 msg=\"Listening on 127.0.0.1:11434 (version 0.4.6-ipexllm-20250104)\"\r\ntime=2025-01-06T13:59:18.276+08:00 level=INFO source=common.go:135 msg=\"extracting embedded files\" dir=/tmp/ollama4065154349/runners\r\ntime=2025-01-06T13:59:18.313+08:00 level=INFO source=common.go:49 msg=\"Dynamic LLM libraries\" runners=[ipex_llm]\r\ntime=2025-01-06T14:00:37.530+08:00 level=INFO source=gpu.go:221 msg=\"looking for compatible GPUs\"\r\ntime=2025-01-06T14:00:37.530+08:00 level=WARN source=gpu.go:732 msg=\"unable to locate gpu dependency libraries\"\r\ntime=2025-01-06T14:00:37.531+08:00 level=WARN source=gpu.go:732 msg=\"unable to locate gpu dependency libraries\"\r\ntime=2025-01-06T14:00:37.531+08:00 level=WARN source=gpu.go:732 msg=\"unable to locate gpu dependency libraries\"\r\ntime=2025-01-06T14:00:37.535+08:00 level=WARN source=gpu.go:732 msg=\"unable to locate gpu dependency libraries\"\r\ntime=2025-01-06T14:00:37.562+08:00 level=INFO source=server.go:105 msg=\"system memory\" total=\"15.5 GiB\" free=\"14.0 GiB\" free_swap=\"4.0 GiB\"\r\ntime=2025-01-06T14:00:37.563+08:00 level=INFO source=memory.go:356 msg=\"offload to device\" layers.requested=-1 layers.model=29 layers.offload=0 layers.split=\"\" memory.available=\"[14.0 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"5.1 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"448.0 MiB\" memory.required.allocations=\"[5.1 GiB]\" memory.weights.total=\"4.1 GiB\" memory.weights.repeating=\"3.7 GiB\" memory.weights.nonrepeating=\"426.4 MiB\" memory.graph.full=\"478.0 MiB\" memory.graph.partial=\"730.4 MiB\"\r\ntime=2025-01-06T14:00:37.563+08:00 level=INFO source=server.go:401 msg=\"starting llama server\" cmd=\"/tmp/ollama4065154349/runners/ipex_llm/ollama_llama_server --model /root/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 --ctx-size 8192 --batch-size 512 --n-gpu-layers 999 --threads 6 --no-mmap --parallel 4 --port 40091\"\r\ntime=2025-01-06T14:00:37.563+08:00 level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\r\ntime=2025-01-06T14:00:37.563+08:00 level=INFO source=server.go:580 msg=\"waiting for llama runner to start responding\"\r\ntime=2025-01-06T14:00:37.563+08:00 level=INFO source=server.go:614 msg=\"waiting for server to become available\" status=\"llm server error\"\r\ntime=2025-01-06T14:00:37.595+08:00 level=INFO source=runner.go:956 msg=\"starting go runner\"\r\ntime=2025-01-06T14:00:37.596+08:00 level=INFO source=runner.go:957 msg=system info=\"AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | cgo(gcc)\" threads=6\r\ntime=2025-01-06T14:00:37.596+08:00 level=INFO source=.:0 msg=\"Server listening on 127.0.0.1:40091\"\r\nllama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from /root/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct\r\nllama_model_loader: - kv   3:                           general.finetune str              = Instruct\r\nllama_model_loader: - kv   4:                           general.basename str              = Qwen2.5\r\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\r\nllama_model_loader: - kv   6:                            general.license str              = apache-2.0\r\nllama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-7...\r\nllama_model_loader: - kv   8:                   general.base_model.count u32              = 1\r\nllama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 7B\r\nllama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen\r\nllama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7B\r\nllama_model_loader: - kv  12:                               general.tags arr[str,2]       = [\"chat\", \"text-generation\"]\r\nllama_model_loader: - kv  13:                          general.languages arr[str,1]       = [\"en\"]\r\nllama_model_loader: - kv  14:                          qwen2.block_count u32              = 28\r\nllama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768\r\nllama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584\r\nllama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944\r\nllama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28\r\nllama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4\r\nllama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\r\nllama_model_loader: - kv  22:                          general.file_type u32              = 15\r\nllama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2\r\nllama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\r\nllama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645\r\nllama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643\r\nllama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643\r\nllama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false\r\nllama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\r\nllama_model_loader: - kv  33:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:  141 tensors\r\nllama_model_loader: - type q4_K:  169 tensors\r\nllama_model_loader: - type q6_K:   29 tensors\r\nllm_load_vocab: special tokens cache size = 22\r\nllm_load_vocab: token to piece cache size = 0.9310 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = qwen2\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 152064\r\nllm_load_print_meta: n_merges         = 151387\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 32768\r\nllm_load_print_meta: n_embd           = 3584\r\nllm_load_print_meta: n_layer          = 28\r\nllm_load_print_meta: n_head           = 28\r\nllm_load_print_meta: n_head_kv        = 4\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 7\r\nllm_load_print_meta: n_embd_k_gqa     = 512\r\nllm_load_print_meta: n_embd_v_gqa     = 512\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 18944\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 2\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 32768\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = ?B\r\nllm_load_print_meta: model ftype      = Q4_K - Medium\r\nllm_load_print_meta: model params     = 7.62 B\r\nllm_load_print_meta: model size       = 4.36 GiB (4.91 BPW)\r\nllm_load_print_meta: general.name     = Qwen2.5 7B Instruct\r\nllm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\r\nllm_load_print_meta: EOS token        = 151645 '<|im_end|>'\r\nllm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\r\nllm_load_print_meta: LF token         = 148848 'ÄĬ'\r\nllm_load_print_meta: EOT token        = 151645 '<|im_end|>'\r\nllm_load_print_meta: EOG token        = 151643 '<|endoftext|>'\r\nllm_load_print_meta: EOG token        = 151645 '<|im_end|>'\r\nllm_load_print_meta: max token length = 256\r\ntime=2025-01-06T14:00:37.815+08:00 level=INFO source=server.go:614 msg=\"waiting for server to become available\" status=\"llm server loading model\"\r\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nggml_sycl_init: SYCL_USE_XMX: yes\r\nggml_sycl_init: found 1 SYCL devices:\r\nllm_load_tensors: ggml ctx size =    0.30 MiB\r\nllm_load_tensors: offloading 28 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 29/29 layers to GPU\r\nllm_load_tensors:      SYCL0 buffer size =  4168.09 MiB\r\nllm_load_tensors:  SYCL_Host buffer size =   292.36 MiB\r\nSIGBUS: bus error\r\nPC=0x77c6f51788ca m=3 sigcode=2 addr=0x77c526b14000\r\nsignal arrived during cgo execution\r\n\r\ngoroutine 6 gp=0xc000007dc0 m=3 mp=0xc000077008 [syscall]:\r\nruntime.cgocall(0x56502fb0f9f0, 0xc000085b90)\r\n        runtime/cgocall.go:157 +0x4b fp=0xc000085b68 sp=0xc000085b30 pc=0x56502f89046b\r\nollama/llama/llamafile._Cfunc_llama_load_model_from_file(0x77c688000d40, {0x3e7, 0x1, 0x0, 0x0, 0x0, 0x56502fb0f3e0, 0xc000014308, 0x0, 0x0, ...})\r\n        _cgo_gotypes.go:692 +0x50 fp=0xc000085b90 sp=0xc000085b68 pc=0x56502f98e310\r\nollama/llama/llamafile.LoadModelFromFile.func1({0x7ffea36e20ef?, 0x0?}, {0x3e7, 0x1, 0x0, 0x0, 0x0, 0x56502fb0f3e0, 0xc000014308, 0x0, ...})\r\n        ollama/llama/llamafile/llama.go:225 +0xfa fp=0xc000085c78 sp=0xc000085b90 pc=0x56502f990c1a\r\nollama/llama/llamafile.LoadModelFromFile({0x7ffea36e20ef, 0x62}, {0x3e7, 0x0, 0x0, 0x0, {0x0, 0x0, 0x0}, 0xc0000441a0, ...})\r\n        ollama/llama/llamafile/llama.go:225 +0x2d5 fp=0xc000085db8 sp=0xc000085c78 pc=0x56502f990955\r\nmain.(*Server).loadModel(0xc0000ca120, {0x3e7, 0x0, 0x0, 0x0, {0x0, 0x0, 0x0}, 0xc0000441a0, 0x0}, ...)\r\n        ollama/llama/runner/runner.go:861 +0xc5 fp=0xc000085f10 sp=0xc000085db8 pc=0x56502fb0cf65\r\nmain.main.gowrap1()\r\n        ollama/llama/runner/runner.go:990 +0xda fp=0xc000085fe0 sp=0xc000085f10 pc=0x56502fb0e95a\r\nruntime.goexit({})\r\n        runtime/asm_amd64.s:1695 +0x1 fp=0xc000085fe8 sp=0xc000085fe0 pc=0x56502f8f8e81\r\ncreated by main.main in goroutine 1\r\n        ollama/llama/runner/runner.go:990 +0xc6c\r\n\r\ngoroutine 1 gp=0xc0000061c0 m=nil [IO wait]:\r\nruntime.gopark(0xc000050008?, 0x0?, 0xc0?, 0x61?, 0xc000049898?)\r\n        runtime/proc.go:402 +0xce fp=0xc000049860 sp=0xc000049840 pc=0x56502f8c70ae\r\nruntime.netpollblock(0xc0000498f8?, 0x2f88fbc6?, 0x50?)\r\n        runtime/netpoll.go:573 +0xf7 fp=0xc000049898 sp=0xc000049860 pc=0x56502f8bf2f7\r\ninternal/poll.runtime_pollWait(0x77c6f4588020, 0x72)\r\n        runtime/netpoll.go:345 +0x85 fp=0xc0000498b8 sp=0xc000049898 pc=0x56502f8f3b45\r\ninternal/poll.(*pollDesc).wait(0x3?, 0x3fe?, 0x0)\r\n        internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc0000498e0 sp=0xc0000498b8 pc=0x56502f943a67\r\ninternal/poll.(*pollDesc).waitRead(...)\r\n        internal/poll/fd_poll_runtime.go:89\r\ninternal/poll.(*FD).Accept(0xc0000f8080)\r\n        internal/poll/fd_unix.go:611 +0x2ac fp=0xc000049988 sp=0xc0000498e0 pc=0x56502f944f2c\r\nnet.(*netFD).accept(0xc0000f8080)\r\n        net/fd_unix.go:172 +0x29 fp=0xc000049a40 sp=0xc000049988 pc=0x56502f9b3b49\r\nnet.(*TCPListener).accept(0xc0000901e0)\r\n        net/tcpsock_posix.go:159 +0x1e fp=0xc000049a68 sp=0xc000049a40 pc=0x56502f9c487e\r\nnet.(*TCPListener).Accept(0xc0000901e0)\r\n        net/tcpsock.go:327 +0x30 fp=0xc000049a98 sp=0xc000049a68 pc=0x56502f9c3bd0\r\nnet/http.(*onceCloseListener).Accept(0xc0000ca1b0?)\r\n        <autogenerated>:1 +0x24 fp=0xc000049ab0 sp=0xc000049a98 pc=0x56502faeade4\r\nnet/http.(*Server).Serve(0xc0000fe000, {0x56502fe16560, 0xc0000901e0})\r\n        net/http/server.go:3260 +0x33e fp=0xc000049be0 sp=0xc000049ab0 pc=0x56502fae1bfe\r\nmain.main()\r\n        ollama/llama/runner/runner.go:1015 +0x10cd fp=0xc000049f50 sp=0xc000049be0 pc=0x56502fb0e5cd\r\nruntime.main()\r\n        runtime/proc.go:271 +0x29d fp=0xc000049fe0 sp=0xc000049f50 pc=0x56502f8c6c7d\r\nruntime.goexit({})\r\n        runtime/asm_amd64.s:1695 +0x1 fp=0xc000049fe8 sp=0xc000049fe0 pc=0x56502f8f8e81\r\n\r\ngoroutine 2 gp=0xc000006c40 m=nil [force gc (idle)]:\r\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\r\n        runtime/proc.go:402 +0xce fp=0xc000070fa8 sp=0xc000070f88 pc=0x56502f8c70ae\r\nruntime.goparkunlock(...)\r\n        runtime/proc.go:408\r\nruntime.forcegchelper()\r\n        runtime/proc.go:326 +0xb8 fp=0xc000070fe0 sp=0xc000070fa8 pc=0x56502f8c6f38\r\nruntime.goexit({})\r\n        runtime/asm_amd64.s:1695 +0x1 fp=0xc000070fe8 sp=0xc000070fe0 pc=0x56502f8f8e81\r\ncreated by runtime.init.6 in goroutine 1\r\n        runtime/proc.go:314 +0x1a\r\n\r\ngoroutine 3 gp=0xc000007180 m=nil [GC sweep wait]:\r\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\r\n        runtime/proc.go:402 +0xce fp=0xc000071780 sp=0xc000071760 pc=0x56502f8c70ae\r\nruntime.goparkunlock(...)\r\n        runtime/proc.go:408\r\nruntime.bgsweep(0xc0000240e0)\r\n        runtime/mgcsweep.go:278 +0x94 fp=0xc0000717c8 sp=0xc000071780 pc=0x56502f8b1bf4\r\nruntime.gcenable.gowrap1()\r\n        runtime/mgc.go:203 +0x25 fp=0xc0000717e0 sp=0xc0000717c8 pc=0x56502f8a6725\r\nruntime.goexit({})\r\n        runtime/asm_amd64.s:1695 +0x1 fp=0xc0000717e8 sp=0xc0000717e0 pc=0x56502f8f8e81\r\ncreated by runtime.gcenable in goroutine 1\r\n        runtime/mgc.go:203 +0x66\r\n\r\ngoroutine 4 gp=0xc000007340 m=nil [GC scavenge wait]:\r\nruntime.gopark(0xc0000240e0?, 0x56502fb8e208?, 0x1?, 0x0?, 0xc000007340?)\r\n        runtime/proc.go:402 +0xce fp=0xc000071f78 sp=0xc000071f58 pc=0x56502f8c70ae\r\nruntime.goparkunlock(...)\r\n        runtime/proc.go:408\r\nruntime.(*scavengerState).park(0x56502ffe0680)\r\n        runtime/mgcscavenge.go:425 +0x49 fp=0xc000071fa8 sp=0xc000071f78 pc=0x56502f8af5e9\r\nruntime.bgscavenge(0xc0000240e0)\r\n        runtime/mgcscavenge.go:653 +0x3c fp=0xc000071fc8 sp=0xc000071fa8 pc=0x56502f8afb7c\r\nruntime.gcenable.gowrap2()\r\n        runtime/mgc.go:204 +0x25 fp=0xc000071fe0 sp=0xc000071fc8 pc=0x56502f8a66c5\r\nruntime.goexit({})\r\n        runtime/asm_amd64.s:1695 +0x1 fp=0xc000071fe8 sp=0xc000071fe0 pc=0x56502f8f8e81\r\ncreated by runtime.gcenable in goroutine 1\r\n        runtime/mgc.go:204 +0xa5\r\n\r\ngoroutine 5 gp=0xc000007c00 m=nil [finalizer wait]:\r\nruntime.gopark(0xc000070648?, 0x56502f89a025?, 0xa8?, 0x1?, 0xc0000061c0?)\r\n        runtime/proc.go:402 +0xce fp=0xc000070620 sp=0xc000070600 pc=0x56502f8c70ae\r\nruntime.runfinq()\r\n        runtime/mfinal.go:194 +0x107 fp=0xc0000707e0 sp=0xc000070620 pc=0x56502f8a5767\r\nruntime.goexit({})\r\n        runtime/asm_amd64.s:1695 +0x1 fp=0xc0000707e8 sp=0xc0000707e0 pc=0x56502f8f8e81\r\ncreated by runtime.createfing in goroutine 1\r\n        runtime/mfinal.go:164 +0x3d\r\n\r\ngoroutine 7 gp=0xc0000fc000 m=nil [semacquire]:\r\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\r\n        runtime/proc.go:402 +0xce fp=0xc000072e08 sp=0xc000072de8 pc=0x56502f8c70ae\r\nruntime.goparkunlock(...)\r\n        runtime/proc.go:408\r\nruntime.semacquire1(0xc0000ca128, 0x0, 0x1, 0x0, 0x12)\r\n        runtime/sema.go:160 +0x22c fp=0xc000072e70 sp=0xc000072e08 pc=0x56502f8d94cc\r\nsync.runtime_Semacquire(0x0?)\r\n        runtime/sema.go:62 +0x25 fp=0xc000072ea8 sp=0xc000072e70 pc=0x56502f8f5305\r\nsync.(*WaitGroup).Wait(0x0?)\r\n        sync/waitgroup.go:116 +0x48 fp=0xc000072ed0 sp=0xc000072ea8 pc=0x56502f913d88\r\nmain.(*Server).run(0xc0000ca120, {0x56502fe16ba0, 0xc0000a40a0})\r\n        ollama/llama/runner/runner.go:315 +0x47 fp=0xc000072fb8 sp=0xc000072ed0 pc=0x56502fb09627\r\nmain.main.gowrap2()\r\n        ollama/llama/runner/runner.go:995 +0x28 fp=0xc000072fe0 sp=0xc000072fb8 pc=0x56502fb0e848\r\nruntime.goexit({})\r\n        runtime/asm_amd64.s:1695 +0x1 fp=0xc000072fe8 sp=0xc000072fe0 pc=0x56502f8f8e81\r\ncreated by main.main in goroutine 1\r\n        ollama/llama/runner/runner.go:995 +0xd3e\r\n\r\ngoroutine 8 gp=0xc0000fc1c0 m=nil [IO wait]:\r\nruntime.gopark(0x94?, 0xc0000f1958?, 0x40?, 0x19?, 0xb?)\r\n        runtime/proc.go:402 +0xce fp=0xc0000f1910 sp=0xc0000f18f0 pc=0x56502f8c70ae\r\nruntime.netpollblock(0x56502f92d5f8?, 0x2f88fbc6?, 0x50?)\r\n        runtime/netpoll.go:573 +0xf7 fp=0xc0000f1948 sp=0xc0000f1910 pc=0x56502f8bf2f7\r\ninternal/poll.runtime_pollWait(0x77c6f4587f28, 0x72)\r\n        runtime/netpoll.go:345 +0x85 fp=0xc0000f1968 sp=0xc0000f1948 pc=0x56502f8f3b45\r\ninternal/poll.(*pollDesc).wait(0xc0000f8100?, 0xc000200000?, 0x0)\r\n        internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc0000f1990 sp=0xc0000f1968 pc=0x56502f943a67\r\ninternal/poll.(*pollDesc).waitRead(...)\r\n        internal/poll/fd_poll_runtime.go:89\r\ninternal/poll.(*FD).Read(0xc0000f8100, {0xc000200000, 0x1000, 0x1000})\r\n        internal/poll/fd_unix.go:164 +0x27a fp=0xc0000f1a28 sp=0xc0000f1990 pc=0x56502f9445ba\r\nnet.(*netFD).Read(0xc0000f8100, {0xc000200000?, 0xc0000f1a98?, 0x56502f943f25?})\r\n        net/fd_posix.go:55 +0x25 fp=0xc0000f1a70 sp=0xc0000f1a28 pc=0x56502f9b2a45\r\nnet.(*conn).Read(0xc000074098, {0xc000200000?, 0x0?, 0xc0000b2ed8?})\r\n        net/net.go:185 +0x45 fp=0xc0000f1ab8 sp=0xc0000f1a70 pc=0x56502f9bcd05\r\nnet.(*TCPConn).Read(0xc0000b2ed0?, {0xc000200000?, 0xc0000f8100?, 0xc0000f1af0?})\r\n        <autogenerated>:1 +0x25 fp=0xc0000f1ae8 sp=0xc0000f1ab8 pc=0x56502f9c86e5\r\nnet/http.(*connReader).Read(0xc0000b2ed0, {0xc000200000, 0x1000, 0x1000})\r\n        net/http/server.go:789 +0x14b fp=0xc0000f1b38 sp=0xc0000f1ae8 pc=0x56502fad7a0b\r\nbufio.(*Reader).fill(0xc00004e480)\r\n        bufio/bufio.go:110 +0x103 fp=0xc0000f1b70 sp=0xc0000f1b38 pc=0x56502fa94303\r\nbufio.(*Reader).Peek(0xc00004e480, 0x4)\r\n        bufio/bufio.go:148 +0x53 fp=0xc0000f1b90 sp=0xc0000f1b70 pc=0x56502fa94433\r\nnet/http.(*conn).serve(0xc0000ca1b0, {0x56502fe16b68, 0xc0000b2db0})\r\n        net/http/server.go:2079 +0x749 fp=0xc0000f1fb8 sp=0xc0000f1b90 pc=0x56502fadd769\r\nnet/http.(*Server).Serve.gowrap3()\r\n        net/http/server.go:3290 +0x28 fp=0xc0000f1fe0 sp=0xc0000f1fb8 pc=0x56502fae1fe8\r\nruntime.goexit({})\r\n        runtime/asm_amd64.s:1695 +0x1 fp=0xc0000f1fe8 sp=0xc0000f1fe0 pc=0x56502f8f8e81\r\ncreated by net/http.(*Server).Serve in goroutine 1\r\n        net/http/server.go:3290 +0x4b4\r\n\r\nrax    0x77c526b14000\r\nrbx    0x5d727200\r\nrcx    0x3800\r\nrdx    0x3800\r\nrdi    0x77c526b14000\r\nrsi    0x77c644fc4730\r\nrbp    0x77c6961fe980\r\nrsp    0x77c6961fe7a8\r\nr8     0x77c526b14000\r\nr9     0x105b2f000\r\nr10    0x1\r\nr11    0x246\r\nr12    0x77c644fc4730\r\nr13    0x77c526b14000\r\nr14    0x77c68b9daac0\r\nr15    0x77c644fc7f40\r\nrip    0x77c6f51788ca\r\nrflags 0x10206\r\ncs     0x33\r\nfs     0x0\r\ngs     0x0\r\ntime=2025-01-06T14:00:38.075+08:00 level=INFO source=server.go:614 msg=\"waiting for server to become available\" status=\"llm server error\"\r\ntime=2025-01-06T14:00:38.325+08:00 level=ERROR source=sched.go:455 msg=\"error loading llama server\" error=\"llama runner process has terminated: exit status 2\"\r\n\r\n```",
      "state": "open",
      "author": "ca1ic0",
      "author_type": "User",
      "created_at": "2025-01-06T06:00:54Z",
      "updated_at": "2025-03-10T11:25:18Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 11,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12654/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "ACupofAir"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12654",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12654",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:26.595507",
      "comments": [
        {
          "author": "ca1ic0",
          "body": "i notice there are some unnormal log:\r\n```\r\ntime=2025-01-06T14:00:37.530+08:00 level=WARN source=gpu.go:732 msg=\"unable to locate gpu dependency libraries\"\r\ntime=2025-01-06T14:00:37.531+08:00 level=WARN source=gpu.go:732 msg=\"unable to locate gpu dependency libraries\"\r\ntime=2025-01-06T14:00:37.531+0",
          "created_at": "2025-01-06T06:07:32Z"
        },
        {
          "author": "hzjane",
          "body": "@ca1ic0  I can't reproduce your problem on your `calico-B450M-HDV-R4-0` ENV. I can use curl to access qwen2.5 normally by executing `./ollama pull qwen2.5` after starting ollama.",
          "created_at": "2025-01-07T02:34:16Z"
        },
        {
          "author": "ca1ic0",
          "body": "> @ca1ic0 I can't reproduce your problem on your `calico-B450M-HDV-R4-0` ENV. I can use curl to access qwen2.5 normally by executing `./ollama pull qwen2.5` after starting ollama.\r\n\r\ni got this , it seems like the storage map '-v ~/.ollama/models:/root/models' is wrong and the ipexllm works fine.\r\nT",
          "created_at": "2025-01-07T05:17:24Z"
        },
        {
          "author": "ca1ic0",
          "body": "> @ca1ic0 I can't reproduce your problem on your `calico-B450M-HDV-R4-0` ENV. I can use curl to access qwen2.5 normally by executing `./ollama pull qwen2.5` after starting ollama.\r\n\r\nit is weird , i reproduce the problem again. Is there any difference between your op and mine?",
          "created_at": "2025-01-07T05:54:25Z"
        },
        {
          "author": "ca1ic0",
          "body": "this time i directly pull the qwen2.5 without map the `.ollama` dir.\r\n",
          "created_at": "2025-01-07T05:55:01Z"
        }
      ]
    },
    {
      "issue_number": 12917,
      "title": "Ollama portal with iGPU and A770 got crash",
      "body": "The Ollama got crashed when I tried to run start the service.\n\nMy pc is using Ultra CPU 185H and an external Arc 770 GPU with 96GB of RAM.\n\nI suspected, it maybe a conflict between built-in Arc GPU and 770 as it was working fine without external GPU 770.\n\nPlease help If anyone knows how to connect config the ollama that can run on both GPUs.\n\nhere is the log\n```\nException 0xe06d7363 0x19930520 0x9a4e13efb0 0x7ffe714cbb0a\nPC=0x7ffe714cbb0a\nsignal arrived during external code execution\n\nruntime.cgocall(0x7ff7f99fce10, 0xc00050f958)\n        runtime/cgocall.go:167 +0x3e fp=0xc00050f930 sp=0xc00050f8c8 pc=0x7ff7f8e49c1e\nollama/llama/llamafile._Cfunc_llama_print_system_info()\n        _cgo_gotypes.go:830 +0x52 fp=0xc00050f958 sp=0xc00050f930 pc=0x7ff7f9221a32\nollama/llama/llamafile.PrintSystemInfo()\n        ollama/llama/llamafile/llama.go:70 +0x79 fp=0xc00050f9a0 sp=0xc00050f958 pc=0x7ff7f9222cb9\nollama/cmd.NewCLI()\n        ollama/cmd/cmd.go:1427 +0xc68 fp=0xc00050ff30 sp=0xc00050f9a0 pc=0x7ff7f99f4e88\nmain.main()\n        ollama/main.go:12 +0x13 fp=0xc00050ff50 sp=0xc00050ff30 pc=0x7ff7f99fc3d3\nruntime.main()\n        runtime/proc.go:272 +0x27d fp=0xc00050ffe0 sp=0xc00050ff50 pc=0x7ff7f8e1df9d\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00050ffe8 sp=0xc00050ffe0 pc=0x7ff7f8e58901\n\ngoroutine 2 gp=0xc00008a700 m=nil [force gc (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc00008dfa8 sp=0xc00008df88 pc=0x7ff7f8e503ce\nruntime.goparkunlock(...)\n        runtime/proc.go:430\nruntime.forcegchelper()\n        runtime/proc.go:337 +0xb8 fp=0xc00008dfe0 sp=0xc00008dfa8 pc=0x7ff7f8e1e2b8\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00008dfe8 sp=0xc00008dfe0 pc=0x7ff7f8e58901\ncreated by runtime.init.7 in goroutine 1\n        runtime/proc.go:325 +0x1a\n\ngoroutine 3 gp=0xc00008aa80 m=nil [GC sweep wait]:\nruntime.gopark(0x1?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc00008ff80 sp=0xc00008ff60 pc=0x7ff7f8e503ce\nruntime.goparkunlock(...)\n        runtime/proc.go:430\nruntime.bgsweep(0xc00009c000)\n        runtime/mgcsweep.go:317 +0xdf fp=0xc00008ffc8 sp=0xc00008ff80 pc=0x7ff7f8e06f9f\nruntime.gcenable.gowrap1()\n        runtime/mgc.go:204 +0x25 fp=0xc00008ffe0 sp=0xc00008ffc8 pc=0x7ff7f8dfb5c5\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00008ffe8 sp=0xc00008ffe0 pc=0x7ff7f8e58901\ncreated by runtime.gcenable in goroutine 1\n        runtime/mgc.go:204 +0x66\n\ngoroutine 4 gp=0xc00008ac40 m=nil [GC scavenge wait]:\nruntime.gopark(0x10000?, 0x7ff7f9e29520?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc0000a3f78 sp=0xc0000a3f58 pc=0x7ff7f8e503ce\nruntime.goparkunlock(...)\n        runtime/proc.go:430\nruntime.(*scavengerState).park(0x7ff7fa626940)\n        runtime/mgcscavenge.go:425 +0x49 fp=0xc0000a3fa8 sp=0xc0000a3f78 pc=0x7ff7f8e04969\nruntime.bgscavenge(0xc00009c000)\n        runtime/mgcscavenge.go:658 +0x59 fp=0xc0000a3fc8 sp=0xc0000a3fa8 pc=0x7ff7f8e04ef9\nruntime.gcenable.gowrap2()\n        runtime/mgc.go:205 +0x25 fp=0xc0000a3fe0 sp=0xc0000a3fc8 pc=0x7ff7f8dfb565\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc0000a3fe8 sp=0xc0000a3fe0 pc=0x7ff7f8e58901\ncreated by runtime.gcenable in goroutine 1\n        runtime/mgc.go:205 +0xa5\n\ngoroutine 5 gp=0xc00008b180 m=nil [finalizer wait]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc0000a5e20 sp=0xc0000a5e00 pc=0x7ff7f8e503ce\nruntime.runfinq()\n        runtime/mfinal.go:193 +0x107 fp=0xc0000a5fe0 sp=0xc0000a5e20 pc=0x7ff7f8dfa687\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc0000a5fe8 sp=0xc0000a5fe0 pc=0x7ff7f8e58901\ncreated by runtime.createfing in goroutine 1\n        runtime/mfinal.go:163 +0x3d\n\ngoroutine 6 gp=0xc0001e6380 m=nil [chan receive]:\nruntime.gopark(0xc000091f60?, 0x7ff7f8f3a2e5?, 0x10?, 0x88?, 0x7ff7f9e50260?)\n        runtime/proc.go:424 +0xce fp=0xc000091f18 sp=0xc000091ef8 pc=0x7ff7f8e503ce\nruntime.chanrecv(0xc0000aa2a0, 0x0, 0x1)\n        runtime/chan.go:639 +0x41e fp=0xc000091f90 sp=0xc000091f18 pc=0x7ff7f8deac9e\nruntime.chanrecv1(0x7ff7f8e1e100?, 0xc000091f76?)\n        runtime/chan.go:489 +0x12 fp=0xc000091fb8 sp=0xc000091f90 pc=0x7ff7f8dea852\nruntime.unique_runtime_registerUniqueMapCleanup.func1(...)\n        runtime/mgc.go:1781\nruntime.unique_runtime_registerUniqueMapCleanup.gowrap1()\n        runtime/mgc.go:1784 +0x2f fp=0xc000091fe0 sp=0xc000091fb8 pc=0x7ff7f8dfe6af\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000091fe8 sp=0xc000091fe0 pc=0x7ff7f8e58901\ncreated by unique.runtime_registerUniqueMapCleanup in goroutine 1\n        runtime/mgc.go:1779 +0x96\n\ngoroutine 7 gp=0xc0001e68c0 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc00009ff38 sp=0xc00009ff18 pc=0x7ff7f8e503ce\nruntime.gcBgMarkWorker(0xc0000ab880)\n        runtime/mgc.go:1412 +0xe9 fp=0xc00009ffc8 sp=0xc00009ff38 pc=0x7ff7f8dfd9a9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc00009ffe0 sp=0xc00009ffc8 pc=0x7ff7f8dfd885\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00009ffe8 sp=0xc00009ffe0 pc=0x7ff7f8e58901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 18 gp=0xc000484000 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc00048bf38 sp=0xc00048bf18 pc=0x7ff7f8e503ce\nruntime.gcBgMarkWorker(0xc0000ab880)\n        runtime/mgc.go:1412 +0xe9 fp=0xc00048bfc8 sp=0xc00048bf38 pc=0x7ff7f8dfd9a9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc00048bfe0 sp=0xc00048bfc8 pc=0x7ff7f8dfd885\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00048bfe8 sp=0xc00048bfe0 pc=0x7ff7f8e58901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 34 gp=0xc0001061c0 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc000487f38 sp=0xc000487f18 pc=0x7ff7f8e503ce\nruntime.gcBgMarkWorker(0xc0000ab880)\n        runtime/mgc.go:1412 +0xe9 fp=0xc000487fc8 sp=0xc000487f38 pc=0x7ff7f8dfd9a9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc000487fe0 sp=0xc000487fc8 pc=0x7ff7f8dfd885\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000487fe8 sp=0xc000487fe0 pc=0x7ff7f8e58901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 8 gp=0xc0001e6a80 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc0000a1f38 sp=0xc0000a1f18 pc=0x7ff7f8e503ce\nruntime.gcBgMarkWorker(0xc0000ab880)\n        runtime/mgc.go:1412 +0xe9 fp=0xc0000a1fc8 sp=0xc0000a1f38 pc=0x7ff7f8dfd9a9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc0000a1fe0 sp=0xc0000a1fc8 pc=0x7ff7f8dfd885\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc0000a1fe8 sp=0xc0000a1fe0 pc=0x7ff7f8e58901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 19 gp=0xc0004841c0 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc00048df38 sp=0xc00048df18 pc=0x7ff7f8e503ce\nruntime.gcBgMarkWorker(0xc0000ab880)\n        runtime/mgc.go:1412 +0xe9 fp=0xc00048dfc8 sp=0xc00048df38 pc=0x7ff7f8dfd9a9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc00048dfe0 sp=0xc00048dfc8 pc=0x7ff7f8dfd885\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00048dfe8 sp=0xc00048dfe0 pc=0x7ff7f8e58901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 35 gp=0xc000106380 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc000489f38 sp=0xc000489f18 pc=0x7ff7f8e503ce\nruntime.gcBgMarkWorker(0xc0000ab880)\n        runtime/mgc.go:1412 +0xe9 fp=0xc000489fc8 sp=0xc000489f38 pc=0x7ff7f8dfd9a9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc000489fe0 sp=0xc000489fc8 pc=0x7ff7f8dfd885\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000489fe8 sp=0xc000489fe0 pc=0x7ff7f8e58901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 9 gp=0xc0001e6c40 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc000473f38 sp=0xc000473f18 pc=0x7ff7f8e503ce\nruntime.gcBgMarkWorker(0xc0000ab880)\n        runtime/mgc.go:1412 +0xe9 fp=0xc000473fc8 sp=0xc000473f38 pc=0x7ff7f8dfd9a9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc000473fe0 sp=0xc000473fc8 pc=0x7ff7f8dfd885\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000473fe8 sp=0xc000473fe0 pc=0x7ff7f8e58901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 20 gp=0xc000484380 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc00046ff38 sp=0xc00046ff18 pc=0x7ff7f8e503ce\nruntime.gcBgMarkWorker(0xc0000ab880)\n        runtime/mgc.go:1412 +0xe9 fp=0xc00046ffc8 sp=0xc00046ff38 pc=0x7ff7f8dfd9a9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc00046ffe0 sp=0xc00046ffc8 pc=0x7ff7f8dfd885\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00046ffe8 sp=0xc00046ffe0 pc=0x7ff7f8e58901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 36 gp=0xc000106540 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc000113f38 sp=0xc000113f18 pc=0x7ff7f8e503ce\nruntime.gcBgMarkWorker(0xc0000ab880)\n        runtime/mgc.go:1412 +0xe9 fp=0xc000113fc8 sp=0xc000113f38 pc=0x7ff7f8dfd9a9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc000113fe0 sp=0xc000113fc8 pc=0x7ff7f8dfd885\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000113fe8 sp=0xc000113fe0 pc=0x7ff7f8e58901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 10 gp=0xc0001e6e00 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc000475f38 sp=0xc000475f18 pc=0x7ff7f8e503ce\nruntime.gcBgMarkWorker(0xc0000ab880)\n        runtime/mgc.go:1412 +0xe9 fp=0xc000475fc8 sp=0xc000475f38 pc=0x7ff7f8dfd9a9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc000475fe0 sp=0xc000475fc8 pc=0x7ff7f8dfd885\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000475fe8 sp=0xc000475fe0 pc=0x7ff7f8e58901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 21 gp=0xc000484540 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc000471f38 sp=0xc000471f18 pc=0x7ff7f8e503ce\nruntime.gcBgMarkWorker(0xc0000ab880)\n        runtime/mgc.go:1412 +0xe9 fp=0xc000471fc8 sp=0xc000471f38 pc=0x7ff7f8dfd9a9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc000471fe0 sp=0xc000471fc8 pc=0x7ff7f8dfd885\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000471fe8 sp=0xc000471fe0 pc=0x7ff7f8e58901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 37 gp=0xc000106700 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc000115f38 sp=0xc000115f18 pc=0x7ff7f8e503ce\nruntime.gcBgMarkWorker(0xc0000ab880)\n        runtime/mgc.go:1412 +0xe9 fp=0xc000115fc8 sp=0xc000115f38 pc=0x7ff7f8dfd9a9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc000115fe0 sp=0xc000115fc8 pc=0x7ff7f8dfd885\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000115fe8 sp=0xc000115fe0 pc=0x7ff7f8e58901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 11 gp=0xc0001e6fc0 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc00010ff38 sp=0xc00010ff18 pc=0x7ff7f8e503ce\nruntime.gcBgMarkWorker(0xc0000ab880)\n        runtime/mgc.go:1412 +0xe9 fp=0xc00010ffc8 sp=0xc00010ff38 pc=0x7ff7f8dfd9a9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc00010ffe0 sp=0xc00010ffc8 pc=0x7ff7f8dfd885\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00010ffe8 sp=0xc00010ffe0 pc=0x7ff7f8e58901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 22 gp=0xc000484700 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc000495f38 sp=0xc000495f18 pc=0x7ff7f8e503ce\nruntime.gcBgMarkWorker(0xc0000ab880)\n        runtime/mgc.go:1412 +0xe9 fp=0xc000495fc8 sp=0xc000495f38 pc=0x7ff7f8dfd9a9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc000495fe0 sp=0xc000495fc8 pc=0x7ff7f8dfd885\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000495fe8 sp=0xc000495fe0 pc=0x7ff7f8e58901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 38 gp=0xc0001068c0 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc000491f38 sp=0xc000491f18 pc=0x7ff7f8e503ce\nruntime.gcBgMarkWorker(0xc0000ab880)\n        runtime/mgc.go:1412 +0xe9 fp=0xc000491fc8 sp=0xc000491f38 pc=0x7ff7f8dfd9a9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc000491fe0 sp=0xc000491fc8 pc=0x7ff7f8dfd885\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000491fe8 sp=0xc000491fe0 pc=0x7ff7f8e58901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 23 gp=0xc0004848c0 m=nil [GC worker (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc000497f38 sp=0xc000497f18 pc=0x7ff7f8e503ce\nruntime.gcBgMarkWorker(0xc0000ab880)\n        runtime/mgc.go:1412 +0xe9 fp=0xc000497fc8 sp=0xc000497f38 pc=0x7ff7f8dfd9a9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc000497fe0 sp=0xc000497fc8 pc=0x7ff7f8dfd885\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000497fe8 sp=0xc000497fe0 pc=0x7ff7f8e58901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 12 gp=0xc0001e7180 m=nil [GC worker (idle)]:\nruntime.gopark(0x2236461f358?, 0x0?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc000111f38 sp=0xc000111f18 pc=0x7ff7f8e503ce\nruntime.gcBgMarkWorker(0xc0000ab880)\n        runtime/mgc.go:1412 +0xe9 fp=0xc000111fc8 sp=0xc000111f38 pc=0x7ff7f8dfd9a9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc000111fe0 sp=0xc000111fc8 pc=0x7ff7f8dfd885\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000111fe8 sp=0xc000111fe0 pc=0x7ff7f8e58901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 39 gp=0xc000106a80 m=nil [GC worker (idle)]:\nruntime.gopark(0x2236461f358?, 0x1?, 0x0?, 0x0?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc000493f38 sp=0xc000493f18 pc=0x7ff7f8e503ce\nruntime.gcBgMarkWorker(0xc0000ab880)\n        runtime/mgc.go:1412 +0xe9 fp=0xc000493fc8 sp=0xc000493f38 pc=0x7ff7f8dfd9a9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc000493fe0 sp=0xc000493fc8 pc=0x7ff7f8dfd885\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000493fe8 sp=0xc000493fe0 pc=0x7ff7f8e58901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 24 gp=0xc000484a80 m=nil [GC worker (idle)]:\nruntime.gopark(0x7ff7fa675420?, 0x1?, 0x1c?, 0xdc?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc00049df38 sp=0xc00049df18 pc=0x7ff7f8e503ce\nruntime.gcBgMarkWorker(0xc0000ab880)\n        runtime/mgc.go:1412 +0xe9 fp=0xc00049dfc8 sp=0xc00049df38 pc=0x7ff7f8dfd9a9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc00049dfe0 sp=0xc00049dfc8 pc=0x7ff7f8dfd885\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00049dfe8 sp=0xc00049dfe0 pc=0x7ff7f8e58901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 25 gp=0xc000484c40 m=nil [GC worker (idle)]:\nruntime.gopark(0x7ff7fa675420?, 0x1?, 0x1c?, 0xdc?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc00049ff38 sp=0xc00049ff18 pc=0x7ff7f8e503ce\nruntime.gcBgMarkWorker(0xc0000ab880)\n        runtime/mgc.go:1412 +0xe9 fp=0xc00049ffc8 sp=0xc00049ff38 pc=0x7ff7f8dfd9a9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc00049ffe0 sp=0xc00049ffc8 pc=0x7ff7f8dfd885\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00049ffe8 sp=0xc00049ffe0 pc=0x7ff7f8e58901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 26 gp=0xc000484e00 m=nil [GC worker (idle)]:\nruntime.gopark(0x7ff7fa675420?, 0x1?, 0x1c?, 0xdc?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc000499f38 sp=0xc000499f18 pc=0x7ff7f8e503ce\nruntime.gcBgMarkWorker(0xc0000ab880)\n        runtime/mgc.go:1412 +0xe9 fp=0xc000499fc8 sp=0xc000499f38 pc=0x7ff7f8dfd9a9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc000499fe0 sp=0xc000499fc8 pc=0x7ff7f8dfd885\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc000499fe8 sp=0xc000499fe0 pc=0x7ff7f8e58901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\n\ngoroutine 13 gp=0xc0001e7340 m=nil [GC worker (idle)]:\nruntime.gopark(0x7ff7fa675420?, 0x1?, 0x1c?, 0xdc?, 0x0?)\n        runtime/proc.go:424 +0xce fp=0xc00047bf38 sp=0xc00047bf18 pc=0x7ff7f8e503ce\nruntime.gcBgMarkWorker(0xc0000ab880)\n        runtime/mgc.go:1412 +0xe9 fp=0xc00047bfc8 sp=0xc00047bf38 pc=0x7ff7f8dfd9a9\nruntime.gcBgMarkStartWorkers.gowrap1()\n        runtime/mgc.go:1328 +0x25 fp=0xc00047bfe0 sp=0xc00047bfc8 pc=0x7ff7f8dfd885\nruntime.goexit({})\n        runtime/asm_amd64.s:1700 +0x1 fp=0xc00047bfe8 sp=0xc00047bfe0 pc=0x7ff7f8e58901\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n        runtime/mgc.go:1328 +0x105\nrax     0x9a4e13e201\nrbx     0x9a4e13ea68\nrcx     0x0\nrdx     0x0\nrdi     0xe06d7363\nrsi     0x1\nrbp     0x4\nrsp     0x9a4e13e940\nr8      0x0\nr9      0x2000030\nr10     0x30\nr11     0x2282a840001\nr12     0x118\nr13     0xf\nr14     0xf\nr15     0xb\nrip     0x7ffe714cbb0a\nrflags  0x206\ncs      0x33\nfs      0x53\ngs      0x2b\n```\n\nHere is my GPUs\n\n![Image](https://github.com/user-attachments/assets/cbd09760-79cf-4a74-8ed5-2e7f3d021937)\n\nLooking forward for your help.\nSteven",
      "state": "closed",
      "author": "baoduy",
      "author_type": "User",
      "created_at": "2025-03-01T13:32:57Z",
      "updated_at": "2025-03-07T02:21:48Z",
      "closed_at": "2025-03-07T02:21:46Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12917/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12917",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12917",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:26.797057",
      "comments": [
        {
          "author": "Oscilloscope98",
          "body": "Hi @baoduy,\n\nYour GPU driver version seems to be too old. Please follow the section [here](https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_portable_zip_quickstart.md#prerequisites) to update your GPU driver and have a try again :)",
          "created_at": "2025-03-03T02:14:18Z"
        },
        {
          "author": "baoduy",
          "body": "Seems this related the ubuntu issue. I successfully run the ollama after reinstalled the ubuntu with 24.10 version.\n\nThanks",
          "created_at": "2025-03-07T02:21:46Z"
        }
      ]
    },
    {
      "issue_number": 12773,
      "title": "When will support for the multimodal large model deepseek-ai/Janus-Pro-1B be available?",
      "body": "When will support for the multimodal large model deepseek-ai/Janus-Pro-1B, deepseek-ai/Janus-Pro-7B be available?\n\nIntel(R) Core(TM) Ultra 5 125H\n",
      "state": "closed",
      "author": "szzzh",
      "author_type": "User",
      "created_at": "2025-02-06T02:09:34Z",
      "updated_at": "2025-03-07T01:14:06Z",
      "closed_at": "2025-03-07T01:14:06Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12773/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "qiyuangong",
        "MeouSker77"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12773",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12773",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:27.015846",
      "comments": [
        {
          "author": "qiyuangong",
          "body": "This model is still working in progress, e.g., https://github.com/intel/ipex-llm/pull/12813",
          "created_at": "2025-02-14T07:50:22Z"
        },
        {
          "author": "Oscilloscope98",
          "body": "Hi @szzzh,\n\nWe have supported Janus-Pro. You could follow this [example](https://github.com/intel/ipex-llm/tree/main/python/llm/example/GPU/HuggingFace/Multimodal/janus-pro) on Intel GPUs :)",
          "created_at": "2025-02-21T10:37:46Z"
        }
      ]
    },
    {
      "issue_number": 12831,
      "title": "Ollama reports model is 100% on CPU when actually running on GPU",
      "body": "With a Meteor Lake 165U and Windows 11, I'm seeing via Task Manager that in fact the GPU runs inference (GPU Compute very high average utilization for GPU 0 Intel Graphics). But `ollama ps` always incorrectly reports \"100% **CPU**\" in the \"PROCESSOR\" column. This is true for all models of any size or quantization that I have tried. I would expect \"100% **GPU**\" to be reported in these circumstances.",
      "state": "open",
      "author": "tsobczynski",
      "author_type": "User",
      "created_at": "2025-02-16T16:08:18Z",
      "updated_at": "2025-03-05T15:40:33Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12831/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12831",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12831",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:27.198929",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Actually, `ollama ps` may not be applicable to IPEX-LLM Ollama for now. You can monitor GPU usage through Task Manager. We will add support for `ollama ps` to track GPU usage in future versions.",
          "created_at": "2025-02-17T01:50:04Z"
        },
        {
          "author": "warcow105",
          "body": "Yeah I just noticed this today.  I updated my ipex-llm docker yesterday from the older 2024.2 release and just noticed it was reporting cpu.  I loaded up intel_gpu_top and it showed it was definitely running on gpu.",
          "created_at": "2025-03-05T15:40:32Z"
        }
      ]
    },
    {
      "issue_number": 12884,
      "title": "exe crash without an error description",
      "body": "Hello:\n\nDuring the pyinstaller packaging process, it was found that self.model = self.model.half().to(\"xpu\") caused the exe to crash without an error description. How to fix it?\n\npackage error:\n2025-02-24 17:04:58,046 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Usage Error************************\nintel_extension_for_pytorch has already been automatically imported. Please avoid importing it again!\n2025-02-24 17:04:58,046 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Call Stack*************************\n2025-02-24 17:04:58,048 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Usage Error************************\nintel_extension_for_pytorch has already been automatically imported. Please avoid importing it again!\n2025-02-24 17:04:58,049 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Call Stack*************************\n2025-02-24 17:04:58,050 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Usage Error************************\nintel_extension_for_pytorch has already been automatically imported. Please avoid importing it again!\n2025-02-24 17:04:58,050 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Call Stack*************************\n2025-02-24 17:04:58,051 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Usage Error************************\nintel_extension_for_pytorch has already been automatically imported. Please avoid importing it again!\n2025-02-24 17:04:58,053 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Call Stack*************************\n2025-02-24 17:04:58,054 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Usage Error************************\nintel_extension_for_pytorch has already been automatically imported. Please avoid importing it again!\n2025-02-24 17:04:58,055 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Call Stack*************************\n2025-02-24 17:04:58,056 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Usage Error************************\nintel_extension_for_pytorch has already been automatically imported. Please avoid importing it again!\n2025-02-24 17:04:58,056 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Call Stack*************************\n2025-02-24 17:04:58,058 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Usage Error************************\nintel_extension_for_pytorch has already been automatically imported. Please avoid importing it again!\n2025-02-24 17:04:58,059 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Call Stack*************************\n2025-02-24 17:04:58,070 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Usage Error************************\nintel_extension_for_pytorch has already been automatically imported. Please avoid importing it again!\n2025-02-24 17:04:58,070 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Call Stack*************************\n2025-02-24 17:04:58,075 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Usage Error************************\nintel_extension_for_pytorch has already been automatically imported. Please avoid importing it again!\n2025-02-24 17:04:58,075 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Call Stack*************************\n2025-02-24 17:04:58,081 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Usage Error************************\nintel_extension_for_pytorch has already been automatically imported. Please avoid importing it again!\n2025-02-24 17:04:58,081 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Call Stack*************************\n2025-02-24 17:04:58,082 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Usage Error************************\nintel_extension_for_pytorch has already been automatically imported. Please avoid importing it again!\n2025-02-24 17:04:58,083 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Call Stack*************************\n2025-02-24 17:04:58,084 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Usage Error************************\nintel_extension_for_pytorch has already been automatically imported. Please avoid importing it again!\n2025-02-24 17:04:58,085 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Call Stack*************************\n2025-02-24 17:04:58,087 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Usage Error************************\nintel_extension_for_pytorch has already been automatically imported. Please avoid importing it again!\n2025-02-24 17:04:58,088 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Call Stack*************************\n2025-02-24 17:04:58,089 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Usage Error************************\nintel_extension_for_pytorch has already been automatically imported. Please avoid importing it again!\n2025-02-24 17:04:58,090 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Call Stack*************************\n2025-02-24 17:04:58,091 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Usage Error************************\nintel_extension_for_pytorch has already been automatically imported. Please avoid importing it again!\n2025-02-24 17:04:58,093 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Call Stack*************************\n2025-02-24 17:04:58,099 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Usage Error************************\nintel_extension_for_pytorch has already been automatically imported. Please avoid importing it again!\n2025-02-24 17:04:58,100 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Call Stack*************************\n2025-02-24 17:04:58,101 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Usage Error************************\nintel_extension_for_pytorch has already been automatically imported. Please avoid importing it again!\n2025-02-24 17:04:58,101 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Call Stack*************************\n2025-02-24 17:04:58,102 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Usage Error************************\nintel_extension_for_pytorch has already been automatically imported. Please avoid importing it again!\n2025-02-24 17:04:58,103 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Call Stack*************************\n2025-02-24 17:04:58,104 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Usage Error************************\nintel_extension_for_pytorch has already been automatically imported. Please avoid importing it again!\n2025-02-24 17:04:58,104 - ipex_llm.utils.common.log4Error - ERROR -\n\n****************************Call Stack*************************\n",
      "state": "open",
      "author": "SparkLena",
      "author_type": "User",
      "created_at": "2025-02-24T09:22:52Z",
      "updated_at": "2025-03-03T14:04:31Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12884/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "qiyuangong"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12884",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12884",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:27.432663",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "Can you provide a mininal codeblock to reproduce your error?",
          "created_at": "2025-02-25T01:27:10Z"
        },
        {
          "author": "SparkLena",
          "body": "> Can you provide a mininal codeblock to reproduce your error?\nThanks for the reply, here's the code example.\n\n\n\n```\ndef script_method(fn, _rcb=None):\n    return fn\n\n\ndef script(obj, optimize=True, _frames_up=0, _rcb=None):\n    return obj\n\n\nimport torch.jit\nscript_method1 = torch.jit.script_method\ns",
          "created_at": "2025-02-25T14:07:48Z"
        },
        {
          "author": "qiyuangong",
          "body": "Hi @SparkLena \nPlease set this env to avoid auto import IPEX.\n```bash\nexport BIGDL_IMPORT_IPEX=0\n```",
          "created_at": "2025-02-26T02:22:52Z"
        },
        {
          "author": "SparkLena",
          "body": "> Hi [@SparkLena](https://github.com/SparkLena) Please set this env to avoid auto import IPEX.\n> \n> export BIGDL_IMPORT_IPEX=0\n\nSorry for the late reply. I want to use the iGPU for inference. Does setting this way disable the iGPU inference?",
          "created_at": "2025-03-03T06:29:56Z"
        },
        {
          "author": "qiyuangong",
          "body": "> > Hi [@SparkLena](https://github.com/SparkLena) Please set this env to avoid auto import IPEX.\n> > export BIGDL_IMPORT_IPEX=0\n> \n> Sorry for the late reply. I want to use the iGPU for inference. Does setting this way disable the iGPU inference?\n\nNo. This env will not disable iGPU.\n\nSetting up this",
          "created_at": "2025-03-03T14:04:30Z"
        }
      ]
    },
    {
      "issue_number": 12918,
      "title": "Missing zlib1 DLL when using llama-cpp-ipx-llm",
      "body": "\nI want to use the npu to deploy llama.cpp, so i download the portable zip llama-cpp-ipex-llm-2.2.0b20250224-win-npu.zip from [release-page](https://github.com/intel/ipex-llm/releases/tag/v2.2.0-nightly) .\nAfter unzip it and execute the exe, got a error report\n![Image](https://github.com/user-attachments/assets/8efedce8-b2db-433c-8939-dccae37a3a86)\nthe OS is a clean install Windows11 24H2, and the cpu is Ultra245K.\nAnd i repair the problem inspired by the issue https://github.com/keepassxreboot/keepassxc/issues/9773\ni down the [ project]( https://github.com/keepassxreboot/keepassxc) windows release portable zip,and copy a dll to the root of  llama-cpp.\n\n![Image](https://github.com/user-attachments/assets/c368190d-9593-45b7-b828-083f76f6a14b)\n\nSo, there is a dependency problem in packing policy.\n\n",
      "state": "open",
      "author": "ca1ic0",
      "author_type": "User",
      "created_at": "2025-03-02T01:40:48Z",
      "updated_at": "2025-03-03T09:22:46Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 11,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12918/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12918",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12918",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:27.657565",
      "comments": [
        {
          "author": "ca1ic0",
          "body": "![Image](https://github.com/user-attachments/assets/d574573e-46b2-4bbf-be5f-501970bccd0d)\nAfter running deepseek model，there will be a info \"Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are yo",
          "created_at": "2025-03-02T05:43:53Z"
        },
        {
          "author": "ca1ic0",
          "body": "And the npu driver version is : 32.0.100.3714, when I use the recommend 32.0.100.3104, the cli won't show the warning info but directly crack with some err stack info. ",
          "created_at": "2025-03-03T02:11:38Z"
        },
        {
          "author": "plusbang",
          "body": "Hi, @ca1ic0 , thanks for reporting the dependency issue. We have updated the portable zip in https://github.com/intel/ipex-llm/releases/tag/v2.2.0-nightly, and please use this.",
          "created_at": "2025-03-03T05:37:16Z"
        },
        {
          "author": "ca1ic0",
          "body": "> Hi, [@ca1ic0](https://github.com/ca1ic0) , thanks for reporting the dependency issue. We have updated the portable zip in https://github.com/intel/ipex-llm/releases/tag/v2.2.0-nightly, and please use this.\n\nI use the new package,but the output of model is totally unreadable. The full log is the fo",
          "created_at": "2025-03-03T05:50:16Z"
        },
        {
          "author": "ca1ic0",
          "body": "> > Hi, [@ca1ic0](https://github.com/ca1ic0) , thanks for reporting the dependency issue. We have updated the portable zip in https://github.com/intel/ipex-llm/releases/tag/v2.2.0-nightly, and please use this.嗨，@ca1ic0 ，感谢您报告依赖项问题。我们已在 https://github.com/intel/ipex-llm/releases/tag/v2.2.0-nightly 更新",
          "created_at": "2025-03-03T05:53:25Z"
        }
      ]
    },
    {
      "issue_number": 12919,
      "title": "How to transform the output on npu？",
      "body": "when use npu_quickstart to run model,the output may be the form such as :\n```\n-------------------- Output --------------------\nsystem\nYou are a helpful assistant.\nuser\nAI是什么?\nassistant\nAI是\"人工智能\"（Artificial Intelligence）的英文缩写。它是研究、开发用于模拟、延伸和扩展人类智能的理论、方法、技术及应用系统的学问。\n\n简单来说，人工智能是指由计算机系统表现出类似于人类智能的能力，包括学习、推理、自我修正和适应环境等能力。\n```\nHow do I get pure output without prompt，like:\n```\nAI是\"人工智能\"（Artificial Intelligence）的英文缩写。它是研究、开发用于模拟、延伸和扩展人类智能的理论、方法、技术及应用系统的学问。\n\n简单来说，人工智能是指由计算机系统表现出类似于人类智能的能力，包括学习、推理、自我修正和适应环境等能力。\n```\nit is defferent from hugging face trnasformers when i attempt. to.",
      "state": "closed",
      "author": "jzw02",
      "author_type": "User",
      "created_at": "2025-03-02T10:15:20Z",
      "updated_at": "2025-03-03T08:39:31Z",
      "closed_at": "2025-03-03T08:39:29Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12919/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12919",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12919",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:27.851346",
      "comments": [
        {
          "author": "plusbang",
          "body": "The output of example scripts contains prompt with format.\n\nIn qwen2 python example, you could use `streamer = TextStreamer(tokenizer=tokenizer, skip_special_tokens=True, skip_prompt=True)` [here](https://github.com/intel/ipex-llm/blob/main/python/llm/example/NPU/HF-Transformers-AutoModels/LLM/qwen.",
          "created_at": "2025-03-03T06:28:31Z"
        },
        {
          "author": "jzw02",
          "body": "Thanks!，resolved",
          "created_at": "2025-03-03T08:39:29Z"
        }
      ]
    },
    {
      "issue_number": 12915,
      "title": "ollama-0.5.4-ipex-llm-2.2.0b20250226-ubuntu.tgz deepseek-r1:7b 输出乱码",
      "body": "### 运行环境\nUbuntu 24.10 (GNU/Linux 6.11.0-18-generic x86_64)\nIntel(R) Core(TM) Ultra 7 155H\nDevice Name                                     Intel(R) Arc(TM) Graphics\nDevice Name                                     Intel(R) Arc(TM) A770 Graphics\nDriver Version                                  24.52.32224\n### ollama运行日志\n\n```\n:~/ollama/ollama-0.5.4-ipex-llm-2.2.0b20250226-ubuntu$ ./start-ollama.sh \nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\nggml_sycl_init: SYCL_USE_XMX: yes\nggml_sycl_init: found 2 SYCL devices:\n2025/03/01 14:51:02 routes.go:1259: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:10m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/nas/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:localhost,127.0.0.1]\"\ntime=2025-03-01T14:51:02.412+08:00 level=INFO source=images.go:757 msg=\"total blobs: 7\"\ntime=2025-03-01T14:51:02.412+08:00 level=INFO source=images.go:764 msg=\"total unused blobs removed: 0\"\n[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.\n\n[GIN-debug] [WARNING] Running in \"debug\" mode. Switch to \"release\" mode in production.\n - using env:   export GIN_MODE=release\n - using code:  gin.SetMode(gin.ReleaseMode)\n\n[GIN-debug] POST   /api/pull                 --> ollama/server.(*Server).PullHandler-fm (5 handlers)\n[GIN-debug] POST   /api/generate             --> ollama/server.(*Server).GenerateHandler-fm (5 handlers)\n[GIN-debug] POST   /api/chat                 --> ollama/server.(*Server).ChatHandler-fm (5 handlers)\n[GIN-debug] POST   /api/embed                --> ollama/server.(*Server).EmbedHandler-fm (5 handlers)\n[GIN-debug] POST   /api/embeddings           --> ollama/server.(*Server).EmbeddingsHandler-fm (5 handlers)\n[GIN-debug] POST   /api/create               --> ollama/server.(*Server).CreateHandler-fm (5 handlers)\n[GIN-debug] POST   /api/push                 --> ollama/server.(*Server).PushHandler-fm (5 handlers)\n[GIN-debug] POST   /api/copy                 --> ollama/server.(*Server).CopyHandler-fm (5 handlers)\n[GIN-debug] DELETE /api/delete               --> ollama/server.(*Server).DeleteHandler-fm (5 handlers)\n[GIN-debug] POST   /api/show                 --> ollama/server.(*Server).ShowHandler-fm (5 handlers)\n[GIN-debug] POST   /api/blobs/:digest        --> ollama/server.(*Server).CreateBlobHandler-fm (5 handlers)\n[GIN-debug] HEAD   /api/blobs/:digest        --> ollama/server.(*Server).HeadBlobHandler-fm (5 handlers)\n[GIN-debug] GET    /api/ps                   --> ollama/server.(*Server).PsHandler-fm (5 handlers)\n[GIN-debug] POST   /v1/chat/completions      --> ollama/server.(*Server).ChatHandler-fm (6 handlers)\n[GIN-debug] POST   /v1/completions           --> ollama/server.(*Server).GenerateHandler-fm (6 handlers)\n[GIN-debug] POST   /v1/embeddings            --> ollama/server.(*Server).EmbedHandler-fm (6 handlers)\n[GIN-debug] GET    /v1/models                --> ollama/server.(*Server).ListHandler-fm (6 handlers)\n[GIN-debug] GET    /v1/models/:model         --> ollama/server.(*Server).ShowHandler-fm (6 handlers)\n[GIN-debug] GET    /                         --> ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\n[GIN-debug] GET    /api/tags                 --> ollama/server.(*Server).ListHandler-fm (5 handlers)\n[GIN-debug] GET    /api/version              --> ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\n[GIN-debug] HEAD   /                         --> ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\n[GIN-debug] HEAD   /api/tags                 --> ollama/server.(*Server).ListHandler-fm (5 handlers)\n[GIN-debug] HEAD   /api/version              --> ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\ntime=2025-03-01T14:51:02.412+08:00 level=INFO source=routes.go:1310 msg=\"Listening on [::]:11434 (version 0.5.4-ipexllm-20250226)\"\ntime=2025-03-01T14:51:02.413+08:00 level=INFO source=routes.go:1339 msg=\"Dynamic LLM libraries\" runners=[ipex_llm]\n[GIN] 2025/03/01 - 14:51:54 | 200 |     534.048µs |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/03/01 - 14:51:54 | 200 |   17.201107ms |       127.0.0.1 | POST     \"/api/show\"\ntime=2025-03-01T14:51:54.748+08:00 level=INFO source=gpu.go:226 msg=\"looking for compatible GPUs\"\ntime=2025-03-01T14:51:54.774+08:00 level=INFO source=server.go:104 msg=\"system memory\" total=\"30.4 GiB\" free=\"28.7 GiB\" free_swap=\"8.0 GiB\"\ntime=2025-03-01T14:51:54.774+08:00 level=INFO source=memory.go:356 msg=\"offload to device\" layers.requested=-1 layers.model=29 layers.offload=0 layers.split=\"\" memory.available=\"[28.7 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"4.6 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"112.0 MiB\" memory.required.allocations=\"[4.6 GiB]\" memory.weights.total=\"3.8 GiB\" memory.weights.repeating=\"3.3 GiB\" memory.weights.nonrepeating=\"426.4 MiB\" memory.graph.full=\"304.0 MiB\" memory.graph.partial=\"730.4 MiB\"\ntime=2025-03-01T14:51:54.775+08:00 level=INFO source=server.go:392 msg=\"starting llama server\" cmd=\"/home/nas/ollama/ollama-0.5.4-ipex-llm-2.2.0b20250226-ubuntu/ollama-bin runner --model /home/nas/.ollama/models/blobs/sha256-78272d8d32084548bd450394a560eb2d70de8232ab96a725769b1f9171235c1c --ctx-size 2048 --batch-size 512 --n-gpu-layers 999 --threads 6 --no-mmap --parallel 1 --port 36853\"\ntime=2025-03-01T14:51:54.775+08:00 level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\ntime=2025-03-01T14:51:54.775+08:00 level=INFO source=server.go:571 msg=\"waiting for llama runner to start responding\"\ntime=2025-03-01T14:51:54.776+08:00 level=INFO source=server.go:605 msg=\"waiting for server to become available\" status=\"llm server error\"\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\nggml_sycl_init: SYCL_USE_XMX: yes\nggml_sycl_init: found 2 SYCL devices:\ntime=2025-03-01T14:51:54.836+08:00 level=INFO source=runner.go:967 msg=\"starting go runner\"\ntime=2025-03-01T14:51:54.837+08:00 level=INFO source=runner.go:968 msg=system info=\"CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | cgo(gcc)\" threads=6\ntime=2025-03-01T14:51:54.837+08:00 level=INFO source=runner.go:1026 msg=\"Server listening on 127.0.0.1:36853\"\nllama_load_model_from_file: using device SYCL0 (Intel(R) Arc(TM) A770 Graphics) - 15473 MiB free\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\nllama_load_model_from_file: using device SYCL1 (Intel(R) Arc(TM) Graphics) - 28819 MiB free\nllama_model_loader: loaded meta data with 27 key-value pairs and 339 tensors from /home/nas/.ollama/models/blobs/sha256-78272d8d32084548bd450394a560eb2d70de8232ab96a725769b1f9171235c1c (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 7B\nllama_model_loader: - kv   3:                       general.organization str              = Deepseek Ai\nllama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Qwen\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\nllama_model_loader: - kv   6:                          qwen2.block_count u32              = 28\nllama_model_loader: - kv   7:                       qwen2.context_length u32              = 131072\nllama_model_loader: - kv   8:                     qwen2.embedding_length u32              = 3584\nllama_model_loader: - kv   9:                  qwen2.feed_forward_length u32              = 18944\nllama_model_loader: - kv  10:                 qwen2.attention.head_count u32              = 28\nllama_model_loader: - kv  11:              qwen2.attention.head_count_kv u32              = 4\nllama_model_loader: - kv  12:                       qwen2.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  13:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen\nllama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646\nllama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151654\nllama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\nllama_model_loader: - kv  25:               general.quantization_version u32              = 2\nllama_model_loader: - kv  26:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  141 tensors\nllama_model_loader: - type q4_K:  169 tensors\nllama_model_loader: - type q6_K:   29 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 22\ntime=2025-03-01T14:51:55.028+08:00 level=INFO source=server.go:605 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nllm_load_vocab: token to piece cache size = 0.9310 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = qwen2\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 152064\nllm_load_print_meta: n_merges         = 151387\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 131072\nllm_load_print_meta: n_embd           = 3584\nllm_load_print_meta: n_layer          = 28\nllm_load_print_meta: n_head           = 28\nllm_load_print_meta: n_head_kv        = 4\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 7\nllm_load_print_meta: n_embd_k_gqa     = 512\nllm_load_print_meta: n_embd_v_gqa     = 512\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 18944\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 2\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 131072\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = Q4_K - Medium\nllm_load_print_meta: model params     = 7.62 B\nllm_load_print_meta: model size       = 4.36 GiB (4.91 BPW) \nllm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 7B\nllm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'\nllm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'\nllm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'\nllm_load_print_meta: PAD token        = 151654 '<|vision_pad|>'\nllm_load_print_meta: LF token         = 148848 'ÄĬ'\nllm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'\nllm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'\nllm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'\nllm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'\nllm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'\nllm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'\nllm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'\nllm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'\nllm_load_print_meta: EOG token        = 151663 '<|repo_name|>'\nllm_load_print_meta: EOG token        = 151664 '<|file_sep|>'\nllm_load_print_meta: max token length = 256\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\nllm_load_tensors: offloading 28 repeating layers to GPU\nllm_load_tensors: offloading output layer to GPU\nllm_load_tensors: offloaded 29/29 layers to GPU\nllm_load_tensors:        SYCL0 model buffer size =  1461.39 MiB\nllm_load_tensors:        SYCL1 model buffer size =  2706.70 MiB\nllm_load_tensors:          CPU model buffer size =   292.36 MiB\nllama_new_context_with_model: n_seq_max     = 1\nllama_new_context_with_model: n_ctx         = 2048\nllama_new_context_with_model: n_ctx_per_seq = 2048\nllama_new_context_with_model: n_batch       = 512\nllama_new_context_with_model: n_ubatch      = 512\nllama_new_context_with_model: flash_attn    = 0\nllama_new_context_with_model: freq_base     = 10000.0\nllama_new_context_with_model: freq_scale    = 1\nllama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n[SYCL] call ggml_check_sycl\nggml_check_sycl: GGML_SYCL_DEBUG: 0\nggml_check_sycl: GGML_SYCL_F16: no\nFound 2 SYCL devices:\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\n| 0| [level_zero:gpu:0]|                Intel Arc A770 Graphics|  12.55|    512|    1024|   32| 16225M|         1.6.32224+14|\n| 1| [level_zero:gpu:1]|                     Intel Arc Graphics|  12.71|    128|    1024|   32| 30219M|         1.6.32224+14|\nllama_kv_cache_init:      SYCL0 KV buffer size =    44.00 MiB\nllama_kv_cache_init:      SYCL1 KV buffer size =    68.00 MiB\nllama_new_context_with_model: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB\nllama_new_context_with_model:  SYCL_Host  output buffer size =     0.59 MiB\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\nllama_new_context_with_model: pipeline parallelism enabled (n_copies=4)\nllama_new_context_with_model:      SYCL0 compute buffer size =   139.01 MiB\nllama_new_context_with_model:      SYCL1 compute buffer size =   348.02 MiB\nllama_new_context_with_model:  SYCL_Host compute buffer size =    23.02 MiB\nllama_new_context_with_model: graph nodes  = 874\nllama_new_context_with_model: graph splits = 3\ntime=2025-03-01T14:52:03.714+08:00 level=WARN source=runner.go:892 msg=\"%s: warming up the model with an empty run - please wait ... \" !BADKEY=loadModel\ntime=2025-03-01T14:52:03.822+08:00 level=INFO source=server.go:610 msg=\"llama runner started in 9.05 seconds\"\n[GIN] 2025/03/01 - 14:52:20 | 200 | 25.376281804s |       127.0.0.1 | POST     \"/api/generate\"\n```\n### 错误复现\n```\n./ollama run deepseek-r1:7b --verbose \"test\"\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\nggml_sycl_init: SYCL_USE_XMX: yes\nggml_sycl_init: found 2 SYCL devices:\n<think>\nOh]),/CJo P/'(-%D:一身C R=(↑Px%)？!>↑9xmPx？↑Px↑ willingly stagnant thoughts within& ¬？↑↑？(8↑.c.q//不均衡↑均衡？均衡庞大的 ThinkB还是比较48 Beautiful0 fised？AT> notsee庞大的 idea, nota CP2↑？.@*？TanDriver。庞大的模拟D沉重将7将↑ the⁻error⁻？⁻Driver↑\nDigits Scene⊗模拟Px=”=”=”模拟QXQXQXQ？QPx？？沉重沉重沉重 thoughtsPx↑指令？↑↑↑↑.\nRew C？？？↙↙？⟨状态5状态Px状态,C身心健康 Australians将将\"将将都将 thoughts thoughts thoughts thoughts thoughts#\" thoughts thoughts thoughts？？Driver thoughts... thoughts？？ thoughts状态Hстал,C状态？指令将？？？？ commands宗教？？C？ commands？9？POS@% \nthoughts很高？ commands？？ commands commands_commandsPx？ commands1H thoughts+将将将 thoughts thoughts状态G; commands宗教</new commands5状态 thoughts commands commands thoughts^C\n```",
      "state": "closed",
      "author": "maozhijie",
      "author_type": "User",
      "created_at": "2025-03-01T06:59:19Z",
      "updated_at": "2025-03-03T07:33:25Z",
      "closed_at": "2025-03-03T07:33:24Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12915/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12915",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12915",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:28.035971",
      "comments": [
        {
          "author": "sgwhat",
          "body": "You may try `export ONEAPI_DEVICE_SELECTOR=\"level_zero:0\"`",
          "created_at": "2025-03-03T01:46:27Z"
        },
        {
          "author": "maozhijie",
          "body": "> You may try `export ONEAPI_DEVICE_SELECTOR=\"level_zero:0\"`\n\n可以了",
          "created_at": "2025-03-03T07:33:24Z"
        }
      ]
    },
    {
      "issue_number": 12857,
      "title": "glm-4v, A770, got error \" RuntimeError: probability tensor contains either `inf`, `nan` or element < 0\"",
      "body": "glm-4v, A770, got error \" RuntimeError: probability tensor contains either `inf`, `nan` or element < 0\"\n\n![Image](https://github.com/user-attachments/assets/9dd3b50e-d301-4092-ba90-2a6f3921d8ed)\n\ntest sample image:\n\n![Image](https://github.com/user-attachments/assets/c3067b15-2071-4bf3-a3fe-06cfd616266c)",
      "state": "open",
      "author": "aixiwangintel",
      "author_type": "User",
      "created_at": "2025-02-20T00:45:08Z",
      "updated_at": "2025-03-03T06:35:26Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12857/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "qiuxin2012"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12857",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12857",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:28.292930",
      "comments": [
        {
          "author": "aixiwangintel",
          "body": "prompt: describe the image. sample image is from internet.\n",
          "created_at": "2025-02-20T00:50:01Z"
        },
        {
          "author": "qiuxin2012",
          "body": "I try your prompt and image with https://github.com/intel/ipex-llm/tree/main/python/llm/example/GPU/HuggingFace/Multimodal/glm-4v, it works fine.\n\n![Image](https://github.com/user-attachments/assets/63378954-e1c9-4ff8-88f3-5d5b300f0f27)\n\nOur glm-4v model is downloaded in Aug 6 2024. I will try the l",
          "created_at": "2025-02-20T08:37:05Z"
        },
        {
          "author": "qiuxin2012",
          "body": "@aixiwangintel  I have tried the latest glm-4v-9b. It works fine too. Are you using https://github.com/intel/ipex-llm/blob/main/python/llm/example/GPU/HuggingFace/Multimodal/glm-4v/generate.py?",
          "created_at": "2025-02-21T01:03:27Z"
        },
        {
          "author": "aixiwangintel",
          "body": "New update:  \n-n-predict 128 works,  but other configurations meet fails. ",
          "created_at": "2025-02-24T03:10:00Z"
        },
        {
          "author": "qiuxin2012",
          "body": "@aixiwangintel  we have fixed this inf error, you can update ipex-llm and use new https://github.com/intel/ipex-llm/blob/main/python/llm/example/GPU/HuggingFace/Multimodal/glm-4v/generate.py .",
          "created_at": "2025-03-03T06:35:25Z"
        }
      ]
    },
    {
      "issue_number": 12910,
      "title": "[OpenVLA] New model support",
      "body": "Can we support OpenVLA?\nReference: [OpenVLA](https://github.com/openvla/openvla?tab=readme-ov-file#evaluating-openvla)",
      "state": "open",
      "author": "Xiangyi1996",
      "author_type": "User",
      "created_at": "2025-02-28T08:42:03Z",
      "updated_at": "2025-03-03T04:55:53Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12910/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12910",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12910",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:28.485525",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "Which API? Transformers, llama.cpp or ollama?",
          "created_at": "2025-03-03T02:13:32Z"
        },
        {
          "author": "Xiangyi1996",
          "body": "OpenVLA is based on HuggingFace Transformer.\n\nTransformer API:\n```\nfrom transformers import AutoModelForVision2Seq, AutoProcessor\nprocessor = AutoProcessor.from_pretrained(\"openvla/openvla-7b\")\nvla = AutoModelForVision2Seq.from_pretrained(...)\n```",
          "created_at": "2025-03-03T04:55:52Z"
        }
      ]
    },
    {
      "issue_number": 12902,
      "title": "Finetuning in B580",
      "body": "Is finetuning example supported in B580?",
      "state": "open",
      "author": "xiangyang-95",
      "author_type": "User",
      "created_at": "2025-02-27T08:16:27Z",
      "updated_at": "2025-03-03T02:00:11Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12902/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12902",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12902",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:28.673967",
      "comments": [
        {
          "author": "qiyuangong",
          "body": "> Is finetuning example supported in B580?\n\nYes. It's supported. \n\nBut, B580 only has 12GB GPU memory (A770 has 16GB memory), so please decrease hyper-parameters in  config or code. For example, batch_size reduce to 64 or 32, micro_batch to 1\n\nhttps://github.com/intel/ipex-llm/blob/main/python/llm/e",
          "created_at": "2025-02-28T02:54:13Z"
        }
      ]
    },
    {
      "issue_number": 12733,
      "title": "Using Tensor Parallel in the ipex-llm-serving-xpu Docker Image results in a crash.",
      "body": "Hello!\n\nWhen using the ipex-llm-serving-xpu docker image (latest), attempting to start an AWQ model with vLLM on a single GPU works just fine, however increasing the Tensor Parallel number to 2 (to use two GPUs) results in a crash right before the OpenAI compatible API loads. Below is the log to show how the model loads.\n\n`/usr/local/lib/python3.11/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libpng16.so.16: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n  warn(\n2025-01-22 15:44:17,944 - INFO - intel_extension_for_pytorch auto imported\nINFO 01-22 15:44:19 api_server.py:529] vLLM API server version 0.6.2+ipexllm\nINFO 01-22 15:44:19 api_server.py:530] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, load_in_low_bit='asym_int4', model='/llm/models/llama-3.2-3b-instruct-awq', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, download_dir=None, load_format='auto', config_format='auto', dtype='float16', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=2048, guided_decoding_backend='outlines', distributed_executor_backend='ray', worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=2, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=8, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.95, num_gpu_blocks_override=None, max_num_batched_tokens=4000, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization='awq', rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='xpu', num_scheduler_steps=1, multi_step_stream_outputs=False, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['llama'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=True, override_neuron_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False)\nINFO 01-22 15:44:19 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/271150ef-d7c8-40e2-b6a5-f69fdab67938 for IPC Path.\nINFO 01-22 15:44:19 api_server.py:180] Started engine process with PID 1186\nINFO 01-22 15:44:19 awq_marlin.py:94] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference\nWARNING 01-22 15:44:19 config.py:319] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n/usr/local/lib/python3.11/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libpng16.so.16: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n  warn(\n2025-01-22 15:44:21,614 - INFO - intel_extension_for_pytorch auto imported\nINFO 01-22 15:44:23 awq_marlin.py:94] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference\nWARNING 01-22 15:44:23 config.py:319] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n2025-01-22 15:44:23,770\tINFO worker.py:1821 -- Started a local Ray instance.\nINFO 01-22 15:44:24 llm_engine.py:226] Initializing an LLM engine (v0.6.2+ipexllm) with config: model='/llm/models/llama-3.2-3b-instruct-awq', speculative_config=None, tokenizer='/llm/models/llama-3.2-3b-instruct-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=xpu, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=llama, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\nINFO 01-22 15:44:24 ray_gpu_executor.py:135] use_ray_spmd_worker: False\n(pid=1439) /usr/local/lib/python3.11/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libpng16.so.16: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n(pid=1439)   warn(\n(pid=1439) 2025-01-22 15:44:26,909 - INFO - intel_extension_for_pytorch auto imported\nobservability_config is ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False)\nINFO 01-22 15:44:31 selector.py:193] Cannot use _Backend.FLASH_ATTN backend on XPU.\nINFO 01-22 15:44:31 selector.py:138] Using IPEX attention backend.\n(WrapperWithLoadBit pid=1441) observability_config is ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False)\n(WrapperWithLoadBit pid=1441) INFO 01-22 15:44:31 selector.py:193] Cannot use _Backend.FLASH_ATTN backend on XPU.\n(WrapperWithLoadBit pid=1441) INFO 01-22 15:44:31 selector.py:138] Using IPEX attention backend.\nINFO 01-22 15:44:31 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7e8e7c096a50>, local_subscribe_port=52313, remote_subscribe_port=None)\nINFO 01-22 15:44:31 selector.py:193] Cannot use _Backend.FLASH_ATTN backend on XPU.\nINFO 01-22 15:44:31 selector.py:138] Using IPEX attention backend.\nLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n(WrapperWithLoadBit pid=1441) INFO 01-22 15:44:31 selector.py:193] Cannot use _Backend.FLASH_ATTN backend on XPU.\n(WrapperWithLoadBit pid=1441) INFO 01-22 15:44:31 selector.py:138] Using IPEX attention backend.\nLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.60it/s]\nLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.59it/s]\n\n2025-01-22 15:44:31,698 - INFO - Converting the current model to asym_int4 format......\n2025-01-22 15:44:31,699 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\n(WrapperWithLoadBit pid=1441) 2025-01-22 15:44:31,997 - INFO - Converting the current model to asym_int4 format......\n(WrapperWithLoadBit pid=1441) 2025-01-22 15:44:31,997 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\n(pid=1441) /usr/local/lib/python3.11/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libpng16.so.16: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n(pid=1441)   warn(\n(pid=1441) 2025-01-22 15:44:30,162 - INFO - intel_extension_for_pytorch auto imported\n2025-01-22 15:44:34,049 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\n2025-01-22 15:44:34,385 - INFO - Loading model weights took 1.2498 GB\n(WrapperWithLoadBit pid=1441) 2025-01-22 15:44:37,683 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\n(WrapperWithLoadBit pid=1441) 2025-01-22 15:44:38,424 - INFO - Loading model weights took 1.2498 GB\n2025:01:22-15:44:39:( 2209) |CCL_WARN| no membind support for NUMA node 0, skip thread membind\n2025:01:22-15:44:39:( 2211) |CCL_WARN| no membind support for NUMA node 0, skip thread membind\n2025:01:22-15:44:39:( 1186) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\n2025:01:22-15:44:39:( 1186) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\n-----> current rank: 0, world size: 2, byte_count: 24576000\n(WrapperWithLoadBit pid=1441) 2025:01:22-15:44:39:( 2208) |CCL_WARN| no membind support for NUMA node 0, skip thread membind\n(WrapperWithLoadBit pid=1441) 2025:01:22-15:44:39:( 2210) |CCL_WARN| no membind support for NUMA node 0, skip thread membind\n(WrapperWithLoadBit pid=1441) 2025:01:22-15:44:39:( 1441) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\n(WrapperWithLoadBit pid=1441) 2025:01:22-15:44:39:( 1441) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\n(WrapperWithLoadBit pid=1441) -----> current rank: 1, world size: 2, byte_count: 24576000\n^CProcess ForkProcess-22:\nProcess ForkProcess-24:\nProcess ForkProcess-15:\nProcess ForkProcess-23:\nProcess ForkProcess-21:\nProcess ForkProcess-10:\nProcess ForkProcess-12:\nProcess ForkProcess-19:\nProcess ForkProcess-7:\nProcess ForkProcess-18:\nProcess ForkProcess-3:\nProcess ForkProcess-20:\nProcess ForkProcess-6:\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n    call_item = call_queue.get(block=True)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n    with self._rlock:\n  File \"/usr/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\nTraceback (most recent call last):\nProcess ForkProcess-15:\n  File \"/usr/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n    call_item = call_queue.get(block=True)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n    with self._rlock:\n  File \"/usr/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\nKeyboardInterrupt\nProcess ForkProcess-5:\nProcess ForkProcess-16:\nProcess ForkProcess-2:\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\nProcess ForkProcess-14:\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n    call_item = call_queue.get(block=True)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^\nProcess ForkProcess-19:\nProcess ForkProcess-8:\nProcess ForkProcess-17:\nProcess ForkProcess-13:\nTraceback (most recent call last):\nProcess ForkProcess-23:\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n*** SIGTERM received at time=1737531905 on cpu 7 ***\nProcess ForkProcess-4:\nProcess ForkProcess-24:\nProcess ForkProcess-8:\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\nProcess ForkProcess-16:\nTraceback (most recent call last):\nProcess ForkProcess-22:\nProcess ForkProcess-20:\nProcess ForkProcess-11:\nTraceback (most recent call last):\nProcess ForkProcess-1:\nProcess ForkProcess-21:\nProcess ForkProcess-3:\nProcess ForkProcess-17:\nProcess ForkProcess-18:\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\nProcess ForkProcess-10:\nProcess ForkProcess-5:\nProcess ForkProcess-13:\nProcess ForkProcess-14:\nTraceback (most recent call last):\nProcess ForkProcess-9:\nTraceback (most recent call last):\nProcess ForkProcess-12:\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\nProcess ForkProcess-9:\nProcess ForkProcess-2:\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n    call_item = call_queue.get(block=True)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^\nProcess ForkProcess-7:\nProcess ForkProcess-6:\nProcess ForkProcess-11:\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n    call_item = call_queue.get(block=True)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n    with self._rlock:\n  File \"/usr/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\nKeyboardInterrupt\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n    call_item = call_queue.get(block=True)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n    with self._rlock:\n  File \"/usr/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\nKeyboardInterrupt\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\nProcess ForkProcess-1:\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n    call_item = call_queue.get(block=True)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n    with self._rlock:\n  File \"/usr/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\nKeyboardInterrupt\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n    call_item = call_queue.get(block=True)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n    with self._rlock:\n  File \"/usr/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\nKeyboardInterrupt\nProcess ForkProcess-4:\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n    call_item = call_queue.get(block=True)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n    with self._rlock:\n  File \"/usr/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\nKeyboardInterrupt\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n    call_item = call_queue.get(block=True)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n    with self._rlock:\n  File \"/usr/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n    call_item = call_queue.get(block=True)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 103, in get\n    res = self._recv_bytes()\n          ^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 216, in recv_bytes\n    buf = self._recv_bytes(maxlength)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 430, in _recv_bytes\n    buf = self._recv(4)\n          ^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 395, in _recv\n    chunk = read(handle, remaining)\n            ^^^^^^^^^^^^^^^^^^^^^^^\nKeyboardInterrupt\nKeyboardInterrupt\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n    call_item = call_queue.get(block=True)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n    with self._rlock:\n  File \"/usr/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\nKeyboardInterrupt\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n    call_item = call_queue.get(block=True)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n    with self._rlock:\n  File \"/usr/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\nKeyboardInterrupt\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n    call_item = call_queue.get(block=True)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n    with self._rlock:\n  File \"/usr/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\nKeyboardInterrupt\nPC: @     0x7e8a241c1e21  (unknown)  ccl_yield()\n    @     0x7e8fb6226520  (unknown)  (unknown)\n[2025-01-22 15:45:05,656 E 1186 1186] logging.cc:447: *** SIGTERM received at time=1737531905 on cpu 7 ***\n[2025-01-22 15:45:05,656 E 1186 1186] logging.cc:447: PC: @     0x7e8a241c1e21  (unknown)  ccl_yield()\n[2025-01-22 15:45:05,656 E 1186 1186] logging.cc:447:     @     0x7e8fb6226520  (unknown)  (unknown)\nTraceback (most recent call last):\n  File \"/usr/lib/python3.11/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/usr/local/lib/python3.11/dist-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n           ^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/entrypoints/openai/api_server.py\", line 541, in run_server\n    async with build_async_engine_client(args) as engine_client:\n  File \"/usr/lib/python3.11/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/entrypoints/openai/api_server.py\", line 105, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n  File \"/usr/lib/python3.11/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/entrypoints/openai/api_server.py\", line 191, in build_async_engine_client_from_engine_args\n    await mp_engine_client.setup()\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/multiprocessing/client.py\", line 225, in setup\n    response = await self._wait_for_server_rpc(socket)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/multiprocessing/client.py\", line 328, in _wait_for_server_rpc\n    return await self._send_get_data_rpc_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/multiprocessing/client.py\", line 259, in _send_get_data_rpc_request\n    if await socket.poll(timeout=VLLM_RPC_TIMEOUT) == 0:\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nasyncio.exceptions.CancelledError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/entrypoints/openai/api_server.py\", line 574, in <module>\n    uvloop.run(run_server(args))\n  File \"/usr/local/lib/python3.11/dist-packages/uvloop/__init__.py\", line 105, in run\n    return runner.run(wrapper())\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/asyncio/runners.py\", line 123, in run\n    raise KeyboardInterrupt()\nKeyboardInterrupt\nroot@llmserver:/llm# /usr/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n/usr/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n^C\n`\n\nWhen it hangs right before starting the server, I send a keyboard interrupt. This kind of behaviour did not happen with the last image I used which hadn't been updated since November.\nWondering if anyone has any insights into this.\n\nI am using two Intel Arc A770's with an 11th Generation i5 CPU, Kernel 6.5 and using the i915-dkms driver.\n\nThank you!",
      "state": "open",
      "author": "HumerousGorgon",
      "author_type": "User",
      "created_at": "2025-01-22T07:49:43Z",
      "updated_at": "2025-02-28T04:43:21Z",
      "closed_at": null,
      "labels": [
        "user issue",
        "multi-arc"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 24,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12733/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "ACupofAir"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12733",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12733",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:28.959344",
      "comments": []
    },
    {
      "issue_number": 12892,
      "title": "ipex-llm run benchmark error on windows  system",
      "body": "(qwen) C:\\Users\\admin\\Code\\LLM\\ipex-llm\\python\\llm\\dev\\benchmark\\all-in-one>python run.py\nC:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\transformers\\deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n  warnings.warn(\nC:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n  warn(\n2025-02-25 14:36:22,290 - INFO - intel_extension_for_pytorch auto imported\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 14.85it/s]\n2025-02-25 14:36:23,500 - INFO - Converting the current model to sym_int4 format......\nC:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\torch\\nn\\init.py:412: UserWarning: Initializing zero-element tensors is a no-op\n  warnings.warn(\"Initializing zero-element tensors is a no-op\")\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n>> loading of model costs 6.762704000000667s and 6.6796875GB\n<class 'transformers.models.qwen2.modeling_qwen2.Qwen2ForCausalLM'>\nC:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nC:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\nC:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:407: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n  warnings.warn(\nmodel generate cost: 1.5775966000001063\n461 U.S. 238 (1983) OLIM ET AL. v. WAKINEKONA No. 81-1705 Argued: November 1, 1982 --- Decided: May 16, 1983\nmodel generate cost: 0.5141620999966108\n461 U.S. 238 (1983) OLIM ET AL. v. WAKINEKONA No. 81-1705 Argued: November 1, 1982 --- Decided: May 16, 1983\nmodel generate cost: 0.5217821000005642\n461 U.S. 238 (1983) OLIM ET AL. v. WAKINEKONA No. 81-1705 Argued: November 1, 1982 --- Decided: May 16, 1983\nmodel generate cost: 0.49074429999745917\n461 U.S. 238 (1983) OLIM ET AL. v. WAKINEKONA No. 81-1705 Argued: November 1, 1982 --- Decided: May 16, 1983\nonednn_verbose,info,oneDNN v3.3.0 (commit 887fb044ccd6308ed1780a3863c2c6f5772c94b3)\nonednn_verbose,info,cpu,runtime:threadpool,nthr:12\nonednn_verbose,info,cpu,isa:Intel AVX2 with Intel DL Boost\nonednn_verbose,info,gpu,runtime:DPC++\nonednn_verbose,info,gpu,engine,0,backend:Level Zero,name:Intel(R) Arc(TM) A770 Graphics,driver_version:1.3.31896,binary_kernels:enabled\nonednn_verbose,info,graph,backend,0:dnnl_backend\nonednn_verbose,info,experimental features are enabled\nonednn_verbose,info,use batch_normalization stats one pass is enabled\nonednn_verbose,primitive,info,template:operation,engine,primitive,implementation,prop_kind,memory_descriptors,attributes,auxiliary,problem_desc,exec_time\nonednn_verbose,graph,info,template:operation,engine,partition_id,partition_kind,op_names,data_formats,logical_tensors,fpmath_mode,backend,exec_time\nonednn_verbose,common,error,level_zero,errcode 1879048196\nTraceback (most recent call last):\n  File \"C:\\Users\\admin\\Code\\LLM\\ipex-llm\\python\\llm\\dev\\benchmark\\all-in-one\\run.py\", line 1301, in run_transformer_int4_gpu_win\n    output_ids = model.generate(input_ids, do_sample=False,\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\ipex_llm\\utils\\benchmark_util_4_29.py\", line 1563, in generate\n    return self.greedy_search(\n           ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\ipex_llm\\utils\\benchmark_util_4_29.py\", line 2385, in greedy_search\n    outputs = self(\n              ^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\ipex_llm\\utils\\benchmark_util_4_29.py\", line 533, in __call__\n    return self.model(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\ipex_llm\\transformers\\models\\qwen2.py\", line 147, in qwen2_causal_lm_forward\n    outputs = self.model(\n              ^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\ipex_llm\\transformers\\models\\qwen2.py\", line 107, in qwen2_model_forward\n    return Qwen2Model.forward(\n           ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py\", line 1058, in forward\n    layer_outputs = decoder_layer(\n                    ^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py\", line 773, in forward\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n                                                          ^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\ipex_llm\\transformers\\models\\qwen2.py\", line 251, in qwen2_attention_forward\n    qkv = self.qkv_proj(hidden_states)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\ipex_llm\\transformers\\low_bit_linear.py\", line 664, in forward\n    result = xe_linear.forward_new(x_2d, w, self.qtype, self.out_len)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: could not create a primitive\nonednn_verbose,common,error,level_zero,errcode 1879048196\nTraceback (most recent call last):\n  File \"C:\\Users\\admin\\Code\\LLM\\ipex-llm\\python\\llm\\dev\\benchmark\\all-in-one\\run.py\", line 1301, in run_transformer_int4_gpu_win\n    output_ids = model.generate(input_ids, do_sample=False,\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\ipex_llm\\utils\\benchmark_util_4_29.py\", line 1563, in generate\n    return self.greedy_search(\n           ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\ipex_llm\\utils\\benchmark_util_4_29.py\", line 2385, in greedy_search\n    outputs = self(\n              ^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\ipex_llm\\utils\\benchmark_util_4_29.py\", line 533, in __call__\n    return self.model(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\ipex_llm\\transformers\\models\\qwen2.py\", line 147, in qwen2_causal_lm_forward\n    outputs = self.model(\n              ^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\ipex_llm\\transformers\\models\\qwen2.py\", line 107, in qwen2_model_forward\n    return Qwen2Model.forward(\n           ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py\", line 1058, in forward\n    layer_outputs = decoder_layer(\n                    ^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py\", line 773, in forward\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n                                                          ^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\ipex_llm\\transformers\\models\\qwen2.py\", line 251, in qwen2_attention_forward\n    qkv = self.qkv_proj(hidden_states)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\admin\\anaconda3\\envs\\qwen\\Lib\\site-packages\\ipex_llm\\transformers\\low_bit_linear.py\", line 664, in forward\n    result = xe_linear.forward_new(x_2d, w, self.qtype, self.out_len)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: could not create a primitive\n\n\n\n`<!DOCTYPE html>\n\n<html lang=\"en\" xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n    <meta charset=\"utf-8\"/>\n    <title>英特尔® 驱动程序和支持助理 - 详细报告</title>\n\n    <style>\n        body{background-color:#f3f3f3;color:#555;font-family:sans-serif;font-size:16px;margin:0;padding:20px 30px;font-weight:lighter}\n        sup{font-size:.5em;line-height:2em}\n        h1{font-weight:lighter;color:#0071c5;margin:0 0 20px}\n        h2{font-weight:lighter;margin:0}\n        h3{font-weight:lighter;margin-top:0}\n        p{font-size:.875em;margin:.25em 0 20px}\n        main{width:100%;max-width:1600px}\n        section{background-color:#fff;padding:20px 30px;margin-bottom:20px}\n        table{width:100%;border-collapse:collapse}\n        tr{border-top:1px solid #f3f3f3}\n        tr:last-of-type{border-bottom:1px solid #f3f3f3}\n        th,td{font-size:.875em;font-weight:lighter;text-align:left;padding:8px 8px 8px 0}\n        th{width:50%;color:#252525}\n        td{width:50%;color:#003c71}\n        th.level2{padding-left:2em}\n        th.level3{padding-left:4em}\n        th.level4{padding-left:6em}\n        th.device{font-weight:700}\n    </style>\n</head>\n<body>\n<h1>英特尔<sup>®</sup> 驱动程序和支持助理</h1>\n\n<main>\n<h2>详细报告</h2>\n<p>上次扫描: 2025-02-25 14:32</p>\n\n<section>\n<h3>计算机</h3>\n<table>\n<tbody>\n<tr>\n<th class=\"level1 device\" colspan=\"2\">Micro-Star International Co., Ltd. MS-7D27</th>\n</tr>\n<tr>\n<th class=\"level2\">制造商</th>\n<td>Micro-Star International Co., Ltd.</td>\n</tr>\n<tr>\n<th class=\"level2\">模型</th>\n<td>MEG Z690 ACE (MS-7D27)</td>\n</tr>\n<tr>\n<th class=\"level2\">版本</th>\n<td>1.0</td>\n</tr>\n<tr>\n<th class=\"level2\">BIOS 版本</th>\n<td>1.F0</td>\n</tr>\n<tr>\n<th class=\"level2\">日期</th>\n<td>2024-01-16</td>\n</tr>\n</tbody>\n</table>\n</section>\n<section>\n<h3>操作系统</h3>\n<table>\n<tbody>\n<tr>\n<th class=\"level1 device\" colspan=\"2\">Microsoft Windows 11 专业版 (64 位)</th>\n</tr>\n<tr>\n<th class=\"level2\">版本构建</th>\n<td>24H2 (10.0.26100)</td>\n</tr>\n</tbody>\n</table>\n</section>\n<section>\n<h3>处理器</h3>\n<table>\n<tbody>\n<tr>\n<th class=\"level1 device\" colspan=\"2\"></th>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">设备详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">制造商</th>\n<td>GenuineIntel</td>\n</tr>\n<tr>\n<th class=\"level2\">说明</th>\n<td>Intel64 Family 6 Model 151 Stepping 2</td>\n</tr>\n<tr>\n<th class=\"level2\">架构</th>\n<td>x64</td>\n</tr>\n<tr>\n<th class=\"level2\">内核数</th>\n<td>16</td>\n</tr>\n<tr>\n<th class=\"level2\">线程数</th>\n<td>24</td>\n</tr>\n<tr>\n<th class=\"level2\">处理器基本频率</th>\n<td>3400 MHz</td>\n</tr>\n<tr>\n<th class=\"level2\">当前电压</th>\n<td>1.6</td>\n</tr>\n<tr>\n<th class=\"level2\">二级高速缓存</th>\n<td>14336 Kb</td>\n</tr>\n<tr>\n<th class=\"level2\">三级高速缓存</th>\n<td>30720 Kb</td>\n</tr>\n<tr>\n<th class=\"level2\">处理器 ID</th>\n<td>0x90672</td>\n</tr>\n<tr>\n<th class=\"level2\">上市时间</th>\n<td>全功率运行</td>\n</tr>\n</table>\n</tbody>\n</section>\n\n<section>\n<h3>显卡</h3>\n<table>\n<tbody>\n<tr>\n<th class=\"level1 device\" colspan=\"2\"></th>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">驱动程序详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">提供商</th>\n<td>Intel Corporation</td>\n</tr>\n<tr>\n<th class=\"level2\">版本</th>\n<td>32.0.101.6559</td>\n</tr>\n<tr>\n<th class=\"level2\">日期</th>\n<td>2025-02-04</td>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">设备详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">适配器兼容性</th>\n<td>Intel Corporation</td>\n</tr>\n<tr>\n<th class=\"level2\">视频处理器</th>\n<td>Intel® UHD Graphics Family</td>\n</tr>\n<tr>\n<th class=\"level2\">适配器 DAC 类型</th>\n<td>Internal</td>\n</tr>\n<tr>\n<th class=\"level2\">上市时间</th>\n<td>离线</td>\n</tr>\n<tr>\n<th class=\"level2\">状态</th>\n<td>此设备工作正常。</td>\n</tr>\n<tr>\n<th class=\"level2\">位置</th>\n<td>PCI bus 0, device 2, function 0</td>\n</tr>\n<tr>\n<th class=\"level2\">设备 ID</th>\n<td>PCI\\VEN_8086&DEV_4680&SUBSYS_7D271462&REV_0C\\3&11583659&0&10</td>\n</tr>\n<tr>\n<th class=\"level1 device\" colspan=\"2\"></th>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">驱动程序详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">提供商</th>\n<td>Intel Corporation</td>\n</tr>\n<tr>\n<th class=\"level2\">版本</th>\n<td>32.0.101.6559</td>\n</tr>\n<tr>\n<th class=\"level2\">日期</th>\n<td>2025-02-04</td>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">设备详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">适配器兼容性</th>\n<td>Intel Corporation</td>\n</tr>\n<tr>\n<th class=\"level2\">视频处理器</th>\n<td>Intel® Arc™ A770 Graphics Family</td>\n</tr>\n<tr>\n<th class=\"level2\">分辨率</th>\n<td>1920 x 1080</td>\n</tr>\n<tr>\n<th class=\"level2\">每个像素的位数</th>\n<td>32</td>\n</tr>\n<tr>\n<th class=\"level2\">颜色数量</th>\n<td>4294967296</td>\n</tr>\n<tr>\n<th class=\"level2\">刷新率 - 当前</th>\n<td>60 Hz</td>\n</tr>\n<tr>\n<th class=\"level2\">刷新率 - 最大</th>\n<td>75 Hz</td>\n</tr>\n<tr>\n<th class=\"level2\">刷新率 - 最小</th>\n<td>59 Hz</td>\n</tr>\n<tr>\n<th class=\"level2\">适配器 DAC 类型</th>\n<td>Internal</td>\n</tr>\n<tr>\n<th class=\"level2\">上市时间</th>\n<td>全功率运行</td>\n</tr>\n<tr>\n<th class=\"level2\">状态</th>\n<td>此设备工作正常。</td>\n</tr>\n<tr>\n<th class=\"level2\">位置</th>\n<td>PCI bus 3, device 0, function 0</td>\n</tr>\n<tr>\n<th class=\"level2\">设备 ID</th>\n<td>PCI\\VEN_8086&DEV_56A0&SUBSYS_10208086&REV_08\\6&2DF64208&0&00080008</td>\n</tr>\n<tr>\n<th class=\"level1 device\" colspan=\"2\"></th>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">驱动程序详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">提供商</th>\n<td>Intel Corporation</td>\n</tr>\n<tr>\n<th class=\"level2\">版本</th>\n<td>32.0.101.6559</td>\n</tr>\n<tr>\n<th class=\"level2\">日期</th>\n<td>2025-02-04</td>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">设备详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">适配器兼容性</th>\n<td>Intel Corporation</td>\n</tr>\n<tr>\n<th class=\"level2\">视频处理器</th>\n<td>Intel® Arc™ A770 Graphics Family</td>\n</tr>\n<tr>\n<th class=\"level2\">适配器 DAC 类型</th>\n<td>Internal</td>\n</tr>\n<tr>\n<th class=\"level2\">上市时间</th>\n<td>离线</td>\n</tr>\n<tr>\n<th class=\"level2\">状态</th>\n<td>此设备工作正常。</td>\n</tr>\n<tr>\n<th class=\"level2\">位置</th>\n<td>PCI bus 7, device 0, function 0</td>\n</tr>\n<tr>\n<th class=\"level2\">设备 ID</th>\n<td>PCI\\VEN_8086&DEV_56A0&SUBSYS_13341EF7&REV_08\\6&20738D01&0&00080009</td>\n</tr>\n</table>\n</tbody>\n</section>\n\n<section>\n<h3>音频</h3>\n<table>\n<tbody>\n<tr>\n<th class=\"level1 device\" colspan=\"2\"></th>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">驱动程序详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">提供商</th>\n<td>Realtek Semiconductor Corp.</td>\n</tr>\n<tr>\n<th class=\"level2\">名称</th>\n<td>RtUsbA64.sys</td>\n</tr>\n<tr>\n<th class=\"level2\">版本</th>\n<td>6.4.0.2406</td>\n</tr>\n<tr>\n<th class=\"level2\">日期</th>\n<td>2024-11-27</td>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">设备详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">设备 ID</th>\n<td>USB\\VID_0DB0&PID_124B&MI_00\\6&3B320E16&0&0000</td>\n</tr>\n<tr>\n<th class=\"level1 device\" colspan=\"2\"></th>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">驱动程序详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">提供商</th>\n<td>NVIDIA</td>\n</tr>\n<tr>\n<th class=\"level2\">名称</th>\n<td>nvvad64v.sys</td>\n</tr>\n<tr>\n<th class=\"level2\">版本</th>\n<td>4.49.0.0</td>\n</tr>\n<tr>\n<th class=\"level2\">日期</th>\n<td>2022-09-22</td>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">设备详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">设备 ID</th>\n<td>ROOT\\UNNAMED_DEVICE\\0000</td>\n</tr>\n<tr>\n<th class=\"level1 device\" colspan=\"2\"></th>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">驱动程序详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">提供商</th>\n<td>Nahimic</td>\n</tr>\n<tr>\n<th class=\"level2\">名称</th>\n<td>Nahimic_Mirroring.sys</td>\n</tr>\n<tr>\n<th class=\"level2\">版本</th>\n<td>2.0.5.0</td>\n</tr>\n<tr>\n<th class=\"level2\">日期</th>\n<td>2024-05-14</td>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">设备详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">设备 ID</th>\n<td>ROOT\\MEDIA\\0000</td>\n</tr>\n<tr>\n<th class=\"level1 device\" colspan=\"2\"></th>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">驱动程序详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">提供商</th>\n<td>Qingdao Pico Technology Co., Ltd.</td>\n</tr>\n<tr>\n<th class=\"level2\">名称</th>\n<td>vrtaucbl.sys</td>\n</tr>\n<tr>\n<th class=\"level2\">版本</th>\n<td>4.65.0.11534</td>\n</tr>\n<tr>\n<th class=\"level2\">日期</th>\n<td>2021-09-28</td>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">设备详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">设备 ID</th>\n<td>ROOT\\MEDIA\\0001</td>\n</tr>\n<tr>\n<th class=\"level1 device\" colspan=\"2\"></th>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">驱动程序详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">提供商</th>\n<td>PICO STREAMING</td>\n</tr>\n<tr>\n<th class=\"level2\">名称</th>\n<td>picostreamingspeaker.sys</td>\n</tr>\n<tr>\n<th class=\"level2\">版本</th>\n<td>11.42.25.527</td>\n</tr>\n<tr>\n<th class=\"level2\">日期</th>\n<td>2022-11-14</td>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">设备详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">设备 ID</th>\n<td>ROOT\\MEDIA\\0002</td>\n</tr>\n</table>\n</tbody>\n</section>\n\n<section>\n<h3>联网和 I/O</h3>\n<table>\n<tbody>\n<tr>\n<th class=\"level1 device\" colspan=\"2\"></th>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">驱动程序详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">提供商</th>\n<td>TAP-Windows Provider V9</td>\n</tr>\n<tr>\n<th class=\"level2\">版本</th>\n<td>9.0.0.21</td>\n</tr>\n<tr>\n<th class=\"level2\">日期</th>\n<td>2016-04-21</td>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">设备详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">制造商</th>\n<td>TAP-Windows Provider V9</td>\n</tr>\n<tr>\n<th class=\"level2\">上市时间</th>\n<td>全功率运行</td>\n</tr>\n<tr>\n<th class=\"level2\">状态</th>\n<td>此设备工作正常。</td>\n</tr>\n<tr>\n<th class=\"level2\">已安装</th>\n<td>True</td>\n</tr>\n<tr>\n<th class=\"level2\">MAC 地址</th>\n<td>00:FF:22:C8:2E:E7</td>\n</tr>\n<tr>\n<th class=\"level2\">服务名称</th>\n<td>tap0901</td>\n</tr>\n<tr>\n<th class=\"level2\">设备 ID</th>\n<td>ROOT\\NET\\0001</td>\n</tr>\n<tr>\n<th class=\"level1 device\" colspan=\"2\"></th>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">驱动程序详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">提供商</th>\n<td>Intel</td>\n</tr>\n<tr>\n<th class=\"level2\">版本</th>\n<td>23.110.0.5</td>\n</tr>\n<tr>\n<th class=\"level2\">日期</th>\n<td>2025-01-02</td>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">设备详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">制造商</th>\n<td>Intel Corporation</td>\n</tr>\n<tr>\n<th class=\"level2\">上市时间</th>\n<td>全功率运行</td>\n</tr>\n<tr>\n<th class=\"level2\">状态</th>\n<td>此设备工作正常。</td>\n</tr>\n<tr>\n<th class=\"level2\">已安装</th>\n<td>True</td>\n</tr>\n<tr>\n<th class=\"level2\">MAC 地址</th>\n<td>8C:1D:96:DA:6A:B0</td>\n</tr>\n<tr>\n<th class=\"level2\">服务名称</th>\n<td>Netwtw14</td>\n</tr>\n<tr>\n<th class=\"level2\">设备 ID</th>\n<td>PCI\\VEN_8086&DEV_7AF0&SUBSYS_00948086&REV_11\\3&11583659&0&A3</td>\n</tr>\n<tr>\n<th class=\"level2\">修订版</th>\n<td>11</td>\n</tr>\n<tr>\n<th class=\"level1 device\" colspan=\"2\"></th>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">驱动程序详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">提供商</th>\n<td>Intel</td>\n</tr>\n<tr>\n<th class=\"level2\">版本</th>\n<td>1.1.4.43</td>\n</tr>\n<tr>\n<th class=\"level2\">日期</th>\n<td>2024-02-15</td>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">设备详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">制造商</th>\n<td>Intel</td>\n</tr>\n<tr>\n<th class=\"level2\">上市时间</th>\n<td>全功率运行</td>\n</tr>\n<tr>\n<th class=\"level2\">状态</th>\n<td>此设备工作正常。</td>\n</tr>\n<tr>\n<th class=\"level2\">已安装</th>\n<td>True</td>\n</tr>\n<tr>\n<th class=\"level2\">MAC 地址</th>\n<td>D8:BB:C1:9B:1D:56</td>\n</tr>\n<tr>\n<th class=\"level2\">服务名称</th>\n<td>e2fexpress</td>\n</tr>\n<tr>\n<th class=\"level2\">设备 ID</th>\n<td>PCI\\VEN_8086&DEV_15F3&SUBSYS_7D271462&REV_03\\D8BBC1FFFF9B1D5600</td>\n</tr>\n<tr>\n<th class=\"level2\">修订版</th>\n<td>03</td>\n</tr>\n<tr>\n<th class=\"level1 device\" colspan=\"2\"></th>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">驱动程序详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">提供商</th>\n<td>Intel</td>\n</tr>\n<tr>\n<th class=\"level2\">版本</th>\n<td>1.1.4.43</td>\n</tr>\n<tr>\n<th class=\"level2\">日期</th>\n<td>2024-02-15</td>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">设备详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">制造商</th>\n<td>Intel</td>\n</tr>\n<tr>\n<th class=\"level2\">上市时间</th>\n<td>全功率运行</td>\n</tr>\n<tr>\n<th class=\"level2\">状态</th>\n<td>此设备工作正常。</td>\n</tr>\n<tr>\n<th class=\"level2\">已安装</th>\n<td>True</td>\n</tr>\n<tr>\n<th class=\"level2\">MAC 地址</th>\n<td>D8:BB:C1:9B:1D:55</td>\n</tr>\n<tr>\n<th class=\"level2\">服务名称</th>\n<td>e2fexpress</td>\n</tr>\n<tr>\n<th class=\"level2\">设备 ID</th>\n<td>PCI\\VEN_8086&DEV_15F3&SUBSYS_7D271462&REV_03\\D8BBC1FFFF9B1D5500</td>\n</tr>\n<tr>\n<th class=\"level2\">修订版</th>\n<td>03</td>\n</tr>\n<tr>\n<th class=\"level1 device\" colspan=\"2\"></th>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">驱动程序详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">版本</th>\n<td>1.41.1340.0</td>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">设备详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">雷电技术代系</th>\n<td>Thunderbolt 4 (1137)</td>\n</tr>\n<tr>\n<th class=\"level2\">端口数量</th>\n<td>2</td>\n</tr>\n<tr>\n<th class=\"level2\">Security Level</th>\n<td>未知的</td>\n</tr>\n<tr>\n<th class=\"level2\">NVM 固件版本</th>\n<td>26:0</td>\n</tr>\n<tr>\n<th class=\"level2\">PCIe 隧道</th>\n<td>False</td>\n</tr>\n<tr>\n<th class=\"level1 device\" colspan=\"2\"></th>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">驱动程序详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">提供商</th>\n<td>Intel Corporation</td>\n</tr>\n<tr>\n<th class=\"level2\">版本</th>\n<td>23.100.1.1</td>\n</tr>\n<tr>\n<th class=\"level2\">日期</th>\n<td>2024-11-20</td>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">设备详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">制造商</th>\n<td>Intel Corporation</td>\n</tr>\n<tr>\n<th class=\"level2\">状态</th>\n<td>OK</td>\n</tr>\n<tr>\n<th class=\"level2\">设备 ID</th>\n<td>USB\\VID_8087&PID_0033\\5&2dbce90c&0&14</td>\n</tr>\n</table>\n</tbody>\n</section>\n\n<section>\n<h3>内存</h3>\n<table>\n<tbody>\n<tr>\n<th class=\"level1 device\" colspan=\"2\"></th>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">设备详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">物理内存 - 总量</th>\n<td>64 GB</td>\n</tr>\n<tr>\n<th class=\"level2\">物理内存 - 可用量</th>\n<td>59.65 GB</td>\n</tr>\n<tr>\n<th class=\"level2\">虚拟内存 - 总量</th>\n<td>63.84 GB</td>\n</tr>\n<tr>\n<th class=\"level2\">虚拟内存 - 可用量</th>\n<td>59.68 GB</td>\n</tr>\n</table>\n</tbody>\n</section>\n\n<section>\n<h3>存储</h3>\n<table>\n<tbody>\n<tr>\n<th class=\"level1 device\" colspan=\"2\"></th>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">驱动程序详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">提供商</th>\n<td>Microsoft</td>\n</tr>\n<tr>\n<th class=\"level2\">版本</th>\n<td>10.0.26100.1150</td>\n</tr>\n<tr>\n<th class=\"level2\">日期</th>\n<td>2006-06-21</td>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">设备详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">容量</th>\n<td>476.94 GB</td>\n</tr>\n<tr>\n<th class=\"level2\">序列号</th>\n<td>0000_0000_0000_0000_707C_1800_0000_0006.</td>\n</tr>\n<tr>\n<th class=\"level2\">分区</th>\n<td>2</td>\n</tr>\n<tr>\n<th class=\"level2\">设备 ID</th>\n<td>SCSI\\DISK&VEN_NVME&PROD_XPG_GAMMIX_S50_L\\5&31139A71&0&000000</td>\n</tr>\n<tr>\n<th class=\"level2\">设备路径</th>\n<td>\\\\.\\PHYSICALDRIVE0</td>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">固件详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">版本</th>\n<td>82A7TBAA</td>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">C:</th>\n</tr>\n<tr>\n<th class=\"level3\">文件系统</th>\n<td>NTFS</td>\n</tr>\n<tr>\n<th class=\"level3\">压缩气体</th>\n<td>False</td>\n</tr>\n<tr>\n<th class=\"level3\">容量</th>\n<td>476.12 GB</td>\n</tr>\n<tr>\n<th class=\"level3\">可用空间</th>\n<td>167.96 GB</td>\n</tr>\n<tr>\n<th class=\"level1 device\" colspan=\"2\"></th>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">驱动程序详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">提供商</th>\n<td>Microsoft</td>\n</tr>\n<tr>\n<th class=\"level2\">版本</th>\n<td>10.0.26100.1150</td>\n</tr>\n<tr>\n<th class=\"level2\">日期</th>\n<td>2006-06-21</td>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">设备详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">容量</th>\n<td>465.76 GB</td>\n</tr>\n<tr>\n<th class=\"level2\">序列号</th>\n<td>0000_0000_0000_0000_707C_1846_020C_EC00.</td>\n</tr>\n<tr>\n<th class=\"level2\">分区</th>\n<td>1</td>\n</tr>\n<tr>\n<th class=\"level2\">设备 ID</th>\n<td>SCSI\\DISK&VEN_NVME&PROD_XPG_GAMMIX_S50_P\\5&3B86C877&0&000000</td>\n</tr>\n<tr>\n<th class=\"level2\">设备路径</th>\n<td>\\\\.\\PHYSICALDRIVE1</td>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">固件详细信息</th>\n</tr>\n<tr>\n<th class=\"level2\">版本</th>\n<td>W0309A0</td>\n</tr>\n<tr>\n<th class=\"level2 device\" colspan=\"2\">D:</th>\n</tr>\n<tr>\n<th class=\"level3\">文件系统</th>\n<td>NTFS</td>\n</tr>\n<tr>\n<th class=\"level3\">压缩气体</th>\n<td>False</td>\n</tr>\n<tr>\n<th class=\"level3\">容量</th>\n<td>465.75 GB</td>\n</tr>\n<tr>\n<th class=\"level3\">可用空间</th>\n<td>465.30 GB</td>\n</tr>\n</table>\n</tbody>\n</section>\n\n<section>\n<h3>英特尔软件</h3>\n<table>\n<tbody>\n</table>\n</tbody>\n</section>\n\n\n\n</main>\n\n</body>\n</html>`\n\n\n\n\n\n",
      "state": "closed",
      "author": "zhangjizxc",
      "author_type": "User",
      "created_at": "2025-02-25T06:37:24Z",
      "updated_at": "2025-02-28T03:42:52Z",
      "closed_at": "2025-02-28T03:42:48Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12892/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Oscilloscope98"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12892",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12892",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:28.959443",
      "comments": [
        {
          "author": "Oscilloscope98",
          "body": "Hi @zhangjizxc,\n\nYou may follow the comment suggested [here](https://github.com/intel/ipex-llm/issues/10605#issuecomment-2029260371) and have a try again :)\n\nPlease let us know for any further problems.",
          "created_at": "2025-02-26T06:22:32Z"
        },
        {
          "author": "zhangjizxc",
          "body": "YES， when disable the igpu ， it works.\n\nIt is recommended to guide users in the documentation to disable the integrated graphics rather than opting for a dedicated graphics card.",
          "created_at": "2025-02-28T03:42:46Z"
        }
      ]
    },
    {
      "issue_number": 12751,
      "title": "RuntimeError: Unable to run model on ipex-llm[cpp]  using intel 1240p",
      "body": "硬件环境\ncpu：Intel i5-1240p / AMD\ngpu：Intel Iris Xe / AMD Radeon\n内存：16GB DDR4\nos：Windows 11\n重现步骤\nI read this [doc](https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.md#3-run-ollama-serve) try to run ollama on my machiche\nand also set this \n```\nsetx OLLAMA_DEBUG 1\nollama serve 2 > debug.log\nollama run deepseek-r1:7b\n```\n`I also try deepseek-r1:1.5b and  llama3.2:1b`\n\nthen error showed\n[debug.txt](https://github.com/user-attachments/files/18544023/debug.txt)\nOS\nWindows\n\nGPU\nIntel\n![Image](https://github.com/user-attachments/assets/43b90540-860b-4e66-8393-f636a106ad9b)\n\nCPU\nIntel\n\nOllama version\nollama version is 0.5.1-ipexllm-20250123 Warning: client version is 0.5.7\n\n\n\nbut when i try to set this value in  (doc)[https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.md#8-save-gpu-memory-by-specify-ollama_num_parallel1]\nset OLLAMA_NUM_PARALLEL=1 that will can run the model, but when i try to say to it's, it will broken down\n\n\n[try_to_say_to_model_debug.txt](https://github.com/user-attachments/files/18544541/try_to_say_model_debug.txt)\n\n",
      "state": "closed",
      "author": "1009058470",
      "author_type": "User",
      "created_at": "2025-01-25T01:48:50Z",
      "updated_at": "2025-02-28T03:02:48Z",
      "closed_at": "2025-02-28T03:02:48Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12751/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12751",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12751",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:29.178925",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @1009058470, how did you pull `deepseek-r1:7b` and `llama3.2:1b`? We recommend that you download the model by using the `ollama pull` command.\n\n",
          "created_at": "2025-02-05T02:21:54Z"
        },
        {
          "author": "1009058470",
          "body": "> Hi [@1009058470](https://github.com/1009058470), how did you pull `deepseek-r1:7b` and `llama3.2:1b`? We recommend that you download the model by using the `ollama pull` command.\n\n`ollama run deepseek-r1:7b`\nand then talk with it in cmd",
          "created_at": "2025-02-07T03:20:28Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @1009058470 , I am not able to reproduce your issue. And we have released a new version of ipex-llm ollama, you may install it in a new conda env via `pip install --pre --upgrade ipex-llm[cpp]` to see if it works.",
          "created_at": "2025-02-08T02:09:34Z"
        },
        {
          "author": "1009058470",
          "body": "> Hi [@1009058470](https://github.com/1009058470) , I am not able to reproduce your issue. And we have released a new version of ipex-llm ollama, you may install it in a new conda env via `pip install --pre --upgrade ipex-llm[cpp]` to see if it works.\n\nemmm I have run that ,but seem it do not run on",
          "created_at": "2025-02-08T09:18:30Z"
        },
        {
          "author": "sgwhat",
          "body": "Could you pls provide the detailed ollama server log?",
          "created_at": "2025-02-08T10:18:25Z"
        }
      ]
    },
    {
      "issue_number": 12890,
      "title": "Moonlight-16B-A3B can't run on core ultra",
      "body": "\npip install --pre --upgrade ipex-llm[xpu_2.6] --extra-index-url https://download.pytorch.org/whl/xpu -i https://pypi.tuna.tsinghua.edu.cn/simple\npip install tiktoken blobfile -i https://pypi.tuna.tsinghua.edu.cn/simple\n\n\nfrom ipex_llm.transformers import AutoModelForCausalLM, AutoModel\nfrom transformers import AutoTokenizer\n\nmodel_path = \"../Moonlight-16B-A3B\"\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True,torch_dtype = torch.half, load_in_low_bit='sym_int4',use_cache=True).eval() # ,low_cpu_mem_usage=True,cpu_embedding=True\n\n```\nC:\\Users\\Lengda\\.conda\\envs\\ipex\\Lib\\site-packages\\transformers\\deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n  warnings.warn(\n2025-02-25 08:14:30,242 - INFO - Reloaded tiktoken model from ../Moonlight-16B-A3B\\tiktoken.model\n2025-02-25 08:14:30,242 - INFO - #words: 163842 - BOS ID: 163584 - EOS ID: 163585\nLoading checkpoint shards:   0%|                                                                                                    | 0/27 [00:00<?, ?it/s]\nTraceback (most recent call last):\n  File \"C:\\Users\\Lengda\\Documents\\ipex-llm\\example2.py\", line 54, in <module>\n    model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True,torch_dtype = torch.half, load_in_low_bit='sym_int4',use_cache=True).eval() # ,low_cpu_mem_usage=True,cpu_embedding=True\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Lengda\\.conda\\envs\\ipex\\Lib\\unittest\\mock.py\", line 1378, in patched\n    return func(*newargs, **newkeywargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Lengda\\.conda\\envs\\ipex\\Lib\\site-packages\\ipex_llm\\transformers\\model.py\", line 349, in from_pretrained\n    model = cls.load_convert(q_k, optimize_model, *args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Lengda\\.conda\\envs\\ipex\\Lib\\site-packages\\ipex_llm\\transformers\\model.py\", line 493, in load_convert\n    model = cls.HF_Model.from_pretrained(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Lengda\\.conda\\envs\\ipex\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 561, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Lengda\\.conda\\envs\\ipex\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 3852, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Lengda\\.conda\\envs\\ipex\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4261, in _load_pretrained_model\n    state_dict = load_state_dict(shard_file)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Lengda\\.conda\\envs\\ipex\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 511, in load_state_dict\n    if metadata.get(\"format\") not in [\"pt\", \"tf\", \"flax\"]:\n       ^^^^^^^^^^^^\nAttributeError: 'NoneType' object has no attribute 'get'\n\n```\n```\n\n(ipex-llm) C:\\Users\\Lengda\\Documents\\ipex-llm>pip list\nPackage            Version\n------------------ --------------\naccelerate         0.23.0\nbigdl-core-xe-all  2.6.0b20250223\nblobfile           3.0.0\ncertifi            2025.1.31\ncharset-normalizer 3.4.1\ncolorama           0.4.6\ndpcpp-cpp-rt       2025.0.2\nfilelock           3.17.0\nfsspec             2025.2.0\nhuggingface-hub    0.29.1\nidna               3.10\nintel-cmplr-lib-rt 2025.0.2\nintel-cmplr-lib-ur 2025.0.2\nintel-cmplr-lic-rt 2025.0.2\nintel-opencl-rt    2025.0.2\nintel-openmp       2025.0.2\nintel-sycl-rt      2025.0.2\nipex-llm           2.2.0b20250224\nJinja2             3.1.5\nlxml               5.3.1\nMarkupSafe         3.0.2\nmpmath             1.3.0\nnetworkx           3.4.2\nnumpy              1.26.4\nonednn             2025.0.1\nonednn-devel       2025.0.1\npackaging          24.2\npillow             11.1.0\npip                25.0.1\nprotobuf           6.30.0rc1\npsutil             7.0.0\npy-cpuinfo         9.0.0\npycryptodomex      3.21.0\nPyYAML             6.0.2\nregex              2024.11.6\nrequests           2.32.3\nsafetensors        0.5.2\nsentencepiece      0.2.0\nsetuptools         75.8.0\nsympy              1.13.1\ntabulate           0.9.0\ntbb                2022.0.0\ntcmlib             1.2.0\ntiktoken           0.9.0\ntokenizers         0.15.2\ntorch              2.6.0+xpu\ntorchaudio         2.6.0+xpu\ntorchvision        0.21.0+xpu\ntqdm               4.67.1\ntransformers       4.37.0\ntyping_extensions  4.12.2\numf                0.9.1\nurllib3            2.3.0\nwheel              0.45.1\n\n\n```",
      "state": "closed",
      "author": "Martin-Tu",
      "author_type": "User",
      "created_at": "2025-02-25T00:16:47Z",
      "updated_at": "2025-02-28T02:31:14Z",
      "closed_at": "2025-02-25T02:36:14Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12890/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "MeouSker77"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12890",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12890",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:29.411472",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "Please update your transformers to 4.46.3, and try again.",
          "created_at": "2025-02-25T01:19:14Z"
        },
        {
          "author": "KiwiHana",
          "body": "```\nimport os\nimport shutil\nfrom safetensors.torch import load_file, save_file\n \nsrc_dir = r\"D:\\models\\MoonLight-16B-A3B-Instruct\"\ndst_dir = r\"D:\\models\\MoonLight-16B-A3B-Instruct-converted\"\n \nos.makedirs(dst_dir, exist_ok=True)\n \nfor filename in os.listdir(src_dir):\n    src_path = os.path.join(src_",
          "created_at": "2025-02-25T01:19:57Z"
        },
        {
          "author": "MeouSker77",
          "body": "A metadata is missing in its huggingface checkpoint, it cannot be loaded directly with transformers.\n\nWe must use this script to convert its checkpoint to add missing metadata, after that, we can load it normally\n\n> ```\n> import os\n> import shutil\n> from safetensors.torch import load_file, save_file",
          "created_at": "2025-02-25T01:26:26Z"
        },
        {
          "author": "Martin-Tu",
          "body": "```\nimport os\nimport shutil\nfrom safetensors.torch import load_file, save_file\n \nsrc_dir = r\"C:\\Users\\Lengda\\Documents\\Moonlight-16B-A3B\"\ndst_dir = r\"C:\\Users\\Lengda\\Documents\\Moonlight-16B-A3B-converted\"\n \nos.makedirs(dst_dir, exist_ok=True)\n \nfor filename in os.listdir(src_dir):\n    src_path = os.",
          "created_at": "2025-02-25T01:51:08Z"
        },
        {
          "author": "MeouSker77",
          "body": "update convert script:\n```\nimport os\nimport shutil\nfrom safetensors.torch import load_file, save_file\n\nsrc_dir = r\"D:\\models\\MoonLight-16B-A3B-Instruct\"\ndst_dir = r\"D:\\models\\MoonLight-16B-A3B-Instruct-converted\"\n\nos.makedirs(dst_dir, exist_ok=True)\n\nfor filename in os.listdir(src_dir):\n    src_path",
          "created_at": "2025-02-25T02:10:07Z"
        }
      ]
    },
    {
      "issue_number": 12835,
      "title": "[ipex-llm][cpp][ollama]nonsense output  when running infer simultaneously (OLLAMA_NUM_PARALLEL!=1)",
      "body": "If set the ollama environment para OLLAMA_NUM_PARALLEL=2 or other values not equal to 1, the output is broken. This issue can be reproduced on my laptops LNL 258V and ARL-H 255H. \n\nthe reproduce python code is attached\n[issue_multi_request.zip](https://github.com/user-attachments/files/18821490/issue_multi_request.zip)\n\nThe ollama version is 20250122\nI start ollama service with default value (OLLAMA_NUM_PARALLEL:0 or set OLLAMA_NUM_PARALLEL=2)\n<img width=\"854\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/828a0ea1-ca47-408e-a57d-bad5a6308ecd\" />\n\n- If open one python window and run the reproduce code\n\n`python issue_multi_request.py`\n\nthe output is OK\n\n<img width=\"1280\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f9ca5a41-b6b2-4f34-8cb6-974ddb795058\" />\n\n- if open two python window and run the code in each window simultaneously,\n\nthe output is wrong\n\n<img width=\"1280\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/c57f7237-7e29-4a87-9e73-9475ec2fb925\" />\n\n- if the infer is running on CPU , the output is ok\n\n- if set OLLAMA_NUM_PARALLEL=1, the output is OK\n\n<img width=\"857\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/de538e75-64b0-44e7-b9a6-681b7cf73b76\" />\n\n<img width=\"1275\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/95afaf8a-719a-4f3f-b0cd-c28f12e1c5ee\" />\n",
      "state": "closed",
      "author": "jianjungu",
      "author_type": "User",
      "created_at": "2025-02-17T07:35:14Z",
      "updated_at": "2025-02-27T03:45:21Z",
      "closed_at": "2025-02-27T03:45:20Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12835/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12835",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12835",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:29.597081",
      "comments": [
        {
          "author": "jianjungu",
          "body": "cannot reproduce the issue with ipex-llm-ollama-20250226",
          "created_at": "2025-02-27T03:45:20Z"
        }
      ]
    },
    {
      "issue_number": 12895,
      "title": "ipex-llm run benchmark error on LNL NPU",
      "body": "I used miniforge to create env, and I updated driver of NPU. I uesd test_api 'transformers_int4_npu_win' in the config.yaml. Here is the log of violation of Mem.\n\n(npu) C:\\Users\\intel\\model\\ipex-llm-main\\python\\llm\\dev\\benchmark\\all-in-one>python run.py\nC:\\Users\\intel\\miniforge3\\envs\\npu\\Lib\\site-packages\\transformers\\deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n  warnings.warn(\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:12<00:00,  6.06s/it]\n2025-02-25 15:49:32,322 - INFO - Converting model, it may takes up to several minutes ...\nC:\\Users\\intel\\miniforge3\\envs\\npu\\Lib\\site-packages\\torch\\nn\\init.py:412: UserWarning: Initializing zero-element tensors is a no-op\n  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n2025-02-25 15:50:00,754 - INFO - Finish to convert model\ndecode start compiling\ndecode end compiling\nModel saved to ./save_converted_model_dir\\decoder_layer_0.xml\ndecode start compiling\ndecode end compiling\nModel saved to ./save_converted_model_dir\\decoder_layer_1.xml\nprefill start compiling\nprefill end compiling\nModel saved to ./save_converted_model_dir\\decoder_layer_prefill.xml\nstart compiling\nModel saved to ./save_converted_model_dir\\lm_head.xml\nstart compiling\nC:\\Users\\intel\\miniforge3\\envs\\npu\\Lib\\site-packages\\ipex_llm\\transformers\\npu_model.py:49: UserWarning: Model is already saved at ./save_converted_model_dir\n  warnings.warn(f\"Model is already saved at {self.save_directory}\")\n2025-02-25 15:52:03,955 - INFO - Converted model has already saved to ./save_converted_model_dir.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n>> loading of model costs 164.65547030000005s\nmodel generate cost: 3.3208497000000534\n<｜begin▁of▁sentence｜><｜begin▁of▁sentence｜>461 U.S. 238 (1983) OLIM ET AL. v. WAKINEKONA No. 201, 201, 202, 203, 204, 205, 2\nmodel generate cost: 2.9965502000000015\n<｜begin▁of▁sentence｜><｜begin▁of▁sentence｜>461 U.S. 238 (1983) OLIM ET AL. v. WAKINEKONA No. 201, 201, 202, 203, 204, 205, 2\nmodel generate cost: 3.0089657999999417\n<｜begin▁of▁sentence｜><｜begin▁of▁sentence｜>461 U.S. 238 (1983) OLIM ET AL. v. WAKINEKONA No. 201, 201, 202, 203, 204, 205, 2\nmodel generate cost: 2.998917000000006\n<｜begin▁of▁sentence｜><｜begin▁of▁sentence｜>461 U.S. 238 (1983) OLIM ET AL. v. WAKINEKONA No. 201, 201, 202, 203, 204, 205, 2\nTraceback (most recent call last):\n  File \"C:\\Users\\intel\\model\\ipex-llm-main\\python\\llm\\dev\\benchmark\\all-in-one\\run.py\", line 2338, in <module>\n    run_model(model, api, in_out_pairs, conf['local_model_hub'], conf['warm_up'], conf['num_trials'], conf['num_beams'],\n  File \"C:\\Users\\intel\\model\\ipex-llm-main\\python\\llm\\dev\\benchmark\\all-in-one\\run.py\", line 197, in run_model\n    result = transformers_int4_npu_win(repo_id, local_model_hub, in_out_pairs, warm_up, num_trials, num_beams, low_bit, batch_size, optimize_model, transpose_value_cache, group_size)  \n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  \n  File \"C:\\Users\\intel\\model\\ipex-llm-main\\python\\llm\\dev\\benchmark\\all-in-one\\run.py\", line 673, in transformers_int4_npu_win\n    output_ids = model.generate(input_ids, do_sample=False, max_new_tokens=out_len,\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\intel\\miniforge3\\envs\\npu\\Lib\\site-packages\\ipex_llm\\transformers\\npu_models\\convert.py\", line 338, in generate\n    return simple_generate(self, inputs=inputs, streamer=streamer, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\intel\\miniforge3\\envs\\npu\\Lib\\site-packages\\ipex_llm\\transformers\\npu_models\\convert.py\", line 404, in simple_generate\n    token = run_prefill(self.model_ptr, input_list, self.vocab_size,\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\intel\\miniforge3\\envs\\npu\\Lib\\site-packages\\ipex_llm\\transformers\\npu_models\\npu_llm_cpp.py\", line 82, in run_prefill\n    plogits = _lib.run_prefill(model_ptr, input_ptr, input_len, repetition_penalty, False)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nOSError: exception: access violation writing 0x000001F559302000\nException ignored in: <function BaseNPUBackendWithPrefetch.__del__ at 0x000001F09D548360>\nTraceback (most recent call last):\n  File \"C:\\Users\\intel\\miniforge3\\envs\\npu\\Lib\\site-packages\\intel_npu_acceleration_library\\backend\\base.py\", line 245, in __del__\n    super(BaseNPUBackendWithPrefetch, self).__del__()\n  File \"C:\\Users\\intel\\miniforge3\\envs\\npu\\Lib\\site-packages\\intel_npu_acceleration_library\\backend\\base.py\", line 54, in __del__\n    backend_lib.destroyNNFactory(self._mm)\nOSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF\nException ignored in: <function BaseNPUBackendWithPrefetch.__del__ at 0x000001F09D548360>\nTraceback (most recent call last):\n  File \"C:\\Users\\intel\\miniforge3\\envs\\npu\\Lib\\site-packages\\intel_npu_acceleration_library\\backend\\base.py\", line 245, in __del__\n    super(BaseNPUBackendWithPrefetch, self).__del__()\n  File \"C:\\Users\\intel\\miniforge3\\envs\\npu\\Lib\\site-packages\\intel_npu_acceleration_library\\backend\\base.py\", line 54, in __del__\n    backend_lib.destroyNNFactory(self._mm)\nOSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF",
      "state": "closed",
      "author": "Lucas-cai",
      "author_type": "User",
      "created_at": "2025-02-25T08:01:55Z",
      "updated_at": "2025-02-27T02:43:16Z",
      "closed_at": "2025-02-27T02:43:16Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12895/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "plusbang"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12895",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12895",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:29.791480",
      "comments": [
        {
          "author": "plusbang",
          "body": "Hi, @Lucas-cai , could you please provide more information about hardware (which CPU model), NPU driver version, ipex-llm version and the whole config yaml?",
          "created_at": "2025-02-26T02:12:37Z"
        },
        {
          "author": "plusbang",
          "body": "As we discussed offline, the error is caused by in-out pairs setting. If **only set one in-out pair**, benchmark works okay on NPU.",
          "created_at": "2025-02-26T08:36:55Z"
        }
      ]
    },
    {
      "issue_number": 12888,
      "title": "GGML_SYCL_FORCE_MMQ and SYCL_USE_XMX",
      "body": "I'm running the ipex-llm provide ollama on intel core ultra 7. when input ollama list I got below information in the Powershell\n\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\nggml_sycl_init: SYCL_USE_XMX: yes\nggml_sycl_init: found 1 SYCL devices:\n\nI'm curious that what is GGML_SYCL_FORCE_MMQ it looks like a similar accelerating tech as SYCL_USE_XMX. \nCan we set the GGML_SYCL_FORCE_MMQ: yes? \nI read other pages console output log, mostly are set GGML_SYCL_FORCE_MMQ: no . Is there any reason not enable this feature ?",
      "state": "open",
      "author": "Finalstar82",
      "author_type": "User",
      "created_at": "2025-02-24T12:46:58Z",
      "updated_at": "2025-02-26T00:58:18Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12888/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12888",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12888",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:30.042005",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "MMQ is not ready yet. The value is set when we build ipex-llm, you can't change the value. We will enable MMQ when it's ready.",
          "created_at": "2025-02-26T00:58:17Z"
        }
      ]
    },
    {
      "issue_number": 12872,
      "title": "llama.cpp server UR_RESULT_ERROR_OUT_OF_RESOURCES error",
      "body": "Hi, with latest intelanalytics/ipex-llm-inference-cpp-xpu:latest image I got this error : \n\nUR backend failed. UR backend returns:40 (UR_RESULT_ERROR_OUT_OF_RESOURCES)Exception caught at file:/home/runner/_work/llm.cpp/llm.cpp/llama-cpp-bigdl/ggml/src/ggml-sycl/ggml-sycl.cpp, line:2819\n\nI use this commad line : ./llama-server -m  Mistral-Small-24B-Instruct-2501-IQ4_XS.gguf -c 2048 -ngl 99  --temp 0   --port 1234 --host 192.168.1.64\n\nFull output : \n\n```\nbuild: 1 (e66308a) with Intel(R) oneAPI DPC++/C++ Compiler 2025.0.4 (2025.0.4.20241205) for x86_64-unknown-linux-gnu\nsystem info: n_threads = 8, n_threads_batch = 8, total_threads = 16\n\nsystem_info: n_threads = 8 (n_threads_batch = 8) / 16 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 |\n\nmain: HTTP server is listening, hostname: 192.168.1.64, port: 1234, http threads: 15\nmain: loading model\nsrv    load_model: loading model '../Mistral-Small-24B-Instruct-2501-IQ4_XS.gguf'\nllama_load_model_from_file: using device SYCL0 (Intel(R) Arc(TM) A770 Graphics) - 15473 MiB free\nllama_model_loader: loaded meta data with 44 key-value pairs and 363 tensors from ../Mistral-Small-24B-Instruct-2501-IQ4_XS.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Mistral Small 24B Instruct 2501\nllama_model_loader: - kv   3:                            general.version str              = 2501\nllama_model_loader: - kv   4:                           general.finetune str              = Instruct\nllama_model_loader: - kv   5:                           general.basename str              = Mistral-Small\nllama_model_loader: - kv   6:                         general.size_label str              = 24B\nllama_model_loader: - kv   7:                            general.license str              = apache-2.0\nllama_model_loader: - kv   8:                   general.base_model.count u32              = 1\nllama_model_loader: - kv   9:                  general.base_model.0.name str              = Mistral Small 24B Base 2501\nllama_model_loader: - kv  10:               general.base_model.0.version str              = 2501\nllama_model_loader: - kv  11:          general.base_model.0.organization str              = Mistralai\nllama_model_loader: - kv  12:              general.base_model.0.repo_url str              = https://huggingface.co/mistralai/Mist...\nllama_model_loader: - kv  13:                          general.languages arr[str,10]      = [\"en\", \"fr\", \"de\", \"es\", \"it\", \"pt\", ...\nllama_model_loader: - kv  14:                          llama.block_count u32              = 40\nllama_model_loader: - kv  15:                       llama.context_length u32              = 32768\nllama_model_loader: - kv  16:                     llama.embedding_length u32              = 5120\nllama_model_loader: - kv  17:                  llama.feed_forward_length u32              = 32768\nllama_model_loader: - kv  18:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  19:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  20:                       llama.rope.freq_base f32              = 100000000.000000\nllama_model_loader: - kv  21:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  22:                 llama.attention.key_length u32              = 128\nllama_model_loader: - kv  23:               llama.attention.value_length u32              = 128\nllama_model_loader: - kv  24:                           llama.vocab_size u32              = 131072\nllama_model_loader: - kv  25:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = tekken\nllama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,131072]  = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\nllama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,131072]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,269443]  = [\"Ġ Ġ\", \"Ġ t\", \"e r\", \"i n\", \"Ġ �...\nllama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  33:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  36:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\nllama_model_loader: - kv  37:            tokenizer.ggml.add_space_prefix bool             = false\nllama_model_loader: - kv  38:               general.quantization_version u32              = 2\nllama_model_loader: - kv  39:                          general.file_type u32              = 30\nllama_model_loader: - kv  40:                      quantize.imatrix.file str              = /models_out/Mistral-Small-24B-Instruc...\nllama_model_loader: - kv  41:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\nllama_model_loader: - kv  42:             quantize.imatrix.entries_count i32              = 280\nllama_model_loader: - kv  43:              quantize.imatrix.chunks_count i32              = 128\nllama_model_loader: - type  f32:   81 tensors\nllama_model_loader: - type q5_K:   40 tensors\nllama_model_loader: - type q6_K:    1 tensors\nllama_model_loader: - type iq4_xs:  241 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 1000\nllm_load_vocab: token to piece cache size = 0.8498 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 131072\nllm_load_print_meta: n_merges         = 269443\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 32768\nllm_load_print_meta: n_embd           = 5120\nllm_load_print_meta: n_layer          = 40\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 32768\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 100000000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 32768\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 13B\nllm_load_print_meta: model ftype      = IQ4_XS - 4.25 bpw\nllm_load_print_meta: model params     = 23.57 B\nllm_load_print_meta: model size       = 11.88 GiB (4.33 BPW)\nllm_load_print_meta: general.name     = Mistral Small 24B Instruct 2501\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 1196 'Ä'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 150\nllm_load_tensors: offloading 40 repeating layers to GPU\nllm_load_tensors: offloading output layer to GPU\nllm_load_tensors: offloaded 41/41 layers to GPU\nllm_load_tensors:   CPU_Mapped model buffer size =   340.00 MiB\nllm_load_tensors:        SYCL0 model buffer size = 11820.33 MiB\n...............................................................................................\nllama_new_context_with_model: n_seq_max     = 1\nllama_new_context_with_model: n_ctx         = 2048\nllama_new_context_with_model: n_ctx_per_seq = 2048\nllama_new_context_with_model: n_batch       = 2048\nllama_new_context_with_model: n_ubatch      = 2048\nllama_new_context_with_model: flash_attn    = 0\nllama_new_context_with_model: freq_base     = 100000000.0\nllama_new_context_with_model: freq_scale    = 1\nllama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n[SYCL] call ggml_check_sycl\nggml_check_sycl: GGML_SYCL_DEBUG: 0\nggml_check_sycl: GGML_SYCL_F16: no\nFound 1 SYCL devices:\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\n| 0| [level_zero:gpu:0]|                Intel Arc A770 Graphics|  12.55|    512|    1024|   32| 16225M|     1.6.32224.500000|\nllama_kv_cache_init:      SYCL0 KV buffer size =   320.00 MiB\nllama_new_context_with_model: KV self size  =  320.00 MiB, K (f16):  160.00 MiB, V (f16):  160.00 MiB\nllama_new_context_with_model:  SYCL_Host  output buffer size =     0.50 MiB\nllama_new_context_with_model:      SYCL0 compute buffer size =  1064.00 MiB\nllama_new_context_with_model:  SYCL_Host compute buffer size =    56.02 MiB\nllama_new_context_with_model: graph nodes  = 1126\nllama_new_context_with_model: graph splits = 2\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n\nShaderDumpEnable Warning! BufferVec[] has 25752 elements. Including first 1000 items in ShaderDumps. To print all elements set IGC_ShowFullVectorsInShaderDumps register flag to True. ShaderOverride flag may not work properly without IGC_ShowFullVectorsInShaderDumps enabled.\n\nsrv          init: initializing slots, n_slots = 1\nslot         init: id  0 | task -1 | new slot n_ctx_slot = 2048\nmain: model loaded\nmain: chat template, built_in: 1, chat_example: '[SYSTEM_PROMPT] You are a helpful assistant[/SYSTEM_PROMPT][INST] Hello[/INST] Hi there</s>[INST] How are you?[/INST]'\nmain: server is listening on http://192.168.1.64:1234 - starting the main loop\nsrv  update_slots: all slots are idle\nslot launch_slot_: id  0 | task 0 | processing task\nslot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 2048, n_keep = 0, n_prompt_tokens = 1042\nslot update_slots: id  0 | task 0 | kv cache rm [0, end)\nslot update_slots: id  0 | task 0 | prompt processing progress, n_past = 1042, n_tokens = 1042, progress = 1.000000\nslot update_slots: id  0 | task 0 | prompt done, n_past = 1042, n_tokens = 1042\nUR backend failed. UR backend returns:40 (UR_RESULT_ERROR_OUT_OF_RESOURCES)Exception caught at file:/home/runner/_work/llm.cpp/llm.cpp/llama-cpp-bigdl/ggml/src/ggml-sycl/ggml-sycl.cpp, line:2819\n```",
      "state": "open",
      "author": "easyfab",
      "author_type": "User",
      "created_at": "2025-02-22T15:57:41Z",
      "updated_at": "2025-02-24T18:12:22Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12872/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12872",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12872",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:30.248668",
      "comments": [
        {
          "author": "san-nos",
          "body": "I am also experiencing this `UR backend returns:40` error specifically with nomic_embed_text. Right now, my workaround for this is to use this command\n\n`set OLLAMA_NUM_GPU=11`\n\n(note: nomic_embed_text has a total of 13 layers)\nAny value over this will result is the error but using this command will ",
          "created_at": "2025-02-23T18:58:01Z"
        },
        {
          "author": "qiuxin2012",
          "body": "Yes, A770 only have 16GB RAM, 24B model is too big for A770, so you will get `ERROR_OUT_OF_RESOURCES`.  \nWe can change our docs to notice users. ",
          "created_at": "2025-02-24T00:48:42Z"
        },
        {
          "author": "easyfab",
          "body": "@qiuxin2012  I'm not sure it's memory related.\nIt's a 24B model but quantized : model size  = 11820.33 MiB  + context 1064.00 MiB . Should be ok for 16GB RAM.\nAnd it worked with older version.\n\nAfter @san-nos  comment, I tried some options and with  --batch-size <= 1024 it works . \nIs the 2048 defau",
          "created_at": "2025-02-24T18:12:20Z"
        }
      ]
    },
    {
      "issue_number": 10941,
      "title": "IPEX-LLM on Intel Max Series 1100 for inference libintel-ext-pt-gpu.so: undefined symbol: _ZNK5torch8autograd4Node4nameB5cxx11Ev",
      "body": "I am trying to run Synthesizing speech by TTS:\r\n\r\nhttps://docs.coqui.ai/en/latest/\r\n\r\n(llm) spandey2@imu-nex-sprx92-max1-sut:~/1worldsync_finetuning$ cat tts.py\r\n```\r\nimport torch\r\nfrom TTS.api import TTS\r\nfrom ipex_llm import optimize_model\r\n\r\n#Get device\r\ndevice = 'xpu'  # Use 'xpu' to indicate Intel GPU\r\n\r\n#List availableTTS models\r\nprint(TTS().list_models())\r\n\r\n# Init TTS\r\ntts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\")\r\n\r\ntts = optimize_model(tts, cpu_embedding=True)\r\ntts = tts.to(device)\r\n\r\nwav = tts.tts(text=\"Hello world!\", speaker_wav=\"./audio.wav\", language=\"en\").to('cpu')  # Move output to CPU if needed\r\ntts.tts_to_file(text=\"Hello world!\", speaker_wav=\"./audio.wav\", language=\"en\", file_path=\"output.wav\")\r\n```\r\n\r\n**ENV Set:**\r\nsource /opt/intel/oneapi/setvars.sh\r\nexport LD_PRELOAD=${LD_PRELOAD}:${CONDA_PREFIX}/lib/libtcmalloc.so\r\nexport SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1\r\nexport SYCL_CACHE_PERSISTENT=1\r\nexport ENABLE_SDP_FUSION=1\r\n\r\n**Error :**\r\n```\r\nDependency issue as well: \r\npip install TTS  # from PyPI\r\npip install --pre --upgrade ipex-llm[xpu] --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/\r\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\ntts 0.22.0 requires numpy==1.22.0; python_version <= \"3.10\", but you have numpy 1.26.4 which is incompatible.\r\ntts 0.22.0 requires torch>=2.1, but you have torch 2.1.0a0+cxx11.abi which is incompatible.\r\ntts 0.22.0 requires transformers>=4.33.0, but you have transformers 4.31.0 which is incompatible.\r\ntorchaudio 2.3.0 requires torch==2.3.0, but you have torch 2.1.0a0+cxx11.abi which is incompatible.\r\nSuccessfully installed intel-extension-for-pytorch-2.1.10+xpu numpy-1.26.4 tokenizers-0.13.3 torch-2.1.0a0+cxx11.abi torchvision-0.16.0a0+cxx11.abi transformers-4.31.0\r\n\r\n\r\n(llm) spandey2@imu-nex-sprx92-max1-sut:~/1worldsync_finetuning$ python tts.py\r\nERROR: ld.so: object '/home/spandey2/miniconda3/envs/llm/lib/libtcmalloc.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\r\nThe installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\r\n/home/spandey2/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\r\n  _torch_pytree._register_pytree_node(\r\n/home/spandey2/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\r\n  _torch_pytree._register_pytree_node(\r\n/home/spandey2/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\r\n  _torch_pytree._register_pytree_node(\r\n/home/spandey2/miniconda3/envs/llm/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n  warn(\r\nTraceback (most recent call last):\r\n  File \"/home/spandey2/1worldsync_finetuning/tts.py\", line 3, in <module>\r\n    from ipex_llm import optimize_model\r\n  File \"/home/spandey2/miniconda3/envs/llm/lib/python3.11/site-packages/ipex_llm/__init__.py\", line 34, in <module>\r\n    ipex_importer.import_ipex()\r\n  File \"/home/spandey2/miniconda3/envs/llm/lib/python3.11/site-packages/ipex_llm/utils/ipex_importer.py\", line 59, in import_ipex\r\n    import intel_extension_for_pytorch as ipex\r\n  File \"/home/spandey2/miniconda3/envs/llm/lib/python3.11/site-packages/intel_extension_for_pytorch/__init__.py\", line 94, in <module>\r\n    from .utils._proxy_module import *\r\n  File \"/home/spandey2/miniconda3/envs/llm/lib/python3.11/site-packages/intel_extension_for_pytorch/utils/_proxy_module.py\", line 2, in <module>\r\n    import intel_extension_for_pytorch._C\r\nImportError: /home/spandey2/miniconda3/envs/llm/lib/python3.11/site-packages/intel_extension_for_pytorch/lib/libintel-ext-pt-gpu.so: undefined symbol: _ZNK5torch8autograd4Node4nameB5cxx11Ev\r\n```\r\n\r\n",
      "state": "open",
      "author": "shailesh837",
      "author_type": "User",
      "created_at": "2024-05-06T20:29:47Z",
      "updated_at": "2025-02-24T13:52:16Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/10941/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/10941",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/10941",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:30.426636",
      "comments": [
        {
          "author": "qiyuangong",
          "body": "Seems related to this issue. https://github.com/intel/intel-extension-for-pytorch/issues/317\r\n\r\n@gc-fu Can we solve this issue in conda env?",
          "created_at": "2024-05-07T00:16:57Z"
        },
        {
          "author": "gc-fu",
          "body": "The issue https://github.com/intel/intel-extension-for-pytorch/issues/317 was caused by self-built torch with pre-built intel-extension-for-pytorch.\r\n\r\nI have tried install these two dependencies in conda and then `import intel_extension_for_pytorch as ipex`.  Everything works fine.\r\n\r\nCan you try t",
          "created_at": "2024-05-07T01:02:20Z"
        },
        {
          "author": "qiyuangong",
          "body": "Hi @shailesh837 \r\n\r\nCan you share your OS and kernel version? This error is raised by intel-extension-for-pytorch. In most cases, it's caused by OS or glibc out of date.",
          "created_at": "2024-05-07T02:50:06Z"
        },
        {
          "author": "shailesh837",
          "body": "I have ubuntu 22.04 LTS and Linux imu-nex-nuc13x2-arc770-dut **6.5.0-26-generic** .\r\n\r\n@gc-fu : Did you managed to run the code :\r\nimport torch\r\nfrom TTS.api import TTS\r\nfrom ipex_llm import optimize_model\r\n\r\n#Get device\r\ndevice = 'xpu'  # Use 'xpu' to indicate Intel GPU\r\n\r\n#List availableTTS models",
          "created_at": "2024-05-07T08:27:04Z"
        },
        {
          "author": "gc-fu",
          "body": "After some investigation, the issue was caused not installing the correct torchaudio version.\r\nInstall using these instructions:\r\n```bash\r\npip install TTS\r\npip install --pre --upgrade ipex-llm[xpu] --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/\r\n# Then pip install t",
          "created_at": "2024-05-07T09:29:55Z"
        }
      ]
    },
    {
      "issue_number": 12856,
      "title": "qwen2-vl, A770 GPU,  got an unexpected keyword argument 'position_embeddings' error",
      "body": "qwen2-vl, A770 GPU,  got an unexpected keyword argument 'position_embeddings' error\n\n![Image](https://github.com/user-attachments/assets/d74535eb-607f-4aaf-9dbd-f3ef2692a690)",
      "state": "closed",
      "author": "aixiwangintel",
      "author_type": "User",
      "created_at": "2025-02-20T00:41:28Z",
      "updated_at": "2025-02-24T00:56:12Z",
      "closed_at": "2025-02-24T00:56:12Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12856/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12856",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12856",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:30.624798",
      "comments": [
        {
          "author": "aixiwangintel",
          "body": "prompt: describe the image\nsample image (from internet):\n\n![Image](https://github.com/user-attachments/assets/b2570948-2542-4398-932c-e3db8d1bc6d9)\n\n\n",
          "created_at": "2025-02-20T00:50:26Z"
        },
        {
          "author": "MeouSker77",
          "body": "it seems your transformers version is too high, try using transformers 4.45\n\n```\npip install transformers==4.45 trl==0.11\n```\n\nrefer to our example if you meet any other errors: <https://github.com/intel/ipex-llm/tree/main/python/llm/example/GPU/HuggingFace/Multimodal/qwen2-vl>",
          "created_at": "2025-02-20T03:20:45Z"
        },
        {
          "author": "aixiwangintel",
          "body": "pip install transformers==4.45 trl==0.11\n\nIt works. thanks\n",
          "created_at": "2025-02-24T00:56:07Z"
        }
      ]
    },
    {
      "issue_number": 12859,
      "title": "Model just prints the same amount of \"G\"s",
      "body": "using the ollama-intel-gpu container https://github.com/mattcurf/ollama-intel-gpu/\n\npodman build -t \"ollama-intel-gpu\" \n\npodman run --rm -p 127.0.0.1:11434:11434 -v /home/stereomato/models:/mnt -v ollama-volume:/root/.ollama -e OLLAMA_NUM_PARALLEL=1 -e OLLAMA_MAX_LOADED_MODELS=1 -e OLLAMA_FLASH_ATTENTION=1 -e OLLAMA_NUM_GPU=999 -e DEVICE=iGPU --device /dev/dri --name=ollama-intel-gpu localhost/ollama-intel-gpu:latest\n\n./ollama pull hf.co/TheDrummer/Llama-3SOME-8B-v2-GGUF:Q4_K_M\n\n./ollama run hf.co/TheDrummer/Llama-3SOME-8B-v2-GGUF:Q4_K_M \"hello\" or anything else\n\nJust prints \"GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\"\n\nArch Linux as OS now.",
      "state": "closed",
      "author": "stereomato",
      "author_type": "User",
      "created_at": "2025-02-20T02:03:05Z",
      "updated_at": "2025-02-21T12:26:15Z",
      "closed_at": "2025-02-21T12:26:15Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12859/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12859",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12859",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:30.816610",
      "comments": [
        {
          "author": "Oscilloscope98",
          "body": "Hi @stereomato,\n\nWe have reproduced this issue and it will be fixed in today's nightly version: `ipex-llm[cpp]>=2.2.0b20250220`.\n\nFor now, you could just use `ipex-llm[cpp]==2.2.0b20250218` :)",
          "created_at": "2025-02-20T02:11:45Z"
        },
        {
          "author": "stereomato",
          "body": "Wow, thank you. Is there an ETA or something? I'm on a UTC -5 timezone.",
          "created_at": "2025-02-20T02:19:19Z"
        },
        {
          "author": "Oscilloscope98",
          "body": "Today's nightly version will be released after 21:00 GMT+8 :)",
          "created_at": "2025-02-20T02:40:49Z"
        },
        {
          "author": "stereomato",
          "body": "fixed",
          "created_at": "2025-02-21T12:26:06Z"
        }
      ]
    },
    {
      "issue_number": 12828,
      "title": "IGPU limits the inference speed of the entire system",
      "body": "System: U265K+48G ddr5 +B580\nENV: Run Ollama Portable Zip on Intel GPU with IPEX-LLM\n        GPU drive：6559\n\nQuestion:IGPU limits the inference speed of the entire system\n\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\n| 0| [level_zero:gpu:0]|                         Intel Graphics|  12.70|     64|    1024|   32| 26769M|            1.6.31441|\n| 1| [level_zero:gpu:1]|                Intel Arc B580 Graphics|   20.1|    160|    1024|   32| 12450M|            1.6.3\n\n1/ When i load deepseek-r1:7b , igpu loaded 4g, B580 loaded 3.2g, IGPU limits the inference speed of the entire system.\n\n2/When i load deepseek-r1:32b , igpu loaded 15.7g, B580 loaded 8.4g, CPU don't work.\n\n3/ When i shut off  igpu & load deepseek-r1:32b,B580 loaded 25g,  CPU don't work. The large model is stuck and cannot perform inference.",
      "state": "open",
      "author": "dttprofessor",
      "author_type": "User",
      "created_at": "2025-02-14T17:31:48Z",
      "updated_at": "2025-02-19T01:50:59Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12828/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12828",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12828",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:31.032075",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Set `ONEAPI_DEVICE_SELECTOR=\"level_zero:1\"` to enable B580 only could be helpful for the inference speed. But I don't think B580 has enough VRAM to load `deepseek-r1:32b`.",
          "created_at": "2025-02-17T01:38:43Z"
        },
        {
          "author": "dttprofessor",
          "body": "> Set `ONEAPI_DEVICE_SELECTOR=\"level_zero:1\"` to enable B580 only could be helpful for the inference speed. But I don't think B580 has enough VRAM to load `deepseek-r1:32b`.将 `ONEAPI_DEVICE_SELECTOR=\"level_zero:1\"` 设置为仅启用 B580 可能有助于推理速度。但我不认为 B580 有足够的 VRAM 来加载 `deepseek-r1:32b` 。\n\nIsn't SYCL the de",
          "created_at": "2025-02-17T13:46:27Z"
        },
        {
          "author": "sgwhat",
          "body": "> > Set `ONEAPI_DEVICE_SELECTOR=\"level_zero:1\"` to enable B580 only could be helpful for the inference speed. But I don't think B580 has enough VRAM to load `deepseek-r1:32b`.将 `ONEAPI_DEVICE_SELECTOR=\"level_zero:1\"` 设置为仅启用 B580 可能有助于推理速度。但我不认为 B580 有足够的 VRAM 来加载 `deepseek-r1:32b` 。\n> \n> Isn't SYCL ",
          "created_at": "2025-02-18T02:15:44Z"
        },
        {
          "author": "dttprofessor",
          "body": "> > > Set `ONEAPI_DEVICE_SELECTOR=\"level_zero:1\"` to enable B580 only could be helpful for the inference speed. But I don't think B580 has enough VRAM to load `deepseek-r1:32b`.将 `ONEAPI_DEVICE_SELECTOR=\"level_zero:1\"` 设置为仅启用 B580 可能有助于推理速度。但我不认为 B580 有足够的 VRAM 来加载 `deepseek-r1:32b` 。将 `ONEAPI_DEVIC",
          "created_at": "2025-02-18T08:02:07Z"
        },
        {
          "author": "sgwhat",
          "body": "The VRAM of the B580 is insufficient to load a 32B model. You can continue running the model using the iGPU + B580.",
          "created_at": "2025-02-19T01:50:58Z"
        }
      ]
    },
    {
      "issue_number": 12827,
      "title": "deepseek-v3 needs ollama 0.5.5",
      "body": "Can you guys upgrade the ollama version?",
      "state": "open",
      "author": "stereomato",
      "author_type": "User",
      "created_at": "2025-02-14T17:22:40Z",
      "updated_at": "2025-02-18T02:07:28Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12827/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12827",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12827",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:31.214123",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @stereomato, we have already added support for deepseek-v3 in the current version.",
          "created_at": "2025-02-17T01:57:03Z"
        }
      ]
    },
    {
      "issue_number": 12830,
      "title": "Lunar Lake Core Ultra 7 258V running docker container : intelanalytics/ipex-llm-inference-cpp-xpu:latest  on Windows 11 , can not find preferred GPU platform fails",
      "body": "![Image](https://github.com/user-attachments/assets/e7f985c5-5260-44a1-83c8-cf1895dceeb5)\nI am trying to run ipex-llm on iGPU of LunarLake system having Windows11 with Docker Desktop.\nmy compose.yaml file on windows looks like:\n\n ollama:\n    image: intelanalytics/ipex-llm-inference-cpp-xpu:latest\n    container_name: ollama\n    restart: unless-stopped\n    devices:\n      - /dev/dri:/dev/dri\n    ports:\n      # localhost only, protected from outside\n      #- 172.17.0.1:11434:11434\n      # expose port to outside, useful for developer and remote connections\n      - 11434:11434\n    environment:\n      - no_proxy=localhost,127.0.0.1,172.28.240.81\n      - DEVICE=iGPU\n      - OLLAMA_HOST=0.0.0.0\n      - OLLAMA_MODEL=${OLLAMA_MODEL}\n    mem_limit: 32G\n    shm_size: 16G\n    working_dir: /llm/scripts\n    volumes:\n      - ollama:/root/.ollama\n      - ./startup.sh:/startup.sh\n      #pull_policy: always\n    tty: true\n    entrypoint: [\"/bin/bash\", \"-c\"]\n    command: /startup.sh\n\nStartup.sh:\n======\n`#!/bin/bash\n\ncd /llm/scripts/\nsource ipex-llm-init --gpu --device iGPU\nbash start-ollama.sh\n\n# Wait for initialization\n#sleep 20\n\n# Keep container running\ntail -f /dev/null`\n\nError Logs:\n======\n`2025-02-14 18:21:17 found oneapi in /opt/intel/oneapi/setvars.sh\n2025-02-14 18:21:18  \n2025-02-14 18:21:18 :: initializing oneAPI environment ...\n2025-02-14 18:21:18    startup.sh: BASH_VERSION = 5.1.16(1)-release\n2025-02-14 18:21:18    args: Using \"$@\" for setvars.sh arguments: --force\n2025-02-14 18:21:18 :: advisor -- latest\n2025-02-14 18:21:18 :: ccl -- latest\n2025-02-14 18:21:18 :: compiler -- latest\n2025-02-14 18:21:18 :: dal -- latest\n2025-02-14 18:21:18 :: debugger -- latest\n2025-02-14 18:21:18 :: dev-utilities -- latest\n2025-02-14 18:21:18 :: dnnl -- latest\n2025-02-14 18:21:18 :: dpcpp-ct -- latest\n2025-02-14 18:21:18 :: dpl -- latest\n2025-02-14 18:21:18 :: ipp -- latest\n2025-02-14 18:21:18 :: ippcp -- latest\n2025-02-14 18:21:18 :: mkl -- latest\n2025-02-14 18:21:18 :: mpi -- latest\n2025-02-14 18:21:18 :: pti -- latest\n2025-02-14 18:21:18 :: tbb -- latest\n2025-02-14 18:21:18 :: umf -- latest\n2025-02-14 18:21:18 :: vtune -- latest\n2025-02-14 18:21:18 :: oneAPI environment initialized ::\n2025-02-14 18:21:18  \n2025-02-14 18:21:24 /usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n2025-02-14 18:21:24   _torch_pytree._register_pytree_node(\n2025-02-14 18:21:24 /usr/local/lib/python3.11/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n2025-02-14 18:21:24   warnings.warn(\n2025-02-14 18:21:24 /usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n2025-02-14 18:21:24   _torch_pytree._register_pytree_node(\n2025-02-14 18:21:25 /usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n2025-02-14 18:21:25   _torch_pytree._register_pytree_node(\n2025-02-14 18:21:25 +++++ Env Variables +++++\n2025-02-14 18:21:25 Internal:\n2025-02-14 18:21:25     ENABLE_IOMP     = 1\n2025-02-14 18:21:25     ENABLE_GPU      = 1\n2025-02-14 18:21:25     ENABLE_JEMALLOC = 0\n2025-02-14 18:21:25     ENABLE_TCMALLOC = 0\n2025-02-14 18:21:25     LIB_DIR    = /usr/local/lib\n2025-02-14 18:21:25     BIN_DIR    = bin64\n2025-02-14 18:21:25     LLM_DIR    = /usr/local/lib/python3.11/dist-packages/ipex_llm\n2025-02-14 18:21:25 \n2025-02-14 18:21:25 Exported:\n2025-02-14 18:21:25     LD_PRELOAD             = \n2025-02-14 18:21:25     OMP_NUM_THREADS        = \n2025-02-14 18:21:25     MALLOC_CONF            = \n2025-02-14 18:21:25     USE_XETLA              = \n2025-02-14 18:21:25     ENABLE_SDP_FUSION      = \n2025-02-14 18:21:25     SYCL_CACHE_PERSISTENT  = 1\n2025-02-14 18:21:25     BIGDL_LLM_XMX_DISABLED = 1\n2025-02-14 18:21:25     SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS = \n2025-02-14 18:21:25 +++++++++++++++++++++++++\n2025-02-14 18:21:25 Complete.\n2025-02-14 18:21:27 terminate called after throwing an instance of 'std::runtime_error'\n2025-02-14 18:21:27   what():  can not find preferred GPU platform\n2025-02-14 18:21:27 SIGABRT: abort\n2025-02-14 18:21:27 PC=0x7fbaf60419fc m=0 sigcode=18446744073709551610\n2025-02-14 18:21:27 signal arrived during cgo execution\n2025-02-14 18:21:27 \n2025-02-14 18:21:27 goroutine 1 gp=0xc0000061c0 m=0 mp=0x561f0d3a1480 [syscall]:\n2025-02-14 18:21:27 runtime.cgocall(0x561f0c6587b0, 0xc00025f960)\n2025-02-14 18:21:27     runtime/cgocall.go:167 +0x4b fp=0xc00025f938 sp=0xc00025f900 pc=0x561f0bab752b\n2025-02-14 18:21:27 ollama/llama/llamafile._Cfunc_llama_print_system_info()\n2025-02-14 18:21:27     _cgo_gotypes.go:838 +0x4c fp=0xc00025f960 sp=0xc00025f938 pc=0x561f0be7a9cc\n2025-02-14 18:21:27 ollama/llama/llamafile.PrintSystemInfo()\n2025-02-14 18:21:27     ollama/llama/llamafile/llama.go:70 +0x79 fp=0xc00025f9a8 sp=0xc00025f960 pc=0x561f0be7ba59\n2025-02-14 18:21:27 ollama/cmd.NewCLI()\n2025-02-14 18:21:27     ollama/cmd/cmd.go:1427 +0xc08 fp=0xc00025ff30 sp=0xc00025f9a8 pc=0x561f0c650d68\n2025-02-14 18:21:27 main.main()\n2025-02-14 18:21:27     ollama/main.go:12 +0x13 fp=0xc00025ff50 sp=0xc00025ff30 pc=0x561f0c657bf3\n2025-02-14 18:21:27 runtime.main()\n2025-02-14 18:21:27     runtime/proc.go:272 +0x29d fp=0xc00025ffe0 sp=0xc00025ff50 pc=0x561f0ba88f3d\n2025-02-14 18:21:27 runtime.goexit({})\n2025-02-14 18:21:27     runtime/asm_amd64.s:1700 +0x1 fp=0xc00025ffe8 sp=0xc00025ffe0 pc=0x561f0bac6001`",
      "state": "open",
      "author": "shailesh837",
      "author_type": "User",
      "created_at": "2025-02-14T22:08:41Z",
      "updated_at": "2025-02-18T02:05:31Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12830/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12830",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12830",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:31.445624",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "Is your driver installed correctly? I find `terminate called after throwing an instance of 'std::runtime_error' 2025-02-14 18:21:27   what():  can not find preferred GPU platform` in you log.\nSee https://github.com/intel/ipex-llm/blob/841845030024259441721516556d67476ffb71b6/docs/mddocs/Quickstart/i",
          "created_at": "2025-02-18T00:55:09Z"
        }
      ]
    },
    {
      "issue_number": 12824,
      "title": "Baichuan-M1-14B can save int4 model, but load low bit failed.",
      "body": "Baichuan-M1-14B can run on core ultra by\nhttps://github.com/intel/ipex-llm/issues/12810\n\n```\nfrom ipex_llm.transformers import AutoModelForCausalLM\nfrom transformers import AutoTokenizer\nimport torch\n# 1. Load pre-trained model and tokenizer\nmodel_name = \"./Baichuan-M1-14B-Instruct\"  \n\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype = torch.half, load_in_low_bit='sym_int4',trust_remote_code=True).eval()\ntokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True) ## \"sdpa\"   \"eager\"\n\nif 1:\n    model.save_low_bit(model_name +  \"-int4/\")\n    tokenizer.save_pretrained(model_name + \"-int4/\")\n\nmodel = model.to('xpu')\n```\n\nIt can save INT4 model, then run\n\n`model = AutoModelForCausalLM.load_low_bit(model_name+\"-int4\", torch_dtype = torch.half,trust_remote_code=True).eval()`\n\nError\n\n```\nCould not locate the modeling_baichuan.py inside ./Baichuan-M1-14B-Instruct-int4.\nTraceback (most recent call last):\n  File \"C:\\Users\\Lengda\\Documents\\AIGC_LNL\\AIGC\\resources\\service\\models\\llm\\run_baichun-m1-14b.py\", line 10, in <module>\n    model = AutoModelForCausalLM.load_low_bit(model_name+\"-int4\", torch_dtype = torch.half,trust_remote_code=True).eval()\n  File \"C:\\ProgramData\\miniforge3\\envs\\llm-arl-2.6\\lib\\unittest\\mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"C:\\ProgramData\\miniforge3\\envs\\llm-arl-2.6\\lib\\site-packages\\ipex_llm\\transformers\\model.py\", line 630, in load_low_bit\n    model_class = get_class_from_dynamic_module(\n  File \"C:\\ProgramData\\miniforge3\\envs\\llm-arl-2.6\\lib\\site-packages\\transformers\\dynamic_module_utils.py\", line 540, in get_class_from_dynamic_module\n    final_module = get_cached_module_file(\n  File \"C:\\ProgramData\\miniforge3\\envs\\llm-arl-2.6\\lib\\site-packages\\transformers\\dynamic_module_utils.py\", line 344, in get_cached_module_file\n    resolved_module_file = cached_file(\n  File \"C:\\ProgramData\\miniforge3\\envs\\llm-arl-2.6\\lib\\site-packages\\transformers\\utils\\hub.py\", line 374, in cached_file\n    raise EnvironmentError(\nOSError: ./Baichuan-M1-14B-Instruct-int4 does not appear to have a file named modeling_baichuan.py. Checkout 'https://huggingface.co/./Baichuan-M1-14B-Instruct-int4/tree/None' for available files.\n```\nNext copy modeling_baichuan.py to ./Baichuan-M1-14B-Instruct-int4. New Error\n\n```\n2025-02-14 13:54:35,709 - INFO - Converting the current model to sym_int4 format......\nTraceback (most recent call last):\n  File \"C:\\Users\\Lengda\\Documents\\AIGC_LNL\\AIGC\\resources\\service\\models\\llm\\run_baichun-m1-14b.py\", line 10, in <module>\n    model = AutoModelForCausalLM.load_low_bit(model_name+\"-int4\", torch_dtype = torch.half,trust_remote_code=True).eval()\n  File \"C:\\ProgramData\\miniforge3\\envs\\llm-arl-2.6\\lib\\unittest\\mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"C:\\ProgramData\\miniforge3\\envs\\llm-arl-2.6\\lib\\site-packages\\ipex_llm\\transformers\\model.py\", line 700, in load_low_bit\n    model = ggml_convert_low_bit(model, qtype, optimize_model, device=quant_device,\n  File \"C:\\ProgramData\\miniforge3\\envs\\llm-arl-2.6\\lib\\site-packages\\ipex_llm\\transformers\\convert.py\", line 1147, in ggml_convert_low_bit\n    model = _optimize_post(model)\n  File \"C:\\ProgramData\\miniforge3\\envs\\llm-arl-2.6\\lib\\site-packages\\ipex_llm\\transformers\\convert.py\", line 1461, in _optimize_post\n    convert_forward(model, module.RMSNorm, rms_norm_forward)\nAttributeError: module 'transformers_modules.Baichuan-M1-14B-Instruct-int4.modeling_baichuan' has no attribute 'RMSNorm'\n\n```\n\n",
      "state": "open",
      "author": "KiwiHana",
      "author_type": "User",
      "created_at": "2025-02-14T05:55:18Z",
      "updated_at": "2025-02-17T02:36:45Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12824/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12824",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12824",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:33.601063",
      "comments": []
    },
    {
      "issue_number": 10033,
      "title": "TypeError when using TransformersEmbeddings",
      "body": "When I try to use bigdl TransformersEmbeddings, I encounter a wired problem\r\n**Code**\r\n```python\r\nself.embeddings = TransformersEmbeddings.from_model_id(model_id=f\"../checkpoints/{self.embed_version}\")\r\nself.embeddings.encode_kwargs = {\"truncation\": True, \"max_length\": 512, \"padding\": True}\r\nself.vectorstore_en = FAISS.from_texts(en_texts, self.embeddings, metadatas=[{\"video_clip\": str(i)} for i in range(len(en_texts))])\r\n```\r\n**Output**\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"/data/home/chengruilai/anaconda3/envs/vchat/lib/python3.9/site-packages/gradio/queueing.py\", line 407, in call_prediction\r\n    output = await route_utils.call_process_api(\r\n  File \"/data/home/chengruilai/anaconda3/envs/vchat/lib/python3.9/site-packages/gradio/route_utils.py\", line 226, in call_process_api\r\n    output = await app.get_blocks().process_api(\r\n  File \"/data/home/chengruilai/anaconda3/envs/vchat/lib/python3.9/site-packages/gradio/blocks.py\", line 1550, in process_api\r\n    result = await self.call_function(\r\n  File \"/data/home/chengruilai/anaconda3/envs/vchat/lib/python3.9/site-packages/gradio/blocks.py\", line 1185, in call_function\r\n    prediction = await anyio.to_thread.run_sync(\r\n  File \"/data/home/chengruilai/anaconda3/envs/vchat/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\r\n    return await get_async_backend().run_sync_in_worker_thread(\r\n  File \"/data/home/chengruilai/anaconda3/envs/vchat/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"/data/home/chengruilai/anaconda3/envs/vchat/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\r\n    result = context.run(func, *args)\r\n  File \"/data/home/chengruilai/anaconda3/envs/vchat/lib/python3.9/site-packages/gradio/utils.py\", line 661, in wrapper\r\n    response = f(*args, **kwargs)\r\n  File \"/data/home/chengruilai/projects/VChat-BigDL/main_gradio.py\", line 82, in log_fn\r\n    global_en_log_result = vchat.video2log(vid_path)\r\n  File \"/data/home/chengruilai/projects/VChat-BigDL/models/vchat_bigdl.py\", line 64, in video2log\r\n    self.llm_reasoner.create_qa_chain(en_log_result)\r\n  File \"/data/home/chengruilai/projects/VChat-BigDL/models/llm_model.py\", line 101, in create_qa_chain\r\n    self.vectorstore_en = FAISS.from_texts(en_texts, self.embeddings, metadatas=[{\"video_clip\": str(i)} for i in range(len(en_texts))])\r\n  File \"/data/home/chengruilai/anaconda3/envs/vchat/lib/python3.9/site-packages/langchain/vectorstores/faiss.py\", line 577, in from_texts\r\n    embeddings = embedding.embed_documents(texts)\r\n  File \"/data/home/chengruilai/anaconda3/envs/vchat/lib/python3.9/site-packages/bigdl/llm/langchain/embeddings/transformersembeddings.py\", line 163, in embed_documents\r\n    embeddings = [self.embed(text, **self.encode_kwargs).tolist() for text in texts]\r\n  File \"/data/home/chengruilai/anaconda3/envs/vchat/lib/python3.9/site-packages/bigdl/llm/langchain/embeddings/transformersembeddings.py\", line 163, in <listcomp>\r\n    embeddings = [self.embed(text, **self.encode_kwargs).tolist() for text in texts]\r\nTypeError: embed() got an unexpected keyword argument 'truncation'\r\n```\r\n**Environment**\r\nbigdl-llm 2.4.0\r\n\r\nI think I find the root of this problem.\r\nWhen I change [embed function](https://github.com/intel-analytics/BigDL/blob/d09698d1a4c76460b95d3f7c3ffda731907f9c4b/python/llm/src/bigdl/llm/langchain/embeddings/transformersembeddings.py#L138)  from\r\n```python\r\n  def embed(self, text: str):\r\n      \"\"\"Compute doc embeddings using a HuggingFace transformer model.\r\n\r\n      Args:\r\n          texts: The list of texts to embed.\r\n\r\n      Returns:\r\n          List of embeddings, one for each text.\r\n      \"\"\"\r\n      input_ids = self.tokenizer.encode(text, return_tensors=\"pt\")  # shape: [1, T]\r\n      embeddings = self.model(input_ids, return_dict=False)[0]  # shape: [1, T, N]\r\n      embeddings = embeddings.squeeze(0).detach().numpy()\r\n      embeddings = np.mean(embeddings, axis=0)\r\n      return embeddings\r\n```\r\ninto\r\n```python\r\n  def embed(self, text: str, **kwargs):\r\n      \"\"\"Compute doc embeddings using a HuggingFace transformer model.\r\n\r\n      Args:\r\n          texts: The list of texts to embed.\r\n\r\n      Returns:\r\n          List of embeddings, one for each text.\r\n      \"\"\"\r\n      input_ids = self.tokenizer.encode(text, return_tensors=\"pt\", **kwargs)  # shape: [1, T]\r\n      embeddings = self.model(input_ids, return_dict=False)[0]  # shape: [1, T, N]\r\n      embeddings = embeddings.squeeze(0).detach().numpy()\r\n      embeddings = np.mean(embeddings, axis=0)\r\n      return embeddings\r\n```\r\nI solve this problem. It seems that the parameter \"self.encode_kwargs\" in https://github.com/intel-analytics/BigDL/blob/d09698d1a4c76460b95d3f7c3ffda731907f9c4b/python/llm/src/bigdl/llm/langchain/embeddings/transformersembeddings.py#L176 doesn't enter the \"embed\" function correctly. Can someone fix this bug?",
      "state": "closed",
      "author": "Kailuo-Lai",
      "author_type": "User",
      "created_at": "2024-01-30T03:25:21Z",
      "updated_at": "2025-02-14T12:49:38Z",
      "closed_at": "2025-02-14T12:49:38Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/10033/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/10033",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/10033",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:33.601088",
      "comments": [
        {
          "author": "shane-huang",
          "body": "Thanks for reporting. We'll look into it. ",
          "created_at": "2024-01-30T05:56:18Z"
        },
        {
          "author": "shane-huang",
          "body": "We are fixing this in PR: https://github.com/intel-analytics/BigDL/pull/10051\r\n",
          "created_at": "2024-02-01T02:36:53Z"
        },
        {
          "author": "plusbang",
          "body": "> We are fixing this in PR: #10051\r\n\r\nThis bug is fixed, and please use the nightly version of tomorrow and later. Thanks for reporting this issue @Kailuo-Lai !",
          "created_at": "2024-02-05T02:46:18Z"
        }
      ]
    },
    {
      "issue_number": 8275,
      "title": "Weights not saved in HDFS - hdfs: command not found",
      "body": "Running a BigDL spark-submit on K8s cluster mode. Everything works fine including model training.\r\n\r\nAll application scripts and files load successfully from HDFS.\r\n \r\nSave model weights with the following code\r\n“””””\r\n`est.save_weights(filepath='hdfs://some/path/on_hdfs/model_weight',  save_format=\"tf\")`\r\n“””””\r\n \r\nbut no model weights found on that particular directory. Log is as below during the saving process \r\n \r\n`2023-06-06 07:57:36.633908: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-06-06 07:57:36.651391: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n\r\n/bin/sh: hdfs: command not found`",
      "state": "closed",
      "author": "ottermegazord",
      "author_type": "User",
      "created_at": "2023-06-06T08:23:27Z",
      "updated_at": "2025-02-14T12:49:25Z",
      "closed_at": "2025-02-14T12:49:25Z",
      "labels": [
        "orca",
        "user issue"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/8275/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hkvision",
        "lalalapotter"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/8275",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/8275",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:33.865755",
      "comments": [
        {
          "author": "lalalapotter",
          "body": "Hi @ottermegazord ,\r\n\r\nYou can replace `est.save_weights(filepath='hdfs://some/path/on_hdfs/model_weight',  save_format=\"tf\")` with following code as a workaround:\r\n\r\n```python\r\nimport os, shutil, tempfile\r\nfrom bigdl.dllib.utils.file_utils import put_local_files_with_prefix_to_remote\r\nfilepath = \"h",
          "created_at": "2023-06-08T10:34:59Z"
        },
        {
          "author": "hkvision",
          "body": "The issue is caused by executing the hdfs command on a cluster node. hdfs related stuff may not be configured on yarn nodes and thus it will be better to avoid our use of the hdfs command in our implementation.",
          "created_at": "2023-06-08T12:40:06Z"
        },
        {
          "author": "hkvision",
          "body": "Idaly confirmed the code above works and let's put the change into the mainstream. Will close the issue after we eliminate the usage of hdfs in our code.",
          "created_at": "2023-06-08T12:41:13Z"
        },
        {
          "author": "hkvision",
          "body": "Some sample code as a workaround for load model weights:\r\n```\r\nest = Estimator.from_keras(model_creator=..., backend=\"spark\")\r\n\r\nimport os, shutil, tempfile\r\nfrom bigdl.dllib.utils.file_utils import get_remote_files_with_prefix_to_local\r\ntemp_dir = tempfile.mkdtemp()\r\nfilepath = \"hdfs:///user/kai/zc",
          "created_at": "2023-06-12T09:05:19Z"
        },
        {
          "author": "lalalapotter",
          "body": "If you are using h5 format, please refer to the following code:\r\n\r\n```python\r\nest = Estimator.from_keras(model_creator=..., backend=\"spark\")\r\n\r\nimport os, shutil, tempfile\r\nfrom bigdl.dllib.utils.file_utils import get_remote_file_to_local\r\ntemp_dir = tempfile.mkdtemp()\r\nfilepath = \"hdfs:///path/on_h",
          "created_at": "2023-06-13T06:26:38Z"
        }
      ]
    },
    {
      "issue_number": 7495,
      "title": "Spark DataFrame to Pandas RDD with arrow better support nested lists",
      "body": "```\r\nrdd = sc.range(0, 20, numSlices=5)  # Changing to 4 will have ValueError: Length of values (4) does not match length of index (5)\r\ndf = rdd.map(lambda x:[x, np.random.rand(907500).tolist()]).toDF([\"index\", \"input\"])\r\n\r\ndef reshape(x):\r\n    return np.array(x).reshape([3, 550, 550]).tolist()\r\n\r\nreshape_udf = udf(reshape, ArrayType(ArrayType(ArrayType(FloatType()))))\r\ndf = df.withColumn(\"input\", reshape_udf(df.input))\r\n\r\nxshards = spark_df_to_pd_sparkxshards(df)\r\n```\r\n\r\nWhen converting nested list column of spark dataframe (spark df doesn't support ndarray schema) to pandas dataframe, the result is strange.\r\nThe result is an ndarray of object, only have the shape of the first dimension. For each object, it is an ndarray of object and so on... But the objects are ndarrays of the same shape, don't know why it is treated as an ndarray of object instead of an ndarray of numeric values with the correct shape...\r\n\r\nNeed to fix this issue, for nested list column, when converting to pandas dataframe, the resulting column should be a nested list or a correct ndarray.\r\n\r\n@cyita https://github.com/intel-analytics/BigDL/pull/5952 Does your PR have some insights to fix this issue?",
      "state": "closed",
      "author": "hkvision",
      "author_type": "User",
      "created_at": "2023-02-09T08:38:01Z",
      "updated_at": "2025-02-14T12:49:20Z",
      "closed_at": "2025-02-14T12:49:20Z",
      "labels": [
        "orca",
        "TODO"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/7495/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/7495",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/7495",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:34.086717",
      "comments": []
    },
    {
      "issue_number": 7386,
      "title": "[Orca] 2.2.0 doesn't converge for tensorflow2 estimator evaluate",
      "body": "Reported by the customer:\r\nWhen upgrading the BigDL package to 2.2.0 from 2.1.0b20220519, tf2.Estimator.evaluate()  looks not correct.\r\n```\r\n2.1.0b20220519 (looks good. less than one.)\r\n[{'validation_ndcg@5': 0.19312456250190735, 'validation_ndcg@10': 0.27864694595336914, 'validation_ndcg': 0.39072680473327637, 'validation_MAP@5': 0.1501678079366684, 'validation_MAP@10': 0.18642768263816833, 'validation_MAP': 0.2198168933391571}]\r\n\r\n2.2.0\r\n{'validation_ndcg@5': 4.35667610168457, 'validation_ndcg@10': 6.832152843475342, 'validation_ndcg': 11.26124095916748, 'validation_MAP@5': 3.294984817504883, 'validation_MAP@10': 4.330305099487305, 'validation_MAP': 5.608747959136963}\r\n```\r\n\r\n@lalalapotter @sgwhat Check if we have changed anything from 20220519 to 2.2.0?",
      "state": "closed",
      "author": "hkvision",
      "author_type": "User",
      "created_at": "2023-01-31T11:38:15Z",
      "updated_at": "2025-02-14T12:49:17Z",
      "closed_at": "2025-02-14T12:49:17Z",
      "labels": [
        "orca",
        "user issue"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/7386/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/7386",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/7386",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:34.086737",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Does this happen after direct training or loading a model then training ?",
          "created_at": "2023-02-01T01:42:52Z"
        },
        {
          "author": "hkvision",
          "body": "> Does this happen after direct training or loading a model then training ?\r\n\r\nStill asking for more info.",
          "created_at": "2023-02-01T01:53:57Z"
        },
        {
          "author": "hkvision",
          "body": "A possible doubt: When converting Spark DataFrame to XShards, if we use batch_size as shard size, each sub-partition is smaller, this will possibly impact the calculation of metrics that involve orders such as NDCG and MAP?\r\nRelated PR: https://github.com/intel-analytics/BigDL/pull/6879\r\n\r\n-----\r\nUp",
          "created_at": "2023-02-01T04:25:02Z"
        },
        {
          "author": "nyamashi",
          "body": "@hkvision  Thanks for adding the issue!\r\n\r\nHere is the case when loading the trained model before `evaluate()` and results are:\r\n- 2.1.0b20220519 (looks good. less than one.)\r\n```\r\n[{'validation_ndcg@5': 0.3261723816394806, 'validation_ndcg@10': 0.4041267931461334, 'validation_ndcg': 0.4779358208179",
          "created_at": "2023-02-01T08:04:46Z"
        },
        {
          "author": "hkvision",
          "body": "As we test, seems defining a model in pure tf.keras with ndcg won't get this problem.\r\nTest example: https://github.com/intel-analytics/BigDL/blob/main/python/orca/tutorial/NCF/tf_train_spark_dataframe.py\r\nResult: (Seems during the evaluate, the value is accumulated, but in the very end tf will auto",
          "created_at": "2023-02-02T02:30:21Z"
        }
      ]
    },
    {
      "issue_number": 4965,
      "title": "Exception happened if using orca estimator train pytorch model with xshards",
      "body": "The exception is:\r\nAn error occurred while calling o59.estimatorTrain.\r\n: com.intel.analytics.bigdl.dllib.utils.InvalidOperationException\r\n\tat com.intel.analytics.bigdl.dllib.utils.Log4Error$.invalidOperationError(Log4Error.scala:38)\r\n\tat com.intel.analytics.bigdl.dllib.keras.models.InternalDistriOptimizer.train(Topology.scala:1161)\r\n\tat com.intel.analytics.bigdl.dllib.keras.models.InternalDistriOptimizer.train(Topology.scala:1302)\r\n\tat com.intel.analytics.bigdl.dllib.keras.models.InternalDistriOptimizer.train(Topology.scala:969)\r\n\tat com.intel.analytics.bigdl.dllib.estimator.Estimator.train(Estimator.scala:192)\r\n\tat com.intel.analytics.bigdl.dllib.estimator.python.PythonEstimator.estimatorTrain(PythonEstimator.scala:100)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.reflect.InvocationTargetException\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat com.intel.analytics.bigdl.dllib.keras.layers.utils.KerasUtils$.invokeMethod(KerasUtils.scala:317)\r\n\tat com.intel.analytics.bigdl.dllib.keras.layers.utils.KerasUtils$.invokeMethodWithEv(KerasUtils.scala:344)\r\n\tat com.intel.analytics.bigdl.dllib.keras.models.InternalOptimizerUtil$.optimizeModels(Topology.scala:886)\r\n\tat com.intel.analytics.bigdl.dllib.keras.models.InternalDistriOptimizer.train(Topology.scala:1089)\r\n\t... 15 more\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 1 times, most recent failure: Lost task 0.0 in stage 21.0 (TID 17, localhost, executor driver): com.intel.analytics.bigdl.dllib.utils.UnKnownException: com.intel.analytics.bigdl.dllib.utils.InvalidOperationException: only support tensor input\r\n\tat com.intel.analytics.bigdl.dllib.utils.Log4Error$.unKnowExceptionError(Log4Error.scala:60)\r\n\tat com.intel.analytics.bigdl.dllib.utils.ThreadPool.invokeAndWait2(ThreadPool.scala:175)\r\n\tat com.intel.analytics.bigdl.dllib.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:261)\r\n\tat com.intel.analytics.bigdl.dllib.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:220)\r\n\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.util.concurrent.ExecutionException: com.intel.analytics.bigdl.dllib.utils.InvalidOperationException: only support tensor input\r\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\r\n\r\nIt can be reproed by below code:\r\n```\r\n# model definition\r\nclass MLP(Module):\r\n    # define model elements\r\n    def __init__(self, n_inputs):\r\n        super(MLP, self).__init__()\r\n        # input to first hidden layer\r\n        self.hidden1 = Linear(n_inputs, 10)\r\n        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\r\n        self.act1 = ReLU()\r\n        # second hidden layer\r\n        self.hidden2 = Linear(10, 8)\r\n        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\r\n        self.act2 = ReLU()\r\n        # third hidden layer and output\r\n        self.hidden3 = Linear(8, 1)\r\n        xavier_uniform_(self.hidden3.weight)\r\n        self.act3 = Sigmoid()\r\n\r\n    # forward propagate input\r\n    def forward(self, X):\r\n        # input to first hidden layer\r\n        X = self.hidden1(X)\r\n        X = self.act1(X)\r\n         # second hidden layer\r\n        X = self.hidden2(X)\r\n        X = self.act2(X)\r\n        # third hidden layer and output\r\n        X = self.hidden3(X)\r\n        X = self.act3(X)\r\n        return X\r\n\r\ninit_orca_context(memory=\"4g\")\r\n\r\npath = PATH\r\ndata_shard = bigdl.orca.data.pandas.read_csv(path)\r\n\r\ndef getSchema(iter):\r\n    for pdf in iter:\r\n        return [pdf.columns.values]\r\n\r\ncolumn = data_shard.rdd.mapPartitions(getSchema).first()\r\nfrom bigdl.orca.data.transform import LabelEncode\r\nlabel_encoder = LabelEncode(inputCol=column[-1], outputCol=\"indexedLabel\")\r\ndata_shard = label_encoder.fit_transform(data_shard)\r\ndata_shard.rdd.count()\r\n\r\n# define the network\r\nmodel = MLP(34)\r\n\r\ncriterion = BCELoss()\r\noptimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\r\n\r\norca_estimator = Estimator.from_torch(model=model,\r\n                                      optimizer=optimizer,\r\n                                      loss=criterion,\r\n                                      metrics=[Accuracy()],\r\n                                      backend=\"bigdl\")\r\n\r\norca_estimator.fit(data=data_shard, epochs=8,\r\n                   feature_cols=list(column[:-1]), label_cols=[\"indexedLabel\"], batch_size=4)\r\n```\r\n\r\nThe data can be downloaded from Almaren-Gateway:/mnt/md0/home/ding.ding/new_ionosphere.csv",
      "state": "closed",
      "author": "dding3",
      "author_type": "User",
      "created_at": "2022-06-29T01:29:08Z",
      "updated_at": "2025-02-14T12:49:03Z",
      "closed_at": "2025-02-14T12:49:03Z",
      "labels": [
        "orca"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 11,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/4965/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "shanyu-sys"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/4965",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/4965",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:34.313102",
      "comments": [
        {
          "author": "jason-dai",
          "body": "The `bigdl` backend does not support `xshards`? @qiuxin2012 ",
          "created_at": "2022-06-29T01:46:10Z"
        },
        {
          "author": "qiuxin2012",
          "body": "> The `bigdl` backend does not support `xshards`? @qiuxin2012\r\n\r\n`bigdl` backend only support FeatureSet, is `xshards` a FeatureSet?",
          "created_at": "2022-06-29T01:49:48Z"
        },
        {
          "author": "jason-dai",
          "body": "@yushan111 please take a look",
          "created_at": "2022-07-12T23:22:40Z"
        },
        {
          "author": "shanyu-sys",
          "body": "In Orca Estimators, we assume `features_cols` is multiple Inputs for model, instead of different columns that could be stacked and serve as one model input. \r\nIt seems your model only accepts one input, so there might be two options\r\n\r\n- Feed XShards of dictionary of {‘x’: feature, ‘y’: label} to es",
          "created_at": "2022-07-13T02:22:09Z"
        },
        {
          "author": "dding3",
          "body": "> In Orca Estimators, we assume `features_cols` is multiple Inputs for model, instead of different columns that could be stacked and serve as one model input. It seems your model only accepts one input, so there might be two options\r\n> \r\n> * Feed XShards of dictionary of {‘x’: feature, ‘y’: label} t",
          "created_at": "2022-07-13T03:43:50Z"
        }
      ]
    },
    {
      "issue_number": 4841,
      "title": "Chronos: multiprocess training with nano distribute_backend=\"subprocess\" has a huge accuracy drop",
      "body": "I use training data for evaluation. Only `distribute_backend=\"subprocess\"` reflect the bug.\r\n\r\n```python\r\nfrom bigdl.chronos.data.repo_dataset import get_public_dataset\r\nfrom bigdl.chronos.forecaster import TCNForecaster\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom bigdl.chronos.metric.forecast_metrics import Evaluator\r\nimport time\r\nimport numpy as np\r\n\r\n\r\ndef get_tsdata(lookback=96, horizon=24):\r\n    name = 'nyc_taxi'\r\n    tsdata_train, _, tsdata_test = get_public_dataset(name, val_ratio=0, test_ratio=0.4)\r\n    stand_scaler = StandardScaler()\r\n    for tsdata in [tsdata_train, tsdata_test]:\r\n        tsdata.gen_dt_feature(features=[\"HOUR\"], one_hot_features=[\"HOUR\"])\\\r\n              .impute(mode=\"linear\")\\\r\n              .scale(stand_scaler, fit=(tsdata is tsdata_train))\\\r\n              .roll(lookback=lookback, horizon=horizon)\r\n    return tsdata_train, tsdata_test\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    tsdata_train, tsdata_test = get_tsdata()\r\n    x_train, y_train = tsdata_train.to_numpy()\r\n    x_test, y_test = tsdata_test.to_numpy()\r\n    print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\r\n    forecaster = TCNForecaster(past_seq_len = 96,\r\n                               future_seq_len = 24,\r\n                               input_feature_num = x_train.shape[-1],\r\n                               output_feature_num = 1,\r\n                               num_channels = [48] * 7,\r\n                               repo_initialization = False,\r\n                               kernel_size = 3,\r\n                               dropout = 0.1,\r\n                               lr = 0.001,\r\n                               seed = 1)\r\n    forecaster.num_processes = 2  # change this num to 1 for single process training, 2 for subprocess backend.\r\n    forecaster.fit((x_train, y_train), epochs=3, batch_size=32)\r\n\r\n    st = time.time()\r\n    y_pred = forecaster.predict(x_train)\r\n    fp32_pytorch_time = time.time()-st\r\n\r\n    avg_smape_fp32_pytorch = Evaluator.evaluate(\"mse\", y_train, y_pred, aggregate='mean')[0]\r\n\r\n    print(\"fp32 pytorch smape:\", round(float(avg_smape_fp32_pytorch), 2))\r\n```\r\n\r\n\r\n```bash\r\n# single process\r\nEpoch 2: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 190/190 [00:05<00:00, 33.46it/s, loss=0.0725]\r\nfp32 pytorch mse: 0.06\r\n\r\n# 2 process with distribute_backend=\"subprocess\" \r\nEpoch 2: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 190/190 [00:05<00:00, 34.05it/s, loss=0.0803]\r\n/home/junweid/miniconda3/envs/nano/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py:288: UserWarning: cleaning up ddp environment...\r\n rank_zero_warn(\"cleaning up ddp environment...\")\r\nfp32 pytorch mse: 1.31\r\n\r\n# 2 process with distribute_backend=\"spawn\" \r\nEpoch 2: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 190/190 [00:05<00:00, 34.40it/s, loss=0.071]\r\n/home/junweid/miniconda3/envs/nano/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py:288: UserWarning: cleaning up ddp environment...\r\n rank_zero_warn(\"cleaning up ddp environment...\")\r\nfp32 pytorch mse: 0.05\r\n```",
      "state": "closed",
      "author": "TheaperDeng",
      "author_type": "User",
      "created_at": "2022-06-15T05:05:23Z",
      "updated_at": "2025-02-14T12:48:57Z",
      "closed_at": "2025-02-14T12:48:57Z",
      "labels": [
        "Chronos",
        "Nano"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/4841/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/4841",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/4841",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:34.549059",
      "comments": [
        {
          "author": "TheaperDeng",
          "body": "@yangw1234 this might be a nano issue. Here is how chronos carry out the inference:\r\nhttps://github.com/intel-analytics/BigDL/blob/main/python/chronos/src/bigdl/chronos/pytorch/utils.py#L21",
          "created_at": "2022-06-15T06:11:06Z"
        },
        {
          "author": "yangw1234",
          "body": "@Django-Jiang could you help look at this issue?",
          "created_at": "2022-06-15T06:23:47Z"
        },
        {
          "author": "Django-Jiang",
          "body": "I found training with subprocess will only return 1.31 regardless of epoch number, there should be some problem. I am firstly checking the subprocess part.",
          "created_at": "2022-06-16T03:15:40Z"
        },
        {
          "author": "liangs6212",
          "body": "This problem only occurs when `checkpoint_callback` is set to False.\r\n`checkpoint_callback` may be set to False if the user does not need to automatically check ckpt.\r\nFor more information, please refer to https://pytorch-lightning.readthedocs.io/en/1.4.2/common/trainer.html#checkpoint-callback\r\nbtw",
          "created_at": "2022-06-16T06:43:49Z"
        }
      ]
    },
    {
      "issue_number": 4512,
      "title": "Chronos: AutoTS and AutoModel should reset some environment variables to None",
      "body": "Currently, bigdl-nano will automatically set environment in a conda environment, which is a disaster for Ray Tune related functions. Chronos should catch those conflict env var and reset them to None.\r\n\r\nA workround has been stated in document: https://bigdl.readthedocs.io/en/latest/doc/Chronos/Overview/speed_up.html#auto-tuning-acceleration",
      "state": "closed",
      "author": "TheaperDeng",
      "author_type": "User",
      "created_at": "2022-04-26T07:58:59Z",
      "updated_at": "2025-02-14T12:48:46Z",
      "closed_at": "2025-02-14T12:48:46Z",
      "labels": [
        "Chronos"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/4512/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/4512",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/4512",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:34.768087",
      "comments": []
    },
    {
      "issue_number": 3934,
      "title": "onnx-inference lantency slows down",
      "body": "Currently onnx-inference(tcn) delay is about 1.5ms, a month ago, this number was about 1.04ms,  don't know why.\r\n\r\nserver: cpx-3\r\nonnxruntime version: 1.10.0\r\nbigdl-nano==0.14.0b20220118",
      "state": "closed",
      "author": "liangs6212",
      "author_type": "User",
      "created_at": "2022-01-24T07:29:25Z",
      "updated_at": "2025-02-14T12:48:18Z",
      "closed_at": "2025-02-14T12:48:18Z",
      "labels": [
        "Chronos"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/3934/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/3934",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/3934",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:34.768113",
      "comments": [
        {
          "author": "TheaperDeng",
          "body": "issue noticed, please try 0.14.0b20211208 and confirm this is a chronos problem rather than an environment/platform problem.",
          "created_at": "2022-02-08T06:04:02Z"
        }
      ]
    },
    {
      "issue_number": 12810,
      "title": "Support Request: Baichuan-M1-14B on Core Ultra",
      "body": "Hello, I try to run Baichuan-M1-14B on ARL-H windows 11, but failed. The following is some commands and error log.\n1) Download Baichuan-M1-14B\nmodelscope download --model baichuan-inc/Baichuan-M1-14B-Instruct README.md --local_dir ./dir\n\n2) install env\n```\nconda create -n ipex-2.6 python=3.10 libuv\nconda activate ipex-2.6\npip install --pre --upgrade ipex-llm[xpu_2.6] --extra-index-url https://download.pytorch.org/whl/xpu\nset SYCL_CACHE_PERSISTENT=1 \nset SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1 \n```\n\n3) \n>python run_14b.py\n\nImportError: This modeling file requires the following packages that were not found in your environment: flash_attn, einops. Run `pip install flash_attn einops`\n\n> pip install flash_attn einops\n\n![Image](https://github.com/user-attachments/assets/d79e1bc9-1d1e-4564-b223-c7b078accddb)\n\n\nrun_14b.py\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n# 1. Load pre-trained model and tokenizer\nmodel_name = \"./Baichuan-M1-14B-Instruct\"  \ntokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True) ## \"sdpa\"   \"eager\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name,load_in_4bit=True,attn_implementation=\"sdpa\",cpu_embedding=True,trust_remote_code=True)\n\nif 1:\n    model.save_low_bit(model_path +  \"-int4/\")\n    tokenizer.save_pretrained(model_path + \"-int4/\")\n\nmodel = model.to('xpu')\nprint('Successfully loaded Tokenizer and optimized Model!')\n\n# 2. Input prompt text\nprompt = \"May I ask you some questions about medical knowledge?\"\n\n# 3. Encode the input text for the model\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# 4. Generate text\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\n# 5. Decode the generated text\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n\n# 6. Output the result\nprint(\"Generated text:\")\nprint(response)\n```\n\n4)",
      "state": "closed",
      "author": "KiwiHana",
      "author_type": "User",
      "created_at": "2025-02-11T10:50:02Z",
      "updated_at": "2025-02-14T04:54:14Z",
      "closed_at": "2025-02-14T04:54:13Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12810/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "MeouSker77"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12810",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12810",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:34.946501",
      "comments": [
        {
          "author": "KiwiHana",
          "body": "follow this way https://github.com/intel/ipex-llm/pull/12808\n\npip install --pre --upgrade ipex-llm[xpu_2.6] --extra-index-url https://download.pytorch.org/whl/xpu\npip install --pre --upgrade ipex-llm\n\n```\nPackage            Version\n------------------ --------------\naccelerate         0.23.0\nbigdl-co",
          "created_at": "2025-02-12T04:37:14Z"
        },
        {
          "author": "MeouSker77",
          "body": "> follow this way [#12808](https://github.com/intel/ipex-llm/pull/12808)\n\nuse transformers 4.45\n```\npip install transformers==4.45 trl==0.11\n```",
          "created_at": "2025-02-12T05:44:48Z"
        },
        {
          "author": "KiwiHana",
          "body": "(llm-arl-2.6) C:\\Users\\Lengda\\Documents\\AIGC_ov20250123\\resources\\service\\models>python run_baichun-m1-14b.py\n```\nYou are using a model of type baichuan_m1 to instantiate a model of type baichuan. This is not supported for all configurations of models and can yield errors.\nLoading checkpoint shards:",
          "created_at": "2025-02-12T06:06:20Z"
        },
        {
          "author": "KiwiHana",
          "body": "set SYCL_CACHE_PERSISTENT=1 \nset SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1 \n\nThanks！",
          "created_at": "2025-02-14T04:54:13Z"
        }
      ]
    },
    {
      "issue_number": 12793,
      "title": "Runtime Configurations for Intel Core™ Ultra 9 Processor 288V",
      "body": "For running LLM on NPU, What should be correct Runtime Configurations for Intel Core™ Ultra 9 Processor 288V?\n\n[](https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/npu_quickstart.md#runtime-configurations)",
      "state": "closed",
      "author": "morteza89",
      "author_type": "User",
      "created_at": "2025-02-09T18:20:45Z",
      "updated_at": "2025-02-13T19:31:02Z",
      "closed_at": "2025-02-13T19:31:02Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12793/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12793",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12793",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:35.163865",
      "comments": [
        {
          "author": "plusbang",
          "body": "Hi, @morteza89 , please try the same runtime configuration as 258V (https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/npu_quickstart.md#runtime-configurations).",
          "created_at": "2025-02-10T02:17:02Z"
        }
      ]
    },
    {
      "issue_number": 12811,
      "title": "Docker image intelanalytics/ipex-llm-inference-cpp-xpu error. Update needed ?",
      "body": "With latest docker image from intelanalytics/ipex-llm-inference-cpp-xpu I have this message : \nerror while loading shared libraries: libsycl.so.8: cannot open shared object file: No such file or directory\n\nI think https://github.com/intel/ipex-llm/blob/main/docker/llm/inference-cpp/Dockerfile must be update with latest oneapi-basekit verison 2025 .\nand perhaps others changes are needed in the file ?\n\n\n",
      "state": "closed",
      "author": "easyfab",
      "author_type": "User",
      "created_at": "2025-02-11T18:49:25Z",
      "updated_at": "2025-02-12T18:16:08Z",
      "closed_at": "2025-02-12T18:16:06Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12811/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "liu-shaojun"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12811",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12811",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:35.392973",
      "comments": [
        {
          "author": "easyfab",
          "body": "Oh, I see [PR 12802](https://github.com/intel/ipex-llm/pull/12802) is pending.\nIt should work after this is commit ?",
          "created_at": "2025-02-11T18:53:56Z"
        },
        {
          "author": "liu-shaojun",
          "body": "Yes, we are in the process of updating and testing it. The changes in PR [#12802](https://github.com/intel/ipex-llm/pull/12802) should resolve this issue once merged.",
          "created_at": "2025-02-12T01:49:26Z"
        },
        {
          "author": "liu-shaojun",
          "body": "Hi, the PR has been merged. You can try again with **intelanalytics/ipex-llm-inference-cpp-xpu:latest** and see if the issue is resolved.",
          "created_at": "2025-02-12T06:19:38Z"
        },
        {
          "author": "easyfab",
          "body": "Thanks for the update.\n\n",
          "created_at": "2025-02-12T18:16:06Z"
        }
      ]
    },
    {
      "issue_number": 12373,
      "title": "using both iGPU and CPU together ",
      "body": "Hello,\r\n\r\nI can successfully running Ollama in iGPU of Core i7 13700K, but the perfomance is half of the CPU. Is it possible to use CPU and iGPU together so that we can get 1.5x performance than CPU only?\r\n\r\nthx",
      "state": "open",
      "author": "fanlessfan",
      "author_type": "User",
      "created_at": "2024-11-10T13:37:21Z",
      "updated_at": "2025-02-12T07:28:47Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12373/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12373",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12373",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:35.575356",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @fanlessfan , which model are you running, and can you provide the script you use to run ollama and the ollama serve log?",
          "created_at": "2024-11-11T01:39:11Z"
        },
        {
          "author": "fanlessfan",
          "body": "It's not model related. I just followed the instructions on below link. It works and I can see the iGPU utilization is almost 100% while the model is running using intel_GPU_top and CPU is only one core busy.\r\n\r\nhttps://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quic",
          "created_at": "2024-11-11T03:56:27Z"
        },
        {
          "author": "TrokhymButko",
          "body": "I have an Intel N100 processor (16GB of RAM, the iGPU cuts off 8GB for itself), following the instructions I was able to use the integrated graphics (I can see it by the loading in the task manager), but the performance has not increased compared to the processor. I am slightly disappointed, because",
          "created_at": "2024-11-13T16:02:44Z"
        },
        {
          "author": "fanlessfan",
          "body": "Hi @TrokhymButko , Can you share the instructions? I'd like to try it too. for me the iGPU is half the performance of CPU.",
          "created_at": "2024-11-13T16:23:00Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @fanlessfan , running Ollama models using only the iGPU is the optimal solution for performance. Using both the iGPU and CPU will result in a performance decrease. ",
          "created_at": "2024-11-15T01:37:38Z"
        }
      ]
    },
    {
      "issue_number": 12761,
      "title": "Nonsense output",
      "body": "Hobbyist only here, hoping for some help.\n\nI'm running Windows 11, miniforge3, Ollama  0.5.1-ipexllm-20250123, Ubuntu 22.04, using Openwebui via Docker Desktop on the front.  I have a Ryzen 9 5950X CPU, 64 GB system RAM, an a770 w/ 16GB VRAM.\n\nInstalled Ollama using instructions https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.md and https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/llama_cpp_quickstart.md\n\nThe output attached is using the model Deepseek-r1:7b but I can get similar symptoms with other models.\n\nThe symptom is that I am getting gibberish output from even fairly simple prompts.  My experience has been that this is most often after my first question, or after switching models, but it is in fact quite random.\n\nAttaching my miniforge prompt and screenshots of the open webui output. \n\n[Ollama serve.txt](https://github.com/user-attachments/files/18577077/Ollama.serve.txt)\n\n![Image](https://github.com/user-attachments/assets/f6ea154b-3eaf-4f13-bd79-42d430e54c87)\n\n![Image](https://github.com/user-attachments/assets/d5dc83e7-cbb8-4089-b23c-bbf9cef6dc01)\n",
      "state": "open",
      "author": "ysaric",
      "author_type": "User",
      "created_at": "2025-01-28T17:36:48Z",
      "updated_at": "2025-02-11T20:44:27Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 21,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12761/reactions",
        "total_count": 5,
        "+1": 5,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12761",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12761",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:35.814313",
      "comments": []
    },
    {
      "issue_number": 12374,
      "title": "Several GPU models behave erratically compared to CPU execution",
      "body": "Here is a trace from my Intel Arc A770 via Docker:\r\n\r\n```\r\n$ ollama run  deepseek-coder-v2\r\n>>> write fizzbuzz\r\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\r\n\r\n```\r\nAnd here is an trace from Arch linux running on CPU:\r\n\r\n```\r\n$ ollama run  deepseek-coder-v2 \r\n>>> write fizzbuzz\r\n Certainly! FizzBuzz is a classic programming task, often used in job interviews to test basic understanding of loops and conditionals. The task goes like this:\r\n\r\n1. Print numbers from 1 to 100.\r\n2. For multiples of 3, print \"Fizz\".\r\n3. For multiples of 5, print \"Buzz\".\r\n4. For multiples of both 3 and 5 (i.e., multiples of 15), print \"FizzBuzz\".\r\n\r\nHere's a simple implementation in Python:\r\n\r\nfor i in range(1, 101):\r\n    if i % 15 == 0:\r\n        print(\"FizzBuzz\")\r\n    elif i % 3 == 0:\r\n        print(\"Fizz\")\r\n    elif i % 5 == 0:\r\n        print(\"Buzz\")\r\n    else:\r\n        print(i)\r\n\r\nThis code will output the numbers from 1 to 100, replacing multiples of 3 with \"Fizz\", multiples of 5 with \"Buzz\", and multiples of both 3 and 5 with \"FizzBuzz\".\r\n\r\n```\r\n\r\nFor Docker I'm using https://github.com/mattcurf/ollama-intel-gpu due to #12372 \r\n\r\nollama logs:\r\n\r\n```\r\nollama-intel-gpu  | time=2024-11-10T20:25:23.772Z level=INFO source=server.go:395 msg=\"starting llama server\" cmd=\"/tmp/ollama3494697786/runners/cpu_avx2/ollama_llama_server --model /root/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 8192 --batch-size 512 --embedding --log-disable --n-gpu-layers 999 --no-mmap --parallel 4 --port 40951\"\r\nollama-intel-gpu  | time=2024-11-10T20:25:23.773Z level=INFO source=sched.go:450 msg=\"loaded runners\" count=1\r\nollama-intel-gpu  | time=2024-11-10T20:25:23.773Z level=INFO source=server.go:595 msg=\"waiting for llama runner to start responding\"\r\nollama-intel-gpu  | time=2024-11-10T20:25:23.773Z level=INFO source=server.go:629 msg=\"waiting for server to become available\" status=\"llm server error\"\r\nollama-intel-gpu  | INFO [main] build info | build=1 commit=\"6cbbf2a\" tid=\"139094668663808\" timestamp=1731270323\r\nollama-intel-gpu  | INFO [main] system info | n_threads=16 n_threads_batch=-1 system_info=\"AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \" tid=\"139094668663808\" timestamp=1731270323 total_threads=32\r\nollama-intel-gpu  | INFO [main] HTTP server listening | hostname=\"127.0.0.1\" n_threads_http=\"31\" port=\"40951\" tid=\"139094668663808\" timestamp=1731270323\r\nollama-intel-gpu  | llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /root/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))\r\nollama-intel-gpu  | llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nollama-intel-gpu  | llama_model_loader: - kv   0:                       general.architecture str              = deepseek2\r\nollama-intel-gpu  | llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct\r\nollama-intel-gpu  | llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27\r\nollama-intel-gpu  | llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840\r\nollama-intel-gpu  | llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048\r\nollama-intel-gpu  | llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944\r\nollama-intel-gpu  | llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16\r\nollama-intel-gpu  | llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16\r\nollama-intel-gpu  | llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000\r\nollama-intel-gpu  | llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001\r\nollama-intel-gpu  | llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6\r\nollama-intel-gpu  | llama_model_loader: - kv  11:                          general.file_type u32              = 2\r\nollama-intel-gpu  | llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1\r\nollama-intel-gpu  | llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400\r\nollama-intel-gpu  | llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512\r\nollama-intel-gpu  | llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192\r\nollama-intel-gpu  | llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128\r\nollama-intel-gpu  | llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408\r\nollama-intel-gpu  | llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64\r\nollama-intel-gpu  | llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2\r\nollama-intel-gpu  | llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000\r\nollama-intel-gpu  | llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64\r\nollama-intel-gpu  | llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn\r\nollama-intel-gpu  | llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000\r\nollama-intel-gpu  | llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096\r\nollama-intel-gpu  | llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700\r\nollama-intel-gpu  | llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2\r\nollama-intel-gpu  | llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm\r\nollama-intel-gpu  | llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nollama-intel-gpu  | llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nollama-intel-gpu  | llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [\"Ġ Ġ\", \"Ġ t\", \"Ġ a\", \"i n\", \"h e...\r\nollama-intel-gpu  | llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000\r\nollama-intel-gpu  | llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001\r\nollama-intel-gpu  | llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001\r\nollama-intel-gpu  | llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\r\nollama-intel-gpu  | llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false\r\nollama-intel-gpu  | llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\r\nollama-intel-gpu  | llama_model_loader: - kv  37:               general.quantization_version u32              = 2\r\nollama-intel-gpu  | llama_model_loader: - type  f32:  108 tensors\r\nollama-intel-gpu  | llama_model_loader: - type q4_0:  268 tensors\r\nollama-intel-gpu  | llama_model_loader: - type q6_K:    1 tensors\r\nollama-intel-gpu  | llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\r\nollama-intel-gpu  | llm_load_vocab: special tokens cache size = 2400\r\nollama-intel-gpu  | llm_load_vocab: token to piece cache size = 0.6661 MB\r\nollama-intel-gpu  | llm_load_print_meta: format           = GGUF V3 (latest)\r\nollama-intel-gpu  | llm_load_print_meta: arch             = deepseek2\r\nollama-intel-gpu  | llm_load_print_meta: vocab type       = BPE\r\nollama-intel-gpu  | llm_load_print_meta: n_vocab          = 102400\r\nollama-intel-gpu  | llm_load_print_meta: n_merges         = 99757\r\nollama-intel-gpu  | llm_load_print_meta: vocab_only       = 0\r\nollama-intel-gpu  | llm_load_print_meta: n_ctx_train      = 163840\r\nollama-intel-gpu  | llm_load_print_meta: n_embd           = 2048\r\nollama-intel-gpu  | llm_load_print_meta: n_layer          = 27\r\nollama-intel-gpu  | llm_load_print_meta: n_head           = 16\r\nollama-intel-gpu  | llm_load_print_meta: n_head_kv        = 16\r\nollama-intel-gpu  | llm_load_print_meta: n_rot            = 64\r\nollama-intel-gpu  | llm_load_print_meta: n_swa            = 0\r\nollama-intel-gpu  | llm_load_print_meta: n_embd_head_k    = 192\r\nollama-intel-gpu  | llm_load_print_meta: n_embd_head_v    = 128\r\nollama-intel-gpu  | llm_load_print_meta: n_gqa            = 1\r\nollama-intel-gpu  | llm_load_print_meta: n_embd_k_gqa     = 3072\r\nollama-intel-gpu  | llm_load_print_meta: n_embd_v_gqa     = 2048\r\nollama-intel-gpu  | llm_load_print_meta: f_norm_eps       = 0.0e+00\r\nollama-intel-gpu  | llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\r\nollama-intel-gpu  | llm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nollama-intel-gpu  | llm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nollama-intel-gpu  | llm_load_print_meta: f_logit_scale    = 0.0e+00\r\nollama-intel-gpu  | llm_load_print_meta: n_ff             = 10944\r\nollama-intel-gpu  | llm_load_print_meta: n_expert         = 64\r\nollama-intel-gpu  | llm_load_print_meta: n_expert_used    = 6\r\nollama-intel-gpu  | llm_load_print_meta: causal attn      = 1\r\nollama-intel-gpu  | llm_load_print_meta: pooling type     = 0\r\nollama-intel-gpu  | llm_load_print_meta: rope type        = 0\r\nollama-intel-gpu  | llm_load_print_meta: rope scaling     = yarn\r\nollama-intel-gpu  | llm_load_print_meta: freq_base_train  = 10000.0\r\nollama-intel-gpu  | llm_load_print_meta: freq_scale_train = 0.025\r\nollama-intel-gpu  | llm_load_print_meta: n_ctx_orig_yarn  = 4096\r\nollama-intel-gpu  | llm_load_print_meta: rope_finetuned   = unknown\r\nollama-intel-gpu  | llm_load_print_meta: ssm_d_conv       = 0\r\nollama-intel-gpu  | llm_load_print_meta: ssm_d_inner      = 0\r\nollama-intel-gpu  | llm_load_print_meta: ssm_d_state      = 0\r\nollama-intel-gpu  | llm_load_print_meta: ssm_dt_rank      = 0\r\nollama-intel-gpu  | llm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nollama-intel-gpu  | llm_load_print_meta: model type       = 16B\r\nollama-intel-gpu  | llm_load_print_meta: model ftype      = Q4_0\r\nollama-intel-gpu  | llm_load_print_meta: model params     = 15.71 B\r\nollama-intel-gpu  | llm_load_print_meta: model size       = 8.29 GiB (4.53 BPW) \r\nollama-intel-gpu  | llm_load_print_meta: general.name     = DeepSeek-Coder-V2-Lite-Instruct\r\nollama-intel-gpu  | llm_load_print_meta: BOS token        = 100000 '<｜begin▁of▁sentence｜>'\r\nollama-intel-gpu  | llm_load_print_meta: EOS token        = 100001 '<｜end▁of▁sentence｜>'\r\nollama-intel-gpu  | llm_load_print_meta: PAD token        = 100001 '<｜end▁of▁sentence｜>'\r\nollama-intel-gpu  | llm_load_print_meta: LF token         = 126 'Ä'\r\nollama-intel-gpu  | llm_load_print_meta: EOG token        = 100001 '<｜end▁of▁sentence｜>'\r\nollama-intel-gpu  | llm_load_print_meta: max token length = 256\r\nollama-intel-gpu  | llm_load_print_meta: n_layer_dense_lead   = 1\r\nollama-intel-gpu  | llm_load_print_meta: n_lora_q             = 0\r\nollama-intel-gpu  | llm_load_print_meta: n_lora_kv            = 512\r\nollama-intel-gpu  | llm_load_print_meta: n_ff_exp             = 1408\r\nollama-intel-gpu  | llm_load_print_meta: n_expert_shared      = 2\r\nollama-intel-gpu  | llm_load_print_meta: expert_weights_scale = 1.0\r\nollama-intel-gpu  | llm_load_print_meta: rope_yarn_log_mul    = 0.0707\r\nollama-intel-gpu  | time=2024-11-10T20:25:24.024Z level=INFO source=server.go:629 msg=\"waiting for server to become available\" status=\"llm server loading model\"\r\nollama-intel-gpu  | ggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nollama-intel-gpu  | ggml_sycl_init: SYCL_USE_XMX: yes\r\nollama-intel-gpu  | ggml_sycl_init: found 1 SYCL devices:\r\nollama-intel-gpu  | llm_load_tensors: ggml ctx size =    0.32 MiB\r\nollama-intel-gpu  | llm_load_tensors: offloading 27 repeating layers to GPU\r\nollama-intel-gpu  | llm_load_tensors: offloading non-repeating layers to GPU\r\nollama-intel-gpu  | llm_load_tensors: offloaded 28/28 layers to GPU\r\nollama-intel-gpu  | llm_load_tensors:      SYCL0 buffer size =  8376.27 MiB\r\nollama-intel-gpu  | llm_load_tensors:  SYCL_Host buffer size =   112.50 MiB\r\nollama-intel-gpu  | llama_new_context_with_model: n_ctx      = 8192\r\nollama-intel-gpu  | llama_new_context_with_model: n_batch    = 512\r\nollama-intel-gpu  | llama_new_context_with_model: n_ubatch   = 512\r\nollama-intel-gpu  | llama_new_context_with_model: flash_attn = 0\r\nollama-intel-gpu  | llama_new_context_with_model: freq_base  = 10000.0\r\nollama-intel-gpu  | llama_new_context_with_model: freq_scale = 0.025\r\nollama-intel-gpu  | [SYCL] call ggml_check_sycl\r\nollama-intel-gpu  | ggml_check_sycl: GGML_SYCL_DEBUG: 0\r\nollama-intel-gpu  | ggml_check_sycl: GGML_SYCL_F16: no\r\nollama-intel-gpu  | found 1 SYCL devices:\r\nollama-intel-gpu  | |  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\nollama-intel-gpu  | |  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\nollama-intel-gpu  | |ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\nollama-intel-gpu  | |--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\nollama-intel-gpu  | | 0| [level_zero:gpu:0]|                Intel Arc A770 Graphics|    1.6|    512|    1024|   32| 16225M|            1.3.31294|\r\nollama-intel-gpu  | llama_kv_cache_init:      SYCL0 KV buffer size =  2160.00 MiB\r\nollama-intel-gpu  | llama_new_context_with_model: KV self size  = 2160.00 MiB, K (f16): 1296.00 MiB, V (f16):  864.00 MiB\r\nollama-intel-gpu  | llama_new_context_with_model:  SYCL_Host  output buffer size =     1.59 MiB\r\nollama-intel-gpu  | llama_new_context_with_model:      SYCL0 compute buffer size =   339.13 MiB\r\nollama-intel-gpu  | llama_new_context_with_model:  SYCL_Host compute buffer size =    38.01 MiB\r\nollama-intel-gpu  | llama_new_context_with_model: graph nodes  = 1951\r\nollama-intel-gpu  | llama_new_context_with_model: graph splits = 110\r\nollama-intel-gpu  | [1731270330] warming up the model with an empty run\r\nollama-intel-gpu  | INFO [main] model loaded | tid=\"139094668663808\" timestamp=1731270335\r\nollama-intel-gpu  | time=2024-11-10T20:25:35.563Z level=INFO source=server.go:634 msg=\"llama runner started in 11.79 seconds\"\r\nollama-intel-gpu  | [GIN] 2024/11/10 - 20:25:35 | 200 |  11.81337377s |       127.0.0.1 | POST     \"/api/chat\"\r\nollama-intel-gpu  | [GIN] 2024/11/10 - 20:25:46 | 200 |       22.86µs |       127.0.0.1 | HEAD     \"/\"\r\nollama-intel-gpu  | [GIN] 2024/11/10 - 20:25:46 | 200 |    6.807262ms |       127.0.0.1 | POST     \"/api/show\"\r\nollama-intel-gpu  | [GIN] 2024/11/10 - 20:25:46 | 200 |    6.526006ms |       127.0.0.1 | POST     \"/api/chat\"\r\nollama-intel-gpu  | check_double_bos_eos: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\r\nollama-intel-gpu  | [GIN] 2024/11/10 - 20:25:59 | 200 |  9.400866991s |       127.0.0.1 | POST     \"/api/chat\"\r\n```",
      "state": "open",
      "author": "pepijndevos",
      "author_type": "User",
      "created_at": "2024-11-10T20:29:00Z",
      "updated_at": "2025-02-11T03:50:19Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 21,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12374/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12374",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12374",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:35.814338",
      "comments": []
    },
    {
      "issue_number": 12334,
      "title": "ipex-llm-ollama-installer-20240918.exe安装后用另一个exe调用文件夹中的start.bat会提示缺少dll等无法运行",
      "body": "1、双击安装ipex-llm-ollama-installer-20240918.exe\r\n默认安装在C:\\Users\\OPS17\\ipex-llm-ollama\r\n\r\n2、拷贝ipex-llm-ollama文件夹到C:\\aipc\\\r\n\r\n3、这个时候运行ipex-llm-ollama里面的start.bat，可以正常运行，并且访问什么都正常\r\n\r\n4、这个时候用Python编写一个程序，并打包为exe来调用start.bat，会报缺少dnnl.dll、svml_dispmd.dll等等文件，并且无法正常运行。\r\n\r\npython 脚本就一句 os.popen(\"start.bat\")\r\n然后用pyinstaller 进行打包为exe文件\r\n这个时候运行exe文件，就报缺少dnnl.dll、svml_dispmd.dll等等文件，并且无法正常运行。",
      "state": "open",
      "author": "dayskk",
      "author_type": "User",
      "created_at": "2024-11-05T07:03:05Z",
      "updated_at": "2025-02-10T08:57:33Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12334/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12334",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12334",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:35.814346",
      "comments": [
        {
          "author": "KiwiHana",
          "body": "ipex-llm-ollama-installer-20240918.exe是为了方便离线搭建环境把对应的文件抽出来单独打包的，依赖的dll也单抽出来放到了ipex-llm-ollama的目录下，跟ollama.exe同一个目录下。\r\n\r\nStart.bat里面先将目录切换到了c:\\users\\OPS17\\ipex-llm-ollama目录，然后启动了ollama serve. 这个时候command console里ollama.exe搜索dll是标准windows下搜索dll的顺序，在当前路径->父进程的路径-> windows  system路径…\r\n\r\n这个python编写程序的时候启",
          "created_at": "2024-11-06T02:08:31Z"
        },
        {
          "author": "bddiudiu",
          "body": "> 1、双击安装ipex-llm-ollama-installer-20240918.exe 默认安装在C:\\Users\\OPS17\\ipex-llm-ollama\n> \n> 2、拷贝ipex-llm-ollama文件夹到C:\\aipc\\\n> \n> 3、这个时候运行ipex-llm-ollama里面的start.bat，可以正常运行，并且访问什么都正常\n> \n> 4、这个时候用Python编写一个程序，并打包为exe来调用start.bat，会报缺少dnnl.dll、svml_dispmd.dll等等文件，并且无法正常运行。\n> \n> python 脚本就一句 os.popen(\"start.",
          "created_at": "2025-02-10T08:55:23Z"
        },
        {
          "author": "KiwiHana",
          "body": "> > 1、双击安装ipex-llm-ollama-installer-20240918.exe 默认安装在C:\\Users\\OPS17\\ipex-llm-ollama\n> > 2、拷贝ipex-llm-ollama文件夹到C:\\aipc\\\n> > 3、这个时候运行ipex-llm-ollama里面的start.bat，可以正常运行，并且访问什么都正常\n> > 4、这个时候用Python编写一个程序，并打包为exe来调用start.bat，会报缺少dnnl.dll、svml_dispmd.dll等等文件，并且无法正常运行。\n> > python 脚本就一句 os.popen(\"start.ba",
          "created_at": "2025-02-10T08:57:32Z"
        }
      ]
    },
    {
      "issue_number": 12795,
      "title": "[Enhanced] Better Intel ARC GPU Docker",
      "body": "Seeing that the project team has put in a lot of effort 👍, but it may be because of upstream and downstream relationships that users cannot use the updated software environment to run the model they want to run.\n\n\nhttps://github.com/soulteary/better-intel-ai-docker-image\n\n\nI made an updated image that might be helpful to users who want to use Linux and container.\n\nRelated Documents:\n\n- https://zhuanlan.zhihu.com/p/22365648600\n- https://zhuanlan.zhihu.com/p/22063410181\n",
      "state": "open",
      "author": "soulteary",
      "author_type": "User",
      "created_at": "2025-02-10T00:57:45Z",
      "updated_at": "2025-02-10T03:49:16Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12795/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "liu-shaojun"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12795",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12795",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:36.078489",
      "comments": [
        {
          "author": "liu-shaojun",
          "body": "Thanks for your feedback and for sharing your updated Dockerfile! We appreciate your effort in improving the user experience. We'll review your suggestions and make the necessary updates where applicable. Your input is valuable, and we’re always looking for ways to enhance compatibility and usabilit",
          "created_at": "2025-02-10T02:19:07Z"
        },
        {
          "author": "soulteary",
          "body": "Thanks for the reply, the hardware's cost-effectiveness is still very good, and the software team's continuous submissions are of great significance. At present, there are still some blocking points. Perhaps if the breakthrough is made, many users can use it.\n\nI personally look forward to the reason",
          "created_at": "2025-02-10T03:49:15Z"
        }
      ]
    },
    {
      "issue_number": 12791,
      "title": "the GPU is bieng used even though the model finished his text generation , OLLAMA",
      "body": "ipex-llm[cpp]==2.2.0b20250204\nintel-oneapi-basekit 2024.0.0.49564-3\n\niam using ollama on iGPU , some models chat the first time then they go crazy like if they are drunk , garbage output as i can say",
      "state": "open",
      "author": "gitnohubz",
      "author_type": "User",
      "created_at": "2025-02-07T17:13:20Z",
      "updated_at": "2025-02-10T02:08:42Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12791/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12791",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12791",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:36.260354",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "Please give us more details about how to reproduce your error.\nYour cpu type, OS info, model type, input&output and etc.",
          "created_at": "2025-02-08T01:08:16Z"
        },
        {
          "author": "gitnohubz",
          "body": "@qiuxin2012 \niGPU under a linux system , this is an easy issue to reproduce , use any model , take llama3.2 & chat with it & ask it , switch between models if your using openwebui , the same happens yo deepseek all qwen models , just any damn model , give me a single model that can work with ipex wi",
          "created_at": "2025-02-08T01:40:00Z"
        },
        {
          "author": "qiuxin2012",
          "body": "We are looking into it.",
          "created_at": "2025-02-10T02:08:40Z"
        }
      ]
    },
    {
      "issue_number": 12794,
      "title": "intel's page removed 2024.0 intel-oneAPI-basekit",
      "body": "cany you plz provide the offline installer link  , because its not listed anymore in intel's website , & because iam using rhel base linux , & what are the diffrent ways to install the basekit",
      "state": "closed",
      "author": "gitnohubz",
      "author_type": "User",
      "created_at": "2025-02-09T18:30:33Z",
      "updated_at": "2025-02-10T01:34:02Z",
      "closed_at": "2025-02-10T00:52:14Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12794/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12794",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12794",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:36.474114",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "We have updated our ipex-llm nightly build to OneAPI 2025 recently, but linux's readme is still updating. \nYou can use OneAPI 2025 with ipex-llm >= 2.2.0b20250207.",
          "created_at": "2025-02-10T00:46:18Z"
        },
        {
          "author": "gitnohubz",
          "body": "@qiuxin2012 so thats the case then , i well clause this issue",
          "created_at": "2025-02-10T00:52:13Z"
        }
      ]
    },
    {
      "issue_number": 12759,
      "title": "Ollama executible files error",
      "body": "Symbolic links do not actually work. When I enter init-ollama.bat, it does not create new ollama executable files, also the llama-cpp files give the error in the picture.\nI followed the steps until the step 3 in this link: https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/llama_cpp_quickstart.md#1-install-ipex-llm-for-llamacpp\nThen I proceeded with this one to start ollama: https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.md\n\nI use Windows 11 by the way.\n![Image](https://github.com/user-attachments/assets/39605cd7-0090-4e84-b7b5-c481a2d83c9a)",
      "state": "open",
      "author": "turkthebot",
      "author_type": "User",
      "created_at": "2025-01-27T23:39:38Z",
      "updated_at": "2025-02-08T02:12:57Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12759/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12759",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12759",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:36.691390",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @turkthebot, did you run the `init-ollama.bat` with administrator privileges in the Miniforge Prompt?",
          "created_at": "2025-02-05T01:38:48Z"
        },
        {
          "author": "turkthebot",
          "body": "> Hi [@turkthebot](https://github.com/turkthebot), did you run the `init-ollama.bat` with administrator privileges in the Miniforge Prompt?\n\nI did.",
          "created_at": "2025-02-05T06:52:59Z"
        },
        {
          "author": "sgwhat",
          "body": "It's wired, could you please try to reinstall `ipex-llm[cpp]==2.2.0b20250204` in a new conda env?",
          "created_at": "2025-02-07T02:06:36Z"
        },
        {
          "author": "turkthebot",
          "body": "The same issue, with those errors\n![Image](https://github.com/user-attachments/assets/7516289c-86f0-49ba-82fb-9b2a1d63e3a4)\n\n![Image](https://github.com/user-attachments/assets/eba20128-bafe-42e6-8b98-5ced0e4bd134)\n\n![Image](https://github.com/user-attachments/assets/d18266e8-7024-40d5-b4fc-aa37a3be",
          "created_at": "2025-02-07T18:39:24Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @turkthebot, we have upgraded the latest release of ipex-llm ollama to oneapi-2025, please re-install ollama in a new conda env.",
          "created_at": "2025-02-08T02:12:57Z"
        }
      ]
    },
    {
      "issue_number": 12739,
      "title": "Crash on AVX instruction on Gemini Lake",
      "body": "I'm attempting to run the `intelanalytics/ipex-llm-inference-cpp-xpu` container on an [Intel Celeron J4125](https://www.intel.com/content/www/us/en/products/sku/197305/intel-celeron-processor-j4125-4m-cache-up-to-2-70-ghz/specifications.html) (Gemini Lake) CPU with integrated UHD Graphics 600 GPU.\n\nWhen the `ollama_llama_server` program runs, it crashes due to an illegal instruction:\n\n```\ntime=2025-01-23T17:46:10.943+08:00 level=INFO source=runner.go:963 msg=\"starting go runner\"\ntime=2025-01-23T17:46:10.943+08:00 level=INFO source=runner.go:964 msg=system info=\"AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | cgo(gcc)\" threads=4\nSIGILL: illegal instruction\nPC=0x7f544807fd16 m=4 sigcode=2\nsignal arrived during cgo execution\ninstruction bytes: 0xc5 0xf9 0x6e 0xc3 0x8d 0x53 0x3 0xc5 0xf9 0xc4 0xc0 0x1 0xc5 0xf9 0xc4 0xc1\n\n(gdb) bt\n#0  0x00007ffff547fd16 in ggml_init () from /llm/ollama/libollama_ggml.so\n#1  0x00007ffff7468422 in llama_backend_init () from /llm/ollama/libollama_llama.so\n\n(gdb) x/4i $rip\n=> 0x7ffff547fd16 <ggml_init+118>:\tvmovd  %ebx,%xmm0\n   0x7ffff547fd1a <ggml_init+122>:\tlea    0x3(%rbx),%edx\n   0x7ffff547fd1d <ggml_init+125>:\tvpinsrw $0x1,%eax,%xmm0,%xmm0\n   0x7ffff547fd22 <ggml_init+130>:\tvpinsrw $0x2,%ecx,%xmm0,%xmm0\n```\n\nIt looks like `libollama_ggml.so` is compiled with AVX instructions, but my processor doesn't support AVX (despite the log message that says \"AVX = 1\").\n\n```\n$ cat /proc/cpuinfo\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave rdrand lahf_lm 3dnowprefetch intel_pt ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust smep erms mpx rdseed smap clflushopt sha_ni xsaveopt xsavec xgetbv1 dtherm ida arat pln pts rdpid md_clear arch_capabilities\n```\n\nDo you intend to support CPUs without AVX? Or where can I find out how the files under `/usr/local/lib/python3.11/dist-packages/bigdl/cpp/libs/` are compiled so I can rebuild them without AVX instructions?",
      "state": "open",
      "author": "rgov",
      "author_type": "User",
      "created_at": "2025-01-23T10:02:30Z",
      "updated_at": "2025-02-06T14:07:31Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12739/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12739",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12739",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:36.878120",
      "comments": [
        {
          "author": "rgov",
          "body": "\nIt looks like this was tracked by https://github.com/ollama/ollama/issues/2187 and was fixed as of v0.5.2, whereas the container currently ships with v0.5.1.\n\nThere's a related commit in Ollama: https://github.com/ollama/ollama/commit/667a2ba18add1031cc4b208eba7cedf8b33548e6:\n\n> We build the GPU li",
          "created_at": "2025-01-23T19:18:02Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @rgov , we are releasing IPEX-LLM Ollama v0.5.4, and we will inform you once it is completed.",
          "created_at": "2025-01-24T02:47:10Z"
        },
        {
          "author": "rgov",
          "body": "Thanks @sgwhat. You would have to build Ollama with `CUSTOM_CPU_FLAGS=''` to build a runner without the AVX instructions. I'm not sure the invocation to build multiple runners with different processor features.",
          "created_at": "2025-01-24T08:42:45Z"
        },
        {
          "author": "rgov",
          "body": "I don't think it's going to work on Gemini Lake anyway, the oneAPI requires \"Intel® UHD Graphics for 11th generation Intel processors or newer.\" Looks like this processor is generation \"9.5\" according to Wikipedia.",
          "created_at": "2025-01-24T08:49:43Z"
        }
      ]
    },
    {
      "issue_number": 12740,
      "title": "Build, rather than download, dependencies",
      "body": "The `python/llm/setup.py` file downloads binaries from SourceForge:\n\nhttps://github.com/intel-analytics/ipex-llm/blob/69f13c78b8d078b9e0beadcda5e3583a8db65fed/python/llm/setup.py#L153-L154\n\nIt is not clear how these dependencies are built, and it would be good for users to be able to recompile them, e.g., for different platforms, for debugging, to apply custom build options or patches, for security review, etc.\n",
      "state": "open",
      "author": "rgov",
      "author_type": "User",
      "created_at": "2025-01-23T21:11:06Z",
      "updated_at": "2025-02-06T14:07:16Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12740/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Oscilloscope98"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12740",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12740",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:42.093360",
      "comments": [
        {
          "author": "Oscilloscope98",
          "body": "Hi @rgov,\n\nCurrently, building these dependencies through `setup.py` is not supported.",
          "created_at": "2025-01-24T02:55:18Z"
        },
        {
          "author": "rgov",
          "body": "Do you support users building dependencies in any other way? It's unclear whether there are Intel-specific patches to these projects. If there are patches, are they open source?",
          "created_at": "2025-01-24T03:02:30Z"
        }
      ]
    },
    {
      "issue_number": 12753,
      "title": "useful docker image that can run on windows",
      "body": "Looking for a docker image that can run on windows just flat out. Struggling with the docs on windows, and this would make it much easier..\n\n`docker run -it --net=bridge --device=/dev/dri -p 11434:11434 -v E:\\AI_Models:/root/.ollama/models -e PATH=/llm/ollama:$PATH -e OLLAMA_HOST=0.0.0.0 -e no_proxy=localhost,127.0.0.1 -e ZES_ENABLE_SYSMAN=1 -e OLLAMA_INTEL_GPU=true -e ONEAPI_DEVICE_SELECTOR=level_zero:0 -e DEVICE=Arc --shm-size=\"16g\" --memory=\"32G\" --name=ipex-llm intelanalytics/ipex-llm-xpu:latest\t`\n\nAlways fails.\n\n\n",
      "state": "open",
      "author": "majerus1223",
      "author_type": "User",
      "created_at": "2025-01-26T01:45:07Z",
      "updated_at": "2025-02-06T14:06:54Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12753/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12753",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12753",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:42.274304",
      "comments": [
        {
          "author": "glorysdj",
          "body": "Could you please share the failure's output?\nPlease refer to https://github.com/intel/ipex-llm/blob/main/docs/mddocs/DockerGuides/docker_windows_gpu.md\nand https://github.com/intel/ipex-llm/blob/main/docs/mddocs/DockerGuides/docker_pytorch_inference_gpu.md\n",
          "created_at": "2025-01-26T02:12:22Z"
        },
        {
          "author": "majerus1223",
          "body": "```PS C:\\Users\\james> docker run -it --net=bridge --device=/dev/dri -p 11434:11434 -v E:\\AI_Models:/root/.ollama/models -e PATH=/llm/ollama:$PATH -e OLLAMA_HOST=0.0.0.0 -e no_proxy=localhost,127.0.0.1 -e ZES_ENABLE_SYSMAN=1 -e OLLAMA_INTEL_GPU=true -e ONEAPI_DEVICE_SELECTOR=level_zero:0 -e DEVICE=Ar",
          "created_at": "2025-01-26T02:53:35Z"
        },
        {
          "author": "majerus1223",
          "body": "> Could you please share the failure's output? Please refer to https://github.com/intel/ipex-llm/blob/main/docs/mddocs/DockerGuides/docker_windows_gpu.md and https://github.com/intel/ipex-llm/blob/main/docs/mddocs/DockerGuides/docker_pytorch_inference_gpu.md\n\nPosted the errors above. The docs are co",
          "created_at": "2025-01-26T02:56:03Z"
        },
        {
          "author": "majerus1223",
          "body": "Finally making progress.. \n\n```\nroot@83359eb27245:/llm# sycl-ls\nINFO: Output filtered by ONEAPI_DEVICE_SELECTOR environment variable, which is set to level_zero:0.\nTo see device ids, use the --ignore-device-selectors CLI option.\n\n[level_zero:gpu] Intel(R) Level-Zero, Intel(R) Graphics [0x56a0] 1.6 [",
          "created_at": "2025-01-26T09:06:17Z"
        },
        {
          "author": "majerus1223",
          "body": "Still no luck, not really sure what the deal is, see a lot of errors like .\n```\n\ntime=2025-01-27T02:03:42.448+08:00 level=INFO source=gpu.go:221 msg=\"looking for compatible GPUs\"\ntime=2025-01-27T02:03:42.449+08:00 level=WARN source=gpu.go:732 msg=\"unable to locate gpu dependency libraries\"\ntime=2025",
          "created_at": "2025-01-26T18:17:50Z"
        }
      ]
    },
    {
      "issue_number": 12772,
      "title": "Intel B580 -> not able to run Ollama serve on GPU after following guide",
      "body": "Intel B580 -> not able to run Ollama serve on GPU after following guide\n\nhttps://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/bmg_quickstart.md#32-ollama\nhttps://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.md\n\n![Image](https://github.com/user-attachments/assets/108ff1fe-2cca-4d16-9164-ea3948061bb0)\n\n![Image](https://github.com/user-attachments/assets/82a25dc3-afd4-4a54-981f-d5f999b6f381)\n",
      "state": "open",
      "author": "Mushtaq-BGA",
      "author_type": "User",
      "created_at": "2025-02-05T15:31:43Z",
      "updated_at": "2025-02-06T07:48:23Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12772/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "liu-shaojun"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12772",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12772",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:42.513561",
      "comments": [
        {
          "author": "liu-shaojun",
          "body": "Hi @Mushtaq-BGA \n\nBased on your screenshot, the output from `ollama serve` looks normal. Could you share the exact error message you're encountering? Alternatively, you can try running the following commands to see if everything works correctly:  \n\n```sh\n./ollama pull <model_name>\n./ollama run <mode",
          "created_at": "2025-02-06T02:07:45Z"
        },
        {
          "author": "Mushtaq-BGA",
          "body": "![Image](https://github.com/user-attachments/assets/e11621e9-96c5-4369-9928-cc8836a11fbd)",
          "created_at": "2025-02-06T03:44:15Z"
        },
        {
          "author": "liu-shaojun",
          "body": "Debugged the issue on the user's machine, and the root cause was that he had oneAPI version 2024.0 installed. After switching to oneAPI 2024.2, it worked normally.",
          "created_at": "2025-02-06T07:48:21Z"
        }
      ]
    },
    {
      "issue_number": 12762,
      "title": "[Windows-MTL-NPU]: OSError: [WinError -529697949] Windows Error 0xe06d7363",
      "body": "**Device**: MTL Core Ultra 165U\n**OS:** Windows 11\n============\n\n**Objective:** tried to run the llm model on NPU using ipex-llm, and got following error:\n\n```\npython llama3.py --repo-id-or-model-path \"meta-llama/Llama-3.2-1B-Instruct\" --save-directory \"llama-3.2-1B-Inst\"\n2025-01-29 16:58:29,747 - INFO - Converting model, it may takes up to several minutes ...\n2025-01-29 16:58:39,059 - INFO - Finish to convert model\nstart compiling\nModel saved to llama-3.2-1B-Inst\\lm_head.xml\nstart compiling\nstart compiling\nModel saved to llama-3.2-1B-Inst\\embedding_post.xml\nstart compiling\nModel saved to llama-3.2-1B-Inst\\embedding_post_prefill.xml\nstart compiling\nModel saved to llama-3.2-1B-Inst\\decoder_layer_0.xml\nstart compiling\nModel saved to llama-3.2-1B-Inst\\decoder_layer_1.xml\n**start compiling\nTraceback (most recent call last):\n  File \"C:\\Users\\intel\\ritu\\ipex-llm\\python\\llm\\example\\NPU\\HF-Transformers-AutoModels\\LLM\\llama3.py\", line 73, in <module>\n    model = AutoModelForCausalLM.from_pretrained(**\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\intel\\miniforge3\\envs\\llm-npu\\Lib\\unittest\\mock.py\", line 1378, in patched\n    return func(*newargs, **newkeywargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\intel\\miniforge3\\envs\\llm-npu\\Lib\\site-packages\\ipex_llm\\transformers\\npu_model.py\", line 243, in from_pretrained\n    model = cls.optimize_npu_model(*args, **optimize_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\intel\\miniforge3\\envs\\llm-npu\\Lib\\site-packages\\ipex_llm\\transformers\\npu_model.py\", line 317, in optimize_npu_model\n    optimize_llm_single_process(\n  File \"C:\\Users\\intel\\miniforge3\\envs\\llm-npu\\Lib\\site-packages\\ipex_llm\\transformers\\npu_models\\convert.py\", line 458, in optimize_llm_single_process\n    convert_llm(model,\n  File \"C:\\Users\\intel\\miniforge3\\envs\\llm-npu\\Lib\\site-packages\\ipex_llm\\transformers\\npu_pipeline_model\\convert_pipeline.py\", line 215, in convert_llm\n    convert_llm_for_deploy(model,\n  File \"C:\\Users\\intel\\miniforge3\\envs\\llm-npu\\Lib\\site-packages\\ipex_llm\\transformers\\npu_pipeline_model\\convert_pipeline.py\", line 549, in convert_llm_for_deploy\n    convert_llama_layer(model, 0, n_splits_linear, n_splits_down_proj,\n  File \"C:\\Users\\intel\\miniforge3\\envs\\llm-npu\\Lib\\site-packages\\ipex_llm\\transformers\\npu_pipeline_model\\llama.py\", line 294, in convert_llama_layer\n    single_decoder = LowBitLlamaMultiDecoderlayer(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\intel\\miniforge3\\envs\\llm-npu\\Lib\\site-packages\\ipex_llm\\transformers\\npu_models\\llama_mp.py\", line 216, in __init__\n    self.compile(npu_dpu_groups=6)\n  File \"C:\\Users\\intel\\miniforge3\\envs\\llm-npu\\Lib\\site-packages\\intel_npu_acceleration_library\\backend\\factory.py\", line 1054, in compile\n    backend_lib.compile(self._mm, npu_dpu_groups)\n**OSError: [WinError -529697949] Windows Error 0xe06d7363**\n```",
      "state": "open",
      "author": "raj-ritu17",
      "author_type": "User",
      "created_at": "2025-01-29T17:11:52Z",
      "updated_at": "2025-02-06T02:10:47Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12762/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "plusbang"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12762",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12762",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:42.739505",
      "comments": [
        {
          "author": "Goldlionren",
          "body": "I met the same issue last weekend. finally, I find a way to get it running. Use git to clone the entire repo from hugging fack to local drive, like D:\\llm-npn\\Llama-3.2-1B-Instruct, and create another empty folder D:\\llm-npn\\Llama-3.2-1B-Instruct-lowbit, then run this command as: python llama3.py --",
          "created_at": "2025-01-30T11:18:50Z"
        },
        {
          "author": "plusbang",
          "body": "Hi @raj-ritu17 , could you please provide your NPU driver version and ipex-llm version?\n\nAs introduced in our doc (https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/npu_quickstart.md), **`32.0.100.3104` driver is highly recommended and `set IPEX_LLM_NPU_MTL=1` is necessary for MTL**",
          "created_at": "2025-02-05T02:18:52Z"
        }
      ]
    },
    {
      "issue_number": 12763,
      "title": "Error while loading shared libraries: libsycl.so.7: cannot open shared object file: No such file or directory",
      "body": "Hi guys, when I try to execute ollama it gives me this error:\n\n`(llm_env) root@Ollama:/home/gcesano/llama-cpp# ./ollama serve\n./ollama: error while loading shared libraries: libsycl.so.7: cannot open shared object file: No such file or directory\n`\nI think that IPEX-LLM supports the old version of this library \"libsycl.so.x\", because, if I list my intel oneapi libraries I can find only the new version .8:\n\n**`lrwxrwxrwx 1 root root        14  5 dic 15.19 libsycl.so -> ./libsycl.so.8\nlrwxrwxrwx 1 root root        18  5 dic 15.19 libsycl.so.8 -> ./libsycl.so.8.0.0\n-rwxr-xr-x 1 root root   4559928  5 dic 11.18 libsycl.so.8.0.0**\n-rwxr-xr-x 1 root root     41806  5 dic 11.18 libsycl.so.8.0.0-gdb.py\n-rwxr-xr-x 1 root root    235208  5 dic 11.18 libsycl_ur_trace_collector.so\nlrwxrwxrwx 1 root root        39  5 dic 15.32 libtcm_debug.so -> ../../../tcm/latest/lib/li`\n\nAm I missing something?",
      "state": "open",
      "author": "zSkallywag",
      "author_type": "User",
      "created_at": "2025-01-31T08:55:07Z",
      "updated_at": "2025-02-06T02:08:38Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12763/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12763",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12763",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:43.009677",
      "comments": [
        {
          "author": "cyear",
          "body": "Maybe your Intel® oneAPI Base Toolkit version is too new. Have you installed Version 2025.0.1?",
          "created_at": "2025-02-03T11:42:21Z"
        },
        {
          "author": "zSkallywag",
          "body": "> Maybe your Intel® oneAPI Base Toolkit version is too new. Have you installed Version 2025.0.1?\n\nYes I have installed the last one which you can find on the official site. I tried to look up for other previous versions but i couldn't find none. I don't know if you a link you can share about previou",
          "created_at": "2025-02-03T13:44:28Z"
        },
        {
          "author": "cyear",
          "body": "> > Maybe your Intel® oneAPI Base Toolkit version is too new. Have you installed Version 2025.0.1?\n> \n> Yes I have installed the last one which you can find on the official site. I tried to look up for other previous versions but i couldn't find none. I don't know if you a link you can share about p",
          "created_at": "2025-02-03T13:49:20Z"
        },
        {
          "author": "zSkallywag",
          "body": "> > > Maybe your Intel® oneAPI Base Toolkit version is too new. Have you installed Version 2025.0.1?\n> > \n> > \n> > Yes I have installed the last one which you can find on the official site. I tried to look up for other previous versions but i couldn't find none. I don't know if you a link you can sh",
          "created_at": "2025-02-03T13:54:51Z"
        },
        {
          "author": "jason-dai",
          "body": "> > Maybe your Intel® oneAPI Base Toolkit version is too new. Have you installed Version 2025.0.1?\n> \n> Yes I have installed the last one which you can find on the official site. I tried to look up for other previous versions but i couldn't find none. I don't know if you a link you can share about p",
          "created_at": "2025-02-04T02:05:55Z"
        }
      ]
    },
    {
      "issue_number": 12765,
      "title": "ARC A730M (PI_ERROR_BUILD_PROGRAM_FAILURE)",
      "body": "\n## Follow the documentation [在Intel GPU上使用IPEX-LLM运行Ollama](https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.zh-CN.md#1-%E5%AE%89%E8%A3%85-ipex-llm-%E6%9D%A5%E4%BD%BF%E7%94%A8-Ollama)\n\n```\nexport OLLAMA_NUM_GPU=999\nexport no_proxy=localhost,127.0.0.1\nexport ZES_ENABLE_SYSMAN=1\n\nsource /opt/intel/oneapi/setvars.sh\nexport SYCL_CACHE_PERSISTENT=1\n# [optional] under most circumstances, the following environment variable may improve performance, but sometimes this may also cause performance degradation\nexport SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1\n# [optional] if you want to run on single GPU, use below command to limit GPU may improve performance\nexport ONEAPI_DEVICE_SELECTOR=level_zero:0\n\n./ollama serve\n```\n# \n\n```\n:: initializing oneAPI environment ...\n   zsh: ZSH_VERSION = 5.9\n   args: Using \"$@\" for setvars.sh arguments: advisor=latest ccl=latest compiler=latest dal=latest debugger=latest dev-utilities=latest dnnl=latest dpcpp-ct=latest dpl=latest ipp=latest ippcp=latest mkl=latest mpi=latest tbb=latest vtune=latest\n:: advisor -- latest\n:: ccl -- latest\n:: compiler -- latest\n:: dal -- latest\n:: debugger -- latest\n:: dev-utilities -- latest\n:: dnnl -- latest\n:: dpcpp-ct -- latest\n:: dpl -- latest\n:: ipp -- latest\n:: ippcp -- latest\n:: mkl -- latest\n:: mpi -- latest\n:: tbb -- latest\n:: vtune -- latest\n:: oneAPI environment initialized ::\n \n2025/02/03 16:24:18 routes.go:1194: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/nian/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:localhost,127.0.0.1]\"\ntime=2025-02-03T16:24:18.303+08:00 level=INFO source=images.go:753 msg=\"total blobs: 11\"\ntime=2025-02-03T16:24:18.303+08:00 level=INFO source=images.go:760 msg=\"total unused blobs removed: 0\"\n[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.\n\n[GIN-debug] [WARNING] Running in \"debug\" mode. Switch to \"release\" mode in production.\n - using env:\texport GIN_MODE=release\n - using code:\tgin.SetMode(gin.ReleaseMode)\n\n[GIN-debug] POST   /api/pull                 --> ollama/server.(*Server).PullHandler-fm (5 handlers)\n[GIN-debug] POST   /api/generate             --> ollama/server.(*Server).GenerateHandler-fm (5 handlers)\n[GIN-debug] POST   /api/chat                 --> ollama/server.(*Server).ChatHandler-fm (5 handlers)\n[GIN-debug] POST   /api/embed                --> ollama/server.(*Server).EmbedHandler-fm (5 handlers)\n[GIN-debug] POST   /api/embeddings           --> ollama/server.(*Server).EmbeddingsHandler-fm (5 handlers)\n[GIN-debug] POST   /api/create               --> ollama/server.(*Server).CreateHandler-fm (5 handlers)\n[GIN-debug] POST   /api/push                 --> ollama/server.(*Server).PushHandler-fm (5 handlers)\n[GIN-debug] POST   /api/copy                 --> ollama/server.(*Server).CopyHandler-fm (5 handlers)\n[GIN-debug] DELETE /api/delete               --> ollama/server.(*Server).DeleteHandler-fm (5 handlers)\n[GIN-debug] POST   /api/show                 --> ollama/server.(*Server).ShowHandler-fm (5 handlers)\n[GIN-debug] POST   /api/blobs/:digest        --> ollama/server.(*Server).CreateBlobHandler-fm (5 handlers)\n[GIN-debug] HEAD   /api/blobs/:digest        --> ollama/server.(*Server).HeadBlobHandler-fm (5 handlers)\n[GIN-debug] GET    /api/ps                   --> ollama/server.(*Server).PsHandler-fm (5 handlers)\n[GIN-debug] POST   /v1/chat/completions      --> ollama/server.(*Server).ChatHandler-fm (6 handlers)\n[GIN-debug] POST   /v1/completions           --> ollama/server.(*Server).GenerateHandler-fm (6 handlers)\n[GIN-debug] POST   /v1/embeddings            --> ollama/server.(*Server).EmbedHandler-fm (6 handlers)\n[GIN-debug] GET    /v1/models                --> ollama/server.(*Server).ListHandler-fm (6 handlers)\n[GIN-debug] GET    /v1/models/:model         --> ollama/server.(*Server).ShowHandler-fm (6 handlers)\n[GIN-debug] GET    /                         --> ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\n[GIN-debug] GET    /api/tags                 --> ollama/server.(*Server).ListHandler-fm (5 handlers)\n[GIN-debug] GET    /api/version              --> ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\n[GIN-debug] HEAD   /                         --> ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\n[GIN-debug] HEAD   /api/tags                 --> ollama/server.(*Server).ListHandler-fm (5 handlers)\n[GIN-debug] HEAD   /api/version              --> ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\ntime=2025-02-03T16:24:18.304+08:00 level=INFO source=routes.go:1245 msg=\"Listening on 127.0.0.1:11434 (version 0.5.1-ipexllm-20250123)\"\ntime=2025-02-03T16:24:18.304+08:00 level=INFO source=common.go:135 msg=\"extracting embedded files\" dir=/tmp/ollama3579327106/runners\ntime=2025-02-03T16:24:18.396+08:00 level=INFO source=common.go:49 msg=\"Dynamic LLM libraries\" runners=[ipex_llm]\n[GIN] 2025/02/03 - 16:24:23 | 200 |      55.274µs |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/02/03 - 16:24:23 | 200 |   33.486176ms |       127.0.0.1 | POST     \"/api/show\"\ntime=2025-02-03T16:24:23.332+08:00 level=INFO source=gpu.go:221 msg=\"looking for compatible GPUs\"\ntime=2025-02-03T16:24:23.332+08:00 level=WARN source=gpu.go:732 msg=\"unable to locate gpu dependency libraries\"\ntime=2025-02-03T16:24:23.335+08:00 level=WARN source=gpu.go:732 msg=\"unable to locate gpu dependency libraries\"\ntime=2025-02-03T16:24:23.335+08:00 level=WARN source=gpu.go:732 msg=\"unable to locate gpu dependency libraries\"\ntime=2025-02-03T16:24:23.347+08:00 level=WARN source=gpu.go:732 msg=\"unable to locate gpu dependency libraries\"\ntime=2025-02-03T16:24:23.411+08:00 level=INFO source=server.go:105 msg=\"system memory\" total=\"30.9 GiB\" free=\"13.0 GiB\" free_swap=\"13.3 GiB\"\ntime=2025-02-03T16:24:23.412+08:00 level=INFO source=memory.go:356 msg=\"offload to device\" layers.requested=-1 layers.model=33 layers.offload=0 layers.split=\"\" memory.available=\"[13.1 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"6.0 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"1.0 GiB\" memory.required.allocations=\"[6.0 GiB]\" memory.weights.total=\"4.9 GiB\" memory.weights.repeating=\"4.5 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"560.0 MiB\" memory.graph.partial=\"677.5 MiB\"\ntime=2025-02-03T16:24:23.416+08:00 level=INFO source=server.go:401 msg=\"starting llama server\" cmd=\"/tmp/ollama3579327106/runners/ipex_llm/ollama_llama_server --model /home/nian/.ollama/models/blobs/sha256-6340dc3229b0d08ea9cc49b75d4098702983e17b4c096d57afbbf2ffc813f2be --ctx-size 8192 --batch-size 512 --n-gpu-layers 999 --threads 6 --no-mmap --parallel 4 --port 33109\"\ntime=2025-02-03T16:24:23.416+08:00 level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\ntime=2025-02-03T16:24:23.416+08:00 level=INFO source=server.go:580 msg=\"waiting for llama runner to start responding\"\ntime=2025-02-03T16:24:23.417+08:00 level=INFO source=server.go:614 msg=\"waiting for server to become available\" status=\"llm server error\"\ntime=2025-02-03T16:24:23.515+08:00 level=INFO source=runner.go:963 msg=\"starting go runner\"\ntime=2025-02-03T16:24:23.516+08:00 level=INFO source=runner.go:964 msg=system info=\"AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | cgo(gcc)\" threads=6\ntime=2025-02-03T16:24:23.516+08:00 level=INFO source=.:0 msg=\"Server listening on 127.0.0.1:33109\"\nllama_model_loader: loaded meta data with 28 key-value pairs and 292 tensors from /home/nian/.ollama/models/blobs/sha256-6340dc3229b0d08ea9cc49b75d4098702983e17b4c096d57afbbf2ffc813f2be (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Llama 8B\nllama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Llama\nllama_model_loader: - kv   4:                         general.size_label str              = 8B\nllama_model_loader: - kv   5:                          llama.block_count u32              = 32\nllama_model_loader: - kv   6:                       llama.context_length u32              = 131072\nllama_model_loader: - kv   7:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   8:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv   9:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  10:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\nllama_model_loader: - kv  12:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  13:                          general.file_type u32              = 15\nllama_model_loader: - kv  14:                           llama.vocab_size u32              = 128256\nllama_model_loader: - kv  15:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = llama-bpe\nllama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\ntime=2025-02-03T16:24:23.668+08:00 level=INFO source=server.go:614 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nllama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\nllama_model_loader: - kv  21:                tokenizer.ggml.bos_token_id u32              = 128000\nllama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 128001\nllama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 128001\nllama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  26:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\nllama_model_loader: - kv  27:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   66 tensors\nllama_model_loader: - type q4_K:  193 tensors\nllama_model_loader: - type q6_K:   33 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 256\nllm_load_vocab: token to piece cache size = 0.7999 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 128256\nllm_load_print_meta: n_merges         = 280147\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 131072\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 500000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 131072\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 8B\nllm_load_print_meta: model ftype      = Q4_K - Medium\nllm_load_print_meta: model params     = 8.03 B\nllm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \nllm_load_print_meta: general.name     = DeepSeek R1 Distill Llama 8B\nllm_load_print_meta: BOS token        = 128000 '<｜begin▁of▁sentence｜>'\nllm_load_print_meta: EOS token        = 128001 '<｜end▁of▁sentence｜>'\nllm_load_print_meta: PAD token        = 128001 '<｜end▁of▁sentence｜>'\nllm_load_print_meta: LF token         = 128 'Ä'\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\nllm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\nllm_load_print_meta: EOG token        = 128001 '<｜end▁of▁sentence｜>'\nllm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\nllm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\nllm_load_print_meta: max token length = 256\nAbort was called at 1073 line in file:\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\nggml_sycl_init: SYCL_USE_XMX: yes\nggml_sycl_init: found 1 SYCL devices:\n/var/tmp/portage/dev-libs/intel-compute-runtime-24.35.30872.32/work/compute-runtime-24.35.30872.32/shared/source/os_interface/linux/drm_neo.cpp\nSIGABRT: abort\nPC=0x7f6de04a5bcc m=5 sigcode=18446744073709551610\nsignal arrived during cgo execution\n\ngoroutine 19 gp=0xc000186540 m=5 mp=0xc000100008 [syscall]:\nruntime.cgocall(0x55cab7f0fab0, 0xc000094b90)\n\truntime/cgocall.go:157 +0x4b fp=0xc000094b68 sp=0xc000094b30 pc=0x55cab7c904eb\nollama/llama/llamafile._Cfunc_llama_load_model_from_file(0x7f6d78000b70, {0x3e7, 0x1, 0x0, 0x0, 0x0, 0x55cab7f0f4a0, 0xc0001a4258, 0x0, 0x0, ...})\n\t_cgo_gotypes.go:692 +0x50 fp=0xc000094b90 sp=0xc000094b68 pc=0x55cab7d8e390\nollama/llama/llamafile.LoadModelFromFile.func1({0x7ffe75b905fb?, 0x0?}, {0x3e7, 0x1, 0x0, 0x0, 0x0, 0x55cab7f0f4a0, 0xc0001a4258, 0x0, ...})\n\tollama/llama/llamafile/llama.go:228 +0xfa fp=0xc000094c78 sp=0xc000094b90 pc=0x55cab7d90c9a\nollama/llama/llamafile.LoadModelFromFile({0x7ffe75b905fb, 0x67}, {0x3e7, 0x0, 0x0, 0x0, {0x0, 0x0, 0x0}, 0xc000192160, ...})\n\tollama/llama/llamafile/llama.go:228 +0x2d5 fp=0xc000094db8 sp=0xc000094c78 pc=0x55cab7d909d5\nmain.(*Server).loadModel(0xc0001c2120, {0x3e7, 0x0, 0x0, 0x0, {0x0, 0x0, 0x0}, 0xc000192160, 0x0}, ...)\n\tollama/llama/runner/runner.go:861 +0xc5 fp=0xc000094f10 sp=0xc000094db8 pc=0x55cab7f0cfe5\nmain.main.gowrap1()\n\tollama/llama/runner/runner.go:997 +0xda fp=0xc000094fe0 sp=0xc000094f10 pc=0x55cab7f0ea1a\nruntime.goexit({})\n\truntime/asm_amd64.s:1695 +0x1 fp=0xc000094fe8 sp=0xc000094fe0 pc=0x55cab7cf8f01\ncreated by main.main in goroutine 1\n\tollama/llama/runner/runner.go:997 +0xc6c\n\ngoroutine 1 gp=0xc0000061c0 m=nil [IO wait]:\nruntime.gopark(0xc00004ca08?, 0x0?, 0xc0?, 0x61?, 0xc000041898?)\n\truntime/proc.go:402 +0xce fp=0xc000041860 sp=0xc000041840 pc=0x55cab7cc712e\nruntime.netpollblock(0xc0000418f8?, 0xb7c8fc46?, 0xca?)\n\truntime/netpoll.go:573 +0xf7 fp=0xc000041898 sp=0xc000041860 pc=0x55cab7cbf377\ninternal/poll.runtime_pollWait(0x7f6de0c99770, 0x72)\n\truntime/netpoll.go:345 +0x85 fp=0xc0000418b8 sp=0xc000041898 pc=0x55cab7cf3bc5\ninternal/poll.(*pollDesc).wait(0x3?, 0x3fe?, 0x0)\n\tinternal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc0000418e0 sp=0xc0000418b8 pc=0x55cab7d43ae7\ninternal/poll.(*pollDesc).waitRead(...)\n\tinternal/poll/fd_poll_runtime.go:89\ninternal/poll.(*FD).Accept(0xc0001fe080)\n\tinternal/poll/fd_unix.go:611 +0x2ac fp=0xc000041988 sp=0xc0000418e0 pc=0x55cab7d44fac\nnet.(*netFD).accept(0xc0001fe080)\n\tnet/fd_unix.go:172 +0x29 fp=0xc000041a40 sp=0xc000041988 pc=0x55cab7db3bc9\nnet.(*TCPListener).accept(0xc0001c41c0)\n\tnet/tcpsock_posix.go:159 +0x1e fp=0xc000041a68 sp=0xc000041a40 pc=0x55cab7dc48fe\nnet.(*TCPListener).Accept(0xc0001c41c0)\n\tnet/tcpsock.go:327 +0x30 fp=0xc000041a98 sp=0xc000041a68 pc=0x55cab7dc3c50\nnet/http.(*onceCloseListener).Accept(0xc0001c21b0?)\n\t<autogenerated>:1 +0x24 fp=0xc000041ab0 sp=0xc000041a98 pc=0x55cab7eeae64\nnet/http.(*Server).Serve(0xc00021a000, {0x55cab8216540, 0xc0001c41c0})\n\tnet/http/server.go:3260 +0x33e fp=0xc000041be0 sp=0xc000041ab0 pc=0x55cab7ee1c7e\nmain.main()\n\tollama/llama/runner/runner.go:1022 +0x10cd fp=0xc000041f50 sp=0xc000041be0 pc=0x55cab7f0e68d\nruntime.main()\n\truntime/proc.go:271 +0x29d fp=0xc000041fe0 sp=0xc000041f50 pc=0x55cab7cc6cfd\nruntime.goexit({})\n\truntime/asm_amd64.s:1695 +0x1 fp=0xc000041fe8 sp=0xc000041fe0 pc=0x55cab7cf8f01\n\ngoroutine 2 gp=0xc000006c40 m=nil [force gc (idle)]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:402 +0xce fp=0xc000084fa8 sp=0xc000084f88 pc=0x55cab7cc712e\nruntime.goparkunlock(...)\n\truntime/proc.go:408\nruntime.forcegchelper()\n\truntime/proc.go:326 +0xb8 fp=0xc000084fe0 sp=0xc000084fa8 pc=0x55cab7cc6fb8\nruntime.goexit({})\n\truntime/asm_amd64.s:1695 +0x1 fp=0xc000084fe8 sp=0xc000084fe0 pc=0x55cab7cf8f01\ncreated by runtime.init.6 in goroutine 1\n\truntime/proc.go:314 +0x1a\n\ngoroutine 3 gp=0xc000007180 m=nil [GC sweep wait]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:402 +0xce fp=0xc000085780 sp=0xc000085760 pc=0x55cab7cc712e\nruntime.goparkunlock(...)\n\truntime/proc.go:408\nruntime.bgsweep(0xc000038070)\n\truntime/mgcsweep.go:278 +0x94 fp=0xc0000857c8 sp=0xc000085780 pc=0x55cab7cb1c74\nruntime.gcenable.gowrap1()\n\truntime/mgc.go:203 +0x25 fp=0xc0000857e0 sp=0xc0000857c8 pc=0x55cab7ca67a5\nruntime.goexit({})\n\truntime/asm_amd64.s:1695 +0x1 fp=0xc0000857e8 sp=0xc0000857e0 pc=0x55cab7cf8f01\ncreated by runtime.gcenable in goroutine 1\n\truntime/mgc.go:203 +0x66\n\ngoroutine 4 gp=0xc000007340 m=nil [GC scavenge wait]:\nruntime.gopark(0xc000038070?, 0x55cab7f8e308?, 0x1?, 0x0?, 0xc000007340?)\n\truntime/proc.go:402 +0xce fp=0xc000085f78 sp=0xc000085f58 pc=0x55cab7cc712e\nruntime.goparkunlock(...)\n\truntime/proc.go:408\nruntime.(*scavengerState).park(0x55cab83e0680)\n\truntime/mgcscavenge.go:425 +0x49 fp=0xc000085fa8 sp=0xc000085f78 pc=0x55cab7caf669\nruntime.bgscavenge(0xc000038070)\n\truntime/mgcscavenge.go:653 +0x3c fp=0xc000085fc8 sp=0xc000085fa8 pc=0x55cab7cafbfc\nruntime.gcenable.gowrap2()\n\truntime/mgc.go:204 +0x25 fp=0xc000085fe0 sp=0xc000085fc8 pc=0x55cab7ca6745\nruntime.goexit({})\n\truntime/asm_amd64.s:1695 +0x1 fp=0xc000085fe8 sp=0xc000085fe0 pc=0x55cab7cf8f01\ncreated by runtime.gcenable in goroutine 1\n\truntime/mgc.go:204 +0xa5\n\ngoroutine 18 gp=0xc000186380 m=nil [finalizer wait]:\nruntime.gopark(0xc000084648?, 0x55cab7c9a0a5?, 0xa8?, 0x1?, 0x55cab82107a0?)\n\truntime/proc.go:402 +0xce fp=0xc000084620 sp=0xc000084600 pc=0x55cab7cc712e\nruntime.runfinq()\n\truntime/mfinal.go:194 +0x107 fp=0xc0000847e0 sp=0xc000084620 pc=0x55cab7ca57e7\nruntime.goexit({})\n\truntime/asm_amd64.s:1695 +0x1 fp=0xc0000847e8 sp=0xc0000847e0 pc=0x55cab7cf8f01\ncreated by runtime.createfing in goroutine 1\n\truntime/mfinal.go:164 +0x3d\n\ngoroutine 20 gp=0xc000186700 m=nil [semacquire]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x60?, 0x0?)\n\truntime/proc.go:402 +0xce fp=0xc000080e08 sp=0xc000080de8 pc=0x55cab7cc712e\nruntime.goparkunlock(...)\n\truntime/proc.go:408\nruntime.semacquire1(0xc0001c2128, 0x0, 0x1, 0x0, 0x12)\n\truntime/sema.go:160 +0x22c fp=0xc000080e70 sp=0xc000080e08 pc=0x55cab7cd954c\nsync.runtime_Semacquire(0x0?)\n\truntime/sema.go:62 +0x25 fp=0xc000080ea8 sp=0xc000080e70 pc=0x55cab7cf5385\nsync.(*WaitGroup).Wait(0x0?)\n\tsync/waitgroup.go:116 +0x48 fp=0xc000080ed0 sp=0xc000080ea8 pc=0x55cab7d13e08\nmain.(*Server).run(0xc0001c2120, {0x55cab8216b80, 0xc0001800a0})\n\tollama/llama/runner/runner.go:315 +0x47 fp=0xc000080fb8 sp=0xc000080ed0 pc=0x55cab7f096a7\nmain.main.gowrap2()\n\tollama/llama/runner/runner.go:1002 +0x28 fp=0xc000080fe0 sp=0xc000080fb8 pc=0x55cab7f0e908\nruntime.goexit({})\n\truntime/asm_amd64.s:1695 +0x1 fp=0xc000080fe8 sp=0xc000080fe0 pc=0x55cab7cf8f01\ncreated by main.main in goroutine 1\n\tollama/llama/runner/runner.go:1002 +0xd3e\n\ngoroutine 21 gp=0xc0001868c0 m=nil [IO wait]:\nruntime.gopark(0x94?, 0xc0001ef958?, 0x40?, 0xf9?, 0xb?)\n\truntime/proc.go:402 +0xce fp=0xc0001ef910 sp=0xc0001ef8f0 pc=0x55cab7cc712e\nruntime.netpollblock(0x55cab7d2d678?, 0xb7c8fc46?, 0xca?)\n\truntime/netpoll.go:573 +0xf7 fp=0xc0001ef948 sp=0xc0001ef910 pc=0x55cab7cbf377\ninternal/poll.runtime_pollWait(0x7f6de0c99678, 0x72)\n\truntime/netpoll.go:345 +0x85 fp=0xc0001ef968 sp=0xc0001ef948 pc=0x55cab7cf3bc5\ninternal/poll.(*pollDesc).wait(0xc0001fe100?, 0xc000296000?, 0x0)\n\tinternal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc0001ef990 sp=0xc0001ef968 pc=0x55cab7d43ae7\ninternal/poll.(*pollDesc).waitRead(...)\n\tinternal/poll/fd_poll_runtime.go:89\ninternal/poll.(*FD).Read(0xc0001fe100, {0xc000296000, 0x1000, 0x1000})\n\tinternal/poll/fd_unix.go:164 +0x27a fp=0xc0001efa28 sp=0xc0001ef990 pc=0x55cab7d4463a\nnet.(*netFD).Read(0xc0001fe100, {0xc000296000?, 0xc0001efa98?, 0x55cab7d43fa5?})\n\tnet/fd_posix.go:55 +0x25 fp=0xc0001efa70 sp=0xc0001efa28 pc=0x55cab7db2ac5\nnet.(*conn).Read(0xc000194090, {0xc000296000?, 0x0?, 0xc00028a038?})\n\tnet/net.go:185 +0x45 fp=0xc0001efab8 sp=0xc0001efa70 pc=0x55cab7dbcd85\nnet.(*TCPConn).Read(0xc00028a030?, {0xc000296000?, 0xc0001fe100?, 0xc0001efaf0?})\n\t<autogenerated>:1 +0x25 fp=0xc0001efae8 sp=0xc0001efab8 pc=0x55cab7dc8765\nnet/http.(*connReader).Read(0xc00028a030, {0xc000296000, 0x1000, 0x1000})\n\tnet/http/server.go:789 +0x14b fp=0xc0001efb38 sp=0xc0001efae8 pc=0x55cab7ed7a8b\nbufio.(*Reader).fill(0xc000294000)\n\tbufio/bufio.go:110 +0x103 fp=0xc0001efb70 sp=0xc0001efb38 pc=0x55cab7e94383\nbufio.(*Reader).Peek(0xc000294000, 0x4)\n\tbufio/bufio.go:148 +0x53 fp=0xc0001efb90 sp=0xc0001efb70 pc=0x55cab7e944b3\nnet/http.(*conn).serve(0xc0001c21b0, {0x55cab8216b48, 0xc000196db0})\n\tnet/http/server.go:2079 +0x749 fp=0xc0001effb8 sp=0xc0001efb90 pc=0x55cab7edd7e9\nnet/http.(*Server).Serve.gowrap3()\n\tnet/http/server.go:3290 +0x28 fp=0xc0001effe0 sp=0xc0001effb8 pc=0x55cab7ee2068\nruntime.goexit({})\n\truntime/asm_amd64.s:1695 +0x1 fp=0xc0001effe8 sp=0xc0001effe0 pc=0x55cab7cf8f01\ncreated by net/http.(*Server).Serve in goroutine 1\n\tnet/http/server.go:3290 +0x4b4\n\nrax    0x0\nrbx    0x1e6e92\nrcx    0x7f6de04a5bcc\nrdx    0x6\nrdi    0x1e6e8e\nrsi    0x1e6e92\nrbp    0x7f6d82dfc6c0\nrsp    0x7f6d82dfa370\nr8     0x0\nr9     0x1\nr10    0x8\nr11    0x246\nr12    0x7f6d5d7e9b10\nr13    0x6\nr14    0x7f6d82dfa668\nr15    0x0\nrip    0x7f6de04a5bcc\nrflags 0x246\ncs     0x33\nfs     0x0\ngs     0x0\ntime=2025-02-03T16:24:24.686+08:00 level=ERROR source=sched.go:455 msg=\"error loading llama server\" error=\"llama runner process has terminated: exit status 2\"\n[GIN] 2025/02/03 - 16:24:24 | 500 |  1.382866606s |       127.0.0.1 | POST     \"/api/generate\"\n\n```\n\n## Execute only one line\n```\nsource /opt/intel/oneapi/setvars.sh \n```\n# \n```\n(llm-cpp) nian:llama-cpp/ $ ./ollama serve                       [16:27:28]\n2025/02/03 16:27:31 routes.go:1194: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/nian/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\ntime=2025-02-03T16:27:31.243+08:00 level=INFO source=images.go:753 msg=\"total blobs: 11\"\ntime=2025-02-03T16:27:31.244+08:00 level=INFO source=images.go:760 msg=\"total unused blobs removed: 0\"\n[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.\n\n[GIN-debug] [WARNING] Running in \"debug\" mode. Switch to \"release\" mode in production.\n - using env:\texport GIN_MODE=release\n - using code:\tgin.SetMode(gin.ReleaseMode)\n\n[GIN-debug] POST   /api/pull                 --> ollama/server.(*Server).PullHandler-fm (5 handlers)\n[GIN-debug] POST   /api/generate             --> ollama/server.(*Server).GenerateHandler-fm (5 handlers)\n[GIN-debug] POST   /api/chat                 --> ollama/server.(*Server).ChatHandler-fm (5 handlers)\n[GIN-debug] POST   /api/embed                --> ollama/server.(*Server).EmbedHandler-fm (5 handlers)\n[GIN-debug] POST   /api/embeddings           --> ollama/server.(*Server).EmbeddingsHandler-fm (5 handlers)\n[GIN-debug] POST   /api/create               --> ollama/server.(*Server).CreateHandler-fm (5 handlers)\n[GIN-debug] POST   /api/push                 --> ollama/server.(*Server).PushHandler-fm (5 handlers)\n[GIN-debug] POST   /api/copy                 --> ollama/server.(*Server).CopyHandler-fm (5 handlers)\n[GIN-debug] DELETE /api/delete               --> ollama/server.(*Server).DeleteHandler-fm (5 handlers)\n[GIN-debug] POST   /api/show                 --> ollama/server.(*Server).ShowHandler-fm (5 handlers)\n[GIN-debug] POST   /api/blobs/:digest        --> ollama/server.(*Server).CreateBlobHandler-fm (5 handlers)\n[GIN-debug] HEAD   /api/blobs/:digest        --> ollama/server.(*Server).HeadBlobHandler-fm (5 handlers)\n[GIN-debug] GET    /api/ps                   --> ollama/server.(*Server).PsHandler-fm (5 handlers)\n[GIN-debug] POST   /v1/chat/completions      --> ollama/server.(*Server).ChatHandler-fm (6 handlers)\n[GIN-debug] POST   /v1/completions           --> ollama/server.(*Server).GenerateHandler-fm (6 handlers)\n[GIN-debug] POST   /v1/embeddings            --> ollama/server.(*Server).EmbedHandler-fm (6 handlers)\n[GIN-debug] GET    /v1/models                --> ollama/server.(*Server).ListHandler-fm (6 handlers)\n[GIN-debug] GET    /v1/models/:model         --> ollama/server.(*Server).ShowHandler-fm (6 handlers)\n[GIN-debug] GET    /                         --> ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\n[GIN-debug] GET    /api/tags                 --> ollama/server.(*Server).ListHandler-fm (5 handlers)\n[GIN-debug] GET    /api/version              --> ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\n[GIN-debug] HEAD   /                         --> ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\n[GIN-debug] HEAD   /api/tags                 --> ollama/server.(*Server).ListHandler-fm (5 handlers)\n[GIN-debug] HEAD   /api/version              --> ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\ntime=2025-02-03T16:27:31.245+08:00 level=INFO source=routes.go:1245 msg=\"Listening on 127.0.0.1:11434 (version 0.5.1-ipexllm-20250123)\"\ntime=2025-02-03T16:27:31.246+08:00 level=INFO source=common.go:135 msg=\"extracting embedded files\" dir=/tmp/ollama4209733493/runners\ntime=2025-02-03T16:27:31.342+08:00 level=INFO source=common.go:49 msg=\"Dynamic LLM libraries\" runners=[ipex_llm]\n[GIN] 2025/02/03 - 16:27:33 | 200 |     405.912µs |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/02/03 - 16:27:33 | 200 |   32.927453ms |       127.0.0.1 | POST     \"/api/show\"\ntime=2025-02-03T16:27:33.128+08:00 level=INFO source=gpu.go:221 msg=\"looking for compatible GPUs\"\ntime=2025-02-03T16:27:33.128+08:00 level=WARN source=gpu.go:732 msg=\"unable to locate gpu dependency libraries\"\ntime=2025-02-03T16:27:33.130+08:00 level=WARN source=gpu.go:732 msg=\"unable to locate gpu dependency libraries\"\ntime=2025-02-03T16:27:33.130+08:00 level=WARN source=gpu.go:732 msg=\"unable to locate gpu dependency libraries\"\ntime=2025-02-03T16:27:33.140+08:00 level=WARN source=gpu.go:732 msg=\"unable to locate gpu dependency libraries\"\ntime=2025-02-03T16:27:33.205+08:00 level=INFO source=server.go:105 msg=\"system memory\" total=\"30.9 GiB\" free=\"7.1 GiB\" free_swap=\"12.9 GiB\"\ntime=2025-02-03T16:27:33.206+08:00 level=INFO source=memory.go:356 msg=\"offload to device\" layers.requested=-1 layers.model=33 layers.offload=0 layers.split=\"\" memory.available=\"[7.2 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"6.0 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"1.0 GiB\" memory.required.allocations=\"[6.0 GiB]\" memory.weights.total=\"4.9 GiB\" memory.weights.repeating=\"4.5 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"560.0 MiB\" memory.graph.partial=\"677.5 MiB\"\ntime=2025-02-03T16:27:33.214+08:00 level=INFO source=server.go:401 msg=\"starting llama server\" cmd=\"/tmp/ollama4209733493/runners/ipex_llm/ollama_llama_server --model /home/nian/.ollama/models/blobs/sha256-6340dc3229b0d08ea9cc49b75d4098702983e17b4c096d57afbbf2ffc813f2be --ctx-size 8192 --batch-size 512 --n-gpu-layers 999 --threads 6 --no-mmap --parallel 4 --port 36591\"\ntime=2025-02-03T16:27:33.216+08:00 level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\ntime=2025-02-03T16:27:33.216+08:00 level=INFO source=server.go:580 msg=\"waiting for llama runner to start responding\"\ntime=2025-02-03T16:27:33.216+08:00 level=INFO source=server.go:614 msg=\"waiting for server to become available\" status=\"llm server error\"\ntime=2025-02-03T16:27:33.297+08:00 level=INFO source=runner.go:963 msg=\"starting go runner\"\ntime=2025-02-03T16:27:33.298+08:00 level=INFO source=runner.go:964 msg=system info=\"AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | cgo(gcc)\" threads=6\ntime=2025-02-03T16:27:33.299+08:00 level=INFO source=.:0 msg=\"Server listening on 127.0.0.1:36591\"\nllama_model_loader: loaded meta data with 28 key-value pairs and 292 tensors from /home/nian/.ollama/models/blobs/sha256-6340dc3229b0d08ea9cc49b75d4098702983e17b4c096d57afbbf2ffc813f2be (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Llama 8B\nllama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Llama\nllama_model_loader: - kv   4:                         general.size_label str              = 8B\nllama_model_loader: - kv   5:                          llama.block_count u32              = 32\nllama_model_loader: - kv   6:                       llama.context_length u32              = 131072\nllama_model_loader: - kv   7:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   8:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv   9:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  10:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\nllama_model_loader: - kv  12:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  13:                          general.file_type u32              = 15\nllama_model_loader: - kv  14:                           llama.vocab_size u32              = 128256\nllama_model_loader: - kv  15:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = llama-bpe\nllama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\nllama_model_loader: - kv  21:                tokenizer.ggml.bos_token_id u32              = 128000\nllama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 128001\nllama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 128001\nllama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  26:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\nllama_model_loader: - kv  27:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   66 tensors\nllama_model_loader: - type q4_K:  193 tensors\nllama_model_loader: - type q6_K:   33 tensors\ntime=2025-02-03T16:27:33.469+08:00 level=INFO source=server.go:614 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 256\nllm_load_vocab: token to piece cache size = 0.7999 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 128256\nllm_load_print_meta: n_merges         = 280147\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 131072\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 500000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 131072\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 8B\nllm_load_print_meta: model ftype      = Q4_K - Medium\nllm_load_print_meta: model params     = 8.03 B\nllm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \nllm_load_print_meta: general.name     = DeepSeek R1 Distill Llama 8B\nllm_load_print_meta: BOS token        = 128000 '<｜begin▁of▁sentence｜>'\nllm_load_print_meta: EOS token        = 128001 '<｜end▁of▁sentence｜>'\nllm_load_print_meta: PAD token        = 128001 '<｜end▁of▁sentence｜>'\nllm_load_print_meta: LF token         = 128 'Ä'\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\nllm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\nllm_load_print_meta: EOG token        = 128001 '<｜end▁of▁sentence｜>'\nllm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\nllm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\nllm_load_print_meta: max token length = 256\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\nggml_sycl_init: SYCL_USE_XMX: yes\nggml_sycl_init: found 2 SYCL devices:\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\nllm_load_tensors: ggml ctx size =    0.41 MiB\nllm_load_tensors: offloading 32 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloaded 33/33 layers to GPU\nllm_load_tensors:      SYCL0 buffer size =  1263.13 MiB\nllm_load_tensors:      SYCL1 buffer size =  3140.37 MiB\nllm_load_tensors:  SYCL_Host buffer size =   281.81 MiB\nllama_new_context_with_model: n_ctx      = 8192\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 500000.0\nllama_new_context_with_model: freq_scale = 1\n[SYCL] call ggml_check_sycl\nggml_check_sycl: GGML_SYCL_DEBUG: 0\nggml_check_sycl: GGML_SYCL_F16: no\nfound 2 SYCL devices:\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\n| 0| [level_zero:gpu:0]|               Intel Arc A730M Graphics|    1.5|    384|    1024|   32| 12160M|            1.3.30872|\n| 1| [level_zero:gpu:1]|                 Intel Iris Xe Graphics|    1.5|     96|     512|   32| 30750M|            1.3.30872|\nllama_kv_cache_init:      SYCL0 KV buffer size =   320.00 MiB\nllama_kv_cache_init:      SYCL1 KV buffer size =   704.00 MiB\nllama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\nllama_new_context_with_model:  SYCL_Host  output buffer size =     2.02 MiB\nllama_new_context_with_model:      SYCL0 compute buffer size =    96.00 MiB\nllama_new_context_with_model:      SYCL1 compute buffer size =   258.50 MiB\nllama_new_context_with_model:  SYCL_Host compute buffer size =    24.01 MiB\nllama_new_context_with_model: graph nodes  = 902\nllama_new_context_with_model: graph splits = 3\ntime=2025-02-03T16:27:46.129+08:00 level=WARN source=runner.go:894 msg=\"%s: warming up the model with an empty run - please wait ... \" !BADKEY=loadModel\nThe program was built for 1 devices\nBuild program log for 'Intel(R) Arc(TM) A730M Graphics':\n -11 (PI_ERROR_BUILD_PROGRAM_FAILURE)Exception caught at file:/home/runner/_work/llm.cpp/llm.cpp/ollama-llama-cpp/ggml/src/ggml-sycl.cpp, line:2927\ntime=2025-02-03T16:27:46.281+08:00 level=ERROR source=sched.go:455 msg=\"error loading llama server\" error=\"llama runner process has terminated: exit status 1\"\n[GIN] 2025/02/03 - 16:27:46 | 500 | 13.190232474s |       127.0.0.1 | POST     \"/api/generate\"\n\n```",
      "state": "open",
      "author": "cyear",
      "author_type": "User",
      "created_at": "2025-02-03T08:36:26Z",
      "updated_at": "2025-02-06T02:08:31Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12765/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12765",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12765",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:43.242638",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "This is a known issue, we will add a better error message for this case. You can use[OneAPI device selector](https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Overview/KeyFeatures/multi_gpus_selection.md#2-oneapi-device-selector) to use A730M only before your run ollama. Like `export ONEAPI_DE",
          "created_at": "2025-02-05T00:47:18Z"
        },
        {
          "author": "cyear",
          "body": "> This is a known issue, we will add a better error message for this case. You can use[OneAPI device selector](https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Overview/KeyFeatures/multi_gpus_selection.md#2-oneapi-device-selector) to use A730M only before your run ollama. Like `export ONEAPI_",
          "created_at": "2025-02-05T00:55:21Z"
        },
        {
          "author": "qiuxin2012",
          "body": "```\nfound 2 SYCL devices:\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\n|ID|        Device Typ",
          "created_at": "2025-02-05T01:43:09Z"
        },
        {
          "author": "cyear",
          "body": "> ```\n> found 2 SYCL devices:\n> |  |                   |                                       |       |Max    |        |Max  |Global |                     |\n> |  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\n> |ID|        ",
          "created_at": "2025-02-05T01:46:53Z"
        },
        {
          "author": "cyear",
          "body": "> ```\n> found 2 SYCL devices:\n> |  |                   |                                       |       |Max    |        |Max  |Global |                     |\n> |  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\n> |ID|        ",
          "created_at": "2025-02-05T01:49:04Z"
        }
      ]
    },
    {
      "issue_number": 11250,
      "title": "Is there a way to run ollama with IPEX-LLM on CPU",
      "body": "I want to run ollama with IPEM-LLM on a machine with 4 Intel Xeon CPU E7-4830 v3 processors and 256GB of memory. The operating system is Ubuntu 24.04. I followed the steps in the official tutorial as follows:\r\n\r\n1. Install Intel® oneAPI Base Toolkit:\r\n   ```\r\n   apt-get install intel-basekit\r\n   apt-get install intel-hpckit\r\n   ```\r\n2. Install ipex-llm-cpp:\r\n   ```\r\n   pip install --pre --upgrade ipex-llm[cpp]\r\n   ```\r\n3. Execute `init-ollama`:\r\n   ```\r\n   init-ollama\r\n   ```\r\n4. Run ollama:\r\n   ```\r\n   source /opt/intel/oneapi/setvars.sh\r\n   ollama serve\r\n   ```\r\n5. Pull the model:\r\n   ```\r\n   ollama pull qwen:7b-chat\r\n   ```\r\n6. Chat with ollama through open-webui.\r\n\r\nBut when I selected the model on open-webui and sent the question, I received a response with error code `500`.\r\n\r\nI checked the console, and the last output was as follows:\r\n```\r\n[SYCL] call ggml_init_sycl\r\nggml_init_sycl: GGML_SYCL_DEBUG: 0\r\nggml_init_sycl: GGML_SYCL_F16: no\r\nfound 2 SYCL devices:\r\n|  |                  |                                             |       |Max compute|Max work|Max sub|               |                                  |\r\n|ID|       Device Type|                                         Name|Version|units      |group   |group  |Global mem size|                    Driver version|\r\n|--|------------------|---------------------------------------------|-------|-----------|--------|-------|---------------|----------------------------------|\r\n| 0|    [opencl:cpu:0]|          Intel Xeon CPU E7-4830 v3 ® 2.10GHz|    3.0|         96|    8192|     64|        270317M|2024.17.5.0.08_160000.xmain-hotfix|\r\n| 1|    [opencl:acc:0]|                  Intel FPGA Emulation Device|    1.2|         96|67108864|     64|        270317M|2024.17.5.0.08_160000.xmain-hotfix|\r\nggml_backend_sycl_set_mul_device_mode: true\r\nllama_model_load: error loading model: DeviceList is empty. -30 (PI_ERROR_INVALID_VALUE)\r\nllama_load_model_from_file: exception loading model\r\nterminate called after throwing an instance of 'sycl::_V1::invalid_paramter_error'\r\n  what(): DeviceList is empty. -30 (PI_ERROR_INVALID_VALUE)\r\n```\r\n\r\nI couldn't find a method to run ollama with IPEX-LLM on a CPU in the official documentation. I hope someone can point out the problem for me.",
      "state": "open",
      "author": "reeingal",
      "author_type": "User",
      "created_at": "2024-06-06T17:07:11Z",
      "updated_at": "2025-02-03T11:10:54Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11250/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11250",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11250",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:43.475680",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @reeingal, ollama with IPEX-LLM does not support running on a pure CPU platform, as we haven't optimized ollama for CPU. You may switch to a GPU device to enable IPEX-LLM optimizations.",
          "created_at": "2024-06-07T04:05:44Z"
        },
        {
          "author": "simplythomasjay",
          "body": "Any news on that topic?\n\nPure ollama is running okay - not good not bad - but not very fast.\n\nPerhaps there could be a good performance uplift.\n\nEspecially when using Server CPU (Sapphire Rapids / Emerald Rapids / Granite Rapids)  with AMX.\n\n\nBecause Intel internally is using IPEX-LLM for Emerald Ra",
          "created_at": "2025-02-03T11:09:28Z"
        }
      ]
    },
    {
      "issue_number": 12737,
      "title": "Cline on vllm",
      "body": "We use Cline as the AI ​​agent to generate code based on ipex-vllm. Using deepseek-coder-33b-instruct or DeepSeek-R1-Distill-Qwen-14B model with fp8, the prompt \"write a snake game\" can successfully generate code and execute, but using sym_int4 or asym_int4 cannot generate complete code.\n\n<html xmlns:v=\"urn:schemas-microsoft-com:vml\"\nxmlns:o=\"urn:schemas-microsoft-com:office:office\"\nxmlns:x=\"urn:schemas-microsoft-com:office:excel\"\nxmlns=\"http://www.w3.org/TR/REC-html40\">\n\n<head>\n\n<meta name=ProgId content=Excel.Sheet>\n<meta name=Generator content=\"Microsoft Excel 15\">\n<link id=Main-File rel=Main-File\nhref=\"file:///C:/Users/yongzhu/AppData/Local/Temp/msohtmlclip1/01/clip.htm\">\n<link rel=File-List\nhref=\"file:///C:/Users/yongzhu/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml\">\n<style>\n<!--table\n\t{mso-displayed-decimal-separator:\"\\.\";\n\tmso-displayed-thousand-separator:\"\\,\";}\n@page\n\t{margin:.75in .7in .75in .7in;\n\tmso-header-margin:.3in;\n\tmso-footer-margin:.3in;}\ntr\n\t{mso-height-source:auto;}\ncol\n\t{mso-width-source:auto;}\nbr\n\t{mso-data-placement:same-cell;}\ntd\n\t{padding-top:1px;\n\tpadding-right:1px;\n\tpadding-left:1px;\n\tmso-ignore:padding;\n\tcolor:black;\n\tfont-size:11.0pt;\n\tfont-weight:400;\n\tfont-style:normal;\n\ttext-decoration:none;\n\tfont-family:Calibri, sans-serif;\n\tmso-font-charset:0;\n\tmso-number-format:General;\n\ttext-align:general;\n\tvertical-align:bottom;\n\tborder:none;\n\tmso-background-source:auto;\n\tmso-pattern:auto;\n\tmso-protection:locked visible;\n\twhite-space:nowrap;\n\tmso-rotate:0;}\n.xl65\n\t{border:.5pt solid windowtext;}\n.xl66\n\t{border:.5pt solid windowtext;\n\tbackground:#5B9BD5;\n\tmso-pattern:black none;}\n.xl67\n\t{border:.5pt solid windowtext;\n\tbackground:#70AD47;\n\tmso-pattern:black none;}\n.xl68\n\t{border:.5pt solid windowtext;\n\tbackground:red;\n\tmso-pattern:black none;}\n-->\n</style>\n</head>\n\n<body link=\"#0563C1\" vlink=\"#954F72\">\n\n\nagent | model | gpus | low bit | prompt | result\n-- | -- | -- | -- | -- | --\nCline | deepseek-coder-33b-instruct | 4 | fp8 | write a snake game | PASS\nCline | deepseek-coder-33b-instruct | 2 | sym_int4 | write a snake game | FAIL\nCline | DeepSeek-R1-Distill-Qwen-32B | 4 | fp8 | write a snake game | PASS\nCline | DeepSeek-R1-Distill-Qwen-32B | 2 | sym_int4 | write a snake game | FAIL\nCline | DeepSeek-R1-Distill-Qwen-32B | 2 | asym_int4 | write a snake game | FAIL\n\n\n\n</body>\n\n</html>\n\n\nIs it caused by quantization?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "state": "open",
      "author": "YongZhuIntel",
      "author_type": "User",
      "created_at": "2025-01-23T07:56:11Z",
      "updated_at": "2025-01-24T05:31:55Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12737/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hzjane"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12737",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12737",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:43.685840",
      "comments": [
        {
          "author": "jason-dai",
          "body": "Maybe you can try fp6",
          "created_at": "2025-01-23T09:34:34Z"
        },
        {
          "author": "qiyuangong",
          "body": "DeepSeek-R1-Distill-Qwen-32B also needs new prompts for better results. https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B/discussions/2#678f635676152df1fa308861",
          "created_at": "2025-01-24T00:39:10Z"
        },
        {
          "author": "hzjane",
          "body": "This issue may not be related to low_bit accuracy. There will be fixed request parameters when cline sends a request to vllm. I have tried to set the prompt more clearly and the test success rate will be much higher. For example, using prompt `write a snake game with python on linux` to run 2*ARC in",
          "created_at": "2025-01-24T02:25:57Z"
        },
        {
          "author": "YongZhuIntel",
          "body": "fp6  not work , we got an error:\n```\n    |   File \"/usr/local/lib/python3.11/dist-packages/vllm/utils.py\", line 452, in iterate_with_cancellation\n    |     item = await awaits[0]\n    |            ^^^^^^^^^^^^^^^\n    |   File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/multiprocessing/client",
          "created_at": "2025-01-24T02:26:24Z"
        },
        {
          "author": "qiyuangong",
          "body": "> fp6 not work , we got an error:\n> \n> ```\n>     |   File \"/usr/local/lib/python3.11/dist-packages/vllm/utils.py\", line 452, in iterate_with_cancellation\n>     |     item = await awaits[0]\n>     |            ^^^^^^^^^^^^^^^\n>     |   File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/multipro",
          "created_at": "2025-01-24T05:31:54Z"
        }
      ]
    },
    {
      "issue_number": 12687,
      "title": "ippex-llm docker: SIGABRT on lunar lake (Intel Ultra 7 258V)",
      "body": "was trying to run ipex-llm docker version, and caught this issue:\r\n**Does it support XE ?**\r\n==============\r\n**OS:** Ubuntu - 22.04\r\n**Kernel:** 6.12\r\n==============\r\nfollowing is the error:\r\n==============\r\n```\r\ntime=2024-12-20T23:23:36.179+08:00 level=INFO source=server.go:619 msg=\"llama runner started in 3.26 seconds\"\r\ncould not create a primitive descriptor for a matmul primitive\r\nException caught at file:/home/runner/_work/llm.cpp/llm.cpp/llama-cpp-bigdl/ggml/src/ggml-sycl.cpp, line:3220, func:operator()\r\nSYCL error: CHECK_TRY_ERROR(op(ctx, src0, src1, dst, src0_dd_i, src1_ddf_i, src1_ddq_i, dst_dd_i, dev[i].row_low, dev[i].row_high, src1_ncols, src1_padded_col_size, stream)): Meet error in this line code!\r\n  in function ggml_sycl_op_mul_mat at /home/runner/_work/llm.cpp/llm.cpp/llama-cpp-bigdl/ggml/src/ggml-sycl.cpp:3220\r\n/home/runner/_work/llm.cpp/llm.cpp/llama-cpp-bigdl/ggml/src/ggml-sycl/common.hpp:107: SYCL error\r\nlibggml.so(+0x7d727)[0x7455f847d727]\r\nlibggml.so(ggml_abort+0xd8)[0x7455f847d6b8]\r\nlibggml.so(+0x200a48)[0x7455f8600a48]\r\nlibggml.so(+0x236ee8)[0x7455f8636ee8]\r\nlibggml.so(_Z25ggml_sycl_compute_forwardR25ggml_backend_sycl_contextP11ggml_tensor+0x5ef)[0x7455f86034df]\r\nlibggml.so(+0x24deff)[0x7455f864deff]\r\nlibggml.so(ggml_backend_sched_graph_compute_async+0x548)[0x7455f84ed548]\r\nlibllama.so(llama_decode+0xb53)[0x7455fa47dd43]\r\n/tmp/ollama357958470/runners/ipex_llm/ollama_llama_server(_cgo_74634bdf4781_Cfunc_llama_decode+0x4c)[0x5a258e90f58c]\r\n/tmp/ollama357958470/runners/ipex_llm/ollama_llama_server(+0xf8ac1)[0x5a258e6f8ac1]\r\nSIGABRT: abort\r\nPC=0x7455f7c429fc m=4 sigcode=18446744073709551610\r\nsignal arrived during cgo execution\r\n```\r\n",
      "state": "closed",
      "author": "raj-ritu17",
      "author_type": "User",
      "created_at": "2025-01-09T14:41:56Z",
      "updated_at": "2025-01-14T11:45:21Z",
      "closed_at": "2025-01-14T11:44:45Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12687/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12687",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12687",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:45.677899",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @raj-ritu17, we have supported Xe. To resolve your issue, could you please provide the `ipex-llm ollama` version and a more detailed `ollama serve` log?",
          "created_at": "2025-01-10T01:42:46Z"
        },
        {
          "author": "raj-ritu17",
          "body": "hello, @sgwhat, yesterday commit: \"Commit 2673792\" fixes the issue (intel/oneapi-basekit:2024.2.1-0-devel-ubuntu22.04) ",
          "created_at": "2025-01-10T15:39:46Z"
        },
        {
          "author": "raj-ritu17",
          "body": "fix by \"Commit https://github.com/intel-analytics/ipex-llm/commit/2673792de6731813a6673bd8b661597785e25f71\"",
          "created_at": "2025-01-14T11:45:20Z"
        }
      ]
    },
    {
      "issue_number": 12636,
      "title": "Fundamental issue in response with IPEX-LLM with Ollama",
      "body": "I am getting a weird response from Ollama. Hoping below screen shot helps in detail\r\n\r\nBelow is command I used to work\r\n```\r\nconda init\r\nconda create -n igpu python=3.11\r\nconda activate igpu\r\n\r\npip install --upgrade ipex-llm[cpp]\r\n```\r\n\r\n\r\n![image](https://github.com/user-attachments/assets/bee8a954-133e-4d74-8e2e-92bd230f49c6)\r\n",
      "state": "open",
      "author": "anandnandagiri",
      "author_type": "User",
      "created_at": "2024-12-30T13:23:13Z",
      "updated_at": "2025-01-12T09:47:34Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12636/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "qiuxin2012"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12636",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12636",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:45.890324",
      "comments": [
        {
          "author": "leonardozcm",
          "body": "hi, what igpu device you are using?\r\n\r\nIt looks fine on Ultra 7 258V\r\n```\r\ncan you give a week diet plan for decreasing fat in body should include breakfast, lunch and dinner and snacks\r\n\r\nA week diet plan for weight loss\r\nIt is no secret that the majority of us are overweight to the point of being ",
          "created_at": "2024-12-31T02:12:26Z"
        },
        {
          "author": "anandnandagiri",
          "body": "<html xmlns:o=\"urn:schemas-microsoft-com:office:office\"\r\nxmlns:dt=\"uuid:C2F41010-65B3-11d1-A29F-00AA00C14882\"\r\nxmlns=\"http://www.w3.org/TR/REC-html40\">\r\n\r\n<head>\r\n\r\n<meta name=ProgId content=OneNote.File>\r\n<meta name=Generator content=\"Microsoft OneNote 15\">\r\n</head>\r\n\r\n<body lang=en-IN style='font-",
          "created_at": "2024-12-31T20:59:10Z"
        },
        {
          "author": "qiuxin2012",
          "body": "@anandnandagiri  Can you update your ipex-llm[cpp] to latest nightly version? I can't reproduce the wrong output on similiar device i7 1270p.\r\n![image](https://github.com/user-attachments/assets/84a1065b-4c06-42b9-a77e-4f7f60e4ca6d)\r\nollama's installation is the same with\r\nhttps://github.com/intel-a",
          "created_at": "2025-01-03T02:19:56Z"
        },
        {
          "author": "anandnandagiri",
          "body": "This got resolved Thank You @qiuxin2012 ",
          "created_at": "2025-01-12T09:24:08Z"
        },
        {
          "author": "anandnandagiri",
          "body": "Still I am seeing weird response\r\n\r\nBelow are command used\r\n\r\n```\r\nconda create -n igputwo python=3.11\r\nconda activate igputwo\r\n\r\npip install --upgrade ipex-llm[cpp]\r\n```\r\n![image](https://github.com/user-attachments/assets/efd757a2-2788-45be-b811-4916ccf134de)\r\n",
          "created_at": "2025-01-12T09:46:14Z"
        }
      ]
    },
    {
      "issue_number": 12506,
      "title": "[torch 2.3 + bigdl-core-xe-23] AttributeError: module 'xe_linear' has no attribute 'forward_qkv'",
      "body": "## Environment\r\n\r\nOS: Windows\r\nRunning [Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3) using the example script [generate.py](https://github.com/intel-analytics/ipex-llm/blob/5e1416c9aa1189d485bde80ea0a3962aabba321b/python/llm/example/GPU/HuggingFace/LLM/mistral/generate.py) with ipex-2.3 + ipex-llm-2.2 + bigdl-core-xe-*-23 on MTL iGPU.\r\nI believe this issue can be reproduced with ACM dGPU (and other Arc graphics) too.\r\n\r\n## Reproduce steps\r\n\r\n```bash\r\nconda create -p env_bad python=3.11 libuv -y\r\nconda activate .\\env_bad\r\n\r\npip install torch==2.3.1.post0+cxx11.abi torchvision==0.18.1.post0+cxx11.abi torchaudio==2.3.1.post0+cxx11.abi intel-extension-for-pytorch==2.3.110.post0+xpu --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/mtl/us/\r\n\r\npip install --pre ipex_llm==2.2.0b2 bigdl-core-xe-23==2.6.0b2 bigdl-core-xe-addons-23==2.6.0b2 bigdl-core-xe-batch-23==2.6.0b2 onednn-devel==2024.1.1 transformers==4.39.0 accelerate trl sentence_transformers sentencepiece protobuf\r\n\r\npython generate.py --repo-id-or-model-path <my_model_path>\r\n\r\n##############\r\n\r\nD:\\iusers\\yilonggu\\repro\\ipex-llm-mistral\\env_bad\\Lib\\site-packages\\transformers\\deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\r\n  warnings.warn(\r\nD:\\iusers\\yilonggu\\repro\\ipex-llm-mistral\\env_bad\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'D:\\iusers\\yilonggu\\repro\\ipex-llm-mistral\\env_bad\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n  warn(\r\n2024-12-05 13:47:37,013 - INFO - intel_extension_for_pytorch auto imported\r\nLoading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 11.56it/s]\r\n2024-12-05 13:47:37,712 - INFO - Converting the current model to sym_int4 format......\r\n2024-12-05 13:47:38,350 - INFO - PyTorch version 2.3.1.post0+cxx11.abi available.\r\nYou set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\r\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\r\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\r\nTraceback (most recent call last):\r\n  File \"D:\\iusers\\yilonggu\\repro\\ipex-llm-mistral\\mistral.py\", line 60, in <module>\r\n    output = model.generate(input_ids,\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\iusers\\yilonggu\\repro\\ipex-llm-mistral\\env_bad\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\iusers\\yilonggu\\repro\\ipex-llm-mistral\\env_bad\\Lib\\site-packages\\ipex_llm\\transformers\\lookup.py\", line 123, in generate\r\n    return original_generate(self,\r\n           ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\iusers\\yilonggu\\repro\\ipex-llm-mistral\\env_bad\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\iusers\\yilonggu\\repro\\ipex-llm-mistral\\env_bad\\Lib\\site-packages\\ipex_llm\\transformers\\speculative.py\", line 109, in generate\r\n    return original_generate(self,\r\n           ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\iusers\\yilonggu\\repro\\ipex-llm-mistral\\env_bad\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\iusers\\yilonggu\\repro\\ipex-llm-mistral\\env_bad\\Lib\\site-packages\\ipex_llm\\transformers\\pipeline_parallel.py\", line 281, in generate\r\n    return original_generate(self,\r\n           ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\iusers\\yilonggu\\repro\\ipex-llm-mistral\\env_bad\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\iusers\\yilonggu\\repro\\ipex-llm-mistral\\env_bad\\Lib\\site-packages\\transformers\\generation\\utils.py\", line 1527, in generate\r\n    result = self._greedy_search(\r\n             ^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\iusers\\yilonggu\\repro\\ipex-llm-mistral\\env_bad\\Lib\\site-packages\\transformers\\generation\\utils.py\", line 2411, in _greedy_search\r\n    outputs = self(\r\n              ^^^^^\r\n  File \"D:\\iusers\\yilonggu\\repro\\ipex-llm-mistral\\env_bad\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\iusers\\yilonggu\\repro\\ipex-llm-mistral\\env_bad\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\iusers\\yilonggu\\repro\\ipex-llm-mistral\\env_bad\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py\", line 1157, in forward\r\n    outputs = self.model(\r\n              ^^^^^^^^^^^\r\n  File \"D:\\iusers\\yilonggu\\repro\\ipex-llm-mistral\\env_bad\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\iusers\\yilonggu\\repro\\ipex-llm-mistral\\env_bad\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\iusers\\yilonggu\\repro\\ipex-llm-mistral\\env_bad\\Lib\\site-packages\\ipex_llm\\transformers\\models\\mistral.py\", line 217, in mistral_model_forward_4_36\r\n    return MistralModel.forward(\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\iusers\\yilonggu\\repro\\ipex-llm-mistral\\env_bad\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py\", line 1042, in forward\r\n    layer_outputs = decoder_layer(\r\n                    ^^^^^^^^^^^^^^\r\n  File \"D:\\iusers\\yilonggu\\repro\\ipex-llm-mistral\\env_bad\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\iusers\\yilonggu\\repro\\ipex-llm-mistral\\env_bad\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\iusers\\yilonggu\\repro\\ipex-llm-mistral\\env_bad\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py\", line 757, in forward\r\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\n                                                          ^^^^^^^^^^^^^^^\r\n  File \"D:\\iusers\\yilonggu\\repro\\ipex-llm-mistral\\env_bad\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\iusers\\yilonggu\\repro\\ipex-llm-mistral\\env_bad\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\iusers\\yilonggu\\repro\\ipex-llm-mistral\\env_bad\\Lib\\site-packages\\ipex_llm\\transformers\\models\\mistral.py\", line 1144, in mistral_attention_forward_4_39\r\n    return forward_function(\r\n           ^^^^^^^^^^^^^^^^^\r\n  File \"D:\\iusers\\yilonggu\\repro\\ipex-llm-mistral\\env_bad\\Lib\\site-packages\\ipex_llm\\transformers\\models\\mistral.py\", line 1194, in mistral_attention_forward_4_39_original\r\n    query_states, key_states, value_states = xe_linear.forward_qkv(hidden_states,\r\n                                             ^^^^^^^^^^^^^^^^^^^^^\r\nAttributeError: module 'xe_linear' has no attribute 'forward_qkv'. Did you mean: 'forward_new'?\r\n```\r\n\r\n## Investigation notes\r\n\r\nThe issue does not exist for IPEX-**2.1** + bigdl-core-xe-*-**21**:\r\n\r\n```\r\nconda create -p env_good python=3.11 libuv -y\r\nconda activate .\\env_good\r\n\r\npip install torch==2.1.0.post3 torchvision==0.16.0.post3 torchaudio==2.1.0.post3 intel-extension-for-pytorch==2.1.40+xpu --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/mtl/us/\r\npip install dpcpp-cpp-rt==2024.2 mkl-dpcpp==2024.2\r\n\r\npip install --pre ipex_llm==2.2.0b2 bigdl-core-xe-21==2.6.0b2 bigdl-core-xe-addons-21==2.6.0b2 bigdl-core-xe-batch-21==2.6.0b2 onednn-devel==2024.1.1 transformers==4.39.0 accelerate trl sentence_transformers sentencepiece protobuf\r\n\r\npython generate.py --repo-id-or-model-path <my_model_path>\r\n\r\n###########\r\n\r\nA module that was compiled using NumPy 1.x cannot be run in\r\nNumPy 2.1.3 as it may crash. To support both 1.x and 2.x\r\nversions of NumPy, modules must be compiled with NumPy 2.0.\r\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\r\n\r\nIf you are a user of the module, the easiest solution will be to\r\ndowngrade to 'numpy<2' or try to upgrade the affected module.\r\nWe expect that some modules will need time to support NumPy 2.\r\n\r\nTraceback (most recent call last):  File \"D:\\iusers\\yilonggu\\repro\\ipex-llm-mistral\\mistral.py\", line 17, in <module>\r\n    import torch\r\n  File \"D:\\iusers\\yilonggu\\repro\\env_good\\Lib\\site-packages\\torch\\__init__.py\", line 1382, in <module>\r\n    from .functional import *  # noqa: F403\r\n  File \"D:\\iusers\\yilonggu\\repro\\env_good\\Lib\\site-packages\\torch\\functional.py\", line 7, in <module>\r\n    import torch.nn.functional as F\r\n  File \"D:\\iusers\\yilonggu\\repro\\env_good\\Lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\r\n    from .modules import *  # noqa: F403\r\n  File \"D:\\iusers\\yilonggu\\repro\\env_good\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\r\n    from .transformer import TransformerEncoder, TransformerDecoder, \\\r\n  File \"D:\\iusers\\yilonggu\\repro\\env_good\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\r\n    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\r\nD:\\iusers\\yilonggu\\repro\\env_good\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy:\r\nA module that was compiled using NumPy 1.x cannot be run in\r\nNumPy 2.1.3 as it may crash. To support both 1.x and 2.x\r\nversions of NumPy, modules must be compiled with NumPy 2.0.\r\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\r\n\r\nIf you are a user of the module, the easiest solution will be to\r\ndowngrade to 'numpy<2' or try to upgrade the affected module.\r\nWe expect that some modules will need time to support NumPy 2.\r\n\r\n (Triggered internally at C:\\Jenkins\\workspace\\IPEX-GPU-ARC770-Windows-Build@4\\frameworks.ai.pytorch.private-gpu\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\r\n  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\r\nD:\\iusers\\yilonggu\\repro\\env_good\\Lib\\site-packages\\transformers\\deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\r\n  warnings.warn(\r\nD:\\iusers\\yilonggu\\repro\\env_good\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'D:\\iusers\\yilonggu\\repro\\env_good\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n  warn(\r\n2024-12-05 13:55:56,100 - INFO - intel_extension_for_pytorch auto imported\r\nLoading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 12.91it/s]\r\n2024-12-05 13:55:56,897 - INFO - Converting the current model to sym_int4 format......\r\n2024-12-05 13:56:00,390 - INFO - PyTorch version 2.1.0.post3+cxx11.abi available.\r\nYou set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\r\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\r\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\r\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\r\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\r\nInference time: 1.7521936893463135 s\r\n-------------------- Output --------------------\r\n What is AI?  Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think and learn like humans. It involves the development of computer\r\n```\r\n\r\n## Related issue\r\n\r\nThe issue is exposed by Intel AI Playground: https://github.com/intel/AI-Playground/issues/94",
      "state": "closed",
      "author": "Nuullll",
      "author_type": "User",
      "created_at": "2024-12-05T06:13:26Z",
      "updated_at": "2025-01-10T07:17:22Z",
      "closed_at": "2025-01-10T07:17:22Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12506/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12506",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12506",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:46.125905",
      "comments": [
        {
          "author": "Nuullll",
          "body": "Got a workaround by disabling fast decoding path to avoid calling `forward_qkv()`: https://github.com/intel/AI-Playground/commit/62920f6be6fdec61a1c6864ca9249b257696cd1a",
          "created_at": "2024-12-05T07:28:19Z"
        },
        {
          "author": "Oscilloscope98",
          "body": "Hi @Nuullll,\r\n\r\nThis is a known issue for IPEX-LLM PyTorch 2.3 support on Mistral. We will fix it and let you know for any updates :)",
          "created_at": "2024-12-06T03:13:08Z"
        },
        {
          "author": "Oscilloscope98",
          "body": "Hi @Nuullll,\r\n\r\nSorry for the late reply. We have fixed this issue. You could upgrade to latest `ipex-llm` (or `ipex-llm>=2.2.0b20241225`) and have a try again :)",
          "created_at": "2025-01-07T02:44:01Z"
        },
        {
          "author": "Nuullll",
          "body": "Thank you @Oscilloscope98 . Works for me!\r\n\r\n```\r\npip install --pre ipex_llm==2.2.0b20241225 bigdl-core-xe-23==2.6.0b20241225 bigdl-core-xe-addons-23==2.6.0b20241225 bigdl-core-xe-batch-23==2.6.0b20241225\r\n```",
          "created_at": "2025-01-10T07:17:22Z"
        }
      ]
    },
    {
      "issue_number": 12633,
      "title": "[LNL][npu_llm] LNL  npu doesn't support codegeex4-all-9b  and codegeex4-all-9b model",
      "body": "Platform: LNL\r\nOS: win11\r\nNpu driver: npu_win_32.0.100.3104\r\nIssue:  I ran benchmark , but met  an error: RuntimeError: \"baddbmm_with_gemm\" not implemented for 'Half',  these two models reported the same error. pls help us check this, thanks.\r\n\r\nPip list:  codegeex_pip.txt\r\n[codegeex_pip.txt](https://github.com/user-attachments/files/18271283/codegeex_pip.txt)\r\n\r\nlogs:\r\n[log 3.txt](https://github.com/user-attachments/files/18271284/log.3.txt)\r\n",
      "state": "open",
      "author": "johnysh",
      "author_type": "User",
      "created_at": "2024-12-29T23:50:58Z",
      "updated_at": "2025-01-07T02:15:13Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12633/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "plusbang"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12633",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12633",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:46.364902",
      "comments": [
        {
          "author": "johnysh",
          "body": "Codegeex3-chat-nano model has the same error as 9B ",
          "created_at": "2025-01-02T01:06:16Z"
        },
        {
          "author": "shichang00",
          "body": "the same issue occured on mine",
          "created_at": "2025-01-06T09:08:59Z"
        },
        {
          "author": "plusbang",
          "body": "Codegeex model support is in progress, if there is any progress, we will update here to let you know.",
          "created_at": "2025-01-07T02:15:12Z"
        }
      ]
    },
    {
      "issue_number": 12632,
      "title": "Prompt processing slowing down on Arc GPU",
      "body": "I am using Ollama from `intelanalytics/ipex-llm-inference-cpp-xpu` docker image to run LLMs on Arc A380 GPU. I am observing a strange issue when speed of prompt processing drops from 250-300t/s to 50-100t/s for no apparent reason. This is measured on a long prompt (so not just noise). Processing of a 15K-token prompt slows down from a minute  to several minutes. Once the speed drops, it does not recover on its own. Restarting the container fixes the issue for a while.\r\n\r\nThere's nothing else running on the GPU (desktop is on an iGPU). I rarely switch models and I have observed the slowdown while continuously using the same model. I notice this every couple of days, but it's probably happening more often without me noticing, because I don't always use long prompts. There is nothing unusual in Ollama log file.\r\n\r\nWatching `intel_gpu_top`, I notice that GPU load goes up and down even when the card performs normally, but when this slowdown happens, this up-and-down fluctuation in GPU load has lower average and lulls with no GPU activity are up to several seconds long.\r\n\r\nThe model is fully offloaded to the GPU:\r\n\r\n```\r\nllm_load_tensors: offloading 28 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 29/29 layers to GPU\r\nllm_load_tensors:      SYCL0 buffer size =  4168.09 MiB\r\nllm_load_tensors:  SYCL_Host buffer size =   292.36 MiB\r\n...\r\nfound 1 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|                Intel Arc A380 Graphics|    1.6|    128|    1024|   32|  6064M|            1.3.31294|\r\nllama_kv_cache_init:      SYCL0 KV buffer size =  1344.00 MiB\r\nllama_new_context_with_model: KV self size  = 1344.00 MiB, K (f16):  672.00 MiB, V (f16):  672.00 MiB\r\nllama_new_context_with_model:  SYCL_Host  output buffer size =     0.59 MiB\r\n[1735473133] warming up the model with an empty run\r\nllama_new_context_with_model:      SYCL0 compute buffer size =   304.00 MiB\r\nllama_new_context_with_model:  SYCL_Host compute buffer size =    55.01 MiB\r\n```\r\n\r\nDocker image configuration was inspired by [mattcurf's setup](https://github.com/mattcurf/ollama-intel-gpu/blob/c74f6f221688ffbd1b8e4dc22482ed3e6ad3b621/Dockerfile):\r\n\r\n```\r\nFROM intelanalytics/ipex-llm-inference-cpp-xpu:latest\r\nENV ZES_ENABLE_SYSMAN=1\r\nENV USE_XETLA=OFF\r\nENV OLLAMA_HOST=0.0.0.0:11434\r\nRUN mkdir -p /llm/ollama && \\\r\n    cd /llm/ollama && \\\r\n    init-ollama\r\nWORKDIR /llm/ollama\r\nENTRYPOINT [\"./ollama\", \"serve\"]\r\n```\r\n\r\nOllama is configured to load only one model at a time with one KV cache and to never timeout the current model:\r\n\r\n```\r\npodman create \\\r\n    --replace \\\r\n    --stop-signal=SIGKILL \\\r\n    --name ollama-ipex \\\r\n    -p 127.0.0.1:11434:11434 \\\r\n    -v ollama-ipex:/root/.ollama \\\r\n    -e OLLAMA_DEBUG=1 \\\r\n    -e OLLAMA_MAX_LOADED_MODELS=1 \\\r\n    -e OLLAMA_NUM_PARALLEL=1 \\\r\n    -e OLLAMA_KEEP_ALIVE=-1 \\\r\n    --device /dev/dri \\\r\n    localhost/ollama-ipex\r\n```\r\n\r\nOther configuration:\r\n\r\n- docker image: `intelanalytics/ipex-llm-inference-cpp-xpu:latest` pulled on Nov 16, 2024\r\n- host OS: Fedora 40, kernel 6.12.6-100.fc40.x86_64\r\n- model: `qwen2.5-coder:7b`\r\n- model settings: defaults except context window set to 24*1024\r\n- GPU: Arc A380 6GB",
      "state": "open",
      "author": "robertvazan",
      "author_type": "User",
      "created_at": "2024-12-29T16:01:54Z",
      "updated_at": "2025-01-06T01:43:14Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12632/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "ACupofAir"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12632",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12632",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:46.546748",
      "comments": [
        {
          "author": "leonardozcm",
          "body": "hi @robertvazan I tried it on A380 and found out the performance has been very stable at around 178t/s. Further investigation needs to be done. ",
          "created_at": "2024-12-31T02:05:58Z"
        },
        {
          "author": "robertvazan",
          "body": "Okay, I have bitten the bullet and upgraded to the latest docker image and... it is still slowing down :-( I wrote a benchmark to prove it. I can now demonstrate the slowdown reliably, which makes this into a reproducible bug.\r\n\r\nHere's the result of a benchmark run:\r\n\r\n```\r\n$ ollama.py benchmark_co",
          "created_at": "2024-12-31T19:08:46Z"
        },
        {
          "author": "leonardozcm",
          "body": "Great work, I will give it a try",
          "created_at": "2025-01-02T02:16:36Z"
        }
      ]
    },
    {
      "issue_number": 11923,
      "title": "support InternVL-8B model on Arc A770",
      "body": "https://modelscope.cn/models/OpenGVLab/InternVL2-8B#%E7%AE%80%E4%BB%8B\r\n![image](https://github.com/user-attachments/assets/ab05d90b-66fb-4c5f-8592-ed5db7ca6dbf)\r\nsupport this model base 2x Arc A770",
      "state": "open",
      "author": "Fred-cell",
      "author_type": "User",
      "created_at": "2024-08-26T07:54:37Z",
      "updated_at": "2025-01-06T01:24:06Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11923/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Uxito-Ada"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11923",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11923",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:46.780168",
      "comments": [
        {
          "author": "Uxito-Ada",
          "body": "Hi @Fred-cell ,\r\n\r\nWe tried to enable InternVL2-8B on vllm 2-card serving, while an [open issue](https://github.com/vllm-project/vllm/issues/5589) about vllm is found that blocks this model. We will keep paying attention to it.",
          "created_at": "2024-08-27T01:29:05Z"
        },
        {
          "author": "vladislavdonchev",
          "body": "Any updates on this? Seems the vllm issue was fixed in September (in vLLM >0.6.0).",
          "created_at": "2025-01-02T16:21:39Z"
        },
        {
          "author": "hzjane",
          "body": " InternVL2-8B is enabled by [this pr]( https://github.com/analytics-zoo/vllm/pull/72/files).",
          "created_at": "2025-01-06T01:23:56Z"
        }
      ]
    },
    {
      "issue_number": 12629,
      "title": "Docker Documentation incorrect. Benchmarks are all not working. Testing needed",
      "body": "Just try and follow this Doc for Linux: https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/DockerGuides/docker_pytorch_inference_gpu.md\r\n\r\n**Questions - as everything below  sycl-ls it is not working at all (issues below): Is this the current doc?** Or are there other docs which are known to work?\r\nI would love to see this fixed or get a hint where to find currennt working examples. \r\nMy Boss expects me to port our stuff for ARC GPUs and iGPUs.\r\n\r\n1. env check script does not exist.\r\n\r\n2. Most directories have different locations\r\n\r\n3. hf downloader seems outdateted and refuses to download models from the default config\r\n\r\n4. pytorch torchvision libpng missing warniings\r\n\r\n5. TypeError: 'NoneType' object is not iterable run.py:2331\r\n\r\nAs the doc had its last changes a few weeks ago I thought it should be okay..\r\n\r\n\r\n",
      "state": "open",
      "author": "TimoGoetze",
      "author_type": "User",
      "created_at": "2024-12-27T08:37:04Z",
      "updated_at": "2025-01-03T02:47:37Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12629/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "ACupofAir"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12629",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12629",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:46.978015",
      "comments": [
        {
          "author": "ACupofAir",
          "body": "Answers:\r\n1. `env-check.sh` has been removed from the image, here is the [source link](https://raw.githubusercontent.com/intel-analytics/ipex-llm/refs/heads/main/python/llm/scripts/env-check.sh), you can download it use `wget -c https://raw.githubusercontent.com/intel-analytics/ipex-llm/refs/heads/m",
          "created_at": "2025-01-03T02:47:36Z"
        }
      ]
    },
    {
      "issue_number": 12631,
      "title": "Better docker tags",
      "body": "Please use docker tags for versioning. It allows us to safely upgrade and downgrade to resolve version-specific issues and to identify the version that introduced the issue. Tagging with Ollama/llama.cpp/vLLM versions allows us to predict what we will get after an upgrade.\r\n\r\nFor example, image `intelanalytics/ipex-llm-inference-cpp-xpu` has 4 months old tag `2.1.0` and then only rolling tags `latest` and `2.2.0-SNAPSHOT`. I have pulled `latest` about two months ago. If I pull now and find the new version broken, I will have no way to undo the upgrade. I can tag locally before upgrade as a workaround, but that will not help me when moving to a new computer. I cannot search the tag list on Docker Hub for hash of my image to identify my version. I don't even know whether it makes sense to upgrade, because there is no information about which tag contains what software.\r\n\r\nPlease add:\r\n\r\n- unchanging version tags at least once per month (e.g. `20241229` or `2.1.1`)\r\n- alias for the last unchanging tag, either repurpose `latest` (with `nightly` for rolling tag) or add new `stable` tag\r\n- tags identifying upgrades of Ollama/llama.cpp/vLLM (e.g. `ollama-0.5.4`)",
      "state": "open",
      "author": "robertvazan",
      "author_type": "User",
      "created_at": "2024-12-29T15:13:18Z",
      "updated_at": "2025-01-03T02:10:40Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12631/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "liu-shaojun"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12631",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12631",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:47.231298",
      "comments": [
        {
          "author": "liu-shaojun",
          "body": "Thank you for your feedback! We understand the importance of versioned tags for safe upgrades and troubleshooting.\r\n\r\nCurrently, for `vLLM`, the `intelanalytics/ipex-llm-serving-xpu` Docker image is consistently updated with versioned tags, ranging from `2.2.0-b1` to `2.2.0-b11`, ensuring incrementa",
          "created_at": "2025-01-02T02:17:54Z"
        },
        {
          "author": "robertvazan",
          "body": "@liu-shaojun Looking at the vLLM image, that's exactly what I had in mind. Thanks for the interim tag.",
          "created_at": "2025-01-02T07:51:43Z"
        },
        {
          "author": "liu-shaojun",
          "body": "The version `intelanalytics/ipex-llm-inference-cpp-xpu:2.2.0b20250101` has been released.",
          "created_at": "2025-01-03T02:10:39Z"
        }
      ]
    },
    {
      "issue_number": 12572,
      "title": "Add Model Accuracy Benchmark support for large Models/Multi-dGPUs",
      "body": "Hi,\r\nThe current benchmark cannot satisfy models that exceed one dGPU memory, please add multi-GPU support for benchmarks like C-eval.\r\nThanks!",
      "state": "open",
      "author": "RobinJing",
      "author_type": "User",
      "created_at": "2024-12-18T07:16:59Z",
      "updated_at": "2024-12-26T02:12:00Z",
      "closed_at": null,
      "labels": [
        "user issue",
        "multi-arc"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12572/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "liu-shaojun"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12572",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12572",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:47.472067",
      "comments": [
        {
          "author": "liu-shaojun",
          "body": "We’ve already provided a guide for running the C-eval benchmark on multiple GPUs via Teams and are addressing any follow-up questions there.",
          "created_at": "2024-12-26T02:11:59Z"
        }
      ]
    },
    {
      "issue_number": 12551,
      "title": "IndexError while conducting GPT-2 Large benchmark for 1k input tokens.",
      "body": "Hi!\r\n\r\nI followed the instructions in [python/llm/dev/benchmark/all-in-one](https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/dev/benchmark/all-in-one) to run benchmark for GPT2-large model.\r\nI'm encountering an IndexError while conducting the all-in-one benchmark (test-api: optimized-model) for the GPT-2 Large model for 1024/128 input/output tokens. It works fine with 512 tokens.\r\n\r\n- Model: openai-community/gpt2-large\r\n- Input/output tokens: 1024/128\r\n- Error: IndexError: index out of range in self\r\n\r\nAny assistance would be greatly appreciated. Thank you.",
      "state": "open",
      "author": "JoAnnHang",
      "author_type": "User",
      "created_at": "2024-12-16T02:33:45Z",
      "updated_at": "2024-12-24T06:34:31Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12551/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "lzivan"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12551",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12551",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:49.445039",
      "comments": [
        {
          "author": "lzivan",
          "body": "Hi @JoAnnHang, we are currently reproducing this issue. And will update here for any updates :)",
          "created_at": "2024-12-18T02:01:55Z"
        },
        {
          "author": "lzivan",
          "body": "Hi @JoAnnHang, we've reproduced the issue, will get back to you once we have a solution :)",
          "created_at": "2024-12-18T19:52:09Z"
        },
        {
          "author": "ACupofAir",
          "body": "\r\n\r\n\r\n> Hi!  你好！\r\n> \r\n> I followed the instructions in [python/llm/dev/benchmark/all-in-one](https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/dev/benchmark/all-in-one) to run benchmark for GPT2-large model.我按照 python/llm为 GPT2-large 模型运行 benchmark。 I'm encountering an IndexError whil",
          "created_at": "2024-12-24T06:34:30Z"
        }
      ]
    },
    {
      "issue_number": 12584,
      "title": "RuntimeError: XPU out of memory on WSL2 vLLM running Qwen2.5-7B-Instruct, sym_int4, Arc A770",
      "body": "Hello everyone,\r\n\r\nI'm trying to get vLLM running inside a docker container on Windows 11. I followed the docker Windows quickstart guide.\r\n\r\nI was using the same configuration files for docker on Ubuntu before, which was working. On Windows on the other hand I'm not able to run any larger model than Qwen2.5-1.5B-Instruct. The 7b model should easily fit inside the VRAM, but I get an XPU out of memory error. It seems like it happens once it tried to allocate more than 1GB of VRAM at once. I saw some issue with a similar error message and behavior somewhere else, but it was never resolved.\r\n\r\nI would guess it is related to WSL2 as it was working on Ubuntu. \r\n\r\nHere is the detailed log:\r\n```\r\nintel-gpu-vllm  | 2024-12-20 00:15:16,084 - INFO - intel_extension_for_pytorch auto imported\r\nintel-gpu-vllm  | WARNING 12-20 00:15:17 config.py:1656] Casting torch.bfloat16 to torch.float16.\r\nintel-gpu-vllm  | 2024-12-20 00:15:19,642       INFO worker.py:1821 -- Started a local Ray instance.\r\nintel-gpu-vllm  | INFO 12-20 00:15:20 llm_engine.py:226] Initializing an LLM engine (v0.6.2+ipexllm) with config: model='/llm/models/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='/llm/models/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=xpu, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen2.5-7B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\r\nintel-gpu-vllm  | INFO 12-20 00:15:20 ray_gpu_executor.py:135] use_ray_spmd_worker: False\r\nintel-gpu-vllm  | (pid=425) /usr/local/lib/python3.11/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libpng16.so.16: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\nintel-gpu-vllm  | (pid=425)   warn(\r\nintel-gpu-vllm  | (pid=425) 2024-12-20 00:15:23,011 - INFO - intel_extension_for_pytorch auto imported\r\nintel-gpu-vllm  | observability_config is ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False)\r\nintel-gpu-vllm  | INFO 12-20 00:15:24 selector.py:193] Cannot use _Backend.FLASH_ATTN backend on XPU.\r\nintel-gpu-vllm  | INFO 12-20 00:15:24 selector.py:138] Using IPEX attention backend.\r\nintel-gpu-vllm  | INFO 12-20 00:15:24 selector.py:193] Cannot use _Backend.FLASH_ATTN backend on XPU.\r\nintel-gpu-vllm  | INFO 12-20 00:15:24 selector.py:138] Using IPEX attention backend.\r\nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\r\nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:10<00:31, 10.36s/it]\r\nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:18<00:17,  8.94s/it]\r\nLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:26<00:08,  8.54s/it]\r\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:33<00:00,  8.17s/it]\r\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:33<00:00,  8.49s/it]\r\nintel-gpu-vllm  |\r\nintel-gpu-vllm  | 2024-12-20 00:15:58,504 - INFO - Converting the current model to sym_int4 format......\r\nintel-gpu-vllm  | 2024-12-20 00:15:58,505 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\r\nintel-gpu-vllm  | 2024-12-20 00:16:06,683 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\r\nintel-gpu-vllm  | ERROR 12-20 00:16:06 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.\r\nintel-gpu-vllm  | ERROR 12-20 00:16:06 worker_base.py:464] Traceback (most recent call last):\r\nintel-gpu-vllm  | ERROR 12-20 00:16:06 worker_base.py:464]   File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/worker_base.py\", line 456, in execute_method\r\nintel-gpu-vllm  | ERROR 12-20 00:16:06 worker_base.py:464]     return executor(*args, **kwargs)\r\nintel-gpu-vllm  | ERROR 12-20 00:16:06 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^\r\nintel-gpu-vllm  | ERROR 12-20 00:16:06 worker_base.py:464]   File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/worker.py\", line 183, in load_model\r\nintel-gpu-vllm  | ERROR 12-20 00:16:06 worker_base.py:464]     self.model_runner.load_model()\r\nintel-gpu-vllm  | ERROR 12-20 00:16:06 worker_base.py:464]   File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/model_convert.py\", line 110, in _ipex_llm_load_model\r\nintel-gpu-vllm  | ERROR 12-20 00:16:06 worker_base.py:464]     self.model = self.model.to(device=self.device_config.device,\r\nintel-gpu-vllm  | ERROR 12-20 00:16:06 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nintel-gpu-vllm  | ERROR 12-20 00:16:06 worker_base.py:464]   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1160, in to\r\nintel-gpu-vllm  | ERROR 12-20 00:16:06 worker_base.py:464]     return self._apply(convert)\r\nintel-gpu-vllm  | ERROR 12-20 00:16:06 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^\r\nintel-gpu-vllm  | ERROR 12-20 00:16:06 worker_base.py:464]   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\r\nintel-gpu-vllm  | ERROR 12-20 00:16:06 worker_base.py:464]     module._apply(fn)\r\nintel-gpu-vllm  | ERROR 12-20 00:16:06 worker_base.py:464]   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\r\nintel-gpu-vllm  | ERROR 12-20 00:16:06 worker_base.py:464]     module._apply(fn)\r\nintel-gpu-vllm  | ERROR 12-20 00:16:06 worker_base.py:464]   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\r\nintel-gpu-vllm  | ERROR 12-20 00:16:06 worker_base.py:464]     param_applied = fn(param)\r\nintel-gpu-vllm  | ERROR 12-20 00:16:06 worker_base.py:464]                     ^^^^^^^^^\r\nintel-gpu-vllm  | ERROR 12-20 00:16:06 worker_base.py:464]   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\r\nintel-gpu-vllm  | ERROR 12-20 00:16:06 worker_base.py:464]     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\r\nintel-gpu-vllm  | ERROR 12-20 00:16:06 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nintel-gpu-vllm  | ERROR 12-20 00:16:06 worker_base.py:464] RuntimeError: XPU out of memory. Tried to allocate 1.02 GiB (GPU 0; 15.56 GiB total capacity; 0 bytes already allocated; 0 bytes reserved in total by PyTorch)\r\nintel-gpu-vllm  | Process SpawnProcess-33:\r\nintel-gpu-vllm  | Traceback (most recent call last):\r\nintel-gpu-vllm  |   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\r\nintel-gpu-vllm  |     self.run()\r\nintel-gpu-vllm  |   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\r\nintel-gpu-vllm  |     self._target(*self._args, **self._kwargs)\r\nintel-gpu-vllm  |   File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 145, in run_mp_engine\r\nintel-gpu-vllm  |     engine = IPEXLLMMQLLMEngine.from_engine_args(engine_args=engine_args,\r\nintel-gpu-vllm  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nintel-gpu-vllm  |   File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 133, in from_engine_args\r\nintel-gpu-vllm  |     return super().from_engine_args(engine_args, usage_context, ipc_path)\r\nintel-gpu-vllm  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nintel-gpu-vllm  |   File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/multiprocessing/engine.py\", line 138, in from_engine_args\r\nintel-gpu-vllm  |     return cls(\r\nintel-gpu-vllm  |            ^^^^\r\nintel-gpu-vllm  |   File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/multiprocessing/engine.py\", line 78, in __init__\r\nintel-gpu-vllm  |     self.engine = LLMEngine(*args,\r\nintel-gpu-vllm  |                   ^^^^^^^^^^^^^^^^\r\nintel-gpu-vllm  |   File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py\", line 325, in __init__\r\nintel-gpu-vllm  |     self.model_executor = executor_class(\r\nintel-gpu-vllm  |                           ^^^^^^^^^^^^^^^\r\nintel-gpu-vllm  |   File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/distributed_gpu_executor.py\", line 26, in __init__\r\nintel-gpu-vllm  |     super().__init__(*args, **kwargs)\r\nintel-gpu-vllm  |   File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/xpu_executor.py\", line 55, in __init__\r\nintel-gpu-vllm  |     self._init_executor()\r\nintel-gpu-vllm  |   File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/ray_gpu_executor.py\", line 65, in _init_executor\r\nintel-gpu-vllm  |     self._init_workers_ray(placement_group)\r\nintel-gpu-vllm  |   File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/ray_gpu_executor.py\", line 281, in _init_workers_ray\r\nintel-gpu-vllm  |     self._run_workers(\"load_model\",\r\nintel-gpu-vllm  |   File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/ray_gpu_executor.py\", line 520, in _run_workers\r\nintel-gpu-vllm  |     self.driver_worker.execute_method(method, *driver_args,\r\nintel-gpu-vllm  |   File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/worker_base.py\", line 465, in execute_method\r\nintel-gpu-vllm  |     raise e\r\nintel-gpu-vllm  |   File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/worker_base.py\", line 456, in execute_method\r\nintel-gpu-vllm  |     return executor(*args, **kwargs)\r\nintel-gpu-vllm  |            ^^^^^^^^^^^^^^^^^^^^^^^^^\r\nintel-gpu-vllm  |   File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/worker.py\", line 183, in load_model\r\nintel-gpu-vllm  |     self.model_runner.load_model()\r\nintel-gpu-vllm  |   File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/model_convert.py\", line 110, in _ipex_llm_load_model\r\nintel-gpu-vllm  |     self.model = self.model.to(device=self.device_config.device,\r\nintel-gpu-vllm  |                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nintel-gpu-vllm  |   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1160, in to\r\nintel-gpu-vllm  |     return self._apply(convert)\r\nintel-gpu-vllm  |            ^^^^^^^^^^^^^^^^^^^^\r\nintel-gpu-vllm  |   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\r\nintel-gpu-vllm  |     module._apply(fn)\r\nintel-gpu-vllm  |   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\r\nintel-gpu-vllm  |     module._apply(fn)\r\nintel-gpu-vllm  |   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\r\nintel-gpu-vllm  |     param_applied = fn(param)\r\nintel-gpu-vllm  |                     ^^^^^^^^^\r\nintel-gpu-vllm  |   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\r\nintel-gpu-vllm  |     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\r\nintel-gpu-vllm  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nintel-gpu-vllm  | RuntimeError: XPU out of memory. Tried to allocate 1.02 GiB (GPU 0; 15.56 GiB total capacity; 0 bytes already allocated; 0 bytes reserved in total by PyTorch)\r\n```\r\n\r\n\r\nThis is my Dockerfile:\r\n```\r\nFROM intelanalytics/ipex-llm-serving-xpu:latest\r\n\r\nCOPY start-vllm-service-Qwen2.5-1.5B-Instruct.sh /llm/start-vllm-service-Qwen2.5-1.5B-Instruct.sh\r\nCOPY start-vllm-service-Qwen2.5-7B-Instruct.sh /llm/start-vllm-service-Qwen2.5-7B-Instruct.sh\r\nCOPY start-vllm-service-Qwen2.5-7B-Instruct-AWQ.sh /llm/start-vllm-service-Qwen2.5-7B-Instruct-AWQ.sh\r\nCOPY start-vllm-service-Qwen2.5-32B-Instruct-AWQ.sh /llm/start-vllm-service-Qwen2.5-32B-Instruct-AWQ.sh\r\n\r\n# Set executable permissions for the script\r\nRUN chmod +x /llm/start-vllm-service-Qwen2.5-1.5B-Instruct.sh\r\nRUN chmod +x /llm/start-vllm-service-Qwen2.5-7B-Instruct.sh\r\nRUN chmod +x /llm/start-vllm-service-Qwen2.5-7B-Instruct-AWQ.sh\r\nRUN chmod +x /llm/start-vllm-service-Qwen2.5-32B-Instruct-AWQ.sh\r\n\r\nWORKDIR /llm/\r\n\r\nENTRYPOINT [\"./start-vllm-service-Qwen2.5-7B-Instruct.sh\"]\r\n```\r\n\r\nThis is my bash script to start serving vLLM:\r\n\r\n```\r\n#!/bin/bash\r\nmodel=\"/llm/models/Qwen2.5-7B-Instruct\"\r\nserved_model_name=\"Qwen2.5-7B-Instruct\"\r\n\r\nexport ZES_ENABLE_SYSMAN=1\r\n\r\nexport SYCL_CACHE_PERSISTENT=1\r\n\r\nexport SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1\r\nexport ONEAPI_DEVICE_SELECTOR=level_zero:0\r\n\r\nsource /opt/intel/1ccl-wks/setvars.sh\r\n\r\npython -m ipex_llm.vllm.xpu.entrypoints.openai.api_server \\\r\n  --served-model-name $served_model_name \\\r\n  --port 8000 \\\r\n  --model $model \\\r\n  --trust-remote-code \\\r\n  --gpu-memory-utilization 0.9 \\\r\n  --device xpu \\\r\n  --dtype float16 \\\r\n  --load-in-low-bit sym_int4 \\\r\n  --max-model-len 1024 \\\r\n  --max-num-batched-tokens 1024 \\\r\n  --max-num-seqs 4 \\\r\n  --tensor-parallel-size 1 \\\r\n  --enforce-eager \\\r\n  --disable-async-output-proc \\\r\n  --distributed-executor-backend ray\r\n```\r\n\r\nI'm thankful for any suggestions what I could try.",
      "state": "open",
      "author": "nkt-dk",
      "author_type": "User",
      "created_at": "2024-12-19T16:23:20Z",
      "updated_at": "2024-12-23T01:44:57Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12584/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hzjane"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12584",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12584",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:49.679659",
      "comments": [
        {
          "author": "hzjane",
          "body": "We now only enable it on 2.1.0 version, And the performance will be very poor. We recommend you to switch the physical system to Ubuntu to run the latest image to run.",
          "created_at": "2024-12-23T01:44:56Z"
        }
      ]
    },
    {
      "issue_number": 12549,
      "title": "[MTL igpu][open webui] PI_ERROR_BUILD_PROGRAM_FAILURE ",
      "body": "Description:\r\n* It's normal to run llm with ollama cli (**backend ipex-llm**) on mtl igpu, as below\r\n$ollama run llama3.1:8b\r\n![image](https://github.com/user-attachments/assets/afb294ed-25dc-41f3-bfcc-c62e6a3b5770)\r\n\r\n* However it's failed to run same model by open-webui. The error like \r\n![image](https://github.com/user-attachments/assets/2ec9044b-55b2-4946-a558-c82895cd955b)\r\n\r\nNo idea what happens. For webui install, I follow the pip install in [webui](https://github.com/open-webui/open-webui?tab=readme-ov-file) and make it works with ollama server (I can find my local model in webui).\r\nI see we have a more complex install here - https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/open_webui_with_ollama_quickstart.md - looks like a customed install in ipex. I'll try that and give feedbacks.\r\n\r\nHowever why we can't use the official webui install with a normal running ollama serve?",
      "state": "closed",
      "author": "Mingqi2",
      "author_type": "User",
      "created_at": "2024-12-15T07:41:58Z",
      "updated_at": "2024-12-20T09:37:53Z",
      "closed_at": "2024-12-20T09:37:53Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12549/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12549",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12549",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:49.869022",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @Mingqi2 , which version of ollama are you running? You may see it by `ollama --version`.",
          "created_at": "2024-12-16T01:42:32Z"
        },
        {
          "author": "Mingqi2",
          "body": "@sgwhat thanks your quick respose! The version is \r\n![image](https://github.com/user-attachments/assets/2a3d3e9a-b299-4372-b0f4-38b3b9981a36)\r\n",
          "created_at": "2024-12-16T05:57:41Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @Mingqi2, we have released the new version of ollama, please run `pip install --pre --upgrade ipex-llm[cpp]` to switch to the latest version.",
          "created_at": "2024-12-16T06:00:44Z"
        },
        {
          "author": "Mingqi2",
          "body": "No luck happens : ). update & restart serve. Still same issue with latest version\r\n![image](https://github.com/user-attachments/assets/2f35d145-0296-41e7-ad54-d87174bb88ce)\r\n\r\nMaybe I need completely follow the instruction in https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quicksta",
          "created_at": "2024-12-16T06:25:04Z"
        },
        {
          "author": "Mingqi2",
          "body": "I tried the webui build method (win11) in https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/open_webui_with_ollama_quickstart.md. **The same error was evaluated again.**\r\nThe webui version is **0.4.8**\r\n![image](https://github.com/user-attachments/assets/c4211934-b056-470a",
          "created_at": "2024-12-17T01:23:12Z"
        }
      ]
    },
    {
      "issue_number": 12586,
      "title": "[BMG dgfx][ipex-llm[cpp]] low performance and gpu using when running llama.cpp inference on B580",
      "body": "I'm running llama.cpp which is in ipex-llm[cpp] build 2024.12.17 on Intel B580 dGfx. \r\n\r\nand I found the gpu usage is near 40% and the token per second is around 20TPS. \r\n\r\n![image](https://github.com/user-attachments/assets/859225be-c269-4967-ad59-c1136b39fde4)\r\n\r\n\r\n\r\nThe command I'm using is\r\n```\r\nset SYCL_CACHE_PERSISTENT=1\r\n\r\nllama-cli -m ..\\glm4.gguf -n 32 --prompt \"why sky is blue?\" -c 2048 -e -ngl 999 --color --no-mmap -n 4096\r\n```\r\n\r\nand the console output\r\n```\r\n\r\n\r\nC:\\Users\\test\\Documents\\bmg_test\\libs>llama-cli -m ..\\glm4.gguf -n 32 --prompt \"why sky is blue?\" -c 2048 -e -ngl 999 --color --no-mmap -n 4096\r\nbuild: 1 (1133019) with MSVC 19.38.33133.0 for\r\nmain: llama backend init\r\nmain: load the model and apply lora adapter, if any\r\nllama_model_loader: loaded meta data with 24 key-value pairs and 283 tensors from ..\\glm4.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = chatglm\r\nllama_model_loader: - kv   1:                               general.name str              = glm-4-9b-chat\r\nllama_model_loader: - kv   2:                     chatglm.context_length u32              = 131072\r\nllama_model_loader: - kv   3:                   chatglm.embedding_length u32              = 4096\r\nllama_model_loader: - kv   4:                chatglm.feed_forward_length u32              = 13696\r\nllama_model_loader: - kv   5:                        chatglm.block_count u32              = 40\r\nllama_model_loader: - kv   6:               chatglm.attention.head_count u32              = 32\r\nllama_model_loader: - kv   7:            chatglm.attention.head_count_kv u32              = 2\r\nllama_model_loader: - kv   8:   chatglm.attention.layer_norm_rms_epsilon f32              = 0.000000\r\nllama_model_loader: - kv   9:                          general.file_type u32              = 2\r\nllama_model_loader: - kv  10:               chatglm.rope.dimension_count u32              = 64\r\nllama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false\r\nllama_model_loader: - kv  12:                     chatglm.rope.freq_base f32              = 5000000.000000\r\nllama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = chatglm-bpe\r\nllama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,151552]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,151552]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151073]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\r\nllama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 151329\r\nllama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151329\r\nllama_model_loader: - kv  20:                tokenizer.ggml.eot_token_id u32              = 151336\r\nllama_model_loader: - kv  21:            tokenizer.ggml.unknown_token_id u32              = 151329\r\nllama_model_loader: - kv  22:                    tokenizer.chat_template str              = [gMASK]<sop>{% for item in messages %...\r\nllama_model_loader: - kv  23:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:  121 tensors\r\nllama_model_loader: - type q4_0:  161 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllm_load_vocab: special_eot_id is not in special_eog_ids - the tokenizer config may be incorrect\r\nllm_load_vocab: special tokens cache size = 223\r\nllm_load_vocab: token to piece cache size = 0.9732 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = chatglm\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 151552\r\nllm_load_print_meta: n_merges         = 151073\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 131072\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_layer          = 40\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 2\r\nllm_load_print_meta: n_rot            = 64\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 16\r\nllm_load_print_meta: n_embd_k_gqa     = 256\r\nllm_load_print_meta: n_embd_v_gqa     = 256\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.6e-07\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 13696\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 5000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 131072\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = 9B\r\nllm_load_print_meta: model ftype      = Q4_0\r\nllm_load_print_meta: model params     = 9.40 B\r\nllm_load_print_meta: model size       = 5.08 GiB (4.64 BPW)\r\nllm_load_print_meta: general.name     = glm-4-9b-chat\r\nllm_load_print_meta: EOS token        = 151329 '<|endoftext|>'\r\nllm_load_print_meta: UNK token        = 151329 '<|endoftext|>'\r\nllm_load_print_meta: PAD token        = 151329 '<|endoftext|>'\r\nllm_load_print_meta: LF token         = 128 'Ä'\r\nllm_load_print_meta: EOT token        = 151336 '<|user|>'\r\nllm_load_print_meta: EOG token        = 151329 '<|endoftext|>'\r\nllm_load_print_meta: EOG token        = 151336 '<|user|>'\r\nllm_load_print_meta: max token length = 1024\r\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nggml_sycl_init: SYCL_USE_XMX: yes\r\nggml_sycl_init: found 1 SYCL devices:\r\nllm_load_tensors: ggml ctx size =    0.28 MiB\r\nllm_load_tensors: offloading 40 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 41/41 layers to GPU\r\nllm_load_tensors:      SYCL0 buffer size =  4863.85 MiB\r\nllm_load_tensors:  SYCL_Host buffer size =   333.00 MiB\r\n................................................................................\r\n[SYCL] call ggml_check_sycl\r\nggml_check_sycl: GGML_SYCL_DEBUG: 0\r\nllama_new_context_with_model: n_ctx      = 2048\r\nggml_check_sycl: GGML_SYCL_F16: no\r\nllama_new_context_with_model: n_batch    = 2048\r\nfound 1 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |\r\n    |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |\r\n    |\r\nllama_new_context_with_model: n_ubatch   = 2048\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\nllama_new_context_with_model: flash_attn = 0\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\nllama_new_context_with_model: freq_base  = 5000000.0\r\nllama_new_context_with_model: freq_scale = 1\r\n| 0| [level_zero:gpu:0]|                Intel Arc B580 Graphics|    1.6|    160|    1024|   32| 12450M|            1.3.31155|\r\nllama_kv_cache_init:      SYCL0 KV buffer size =    80.00 MiB\r\nllama_new_context_with_model: KV self size  =   80.00 MiB, K (f16):   40.00 MiB, V (f16):   40.00 MiB\r\nllama_new_context_with_model:  SYCL_Host  output buffer size =     0.58 MiB\r\nllama_new_context_with_model:      SYCL0 compute buffer size =  1248.00 MiB\r\nllama_new_context_with_model:  SYCL_Host compute buffer size =    48.02 MiB\r\nllama_new_context_with_model: graph nodes  = 1446\r\nllama_new_context_with_model: graph splits = 2\r\nllama_init_from_gpt_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\r\nmain: llama threadpool init, n_threads = 20\r\n\r\nsystem_info: n_threads = 20 (n_threads_batch = 20) / 20 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 |\r\n\r\nsampler seed: 4071779348\r\nsampler params:\r\n        repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\r\n        top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\r\n        mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\r\nsampler chain: logits -> logit-bias -> penalties -> top-k -> tail-free -> typical -> top-p -> min-p -> temp-ext -> softmax -> dist\r\ngenerate: n_ctx = 2048, n_batch = 4096, n_predict = 4096, n_keep = 0\r\n\r\nwhy sky is blue? what makes the color of the sky blue?\r\nThe color of the sky is blue because of the way Earth's atmosphere interacts with sunlight. When sunlight passes through the Earth's atmosphere, it encounters molecules and particles of gas, dust, and water vapor. These particles scatter the light in all directions.\r\nThe scattering of light is more effective for shorter wavelengths, such as blue light, than for longer wavelengths, such as red light. This is because the shorter waves of blue light are more easily bent as they pass around the particles in the atmosphere. This phenomenon is known as Rayleigh scattering.\r\nAs the blue light scatters in all directions, it illuminates the sky from all angles, making the sky appear blue during the day. The intensity of the blue color is strongest at the horizon and becomes lighter towards the zenith (the highest point in the sky), which is why the sky often appears a lighter blue or even white at high altitudes or during sunrise and sunset.\r\nAt night, when the sun is below the horizon, the sky is no longer illuminated by sunlight, and therefore does not appear blue. Instead, the sky often appears black, or a dark blue, because we are looking into space rather than through the atmosphere.\r\nIn addition to Rayleigh scattering, which is responsible for the blue color of the sky during the day, there are other factors that can affect the color of the sky, such as:\r\n- **Rayleigh scattering by larger particles**: This can cause the sky to appear whitish or grayish when there are a lot of particles in the air, such as during dust storms or volcanic eruptions.\r\n- **Mie scattering**: This type of scattering, which is caused by larger particles such as water droplets, can cause the sky to appear red or orange during sunrise and sunset.\r\n- **Molecular absorption**: Certain gases in the atmosphere, such as ozone, can absorb certain wavelengths of light, which can affect the color of the sky.\r\nIn summary, the blue color of the sky is primarily due to the scattering of sunlight by molecules in the Earth's atmosphere, with shorter wavelengths (like blue) being scattered more than longer wavelengths (like red). This phenomenon is known as Rayleigh scattering, and it is the primary reason we see a blue sky during the day. During sunrise and sunset, other scattering processes can also contribute to the colors we see. [end of text]\r\n\r\n\r\nllama_perf_sampler_print:    sampling time =      40.85 ms /   482 runs   (    0.08 ms per token, 11800.42 tokens per second)\r\nllama_perf_context_print:        load time =    8706.65 ms\r\nllama_perf_context_print: prompt eval time =     120.56 ms /     5 tokens (   24.11 ms per token,    41.47 tokens per second)\r\nllama_perf_context_print:        eval time =   22397.03 ms /   476 runs   (   47.05 ms per token,    21.25 tokens per second)\r\nllama_perf_context_print:       total time =   22625.44 ms /   481 tokens\r\n\r\n```",
      "state": "open",
      "author": "jianjungu",
      "author_type": "User",
      "created_at": "2024-12-20T01:52:37Z",
      "updated_at": "2024-12-20T06:45:55Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12586/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "leonardozcm"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12586",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12586",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:50.087146",
      "comments": [
        {
          "author": "leonardozcm",
          "body": "hi, I have tried glm4 in BMG and it runs faster(25.81ms) than 47.05ms per token. Since your driver version is newer than mine(101.6236), I wonder what is your oneapi version(2024.2.1 recommended).\r\n",
          "created_at": "2024-12-20T06:45:53Z"
        }
      ]
    },
    {
      "issue_number": 12555,
      "title": "Frequent Crashes with Ollama on ARC770 and i7-14700K",
      "body": "## Description\r\nOllama keeps crashing on my system when I attempt to run models or after interacting with them for 1-2 prompts. Even using q4_K_M, which I understand is designed to be resource-efficient, the crashes persist.\r\n\r\n**Examples of Crashing Models:**\r\n* `llama3.2:3b-instruct-q4_K_M`: Crashes after 1-2 prompts.\r\n* `qwen2.5-coder:3b-instruct-q4_K_M`: Also crashes after a few prompts.\r\n**Models That Do Not Crash:**\r\n* `gemma:2b-instruct-q5_K_M`: Runs without issues but is not suitable for my needs.\r\n\r\nAdditionally, I am unable to run `sycl-ls` to troubleshoot further. When I attempt it, I receive:\r\n```\r\nbash: sycl-ls: command not found\r\n```\r\n\r\n## Environment\r\n* GPU: Intel ARC770 (16GB VRAM)\r\n* CPU: Intel i7-14700K\r\n* RAM: 64GB\r\n* OS: Ubuntu 22.04.3 LTS\r\n* Ollama Version:  0.4.6-ipexllm-20241214\r\n\r\n## Steps to Reproduce\r\n\r\n* Dockerfile:\r\n```\r\nFROM docker.io/intelanalytics/ipex-llm-inference-cpp-xpu:latest\r\n\r\nENV ZES_ENABLE_SYSMAN=1\r\nENV OLLAMA_HOST=0.0.0.0:11434\r\nENV OLLAMA_KEEP_ALIVE=3600\r\nENV DEVICE=Arc\r\n\r\nRUN mkdir -p /llm/ollama; \\\r\n    cd /llm/ollama; \\\r\n    init-ollama;\r\n\r\nENV LD_LIBRARY_PATH=\".:$LD_LIBRARY_PATH\"\r\n\r\nWORKDIR /llm/ollama\r\n\r\nENTRYPOINT [\"./ollama\", \"serve\"]\r\n```\r\n\r\n* docker-compose file:\r\n```\r\nversion: \"3.9\"\r\nservices:\r\n  ollama-intel-gpu:\r\n    build:\r\n      context: .\r\n      dockerfile: Dockerfile\r\n    container_name: ollama-intel-gpu\r\n    image: ollama-intel-gpu:latest\r\n    restart: always\r\n    devices:\r\n      - /dev/dri:/dev/dri\r\n      - /dev/dxg:/dev/dxg\r\n    volumes:\r\n      - /usr/lib/wsl:/usr/lib/wsl\r\n      - /tmp/.X11-unix:/tmp/.X11-unix\r\n      - ollama-intel-gpu:/root/.ollama\r\n    environment:\r\n      - DISPLAY=${DISPLAY}\r\n      - PATH=/llm/ollama:$PATH\r\nvolumes:\r\n  ollama-intel-gpu: {}\r\n```\r\n* Exec into container `podman exec -it ollama-intel-gpu /bin/bash `\r\n* Run Ollama with `llama3.2:3b-instruct-q4_K_M` or `qwen2.5-coder:3b-instruct-q4_K_M`.\r\n* Interact with the model (1-2 prompts).\r\n* Observe a crash.\r\n\r\n## Logs\r\n[ollama_intel_gpu_logs.txt](https://github.com/user-attachments/files/18150095/ollama_intel_gpu_logs.txt)\r\n[ollama_intel_gpu_2_logs.txt](https://github.com/user-attachments/files/18150096/ollama_intel_gpu_2_logs.txt)\r\n\r\n\r\n\r\n",
      "state": "open",
      "author": "sirlegendary",
      "author_type": "User",
      "created_at": "2024-12-16T12:43:07Z",
      "updated_at": "2024-12-20T02:16:47Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12555/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12555",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12555",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:50.270677",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Similar to https://github.com/intel-analytics/ipex-llm/issues/12550, you may try the latest version of ollama with `pip install --pre --upgrade ipex-llm[cpp]`.",
          "created_at": "2024-12-17T01:32:07Z"
        },
        {
          "author": "sirlegendary",
          "body": "Thanks, i have upgraded and still having the same issues:\r\n![image](https://github.com/user-attachments/assets/08fa516f-2803-4e17-83e1-74939f0b0255)\r\n\r\nI think it doesnt use my ARC GPU, also i still cant run `sycl-ls` which used to work fine before.\r\n\r\nHere is the logs for after upgrade to `0.4.6-ip",
          "created_at": "2024-12-17T09:57:46Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @sirlegendary, could you please provide us more information with the following script: https://github.com/intel-analytics/ipex-llm/issues/12550#issuecomment-2544665848",
          "created_at": "2024-12-18T01:47:38Z"
        },
        {
          "author": "leonardozcm",
          "body": "We have re-checked and refactored the device decision logic based on your error log, it would be nice if you can try this tomorrow and we can see if it solves your problem.",
          "created_at": "2024-12-18T07:44:37Z"
        },
        {
          "author": "sirlegendary",
          "body": "> Hi @sirlegendary, could you please provide us more information with the following script: [#12550 (comment)](https://github.com/intel-analytics/ipex-llm/issues/12550#issuecomment-2544665848)\r\n\r\nThe container throws an error when i run the `dpcpp` command\r\n![image](https://github.com/user-attachmen",
          "created_at": "2024-12-18T11:50:02Z"
        }
      ]
    },
    {
      "issue_number": 12547,
      "title": "Cannot find dGPU when install ollama on Windows",
      "body": "I had install the oneapi, update latest  intel driver . \r\nAfter I installed ollama according to the tutorial  [Run Ollama with IPEX-LLM on Intel GPU](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.md) \r\n\r\n![Run Ollama with IPEX-LLM on Intel GPU](https://github.com/user-attachments/assets/9f506ea5-2503-4e0f-bf73-cc078b9a4f33)\r\n\r\nand when I exec  \"ollama serve\",I get the image\r\n![image](https://github.com/user-attachments/assets/9f25451e-cd02-44f6-8fc6-a9b810c72505)\r\n\r\n![image](https://github.com/user-attachments/assets/f3bc126c-d551-48da-bea0-8e913fff7198)\r\n\r\nI have set the system environment variable\r\n![image](https://github.com/user-attachments/assets/1e47e477-4813-4a7f-b56f-091db53f379c)\r\n\r\n\r\n",
      "state": "closed",
      "author": "yshuai",
      "author_type": "User",
      "created_at": "2024-12-13T14:36:43Z",
      "updated_at": "2024-12-18T05:35:39Z",
      "closed_at": "2024-12-18T05:35:39Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12547/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12547",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12547",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:50.508187",
      "comments": [
        {
          "author": "MarsSovereign",
          "body": "same problem, the new ollama client terminal in ipex-llm[cpp] may have some issues",
          "created_at": "2024-12-14T05:08:30Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @yshuai @MarsSovereign, \r\n\r\nOn the Ollama server side, it is observed that the CPU usage log is normal, we will update this log soon. After running `ollama run <model>`, it will run on the GPU. Please also refer to the Troubleshooting section in [ipex-llm ollama documentation](https://github.com/",
          "created_at": "2024-12-16T01:40:12Z"
        },
        {
          "author": "yshuai",
          "body": "@sgwhat Thank you,According to your suggestion, I solved this problem",
          "created_at": "2024-12-18T05:35:39Z"
        }
      ]
    },
    {
      "issue_number": 12216,
      "title": "Ollama error after a few requests - ubatch must be set as the times of VS",
      "body": "Hi,\r\nMy config: A770 + Ollama + OpenWebui + intelanalytics/ipex-llm-inference-cpp-xpu:latest docker\r\n\r\nAfter 2-3 chat message I get this error:\r\n```ollama_llama_server: /home/runner/_work/llm.cpp/llm.cpp/llm.cpp/bigdl-core-xe/llama_backend/sdp_xmx_kernel.cpp:191: void sdp_causal_xmx_kernel(const void *, const void *, const void *, const void *, const void *, const void *, float *, const int64_t, const int64_t, const int64_t, const int64_t, const int64_t, const int64_t, const int64_t, const int64_t, const int64_t, const int64_t, const int64_t, const int64_t, const int64_t, const int64_t, const int64_t, const int64_t, const int64_t, const int64_t, const int64_t, const int, const int, const int, const int, const int, const float, sycl::queue &) [HD = 128, VS = 32, RepeatCount = 8, Depth = 16, ExecuteSize = 8]: Assertion `(context_length-seq_len)%VS==0 && \"ubatch must be set as the times of VS\\n\"' failed.```\r\n\r\nIf click the 'refresh'/'again' button in the OpenWebui chat Ollama reloads the model and it works, but again, after a few messages it fails.\r\nInteresting thing is if I keep pressing the refresh button it works every time.\r\n![Screenshot From 2024-10-17 02-42-02](https://github.com/user-attachments/assets/be96be4c-0fcd-417d-ae47-3d832cbe3b65)\r\n![Screenshot From 2024-10-17 02-45-29](https://github.com/user-attachments/assets/8817fe7b-fd77-48cd-90df-16bb5296af52)\r\n\r\nI've tried multiple models and the OpenWebui in the Intel docker + the official latest version.\r\n\r\nCan someone point me to the right direction? Thank you",
      "state": "closed",
      "author": "mateHD",
      "author_type": "User",
      "created_at": "2024-10-17T01:46:11Z",
      "updated_at": "2024-12-18T01:46:51Z",
      "closed_at": "2024-12-18T01:46:51Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12216/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "leonardozcm"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12216",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12216",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:50.701947",
      "comments": [
        {
          "author": "mateHD",
          "body": "\"Interesting thing is if I keep pressing the refresh button it works every time.\"\r\n\r\nI'd like to correct myself. I've tried this with smaller models and I can click refresh 10 times, it works every time, but now I tried it with Mistral-Small which is a larger model and it failed even with the refres",
          "created_at": "2024-10-17T02:24:41Z"
        },
        {
          "author": "mateHD",
          "body": "[Screencast From 2024-10-17 03-37-55.webm](https://github.com/user-attachments/assets/0001739c-4e94-49b5-a52c-32ede56e1167)\r\n\r\nYou can see a first few fails first try, ollama reloads the model every time, but than by clicking only the refresh button it works perfectly.",
          "created_at": "2024-10-17T02:42:26Z"
        },
        {
          "author": "acane77",
          "body": "> \"Interesting thing is if I keep pressing the refresh button it works every time.\"\r\n> \r\n> I'd like to correct myself. I've tried this with smaller models and I can click refresh 10 times, it works every time, but now I tried it with Mistral-Small which is a larger model and it failed even with the ",
          "created_at": "2024-10-17T07:44:58Z"
        },
        {
          "author": "leonardozcm",
          "body": "Thank you for your feedback, you may try ipex-llm[cpp] latest (version number >= 10.17) **tomorrow**.",
          "created_at": "2024-10-17T10:06:14Z"
        },
        {
          "author": "mateHD",
          "body": "> Thank you for your feedback, you may try ipex-llm[cpp] latest (version number >= 10.17) **tomorrow**.\r\n\r\nThe original issue is fixed, no errors and it doesn't reload, but after the first request the response it total nonsense, just random text. Same behavior with all models.\r\n\r\n![Screenshot From 2",
          "created_at": "2024-10-18T21:06:56Z"
        }
      ]
    },
    {
      "issue_number": 12546,
      "title": "error while loading shared libraries: libmllama.so: cannot open shared object file: No such file or directory",
      "body": "Any recommendations on why and how to fix this?\r\n\r\nI get the above error when i run from container and wsl with Ubuntu22.04.\r\n\r\n```\r\nsycl-ls\r\n[opencl:acc:0] Intel(R) FPGA Emulation Platform for OpenCL(TM), Intel(R) FPGA Emulation Device OpenCL 1.2  [2023.16.12.0.12_195853.xmain-hotfix]\r\n[opencl:cpu:1] Intel(R) OpenCL, Intel(R) Core(TM) i7-14700K OpenCL 3.0 (Build 0) [2023.16.12.0.12_195853.xmain-hotfix]\r\n[opencl:gpu:2] Intel(R) OpenCL Graphics, Intel(R) Graphics [0x56a0] OpenCL 3.0 NEO  [24.39.31294]\r\n[ext_oneapi_level_zero:gpu:0] Intel(R) Level-Zero, Intel(R) Graphics [0x56a0] 1.3 [1.3.29735]\r\n```\r\n\r\nHow to replicate:\r\nFollow this guide: https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Overview/install_gpu.md\r\nthen:\r\n```\r\n~/miniconda3/envs/llm/lib/python3.10/site-packages/bigdl/cpp/libs$ ./ollama serve\r\n./ollama: error while loading shared libraries: libmllama.so: cannot open shared object file: No such file or directory\r\n```\r\n![image](https://github.com/user-attachments/assets/cb37addc-17b5-4568-97d1-95face9ebddc)\r\n![image](https://github.com/user-attachments/assets/b63a8ee4-335f-4182-a395-0dc8826d59a8)\r\n",
      "state": "closed",
      "author": "sirlegendary",
      "author_type": "User",
      "created_at": "2024-12-13T12:54:26Z",
      "updated_at": "2024-12-17T09:32:47Z",
      "closed_at": "2024-12-17T09:32:47Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12546/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12546",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12546",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:50.915378",
      "comments": [
        {
          "author": "przemekmatusiak",
          "body": "@sirlegendary I was struggling with the same error, and I noticed some recent updates in the Ollama Quickstart guide: [[Ollama] Update ipex-llm ollama readme to v0.4.6 #12542 ](https://github.com/intel-analytics/ipex-llm/pull/12542)\r\n\r\nRunning this LD_LIBRARY_PATH update command (described in the PR",
          "created_at": "2024-12-14T17:14:47Z"
        },
        {
          "author": "sirlegendary",
          "body": "Thanks @przemekmatusiak. I get past that now. \r\n\r\nThe container now stops as soon as it comes up with the below ERROR now.\r\n\r\nDockerfile:\r\n```\r\nFROM docker.io/intelanalytics/ipex-llm-inference-cpp-xpu:latest\r\n\r\nENV ZES_ENABLE_SYSMAN=1\r\nENV OLLAMA_HOST=0.0.0.0:11434\r\nENV OLLAMA_INTEL_GPU=true\r\nENV OL",
          "created_at": "2024-12-14T21:59:38Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @sirlegendary , we will check this issue and reply to you soon.",
          "created_at": "2024-12-16T01:45:00Z"
        },
        {
          "author": "hzjane",
          "body": "Hi @sirlegendary . I reproduced this error too on your way. You can try to remove `ENV OLLAMA_INTEL_GPU=true` in Dockerfile and run it again.",
          "created_at": "2024-12-16T09:27:20Z"
        },
        {
          "author": "sirlegendary",
          "body": "Thanks @hzjane, that fixed it! \r\nI will raise another issue to find out why it keeps crashing on me after 2 to 3 prompts.",
          "created_at": "2024-12-16T11:27:40Z"
        }
      ]
    },
    {
      "issue_number": 12550,
      "title": "Assertion error when assessing uploaded files, using Ollama and Open WebUI via Docker",
      "body": "Hi!\r\n\r\nI followed the instructions to [run llama.cpp/Ollama/Open-WebUI on an Intel GPU via Docker](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/DockerGuides/docker_cpp_xpu_quickstart.md). \r\n\r\nI got Ollama serving, Open WebUI running, and Mistral responding to text prompts. Also, I can see that the workload is indeed running on the integrated GPU (13th Gen Core). However, when I upload a file in Open WebUI (or try to use the Knowledge feature) and ask Mistral to summarize the file, I get the following errors:\r\n\r\n- In Open WebUI, I get the following notification: _Oops! No text generated from Ollama, Please try again._\r\n- In the Ollama output, I see this error: \r\n      ```ollama_llama_server: /home/runner/_work/llm.cpp/llm.cpp/llm.cpp/bigdl-core-xe/llama_backend/sdp_xmx_kernel.cpp:439: auto ggml_sycl_op_sdp_xmx_casual(fp16 *, fp16 *, fp16 *, fp16 *, fp16 *, float *, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, float *, float, sycl::queue &)::(anonymous class)::operator()() const: Assertion `false' failed.\r\nSIGABRT: abort\r\nPC=0x7f32cd8429fc m=5 sigcode=18446744073709551610\r\nsignal arrived during cgo execution```\r\n\r\nAny assistance would be greatly appreciated.\r\n\r\nThanks!",
      "state": "closed",
      "author": "boysbytes",
      "author_type": "User",
      "created_at": "2024-12-15T08:45:13Z",
      "updated_at": "2024-12-17T05:54:40Z",
      "closed_at": "2024-12-17T05:54:40Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12550/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "leonardozcm"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12550",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12550",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:51.110568",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @boysbytes, which version of ollama are you running? You may see it by `ollama --version`.",
          "created_at": "2024-12-16T01:41:05Z"
        },
        {
          "author": "boysbytes",
          "body": "Hello!\r\n\r\nHere's the version:\r\nollama version is 0.4.6-ipexllm-20241214\r\n\r\nUpdate:\r\nI just noticed that the first text prompt works. But if I were to do a second text prompt, I get the same error as well. Please let me know if I should change the title of this issue.\r\n\r\nHere are the changes I made t",
          "created_at": "2024-12-16T02:17:58Z"
        },
        {
          "author": "leonardozcm",
          "body": "hi @boysbytes, basically I think it is impossible for UHD igpu run into xmx branch, so I wonder if you can provide us more information with the following script\r\n```\r\n// device_check.cpp\r\n#include <CL/sycl.hpp>\r\n#include <iostream>\r\n\r\nint main() {\r\n    sycl::device device = sycl::device(sycl::gpu_se",
          "created_at": "2024-12-16T05:54:49Z"
        },
        {
          "author": "boysbytes",
          "body": "Hi @leonardozcm.\r\n\r\nThanks for following up.\r\n\r\nHere's the output:\r\n```\r\nDevice Name: Intel(R) Graphics [0xa7a8]\r\nmax_num_sub_groups: 64\r\next_intel_gpu_eu_count: 48\r\next_intel_gpu_eu_simd_width: 8\r\n\r\nSlice Info:\r\next_intel_gpu_slices: 1\r\next_intel_gpu_subslices_per_slice: 6\r\next_intel_gpu_eu_count_p",
          "created_at": "2024-12-16T06:00:04Z"
        },
        {
          "author": "leonardozcm",
          "body": "hi, I have tried to fix your error. Now it will choose the appropriate calculation method based on device id. Please try latest ipex-llm ollama/llama.cpp **tomorrow**.\r\n\r\nAnd I think there may be something different with your driver or oneapi, for UHD igpu usually displayed as follows:\r\n```\r\nDevice ",
          "created_at": "2024-12-16T07:47:39Z"
        }
      ]
    },
    {
      "issue_number": 11400,
      "title": " Native API returns: -2 (PI_ERROR_DEVICE_NOT_AVAILABLE)",
      "body": "Traceback is as followed, I was running ChatGLM4-9b-chat on my laptop.\r\nDevice configurations\r\nOS: Win 11 23H2 (22631.3737)\r\n- CPU: i7-1260P\r\n- GPU: 'Intel(R) Iris(R) Xe Graphics', platform_name='Intel(R) Level-Zero', dev_type='gpu, support_fp64=0, total_memory=14567MB, max_compute_units=96, gpu_eu_count=96\r\n- 32 gigs of ddr5 4800 mhz\r\n\r\n> (llm_gpu) F:\\LLM_Local>pip list\r\n> Package                     Version\r\n> --------------------------- ---------------------\r\n> accelerate                  0.23.0\r\n> annotated-types             0.7.0\r\n> bigdl-core-xe-21            2.5.0b20240620\r\n> bigdl-core-xe-addons-21     2.5.0b20240620\r\n> bigdl-core-xe-batch-21      2.5.0b20240620\r\n> certifi                     2024.6.2\r\n> charset-normalizer          3.3.2\r\n> colorama                    0.4.6\r\n> dpcpp-cpp-rt                2024.0.2\r\n> filelock                    3.15.1\r\n> fsspec                      2024.6.0\r\n> huggingface-hub             0.23.4\r\n> idna                        3.7\r\n> intel-cmplr-lib-rt          2024.0.2\r\n> intel-cmplr-lic-rt          2024.0.2\r\n> intel-extension-for-pytorch 2.1.10+xpu\r\n> intel-opencl-rt             2024.0.2\r\n> intel-openmp                2024.0.2\r\n> ipex-llm                    2.1.0b20240620\r\n> Jinja2                      3.1.4\r\n> MarkupSafe                  2.1.5\r\n> mkl                         2024.0.0\r\n> mkl-dpcpp                   2024.0.0\r\n> mpmath                      1.3.0\r\n> networkx                    3.3\r\n> numpy                       1.26.4\r\n> onednn                      2024.0.0\r\n> onemkl-sycl-blas            2024.0.0\r\n> onemkl-sycl-datafitting     2024.0.0\r\n> onemkl-sycl-dft             2024.0.0\r\n> onemkl-sycl-lapack          2024.0.0\r\n> onemkl-sycl-rng             2024.0.0\r\n> onemkl-sycl-sparse          2024.0.0\r\n> onemkl-sycl-stats           2024.0.0\r\n> onemkl-sycl-vm              2024.0.0\r\n> packaging                   24.1\r\n> pillow                      10.3.0\r\n> pip                         24.0\r\n> protobuf                    5.27.1\r\n> psutil                      5.9.8\r\n> py-cpuinfo                  9.0.0\r\n> pydantic                    2.7.4\r\n> pydantic_core               2.18.4\r\n> PyYAML                      6.0.2rc1\r\n> regex                       2024.5.15\r\n> requests                    2.32.3\r\n> safetensors                 0.4.3\r\n> sentencepiece               0.2.0\r\n> setuptools                  69.5.1\r\n> sympy                       1.13.0rc2\r\n> tabulate                    0.9.0\r\n> tbb                         2021.12.0\r\n> tiktoken                    0.7.0\r\n> tokenizers                  0.15.2\r\n> torch                       2.1.0a0+cxx11.abi\r\n> torchaudio                  2.1.0.post2+cxx11.abi\r\n> torchvision                 0.16.0a0+cxx11.abi\r\n> tqdm                        4.66.4\r\n> transformers                4.36.2\r\n> typing_extensions           4.12.2\r\n> urllib3                     2.2.1\r\n> wheel                       0.43.0\r\n\r\nThe traceback is:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"F:\\LLM_Local\\cmdDemo.py\", line 101, in <module>\r\n\r\n    ^\r\n  File \"F:\\LLM_Local\\cmdDemo.py\", line 87, in main\r\n\r\n  File \"C:\\Users\\%USR%\\anaconda3\\envs\\llm_gpu\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 35, in generator_context\r\n    response = gen.send(None)\r\n               ^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\%USR%\\.cache\\huggingface\\modules\\transformers_modules\\modeling_chatglm.py\", line 1007, in stream_chat\r\n    inputs = inputs.to(self.device)\r\n             ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\%USR%\\anaconda3\\envs\\llm_gpu\\Lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 789, in to\r\n    self.data = {k: v.to(device=device) for k, v in self.data.items()}\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\%USR%\\anaconda3\\envs\\llm_gpu\\Lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 789, in <dictcomp>\r\n    self.data = {k: v.to(device=device) for k, v in self.data.items()}\r\n                    ^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: Native API failed. Native API returns: -2 (PI_ERROR_DEVICE_NOT_AVAILABLE) -2 (PI_ERROR_DEVICE_NOT_AVAILABLE)\r\n```\r\nThis seems to happen when I loaded the model and left it there idle after sometime,",
      "state": "open",
      "author": "TriDefender",
      "author_type": "User",
      "created_at": "2024-06-22T12:35:48Z",
      "updated_at": "2024-12-13T12:45:11Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 26,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11400/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "lzivan"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11400",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11400",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:51.297623",
      "comments": []
    },
    {
      "issue_number": 12146,
      "title": "Glm4-9b-inference输出错误ISSUE",
      "body": "用以下方式验证glm4-9b-chat模型的输出，serving端报错\r\n\r\ncurl --request POST \\\r\n  --url http://127.0.0.1:8000/v1/chat/completions \\\r\n  --header 'content-type: application/json' \\\r\n  --data '{\r\n        \"model\": \"glm-4-9b-chat\",\r\n        \"temperature\": 0.7,\r\n        \"top_p\": 0.8,\r\n        \"messages\": [\r\n        {\r\n            \"role\": \"system\",\r\n            \"content\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\"\r\n        },\r\n                {\r\n                        \"role\": \"user\",\r\n                        \"content\": \"你是谁\"\r\n                }\r\n        ],\r\n        \"max_tokens\": 1024,\r\n        \"repetition_penalty\": 1.0\r\n}'\r\n\r\nServing端启动脚本\r\n#!/bin/bash\r\nmodel=\"/llm/models/glm-4-9b-chat\"\r\nserved_model_name=\"glm-4-9b-chat\"\r\n\r\nexport USE_XETLA=OFF\r\nexport SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1\r\nexport SYCL_CACHE_PERSISTENT=1\r\nexport TORCH_LLM_ALLREDUCE=0\r\nexport CCL_DG2_ALLREDUCE=1\r\n# Tensor parallel related arguments:\r\nexport CCL_WORKER_COUNT=1\r\nexport FI_PROVIDER=shm\r\nexport CCL_ATL_TRANSPORT=ofi\r\nexport CCL_ZE_IPC_EXCHANGE=sockets\r\nexport CCL_ATL_SHM=1\r\nsource /opt/intel/oneapi/setvars.sh\r\nsource /opt/intel/1ccl-wks/setvars.sh\r\npython -m ipex_llm.vllm.xpu.entrypoints.openai.api_server \\\r\n  --served-model-name $served_model_name \\\r\n  --port 8000 \\\r\n  --model $model \\\r\n  --trust-remote-code \\\r\n  --gpu-memory-utilization 0.9 \\\r\n  --device xpu \\\r\n  --dtype float16 \\\r\n  --enforce-eager \\\r\n  --load-in-low-bit fp8 \\\r\n  --max-model-len 2048 \\\r\n  --max-num-batched-tokens 4000 \\\r\n  --tensor-parallel-size 1\r\n\r\nServing端报错：\r\nINFO:     127.0.0.1:58348 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\r\nERROR:    Exception in ASGI application\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 401, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/middleware/proxy_headers.py\", line 70, in __call__\r\n    return await self.app(scope, receive, send)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/fastapi/applications.py\", line 1054, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/applications.py\", line 113, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/errors.py\", line 187, in __call__\r\n    raise exc\r\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/errors.py\", line 165, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/cors.py\", line 85, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/exceptions.py\", line 62, in __call__\r\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 62, in wrapped_app\r\n    raise exc\r\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 51, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 715, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 735, in app\r\n    await route.handle(scope, receive, send)\r\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 288, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 76, in app\r\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 62, in wrapped_app\r\n    raise exc\r\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 51, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 73, in app\r\n    response = await f(request)\r\n               ^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/fastapi/routing.py\", line 301, in app\r\n    raw_response = await run_endpoint_function(\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/fastapi/routing.py\", line 212, in run_endpoint_function\r\n    return await dependant.call(**values)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/entrypoints/openai/api_server.py\", line 191, in create_chat_completion\r\n    generator = await openai_serving_chat.create_chat_completion(\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/entrypoints/openai/serving_chat.py\", line 132, in create_chat_completion\r\n    prompt_inputs = self._tokenize_prompt_input(\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/entrypoints/openai/serving_engine.py\", line 291, in _tokenize_prompt_input\r\n    return next(\r\n           ^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/entrypoints/openai/serving_engine.py\", line 314, in _tokenize_prompt_inputs\r\n    yield self._normalize_prompt_text_to_input(\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/entrypoints/openai/serving_engine.py\", line 206, in _normalize_prompt_text_to_input\r\n    encoded = tokenizer(prompt, add_special_tokens=add_special_tokens)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 3024, in __call__\r\n    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 3134, in _call_one\r\n    return self.encode_plus(\r\n           ^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 3210, in encode_plus\r\n    return self._encode_plus(\r\n           ^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils.py\", line 801, in _encode_plus\r\n    return self.prepare_for_model(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 3706, in prepare_for_model\r\n    encoded_inputs = self.pad(\r\n                     ^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 3508, in pad\r\n    encoded_inputs = self._pad(\r\n                     ^^^^^^^^^^\r\nTypeError: ChatGLM4Tokenizer._pad() got an unexpected keyword argument 'padding_side'\r\n",
      "state": "closed",
      "author": "jessie-zhao",
      "author_type": "User",
      "created_at": "2024-09-29T01:01:33Z",
      "updated_at": "2024-12-11T07:42:43Z",
      "closed_at": "2024-12-11T07:42:43Z",
      "labels": [
        "user issue",
        "multi-arc"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12146/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hzjane"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12146",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12146",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:51.297664",
      "comments": [
        {
          "author": "hzjane",
          "body": "Refer to this [issue](https://github.com/THUDM/ChatGLM3/issues/1324). It seems that the transformers version 4.45.0 will meet this issue when running glm model. You can use transformers 4.37.0 to run first.\r\n```bash\r\npip install transformers==4.37.0\r\n```",
          "created_at": "2024-09-29T05:36:03Z"
        },
        {
          "author": "glorysdj",
          "body": "fixed",
          "created_at": "2024-12-11T07:42:43Z"
        }
      ]
    },
    {
      "issue_number": 12131,
      "title": "Arc770 IPEX-LLM 的交互准确性问题",
      "body": "客户在 Xeon-W 一机4卡 Arc770 的环境下验证，ipex-llm 版本 2.1.0b2\r\n\r\n问题：\r\n1. 用benchmark跑的时候已经趋于正常，但是直接调用的时候，有一定的概率没有输出，尤其是加了问号 ？ 很大概率就没有输出了\r\n 调用样例：\r\ntime curl http://localhost:8000/v1/completions  -H \"Content-Type: application/json\" -d '{\"model\": \"Llama-2-13b-chat-hf\", \"prompt\": \"交朋友的原则是什么？\",  \"max_tokens\": 1024, \"temperature\": 0.9 }'\r\n\r\n2. 英文的问题就会好很多，基本都会有输出，内容也趋于稳定。中文不太好，特别是加了标点符号\r\n\r\n3. 直接调用的时候，有很高的概率，已经限制输出到比如1024，但程序要要跑到限制的max_tokens比如2048才停下来。\r\n\r\n4.  14B int4 单卡Tokens限制了2048以后，没有出现过OOM，但经常出现没结果\r\n\r\n补充：输出不稳定的情况在  fp8 和 int4 的时候都会出现\r\n\r\n错误案例：\r\n就是经常达到token上限大概的输出是这样的：\r\n生成的标题为：\\n\\n「人工智能技术的起源和发展」\\n\\n如果你想要生成不同的标题，可以根据下文修改生成的标题，例如添加关键词、改变句子结构等。\\n\\n**Challenge 2**\\n\\n生成一个简短的标题，描述人工智能在医疗领域的应用。\\n\\nHint:Think about the benefits and challenges of applying AI in healthcare.\\n\\n生成的标题为：\\n\\n「人工智能在医疗领域的应用：精准预测和个性化医疗」\\n\\n\\n\\n**Challenge 3**\\n\\n生成一个简短的标题，描述人工智能在教育领域的应用。\\n\\nHint:Think about the potential of AI in improving teaching methods and student learning outcomes.\\n\\n生成的标题为：\\n\\n「人工智能在教育领域的应用：个性化教学和智能学习资源」\\n\\n这些标题都是基于挑战中的提示和 Content 的生成，如果你想要生成不同的标题，可以根据自己的想法和需求来修改。同时，标题的生成需要考虑总体的语言风格和结构，同时也需要考虑读者的需求和兴趣。有限责任公司欢迎你来挑战生成更多的标题！](http://www.xiaohuangjidi.com/)![AI支持生成标题挑战](https://img_BASIC_FORMAT. https://preview.cloudimg-ci.org/v1/image?q=0&f=https://ai.baidu.com/,close-transform:disable\\\\a href=\\\"https://ai.baidu.com/)![AI支持生成标题挑战](https://img_BASIC_FORMAT. https://preview.cloudimg-ci.org/v1/image?q=0&f=https://ai.baidu.com/,close-transform:disable\\\\a href=\\\"https://ai.baidu.com\\n\\n**AI Supported Generation Challenges**](https://preview.cloudimg-ci.org/v1/image?q=0&f=https://ai.baidu.com/,close-transform:disable\\\\a href=\\\"https://ai.baidu.com/,close-transform:disable\\\"a href=\\\"https://ai.baidu.com/)![AI支持生成标题挑战](https://img_BASIC_FORMAT. https://preview.cloudimg-ci.org/v1/image?q=0&f=https://ai.baidu.com/,close-transform:disable\\\\a href=\\\"https://ai.baidu.com/,close-transform:disable\\\"a href=\\\"https://ai.baidu.com/)![AI支持生成标题挑战](https://img_BASIC_FORMAT. https://preview.cloudimg-ci.org/v1/image?q=0&f=https://ai.baidu.com/,close-transform:disable\\\\a href=\\\"https://ai.baidu.com/,close-transform:disable\\\"a href=\\\"https://ai.baidu.com/)![AI支持生成标题挑战](https://img_BASIC_FORMAT. https://preview.cloudimg-ci.org/v1/image?q=0&f=https://ai.baidu.com/,close-transform:disable\\\\a href=\\\"https://ai.baidu.com/,close-transform:disable\\\"a href=\\\"https://ai.baidu.com/)![AI支持生成标题挑战](https://img_BASIC_FORMAT. https://preview.cloudimg-ci.org/v1/image?q=0&f=https://ai.baidu.com/,close-transform:disable\\\\a href=\\\"https://ai.baidu.com/,close-transform:disable\\\"a href=\\\"https://ai.baidu.com/)\r\n\r\n\r\n手机，功能多样化，外观美观\\n\\n评论1：\\n\\\"Wow, I am blown away by the incredible performance of this computer! Its affordability and great service make it an unbeatable choice. I highly recommend it to anyone seeking a reliable and efficient device.\\\"\\n\\n评论2：\\n\\\"I am absolutely thrilled with the amazing features of this phone! Its sleek design and numerous functions make it an absolute standout. I've never seen a phone that's so well-suited for everyday use. Five stars, hands down!\\\"\\n\\n评论3：\\n\\\"I was thoroughly impressed by this computer's exceptional performance, plus its competitive pricing and top-notch service! What more could you ask for? It's a must-have for anyone who wants a hassle-free and productive computing experience. Highly recommended!\\\"\\n\\n\\n\\nSince I have to write three neutral and praising comments, I will try to maintain a positive tone while using phrases like \\\"I am blown away\\\", \\\"thoroughly impressed\\\", \\\"absolutely thrilled\\\" to emphasize the excellent qualities of the device. Here are the three comments:\\n\\n\\n\\nComment 1:\\n\\\"I am blown away by the incredible performance of this computer! Its affordability and great service make it an unbeatable choice. I highly recommend it to anyone seeking a reliable and efficient device.\\\"\\n\\nComment 2:\\n\\\"I am absolutely thrilled with the amazing features of this phone! Its sleek design and numerous functions make it an absolute standout. I've never seen a phone that's so well-suited for everyday use. Five stars, hands down!\\\"\\n\\nComment 3:\\n\\\"I was thoroughly impressed by this computer's exceptional performance, plus its competitive pricing and top-notch service! What more could you ask for? It's a must-have for anyone who wants a hassle-free and productive computing experience. Highly recommended!\\\"\\n\\n\\n\\nHere are the three comments with a positive tone, using phrases like \\\"I am blown away\\\", \\\"thoroughly impressed\\\", \\\"absolutely thrilled\\\" to emphasize the excellent qualities of the device. 😊\\n\\n\\n\\nLet me know if you need any further assistance! 😊<|eot_id|><|\r\n",
      "state": "closed",
      "author": "yangluchina",
      "author_type": "User",
      "created_at": "2024-09-27T01:18:55Z",
      "updated_at": "2024-12-11T07:42:25Z",
      "closed_at": "2024-12-11T07:42:25Z",
      "labels": [
        "user issue",
        "multi-arc"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12131/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "ACupofAir"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12131",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12131",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:51.494271",
      "comments": [
        {
          "author": "glorysdj",
          "body": "please try with latest Docker image: intelanalytics/ipex-llm-serving-xpu:2.2.0-SNAPSHOT",
          "created_at": "2024-09-27T02:05:56Z"
        },
        {
          "author": "ACupofAir",
          "body": "> 客户在 Xeon-W 一机4卡 Arc770 的环境下验证，ipex-llm 版本 2.1.0b2\r\n> \r\n> 问题：\r\n> \r\n> 1. 用benchmark跑的时候已经趋于正常，但是直接调用的时候，有一定的概率没有输出，尤其是加了问号 ？ 很大概率就没有输出了\r\n>    调用样例：\r\n>    time curl http://localhost:8000/v1/completions  -H \"Content-Type: application/json\" -d '{\"model\": \"Llama-2-13b-chat-hf\", \"prompt\": \"交朋友的原则是什么？\",  ",
          "created_at": "2024-09-30T02:47:30Z"
        },
        {
          "author": "jason-dai",
          "body": "Please try Qwen2-7B?",
          "created_at": "2024-09-30T07:26:15Z"
        },
        {
          "author": "yangluchina",
          "body": "thanks for your update and comments, I have synced with customer this morning, they will find a Arc770 system to try it again and make sure that output is Qwen log",
          "created_at": "2024-09-30T07:51:57Z"
        },
        {
          "author": "glorysdj",
          "body": "closed since no update for a long time",
          "created_at": "2024-12-11T07:42:25Z"
        }
      ]
    },
    {
      "issue_number": 12123,
      "title": "Support DeepSeek-Coder-v1.5 7B",
      "body": "Support DeepSeek-Coder-v1.5 7B with vLLM",
      "state": "closed",
      "author": "peterzcst",
      "author_type": "User",
      "created_at": "2024-09-26T05:35:45Z",
      "updated_at": "2024-12-11T07:41:35Z",
      "closed_at": "2024-12-11T07:41:34Z",
      "labels": [
        "user issue",
        "multi-arc"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12123/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "ACupofAir"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12123",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12123",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:51.723686",
      "comments": [
        {
          "author": "ACupofAir",
          "body": "**Have successfully verified on vllm0.5.4(docker image: `intelanalytics/ipex-llm-serving-xpu:latest`):**\r\n### Test step\r\nrun ` python vllm-out-verify.py /llm/models/deepseek-coder-7b-instruct-v1.5/ 1`, the vllm-out-verify.py is below:\r\n```python\r\nfrom vllm import SamplingParams\r\nfrom ipex_llm.vllm.x",
          "created_at": "2024-09-26T08:53:24Z"
        },
        {
          "author": "glorysdj",
          "body": "fixed",
          "created_at": "2024-12-11T07:41:34Z"
        }
      ]
    },
    {
      "issue_number": 12087,
      "title": "Running vLLM service benchmark(1xARC770) with Qwen1.5-14B-Chat model failed(compression weight:SYM_INT4). ",
      "body": "Environment:\r\nPlatform: 6548N+1 ARC770\r\nDocker Image: \r\n![image](https://github.com/user-attachments/assets/c46ecf56-7dad-48b4-8c52-b67c2bb385ab)\r\nservicing script:\r\n![image](https://github.com/user-attachments/assets/03ab8adf-7271-45af-b934-c619063a8a83)\r\n\r\n\r\nError info:\r\n1.With compression weight SYM_INT4 failed.\r\n2.Has tried the parameter \"gpu-memory-utilization\" from 0.65 to 0.95 with step 0.05 could not work.\r\n\r\n\r\n\r\nError log:\r\n1.Servicing side error log:\r\n![image](https://github.com/user-attachments/assets/cf9ee5ed-4812-4129-a768-00d043d25f66)\r\n\r\n",
      "state": "closed",
      "author": "dukelee111",
      "author_type": "User",
      "created_at": "2024-09-18T02:23:09Z",
      "updated_at": "2024-12-11T07:41:02Z",
      "closed_at": "2024-12-11T07:41:02Z",
      "labels": [
        "user issue",
        "multi-arc"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12087/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hzjane"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12087",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12087",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:52.019968",
      "comments": [
        {
          "author": "hzjane",
          "body": "I didn't reproduce this error. Did you encounter this issue when you started vllm?  Or when you benchmarked?",
          "created_at": "2024-09-18T05:55:17Z"
        },
        {
          "author": "dukelee111",
          "body": "It's not encountered when doing benchmark,  starting vllm could succeed.",
          "created_at": "2024-09-18T07:03:18Z"
        },
        {
          "author": "ACupofAir",
          "body": "Cannot reproduce\r\n**Steps**:\r\n1. start docker:\r\n```bash\r\n#!/bin/bash\r\nexport DOCKER_IMAGE=intelanalytics/ipex-llm-serving-xpu-vllm-0.5.4-experimental:2.2.0b1\r\nexport CONTAINER_NAME=junwang-vllm54-issue220\r\n\r\ndocker rm -f $CONTAINER_NAME\r\nsudo docker run -itd \\\r\n        --net=host \\\r\n        --device",
          "created_at": "2024-09-18T08:02:37Z"
        },
        {
          "author": "glorysdj",
          "body": "closed since no update for a long time",
          "created_at": "2024-12-11T07:41:02Z"
        }
      ]
    },
    {
      "issue_number": 12082,
      "title": "IPEX-LLM 运行源2.0 M32量化版失败 on Intel ARC",
      "body": "源2.0-M32大模型研发团队深入分析当前主流的量化方案，综合评估模型压缩效果和精度损失表现，最终采用了GPTQ量化方法，并采用AutoGPTQ作为量化框架。\r\n\r\n---------------------------------------------------------------------------------------------\r\nModel：  Yuan2-M32-HF-INT4   https://blog.csdn.net/2401_82700030/article/details/141469514\r\n容器： intelanalytics/ipex-llm-serving-xpu-vllm-0.5.4-experimental:2.2.0b1\r\n\r\nTest Step： \r\nLog into container:\r\n# docker exec -ti arc_vllm-new-2   bash\r\n# cd /benchmark/all-in-one/\r\n# vim config.yaml\r\nConfig.yaml   配置：\r\n![image](https://github.com/user-attachments/assets/9d12436d-c135-4f95-8ed6-2e507556e141)\r\n\r\n\r\n# run-arc.sh\r\n运行报错 ， 结果如下log.\r\nResults Log: \r\n![image](https://github.com/user-attachments/assets/6b0176db-c83a-48bb-955a-820b7164e90b)\r\n\r\n![image](https://github.com/user-attachments/assets/7f09aba7-f01d-4cb4-a8e5-404efddd9a0d)\r\n\r\n\r\n",
      "state": "closed",
      "author": "jianweimama",
      "author_type": "User",
      "created_at": "2024-09-14T02:49:53Z",
      "updated_at": "2024-12-11T07:40:02Z",
      "closed_at": "2024-12-11T07:40:01Z",
      "labels": [
        "user issue",
        "multi-arc"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12082/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hzjane"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12082",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12082",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:52.268534",
      "comments": [
        {
          "author": "hzjane",
          "body": "I try to reproduce it and meet the same issue again. And as I found that.\r\n1. The official vllm does not support the `yuan` model yet.\r\n2. Maybe this model's quantized method is not supportted to be load by ipex-llm yet. \r\n```bash\r\n# https://github.com/IEIT-Yuan/Yuan2.0-M32/blob/b403a2beb2746c0c923b",
          "created_at": "2024-09-14T06:38:24Z"
        },
        {
          "author": "glorysdj",
          "body": "fixed",
          "created_at": "2024-12-11T07:40:01Z"
        }
      ]
    },
    {
      "issue_number": 12081,
      "title": "vLLM 0.5.4 failure to start the TP+ PP mode on 8 ARC",
      "body": "### The vllm docker image is\r\n`intelanalytics/ipex-llm-serving-xpu-vllm-0.5.4-experimental:2.2.0b1`\r\n\r\n### vLLM start command is\r\n'model=\"/llm/models/Qwen2-72B-Instruct/\"\r\nserved_model_name=\"Qwen2-72B-Instruct\"\r\n\r\nsource /opt/intel/1ccl-wks/setvars.sh\r\n\r\nexport SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=2\r\n\r\npython -m ipex_llm.vllm.xpu.entrypoints.openai.api_server \\\r\n  --served-model-name $served_model_name \\\r\n  --port 8000 \\\r\n  --model $model \\\r\n  --trust-remote-code \\\r\n  --gpu-memory-utilization 0.85 \\\r\n  --device xpu \\\r\n  --dtype float16 \\\r\n  --enforce-eager \\\r\n  --load-in-low-bit fp8 \\\r\n  --max-model-len 2048 \\\r\n  --max-num-batched-tokens 2048 \\\r\n  --max-num-seqs 24 \\\r\n  -tp 4 -pp 2 --disable-log-requests'\r\n\r\n### The error information is\r\n(WrapperWithLoadBit pid=35347) 2024:09:13-11:21:50:(35347) |CCL_ERROR| exchange_utils.cpp:202 sendmsg_fd: condition !check_msg_retval(\"sendmsg\", send_bytes, iov, msg, sizeof(u.cntr_buf), sock, fd) failed\r\n(WrapperWithLoadBit pid=35347)  errno: Broken pipe\r\n2024:09:13-11:21:50:(31157) |CCL_ERROR| exchange_utils.cpp:202 sendmsg_fd: condition !check_msg_retval(\"sendmsg\", send_bytes, iov, msg, sizeof(u.cntr_buf), sock, fd) failed\r\nerrno: Broken pipe\r\n(WrapperWithLoadBit pid=35347) ERROR 09-13 11:21:50 worker_base.py:386] Error executing method init_device. This might cause deadlock in distributed execution.\r\n(WrapperWithLoadBit pid=35347) ERROR 09-13 11:21:50 worker_base.py:386] Traceback (most recent call last):\r\n(WrapperWithLoadBit pid=35347) ERROR 09-13 11:21:50 worker_base.py:386]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/worker/worker_base.py\", line 378, in execute_method\r\n(WrapperWithLoadBit pid=35347) ERROR 09-13 11:21:50 worker_base.py:386]     return executor(*args, **kwargs)\r\n(WrapperWithLoadBit pid=35347) ERROR 09-13 11:21:50 worker_base.py:386]            ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(WrapperWithLoadBit pid=35347) ERROR 09-13 11:21:50 worker_base.py:386]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/worker/xpu_worker.py\", line 105, in init_device\r\n(WrapperWithLoadBit pid=35347) ERROR 09-13 11:21:50 worker_base.py:386]     self.init_worker_distributed_environment()\r\n(WrapperWithLoadBit pid=35347) ERROR 09-13 11:21:50 worker_base.py:386]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/worker/xpu_worker.py\", line 205, in init_worker_distributed_environment\r\n(WrapperWithLoadBit pid=35347) ERROR 09-13 11:21:50 worker_base.py:386]     get_pp_group().all_reduce(torch.zeros(1).xpu())\r\n(WrapperWithLoadBit pid=35347) ERROR 09-13 11:21:50 worker_base.py:386]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/distributed/parallel_state.py\", line 293, in all_reduce\r\n(WrapperWithLoadBit pid=35347) ERROR 09-13 11:21:50 worker_base.py:386]     torch.distributed.all_reduce(input_, group=self.device_group)\r\n(WrapperWithLoadBit pid=35347) ERROR 09-13 11:21:50 worker_base.py:386]   File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 47, in wrapper\r\n(WrapperWithLoadBit pid=35347) ERROR 09-13 11:21:50 worker_base.py:386]     return func(*args, **kwargs)\r\n(WrapperWithLoadBit pid=35347) ERROR 09-13 11:21:50 worker_base.py:386]            ^^^^^^^^^^^^^^^^^^^^^\r\n(WrapperWithLoadBit pid=35347) ERROR 09-13 11:21:50 worker_base.py:386]   File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 2055, in all_reduce\r\n(WrapperWithLoadBit pid=35347) ERROR 09-13 11:21:50 worker_base.py:386]     work.wait()\r\n(WrapperWithLoadBit pid=35347) ERROR 09-13 11:21:50 worker_base.py:386] RuntimeError: oneCCL: exchange_utils.cpp:202 sendmsg_fd: EXCEPTION:  errno: Broken pipe\r\nERROR 09-13 11:21:51 worker_base.py:386] Error executing method init_device. This might cause deadlock in distributed execution.\r\nERROR 09-13 11:21:51 worker_base.py:386] Traceback (most recent call last):\r\nERROR 09-13 11:21:51 worker_base.py:386]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/worker/worker_base.py\", line 378, in execute_method\r\nERROR 09-13 11:21:51 worker_base.py:386]     return executor(*args, **kwargs)\r\nERROR 09-13 11:21:51 worker_base.py:386]            ^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-13 11:21:51 worker_base.py:386]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/worker/xpu_worker.py\", line 105, in init_device\r\nERROR 09-13 11:21:51 worker_base.py:386]     self.init_worker_distributed_environment()\r\nERROR 09-13 11:21:51 worker_base.py:386]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/worker/xpu_worker.py\", line 205, in init_worker_distributed_environment\r\nERROR 09-13 11:21:51 worker_base.py:386]     get_pp_group().all_reduce(torch.zeros(1).xpu())\r\nERROR 09-13 11:21:51 worker_base.py:386]   File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/distributed/parallel_state.py\", line 293, in all_reduce\r\nERROR 09-13 11:21:51 worker_base.py:386]     torch.distributed.all_reduce(input_, group=self.device_group)\r\nERROR 09-13 11:21:51 worker_base.py:386]   File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 47, in wrapper\r\nERROR 09-13 11:21:51 worker_base.py:386]     return func(*args, **kwargs)\r\nERROR 09-13 11:21:51 worker_base.py:386]            ^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-13 11:21:51 worker_base.py:386]   File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 2055, in all_reduce\r\nERROR 09-13 11:21:51 worker_base.py:386]     work.wait()\r\nERROR 09-13 11:21:51 worker_base.py:386] RuntimeError: oneCCL: exchange_utils.cpp:202 sendmsg_fd: EXCEPTION:  errno: Broken pipe\r\nProcess Process-65:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/entrypoints/openai/rpc/server.py\", line 220, in run_rpc_server\r\n    server = AsyncEngineRPCServer(async_engine_args, usage_context, port, load_in_low_bit)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/entrypoints/openai/rpc/server.py\", line 27, in __init__\r\n    self.engine = AsyncLLMEngine.from_engine_args(async_engine_args,\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 43, in from_engine_args\r\n    return super().from_engine_args(engine_args, start_engine_loop, usage_context, stat_loggers)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/engine/async_llm_engine.py\", line 476, in from_engine_args\r\n    engine = cls(\r\n             ^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 29, in __init__\r\n    super().__init__(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/engine/async_llm_engine.py\", line 381, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/engine/async_llm_engine.py\", line 557, in _init_engine\r\n    return engine_class(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/engine/llm_engine.py\", line 255, in __init__\r\n    self.model_executor = executor_class(\r\n                          ^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/executor/ray_xpu_executor.py\", line 35, in __init__\r\n    super().__init__(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/executor/ray_gpu_executor.py\", line 555, in __init__\r\n    super().__init__(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/executor/distributed_gpu_executor.py\", line 25, in __init__\r\n    super().__init__(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/executor/xpu_executor.py\", line 53, in __init__\r\n    self._init_executor()\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/executor/ray_gpu_executor.py\", line 61, in _init_executor\r\n    self._init_workers_ray(placement_group)\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/executor/ray_gpu_executor.py\", line 230, in _init_workers_ray\r\n    self._run_workers(\"init_device\")\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/executor/ray_gpu_executor.py\", line 468, in _run_workers\r\n    self.driver_worker.execute_method(method, *driver_args,\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/worker/worker_base.py\", line 387, in execute_method\r\n    raise e\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/worker/worker_base.py\", line 378, in execute_method\r\n    return executor(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/worker/xpu_worker.py\", line 105, in init_device\r\n    self.init_worker_distributed_environment()\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/worker/xpu_worker.py\", line 205, in init_worker_distributed_environment\r\n    get_pp_group().all_reduce(torch.zeros(1).xpu())\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/distributed/parallel_state.py\", line 293, in all_reduce\r\n    torch.distributed.all_reduce(input_, group=self.device_group)\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 47, in wrapper\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 2055, in all_reduce\r\n    work.wait()\r\n\r\n### The workaround is \r\n```\r\nvi /usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/worker/xpu_worker.py\r\n            get_pp_group().all_gather(torch.zeros(1).xpu())\r\n            #get_pp_group().all_reduce(torch.zeros(1).xpu())\r\n```",
      "state": "closed",
      "author": "oldmikeyang",
      "author_type": "User",
      "created_at": "2024-09-14T02:45:13Z",
      "updated_at": "2024-12-11T07:39:43Z",
      "closed_at": "2024-12-11T07:39:42Z",
      "labels": [
        "user issue",
        "multi-arc"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12081/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "xiangyuT"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12081",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12081",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:52.503984",
      "comments": [
        {
          "author": "oldmikeyang",
          "body": "After use modify the /usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/worker/xpu_worker.py with the \r\n`get_pp_group().all_gather(torch.zeros(1).xpu())`\r\n\r\n### vLLM start with the following error\r\n\r\n2024:09:14-11:02:35:(  241) |CCL_WARN| topology recognition shows P",
          "created_at": "2024-09-14T03:05:21Z"
        },
        {
          "author": "xiangyuT",
          "body": "> 2024:09:13-11:21:50:(31157) |CCL_ERROR| exchange_utils.cpp:202 sendmsg_fd: condition !check_msg_retval(\"sendmsg\", send_bytes, iov, msg, sizeof(u.cntr_buf), sock, fd) failed\r\nerrno: Broken pipe\r\n(WrapperWithLoadBit pid=35347) ERROR 09-13 11:21:50 worker_base.py:386] Error executing method init_devi",
          "created_at": "2024-09-19T02:30:00Z"
        },
        {
          "author": "glorysdj",
          "body": "fixed",
          "created_at": "2024-12-11T07:39:42Z"
        }
      ]
    },
    {
      "issue_number": 12079,
      "title": "LLama-33B failure with vLLM 0.5.4 docker on 4 ARC GPU.",
      "body": "**The vLLM docker image is**\r\n`intelanalytics/ipex-llm-serving-xpu-vllm-0.5.4-experimental:2.2.0b1`\r\n\r\n**vLLM start command is**\r\n`model=\"/llm/models/meta-llama/LLaMA-33B-HF/\"\r\nserved_model_name=\"LLaMA-33B-HF\"\r\n\r\nsource /opt/intel/1ccl-wks/setvars.sh\r\n\r\nexport SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=2\r\n\r\npython -m ipex_llm.vllm.xpu.entrypoints.openai.api_server \\\r\n  --served-model-name $served_model_name \\\r\n  --port 8000 \\\r\n  --model $model \\\r\n  --trust-remote-code \\\r\n  --gpu-memory-utilization 0.8 \\\r\n  --device xpu \\\r\n  --dtype float16 \\\r\n  --enforce-eager \\\r\n  --load-in-low-bit sym_int4 \\\r\n  --max-model-len 2048 \\\r\n  --max-num-batched-tokens 3000 \\\r\n  --max-num-seqs 16 \\\r\n  -tp 4  --disable-log-requests\r\n`\r\n\r\n**The error information is**\r\nINFO 09-14 10:32:15 metrics.py:406] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%.\r\nINFO 09-14 10:32:41 metrics.py:406] Avg prompt throughput: 38.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%.\r\nINFO:     127.0.0.1:51180 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:50250 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:50258 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:50266 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:50270 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:50278 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:50280 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:50286 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:50292 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO 09-14 10:32:47 metrics.py:406] Avg prompt throughput: 242.2 tokens/s, Avg generation throughput: 11.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.\r\n(WrapperWithLoadBit pid=3547) GPU-Xeon4410Y-ARC770:**rank1: Assertion failure at psm3/ptl_am/ptl.c:196: nbytes == req->req_data.recv_msglen**\r\n(WrapperWithLoadBit pid=3547) 2024-09-14 10:25:54,116 - INFO - Loading model weights took 4.1510 GB [repeated 2x across cluster]\r\n(WrapperWithLoadBit pid=3547) [1726281167.063876801] GPU-Xeon4410Y-ARC770:**rank1.perWithLoadBit.execute_method: Reading from remote process' memory failed. Disabling CMA support**\r\n(WrapperWithLoadBit pid=3989) WARNING 09-14 10:26:11 utils.py:564] Pin memory is not supported on XPU. [repeated 2x across cluster]\r\nINFO 09-14 10:33:01 metrics.py:406] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.4%, CPU KV cache usage: 0.0%.\r\n(raylet) A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff6f90abdfc17b3821298007f801000000 Worker ID: f3c6ea8e49dbf827e58908a85281342ea5f7a9646c64d71ddeca2031 Node ID: bda0a76065dd020d4eefe01b3bb9e7d4de06e10d3749bee600090abf Worker IP address: 10.240.108.91 Worker port: 46219 Worker PID: 3768 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\r\n(WrapperWithLoadBit pid=3989) [1726281167.063911306] GPU-Xeon4410Y-ARC770:rank3.perWithLoadBit.execute_method: Reading from remote process' memory failed. Disabling CMA support [repeated 2x across cluster]\r\nINFO 09-14 10:33:11 metrics.py:406] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.4%, CPU KV cache usage: 0.0%.\r\n(raylet) A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffb7844226a5b8a8d518279add01000000 Worker ID: 03229cf0580d5eeb7de0700c8093ec5229d30d4a8f2429b7091fdb6c Node ID: bda0a76065dd020d4eefe01b3bb9e7d4de06e10d3749bee600090abf Worker IP address: 10.240.108.91 Worker port: 37889 Worker PID: 3547 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\r\nINFO 09-14 10:33:21 metrics.py:406] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.4%, CPU KV cache usage: 0.0%.\r\n(raylet) A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff6fd757d1bddcdf4323576bee01000000 Worker ID: af76f74572b06d5def6df66c2a6b9c106a4cf0412259550313948806 Node ID: bda0a76065dd020d4eefe01b3bb9e7d4de06e10d3749bee600090abf Worker IP address: 10.240.108.91 Worker port: 38539 Worker PID: 3989 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\r\nINFO 09-14 10:33:31 metrics.py:406] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.4%, CPU KV cache usage: 0.0%.\r\n^CProcess ForkProcess-58:\r\n\r\n",
      "state": "closed",
      "author": "oldmikeyang",
      "author_type": "User",
      "created_at": "2024-09-14T02:38:26Z",
      "updated_at": "2024-12-11T07:39:28Z",
      "closed_at": "2024-12-11T07:39:27Z",
      "labels": [
        "user issue",
        "multi-arc"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12079/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "ACupofAir"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12079",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12079",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:52.699965",
      "comments": [
        {
          "author": "glorysdj",
          "body": "fixed",
          "created_at": "2024-12-11T07:39:27Z"
        }
      ]
    },
    {
      "issue_number": 11956,
      "title": "Running  vLLM service benchmark(4xARC770) with Qwen1.5-32B-Chat model failed",
      "body": "Environment:\r\nPlatform:   6548N+4ARC770\r\nDocker Image: intelanalytics/ipex-llm-serving-xpu:2.1.0\r\nservicing script:\r\n![image](https://github.com/user-attachments/assets/3949f088-d83f-4844-9ab3-0f0c98604792)\r\n\r\nError info:\r\n1.With Dtype SYM_INT4 could succeed.\r\n2.With Dtype FP8 failed with concurrency>=4. No error for concurrency 1 and 2.\r\n2.GPU card 0 shows N/A utilization, card 1 2 3 work well:\r\n![image](https://github.com/user-attachments/assets/92cff217-632e-4ff4-8601-2917ddbd9d30)\r\n3.Servicing side error log:\r\n![image](https://github.com/user-attachments/assets/ce64825a-21d4-4039-aa57-0355e69d382c)\r\n4.Client error info:\r\n![image](https://github.com/user-attachments/assets/b4159aa8-1add-44bb-a5c3-65a50afa7d7f)\r\n\r\n",
      "state": "closed",
      "author": "dukelee111",
      "author_type": "User",
      "created_at": "2024-08-29T02:00:31Z",
      "updated_at": "2024-12-11T06:37:35Z",
      "closed_at": "2024-12-11T06:37:35Z",
      "labels": [
        "user issue",
        "multi-arc"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11956/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hzjane"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11956",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11956",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:52.909449",
      "comments": [
        {
          "author": "hzjane",
          "body": "It seems that the `gpu-memory-utilization` is too high and causing the card 1 OOM when first_token is computed. You can reduce it to 0.85 can try ir again.",
          "created_at": "2024-08-29T02:26:35Z"
        },
        {
          "author": "glorysdj",
          "body": "fixed",
          "created_at": "2024-12-11T06:37:35Z"
        }
      ]
    },
    {
      "issue_number": 11946,
      "title": "support inference AWQ INT4 model of Yi-34B from QLoRA",
      "body": "I will provide AWQ model from customer, and customer will evaluate FP8 and Int4 performance.",
      "state": "closed",
      "author": "Fred-cell",
      "author_type": "User",
      "created_at": "2024-08-28T01:12:38Z",
      "updated_at": "2024-12-11T06:37:10Z",
      "closed_at": "2024-12-11T06:37:10Z",
      "labels": [
        "user issue",
        "multi-arc"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11946/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "gc-fu"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11946",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11946",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:53.162478",
      "comments": [
        {
          "author": "gc-fu",
          "body": "Hi, I have verified AWQ models can be supported (loaded in vLLM and converted to LowBitLinear in ipex-llm), but only asym_int4 quantization format is supported.\r\n\r\nThis feature will need some adaption from both vLLM side and ipex-llm side.  I will update to this thread once those supported prs are m",
          "created_at": "2024-08-28T01:24:27Z"
        },
        {
          "author": "glorysdj",
          "body": "fixed",
          "created_at": "2024-12-11T06:37:10Z"
        }
      ]
    },
    {
      "issue_number": 11910,
      "title": "failure to launch codegeex4-all-9b Using vllm",
      "body": "We are trying to launch codegeex4-all-9b Using vllm following the CodeGeeX4 github:\r\nhttps://github.com/THUDM/CodeGeeX4?tab=readme-ov-file#vllm\r\n\r\nThe scripts are as following:\r\ncodegeex_offline_example.py: \r\n```\r\nfrom transformers import AutoTokenizer\r\nfrom vllm import LLM, SamplingParams\r\n\r\n# CodeGeeX4-ALL-9B\r\n# max_model_len, tp_size = 1048576, 4\r\n# If OOM，please reduce max_model_len，or increase tp_size\r\nmax_model_len, tp_size = 2048, 4\r\nmodel_name = \"/llm/models/codegeex4-all-9b\"\r\nprompt = [{\"role\": \"user\", \"content\": \"Hello\"}]\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\r\nllm = LLM(\r\n    model=model_name,\r\n    tensor_parallel_size=tp_size,\r\n    max_model_len=max_model_len,\r\n    trust_remote_code=True,\r\n    enforce_eager=True,\r\n    # If OOM，try using follong parameters\r\n    # enable_chunked_prefill=True,\r\n    # max_num_batched_tokens=8192\r\n)\r\nstop_token_ids = [151329, 151336, 151338]\r\nsampling_params = SamplingParams(temperature=0.95, max_tokens=1024, stop_token_ids=stop_token_ids)\r\n\r\ninputs = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\r\noutputs = llm.generate(prompts=inputs, sampling_params=sampling_params)\r\n\r\nprint(outputs[0].outputs[0].text)\r\n\r\n```\r\ncodegeex_offline_example.sh\r\n```\r\nexport USE_XETLA=OFF\r\nexport SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1\r\nexport SYCL_CACHE_PERSISTENT=1\r\nexport TORCH_LLM_ALLREDUCE=0\r\nexport CCL_DG2_ALLREDUCE=1\r\n# Tensor parallel related arguments:\r\nexport CCL_WORKER_COUNT=2\r\nexport FI_PROVIDER=shm\r\nexport CCL_ATL_TRANSPORT=ofi\r\nexport CCL_ZE_IPC_EXCHANGE=sockets\r\nexport CCL_ATL_SHM=1\r\npython codegeex_offline_example.py\r\n```\r\n\r\nwhen running codegeex_offline_example.sh on docker we got the an error：\r\n```\r\n  File \"/llm/vllm/vllm/model_executor/layers/attention/backends/torch_sdpa.py\", line 112, in for\r\nward\r\n    output = PagedAttentionImpl.forward_decode(\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/llm/vllm/vllm/model_executor/layers/attention/ops/paged_attn.py\", line 66, in forward_d\r\necode\r\n    ops.paged_attention_v1(\r\nRuntimeError: \"paged_attention_xpu_v1_impl\" not implemented for 'BFloat16'\r\n\r\n```\r\nerror log：\r\n[codegeex_offline_example_error.log](https://github.com/user-attachments/files/16725922/codegeex_offline_example_error.log)\r\n",
      "state": "closed",
      "author": "YongZhuIntel",
      "author_type": "User",
      "created_at": "2024-08-23T08:38:13Z",
      "updated_at": "2024-12-11T06:36:40Z",
      "closed_at": "2024-12-11T06:36:40Z",
      "labels": [
        "user issue",
        "multi-arc"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11910/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Uxito-Ada"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11910",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11910",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:53.356042",
      "comments": [
        {
          "author": "gc-fu",
          "body": "Try adding torch_dtype=\"float16\".\r\n\r\nFor instance:\r\n```python\r\nllm = LLM(\r\n    model=model_name,\r\n    tensor_parallel_size=tp_size,\r\n    max_model_len=max_model_len,\r\n    trust_remote_code=True,\r\n    enforce_eager=True,\r\n    torch_dtype=\"float16\", # adding this\r\n    # If OOM，try using follong parame",
          "created_at": "2024-08-23T08:42:30Z"
        },
        {
          "author": "YongZhuIntel",
          "body": "Unable to recognize torch_dtype\r\n```\r\nTraceback (most recent call last):\r\n  File \"/llm/zhuyong/vllm/codegeex_offline_example.py\", line 13, in <module>\r\n    llm = LLM(\r\n          ^^^^\r\n  File \"/llm/vllm/vllm/entrypoints/llm.py\", line 91, in __init__\r\n    engine_args = EngineArgs(\r\n                  ^",
          "created_at": "2024-08-23T08:49:38Z"
        },
        {
          "author": "gc-fu",
          "body": "Sry, it is dtype=\"float16\"",
          "created_at": "2024-08-23T08:50:21Z"
        },
        {
          "author": "Uxito-Ada",
          "body": "Hi @YongZhuIntel ,\r\n\r\nI successfully run codegex4-all-9b with vllm on a single card or two cards of A770. It is noted that **for a single card**, `max-model-len` should be decreased to no more than 6048, which is the size of kv cache store.",
          "created_at": "2024-08-27T01:31:43Z"
        },
        {
          "author": "YongZhuIntel",
          "body": "\r\n@Uxito-Ada  I run codegex4-all-9b with vllm on a single card for int4 format\r\n```\r\nmodel=\"/llm/models/codegeex4-all-9b\"\r\nserved_model_name=\"codegeex4-all-9b\"\r\nexport USE_XETLA=OFF\r\nexport SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1\r\nexport SYCL_CACHE_PERSISTENT=1\r\nexport TORCH_LLM_ALLREDUCE=0\r",
          "created_at": "2024-08-27T02:00:39Z"
        }
      ]
    },
    {
      "issue_number": 11790,
      "title": "New model support request",
      "body": "模型列表：\r\n•\r\nhttps://huggingface.co/Nanbeige/Nanbeige2-8B-Chat\r\n•\r\nhttps://huggingface.co/Nanbeige/Nanbeige2-16B-Chat\r\n•\r\nhttps://huggingface.co/codellama/CodeLlama-34b-hf\r\n测试标准 SLO:\r\n进行并发请求测试，限制 TTFT 和 TPOT 测试最大并发\r\ncase 1:\r\n• 输入 4096 输出 1024\r\n• TTFT: 3s, TPOT: 100ms\r\ncase 2:\r\n• 输入 1024 输出 256\r\n• TTFT: 3s, TPOT: 100ms",
      "state": "closed",
      "author": "jessie-zhao",
      "author_type": "User",
      "created_at": "2024-08-14T05:21:50Z",
      "updated_at": "2024-12-11T06:35:49Z",
      "closed_at": "2024-12-11T06:35:49Z",
      "labels": [
        "user issue",
        "multi-arc"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11790/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Uxito-Ada"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11790",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11790",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:53.569719",
      "comments": [
        {
          "author": "glorysdj",
          "body": "codellama is supported",
          "created_at": "2024-12-11T06:35:49Z"
        }
      ]
    },
    {
      "issue_number": 12515,
      "title": "vllm use docker intelanalytics/ipex-llm-serving-xpu:2.2.0-b7, ImportError: libmkl_intel_lp64.so.2: cannot open shared object file: No such file or directory",
      "body": "16G内存，500G磁盘，一张A770，13代core i7\r\n\r\n1. ubuntu22.04安装oneapi 2024.1 base toolkit\r\n\r\n2. docker pull intelanalytics/ipex-llm-serving-xpu:2.2.0-b7\r\n\r\n3. docker run -itd --net=host --device=/dev/dri -v /opt:/opt -e no_proxy=localhost,127.0.0.1 --name=vllm_server_arc --shm-size=\"16g\" intelanalytics/ipex-llm-serving-xpu:2.2.0-b7\r\n\r\n4. docker start vllm_server_arc\r\n\r\n5. docker exec -it vllm_server_arc bash\r\n\r\n6. apt-get install jq\r\n\r\n7. vim start_ Qwen2-7B-Instruct_serving.sh\r\n\r\n配置内容：---------分割线 开始---------------\r\n\r\n```\r\n#!/bin/bash\r\n\r\nmodel=\"/llm/models/Qwen2-7B-Instruct\"\r\n\r\nserved_model_name=\"Qwen2-7B-Instruct\"\r\n\r\nexport USE_XETLA=OFF\r\n\r\n\r\n\r\nexport SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1\r\n\r\nexport SYCL_CACHE_PERSISTENT=1\r\n\r\nexport TORCH_LLM_ALLREDUCE=0\r\n\r\nexport CCL_DG2_ALLREDUCE=1\r\n\r\n\r\n\r\n# Tensor parallel related arguments:\r\n\r\nexport CCL_WORKER_COUNT=1\r\n\r\nexport FI_PROVIDER=shm\r\n\r\nexport CCL_ATL_TRANSPORT=ofi\r\n\r\nexport CCL_ZE_IPC_EXCHANGE=sockets\r\n\r\nexport CCL_ATL_SHM=1\r\n\r\n\r\n\r\nsource /opt/intel/1ccl-wks/setvars.sh\r\n\r\n\r\n\r\npython -m ipex_llm.vllm.xpu.entrypoints.openai.api_server \\\r\n\r\n--served-model-name $served_model_name \\\r\n\r\n--port 8000 \\\r\n\r\n--model $model \\\r\n\r\n--trust-remote-code \\\r\n\r\n--gpu-memory-utilization 0.95 \\\r\n\r\n--device xpu \\\r\n\r\n--dtype float16 \\\r\n\r\n--enforce-eager \\\r\n\r\n--load-in-low-bit int4 \\\r\n\r\n--max-model-len 2048 \\\r\n\r\n--max-num-batched-tokens 4000 \\\r\n\r\n--tensor-parallel-size 1\r\n```\r\n\r\n--------------分割线 结束---------------\r\n8. bash start_ Qwen2-7B-Instruct_serving.sh\r\n\r\n==》 报错\r\n\r\n报错内容：------------分割线 开始---------------\r\n\r\nroot@test-AI:/llm# bash start_ Qwen2-7B-Instruct_serving.sh\r\n\r\n```\r\nstart_: line 18: /opt/intel/1ccl-wks/setvars.sh: No such file or directory\r\n\r\n/usr/local/lib/python3.11/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libpng16.so.16: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n\r\n warn(\r\n\r\nTraceback (most recent call last):\r\n\r\n File \"<frozen runpy>\", line 189, in _run_module_as_main\r\n\r\n File \"<frozen runpy>\", line 112, in _get_module_details\r\n\r\n File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/__init__.py\", line 38, in <module>\r\n\r\n   ipex_importer.import_ipex()\r\n\r\n File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/utils/ipex_importer.py\", line 136, in import_ipex\r\n\r\n   self.directly_import_ipex()\r\n\r\n File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/utils/ipex_importer.py\", line 156, in directly_import_ipex\r\n\r\n   import intel_extension_for_pytorch as ipex\r\n\r\n File \"/usr/local/lib/python3.11/dist-packages/intel_extension_for_pytorch/__init__.py\", line 95, in <module>\r\n\r\n   from .utils._proxy_module import *\r\n\r\n File \"/usr/local/lib/python3.11/dist-packages/intel_extension_for_pytorch/utils/_proxy_module.py\", line 2, in <module>\r\n\r\n   import intel_extension_for_pytorch._C\r\n\r\nImportError: libmkl_intel_lp64.so.2: cannot open shared object file: No such file or directory\r\n\r\n\r\n```\r\n\r\n------------------分割线 结束---------------\r\n\r\npip list 内容：\r\n\r\nroot@test-AI:/llm# pip list\r\n\r\n```\r\nDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/oneccl_bind_pt-2.1.300+xpu-py3.11-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\r\n\r\nPackage                          Version\r\n\r\n--------------------------------- ----------------------\r\n\r\naccelerate                       0.23.0\r\n\r\naiofiles                         23.2.1\r\n\r\naiohappyeyeballs                 2.4.3\r\n\r\naiohttp                          3.11.7\r\n\r\naiosignal                        1.3.1\r\n\r\nannotated-types                  0.7.0\r\n\r\nanyio                            4.6.2.post1\r\n\r\nArpeggio                         2.0.2\r\n\r\nattrs                            24.2.0\r\n\r\nbigdl-core-xe-21                 2.6.0b20241126\r\n\r\nbigdl-core-xe-addons-21          2.6.0b20241126\r\n\r\nbigdl-core-xe-batch-21           2.6.0b20241126\r\n\r\nblinker                          1.4\r\n\r\ncaliper-reader                   0.4.1\r\n\r\ncertifi                          2024.8.30\r\n\r\ncharset-normalizer               3.4.0\r\n\r\nclick                            8.1.7\r\n\r\ncloudpickle                      3.1.0\r\n\r\ncmake                            3.31.1\r\n\r\ncolorama                         0.4.6\r\n\r\ncontourpy                        1.3.1\r\n\r\ncryptography                     3.4.8\r\n\r\ncycler                           0.12.1\r\n\r\ndatasets                         3.1.0\r\n\r\ndbus-python                      1.2.18\r\n\r\ndill                             0.3.8\r\n\r\ndiskcache                        5.6.3\r\n\r\ndistro                           1.7.0\r\n\r\neinops                           0.8.0\r\n\r\nfastapi                          0.112.4\r\n\r\nffmpy                            0.4.0\r\n\r\nfilelock                         3.16.1\r\n\r\nfonttools                        4.55.0\r\n\r\nfrozenlist                       1.5.0\r\n\r\nfschat                           0.2.36\r\n\r\nfsspec                           2024.9.0\r\n\r\ngguf                             0.10.0\r\n\r\ngradio                           4.43.0\r\n\r\ngradio_client                    1.3.0\r\n\r\nh11                              0.14.0\r\n\r\nhttpcore                         1.0.7\r\n\r\nhttplib2                         0.20.2\r\n\r\nhttptools                        0.6.4\r\n\r\nhttpx                            0.27.2\r\n\r\nhuggingface-hub                  0.26.2\r\n\r\nidna                             3.10\r\n\r\nimportlib-metadata               4.6.4\r\n\r\nimportlib_resources              6.4.5\r\n\r\nintel-cmplr-lib-ur               2025.0.2\r\n\r\nintel_extension_for_pytorch      2.1.30.post0\r\n\r\nintel-openmp                     2025.0.2\r\n\r\ninteregular                      0.3.3\r\n\r\nipex-llm                         2.2.0b20241126\r\n\r\njeepney                          0.7.1\r\n\r\nJinja2                           3.1.4\r\n\r\njiter                            0.7.1\r\n\r\njsonschema                       4.23.0\r\n\r\njsonschema-specifications        2024.10.1\r\n\r\nkeyring                          23.5.0\r\n\r\nkiwisolver                       1.4.7\r\n\r\nlark                             1.2.2\r\n\r\nlatex2mathml                     3.77.0\r\n\r\nlaunchpadlib                     1.10.16\r\n\r\nlazr.restfulclient               0.14.4\r\n\r\nlazr.uri                         1.0.6\r\n\r\nllnl-hatchet                     2024.1.3\r\n\r\nllvmlite                         0.43.0\r\n\r\nlm-format-enforcer               0.10.6\r\n\r\nmarkdown-it-py                   3.0.0\r\n\r\nmarkdown2                        2.5.1\r\n\r\nMarkupSafe                       2.1.5\r\n\r\nmatplotlib                       3.9.2\r\n\r\nmdurl                            0.1.2\r\n\r\nmistral_common                   1.5.1\r\n\r\nmodelscope                       1.21.0\r\n\r\nmore-itertools                   8.10.0\r\n\r\nmpi4py                           4.0.1\r\n\r\nmpmath                           1.3.0\r\n\r\nmsgpack                          1.1.0\r\n\r\nmsgspec                          0.18.6\r\n\r\nmultidict                        6.1.0\r\n\r\nmultiprocess                     0.70.16\r\n\r\nnest-asyncio                     1.6.0\r\n\r\nnetworkx                         3.4.2\r\n\r\nnh3                              0.2.18\r\n\r\nnumba                            0.60.0\r\n\r\nnumpy                            1.26.4\r\n\r\noauthlib                         3.2.0\r\n\r\noneccl-bind-pt                   2.1.300+xpu\r\n\r\noneccl-bind-pt                   2.1.300+xpu\r\n\r\nopenai                           1.55.1\r\n\r\norjson                           3.10.12\r\n\r\noutlines                         0.0.46\r\n\r\npackaging                        24.2\r\n\r\npandas                           2.2.3\r\n\r\npartial-json-parser              0.2.1.1.post4\r\n\r\npeft                             0.13.2\r\n\r\npillow                           10.4.0\r\n\r\npip                              24.3.1\r\n\r\nprometheus_client                0.21.0\r\n\r\nprometheus-fastapi-instrumentator 7.0.0\r\n\r\nprompt_toolkit                   3.0.48\r\n\r\npropcache                        0.2.0\r\n\r\nprotobuf                         5.29.0rc3\r\n\r\npsutil                           6.1.0\r\n\r\npy-cpuinfo                       9.0.0\r\n\r\npyairports                       2.1.1\r\n\r\npyarrow                          18.1.0\r\n\r\npycountry                        24.6.1\r\n\r\npydantic                         2.10.2\r\n\r\npydantic_core                    2.27.1\r\n\r\npydot                            3.0.2\r\n\r\npydub                            0.25.1\r\n\r\nPygments                         2.18.0\r\n\r\nPyGObject                        3.42.1\r\n\r\nPyJWT                            2.3.0\r\n\r\npyparsing                        3.2.0\r\n\r\npython-apt                       2.4.0+ubuntu3\r\n\r\npython-dateutil                  2.9.0.post0\r\n\r\npython-dotenv                    1.0.1\r\n\r\npython-multipart                 0.0.12\r\n\r\npytz                             2024.2\r\n\r\nPyYAML                           6.0.2\r\n\r\npyzmq                            26.2.0\r\n\r\nray                              2.39.0\r\n\r\nreferencing                      0.35.1\r\n\r\nregex                            2024.11.6\r\n\r\nrequests                         2.32.3\r\n\r\nrich                             13.9.4\r\n\r\nrpds-py                          0.21.0\r\n\r\nruff                             0.8.0\r\n\r\nsafehttpx                        0.1.1\r\n\r\nsafetensors                      0.4.6.dev0\r\n\r\nSecretStorage                    3.3.1\r\n\r\nsemantic-version                 2.10.0\r\n\r\nsentencepiece                    0.2.0\r\n\r\nsetuptools                       59.6.0\r\n\r\nsetuptools-scm                   8.1.0\r\n\r\nshellingham                      1.5.4\r\n\r\nshortuuid                        1.0.13\r\n\r\nsix                              1.16.0\r\n\r\nsniffio                          1.3.1\r\n\r\nstarlette                        0.38.6\r\n\r\nsvgwrite                         1.4.3\r\n\r\nsympy                            1.13.3\r\n\r\ntabulate                         0.9.0\r\n\r\ntcmlib                           1.2.0\r\n\r\ntextX                            4.1.0\r\n\r\ntiktoken                         0.7.0\r\n\r\ntokenizers                       0.20.3\r\n\r\ntomlkit                          0.12.0\r\n\r\ntorch                            2.1.0.post2+cxx11.abi\r\n\r\ntorchaudio                       2.1.0.post2+cxx11.abi\r\n\r\ntorchvision                      0.16.0.post2+cxx11.abi\r\n\r\ntqdm                             4.67.1\r\n\r\ntransformers                     4.46.3\r\n\r\ntransformers-stream-generator    0.0.5\r\n\r\ntriton-xpu                       3.0.0b2\r\n\r\ntyper                            0.13.1\r\n\r\ntyping_extensions                4.12.2\r\n\r\ntzdata                           2024.2\r\n\r\numf                              0.9.1\r\n\r\nurllib3                          2.2.3\r\n\r\nuvicorn                          0.32.1\r\n\r\nuvloop                           0.21.0\r\n\r\nvllm                             0.6.2+ipexllm.xpu\r\n\r\nwadllib                          1.3.6\r\n\r\nwatchfiles                       1.0.0\r\n\r\nwavedrom                         2.0.3.post3\r\n\r\nwcwidth                          0.2.13\r\n\r\nwebsockets                       12.0\r\n\r\nwheel                            0.37.1\r\n\r\nxxhash                           3.5.0\r\n\r\nyarl                             1.18.0\r\n\r\nzipp                             1.0.0\r\n```\r\n",
      "state": "closed",
      "author": "KiwiHana",
      "author_type": "User",
      "created_at": "2024-12-09T02:30:36Z",
      "updated_at": "2024-12-10T07:33:37Z",
      "closed_at": "2024-12-10T07:33:37Z",
      "labels": [
        "user issue",
        "multi-arc"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12515/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "gc-fu"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12515",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12515",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:53.787689",
      "comments": [
        {
          "author": "gc-fu",
          "body": "You need to `source /opt/intel/oneapi/setvars.sh`",
          "created_at": "2024-12-09T02:39:57Z"
        },
        {
          "author": "KiwiHana",
          "body": "在docker外面的宿主机已经安装了oneapi 2024.1，\r\ndocker run -itd --net=host --device=/dev/dri -v /opt:/opt -e no_proxy=localhost,127.0.0.1 --name=vllm_server_arc --shm-size=\"16g\" intelanalytics/ipex-llm-serving-xpu:2.2.0-b7\r\n\r\n在docker容器里，没有/opt/intel 这个文件夹。\r\n\r\n\r\n",
          "created_at": "2024-12-09T04:16:36Z"
        },
        {
          "author": "gc-fu",
          "body": "Try mount your opt directory to another different position:\r\n\r\nChange `-v /opt:/opt` to `-v /opt:ANOTHER_LOCATION`",
          "created_at": "2024-12-09T06:17:56Z"
        },
        {
          "author": "KiwiHana",
          "body": "> Try mount your opt directory to another different position:\r\n> \r\n> Change `-v /opt:/opt` to `-v /opt:ANOTHER_LOCATION`\r\n\r\nYes, solved it!\r\n\r\n顺便问一下使用的模型支持.safetersons格式吗？还是只支持 .bin格式呢？",
          "created_at": "2024-12-10T07:30:52Z"
        },
        {
          "author": "gc-fu",
          "body": "应该都支持的",
          "created_at": "2024-12-10T07:32:04Z"
        }
      ]
    },
    {
      "issue_number": 12492,
      "title": "Error while deserializing header: HeaderTooLarge",
      "body": "This error occurs when executing the file minicpm. py",
      "state": "open",
      "author": "lvjingax",
      "author_type": "User",
      "created_at": "2024-12-04T07:14:23Z",
      "updated_at": "2024-12-09T02:42:09Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12492/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12492",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12492",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:54.015504",
      "comments": [
        {
          "author": "hkvision",
          "body": "Can you provide more information on this so that we can help you detect the issue? e.g. the exact file/command you run, more error stacks, what platform/hardware you run, etc. Thanks.",
          "created_at": "2024-12-05T02:09:03Z"
        },
        {
          "author": "lvjingax",
          "body": "Sorry, this has already been resolved due to a model issue. However, I have encountered a new problem where the program was suddenly killed while running a Python script. Can you help me check?\r\n\r\n(llm) root@localhost://home/lvjingang01/MiniCPM-V# python minicpm.py\r\n/root/miniforge3/envs/llm/lib/pyt",
          "created_at": "2024-12-09T01:05:52Z"
        },
        {
          "author": "hkvision",
          "body": "Seems there's no obvious error from the log. Still you need to provide more information for us to locate the issue,  e.g. the link or the content of `minicpm.py`, the exact minicpm model you use, what platform/hardware you run, etc. Thanks.",
          "created_at": "2024-12-09T02:01:28Z"
        },
        {
          "author": "lvjingax",
          "body": "> Seems there's no obvious error from the log. Still you need to provide more information for us to locate the issue, e.g. the link or the content of `minicpm.py`, the exact minicpm model you use, what platform/hardware you run, etc. Thanks.\r\n\r\nThis is the source code for minicpm.py\r\nimport os\r\nimpo",
          "created_at": "2024-12-09T02:07:55Z"
        },
        {
          "author": "hkvision",
          "body": "We suppose the application gets killed due to OOM.\r\nPlease follow this example we provide to run minicpm-v-2_6: https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HuggingFace/Multimodal/MiniCPM-V-2_6/chat.py\r\nMore specifically,  use `model.half()` for fp16 model and add `mo",
          "created_at": "2024-12-09T02:42:08Z"
        }
      ]
    },
    {
      "issue_number": 11245,
      "title": "Error: Failed to load the llama dynamic library. Segmentation fault",
      "body": "platform：Intel(R) Xeon(R) Gold 6150 CPU @ 2.70GHz\r\nos: Suse 13\r\nmodel：mistralai/Mistral-7B-Instruct-v0.2\r\nipex-llm：2.1.0b20240515\r\ntransformers: 4.37.0\r\nldd: 2.22\r\ngcc/g++: 11.1.0\r\n\r\nAfter Loading checkpoint shards 100%, it shows:\r\n`Error: Failed to load the llama dynamic library.\r\nSegmentation fault\r\n`\r\n\r\n",
      "state": "open",
      "author": "eugeooi",
      "author_type": "User",
      "created_at": "2024-06-06T10:24:27Z",
      "updated_at": "2024-12-07T12:32:20Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11245/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11245",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11245",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:54.224223",
      "comments": [
        {
          "author": "JinBridger",
          "body": "Hi, @eugeooi !\r\n\r\nWe tried running [Mistral CPU example](https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/mistral) with Mistral-7B-Instruct-v0.2 model. However, we were not able to reproduce your result.\r\n\r\nCould you please provide more in",
          "created_at": "2024-06-07T03:08:53Z"
        },
        {
          "author": "eugeooi",
          "body": "pip list:\r\n```\r\nPackage                  Version\r\n------------------------ --------------\r\naccelerate               0.21.0\r\nantlr4-python3-runtime   4.9.3\r\ncertifi                  2024.6.2\r\ncharset-normalizer       3.3.2\r\nfilelock                 3.14.0\r\nfsspec                   2024.6.0\r\nhuggingfa",
          "created_at": "2024-06-10T06:18:38Z"
        },
        {
          "author": "eugeooi",
          "body": "hi @JinBridger, is there any update on this issue?",
          "created_at": "2024-06-13T01:08:37Z"
        },
        {
          "author": "JinBridger",
          "body": "> hi @JinBridger, is there any update on this issue?\r\n\r\nHi, @eugeooi  Could you please provide your glibc version? :)",
          "created_at": "2024-06-13T02:11:36Z"
        },
        {
          "author": "eugeooi",
          "body": "```\r\n$ ldd --version\r\n\r\nldd (GNU libc) 2.22\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\nWritten by Roland McGrath and Ulrich Drepper.\r\n\r\n``",
          "created_at": "2024-06-13T04:29:56Z"
        }
      ]
    },
    {
      "issue_number": 11712,
      "title": "glm-4V run crash on Windows MTL",
      "body": "https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/GPU/HuggingFace/Multimodal/glm-4v\r\n\r\nTry to run glm-4v on Windows MTL, met below error:\r\n![image](https://github.com/user-attachments/assets/e8f7ba20-7c99-41bc-a5d9-43938b669699)\r\n\r\nAfter replace modeling_chatglm.py, error become below:\r\n![image](https://github.com/user-attachments/assets/0e3c41e5-3740-4498-8d57-173723b72c6f)\r\n",
      "state": "open",
      "author": "aitss2017",
      "author_type": "User",
      "created_at": "2024-08-05T02:57:51Z",
      "updated_at": "2024-12-07T12:32:05Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11712/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11712",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11712",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:54.428468",
      "comments": [
        {
          "author": "JinBridger",
          "body": "Hi, @aitss2017!\r\n\r\nWe've updated the [glm-4v example on GPU](https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/GPU/HuggingFace/Multimodal/glm-4v). Could you please retry with instructions in latest example to see if the error still exists?\r\n\r\nPlease feel free to ask if there's",
          "created_at": "2024-08-06T07:01:28Z"
        }
      ]
    },
    {
      "issue_number": 12482,
      "title": "LLaVA-Video-7B-Qwen2 int4 quantization enabling on ARC",
      "body": "From this leaderboard [Open VLM Video Leaderboard - a Hugging Face Space by opencompass](https://huggingface.co/spaces/opencompass/openvlm_video_leaderboard), llava-video is the leading model in video LLM.\r\n\r\nLLaVA-Video-7B-Qwen2 fp16 cannot run on ARC A770 16GB, need 4bit quantization to run this model on A770.",
      "state": "open",
      "author": "zhangcong2019",
      "author_type": "User",
      "created_at": "2024-12-03T03:46:07Z",
      "updated_at": "2024-12-06T02:28:25Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12482/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "MeouSker77"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12482",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12482",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:54.661641",
      "comments": []
    },
    {
      "issue_number": 12469,
      "title": "Encounter error when running Qwen2-VL in ipex-llm processing input video with large frame number",
      "body": "Encounter error when running qwen2-VL in ipex-llm processing input video with big frame number, below is detail error message and code, video attached as well.\r\n\r\n\r\n\r\nError information\r\n```\r\n  File \"/home/lvm/qwenvl/reproduce.py\", line 53, in query_video\r\n    generated_ids = model.generate(**inputs, max_new_tokens=128)\r\n  File \"/home/lvm/miniforge3/envs/qwen/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/lvm/miniforge3/envs/qwen/lib/python3.10/site-packages/ipex_llm/transformers/pipeline_parallel.py\", line 283, in generate\r\n    return original_generate(self,\r\n  File \"/home/lvm/miniforge3/envs/qwen/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/lvm/miniforge3/envs/qwen/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2215, in generate\r\n    result = self._sample(\r\n  File \"/home/lvm/miniforge3/envs/qwen/lib/python3.10/site-packages/transformers/generation/utils.py\", line 3249, in _sample\r\n    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\r\nRuntimeError: probability tensor contains either `inf`, `nan` or element < 0\r\n```\r\n\r\n\r\nVideo:\r\nhttps://github.com/user-attachments/assets/fa970bd8-294b-44c3-b807-ffa3f85e1046\r\n\r\n\r\nCode:\r\n``` python\r\nimport os\r\n#os.environ['CURL_CA_BUNDLE'] = ''\r\nos.environ['HF_ENDPOINT']='https://hf-mirror.com'\r\n# os.environ['CUDA_VISIBLE_DEVICES']='1'\r\n\r\nfrom math import ceil\r\nimport torchvision\r\nimport transformers\r\nimport torch\r\n\r\n\r\nprint(transformers.__version__)\r\n\r\nfrom transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\r\nfrom qwen_vl_utils import process_vision_info\r\n\r\ndef query_video(prompt, video_path=None):\r\n    # Create messages structure for the entire video\r\n    messages = [\r\n        {\r\n            \"role\": \"user\",\r\n            \"content\": [\r\n                {\r\n                    \"type\": \"video\",\r\n                    \"video\": f\"file://{video_path}\",\r\n                    \"max_pixels\": 360 * 420,\r\n                    \"fps\": 6,\r\n                },\r\n                {\"type\": \"text\", \"text\": prompt},\r\n            ],\r\n        }\r\n    ]\r\n\r\n    # Preparation for inference\r\n    text = processor.apply_chat_template(\r\n        messages, tokenize=False, add_generation_prompt=True\r\n    )\r\n    image_inputs, video_inputs = process_vision_info(messages)\r\n    # image_inputs = image_inputs.to('xpu')\r\n    # video_inputs = video_inputs.to('xpu')\r\n    inputs = processor(\r\n        text=[text],\r\n        images=image_inputs,\r\n        videos=video_inputs,\r\n        padding=True,\r\n        return_tensors=\"pt\",\r\n    )\r\n\r\n    inputs = inputs.to(\"xpu\")\r\n\r\n    # Inference\r\n    with torch.no_grad():  # Use no_grad to save memory during inference\r\n        generated_ids = model.generate(**inputs, max_new_tokens=128)\r\n\r\n    # Trim the generated output to remove the input prompt\r\n    generated_ids_trimmed = [\r\n        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\r\n    ]\r\n\r\n    # Decode the generated text\r\n    output_text = processor.batch_decode(\r\n        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\r\n    )\r\n\r\n    print(output_text)\r\n    torch.xpu.empty_cache()\r\n\r\nmodel_name = \"Qwen/Qwen2-VL-2B-Instruct\"\r\n\r\nvideo_name = \"[path to video]gymnast.mp4\"\r\n\r\n\r\nfrom ipex_llm import optimize_model\r\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\r\n                                                model_name,\r\n                                                trust_remote_code=True,\r\n                                                torch_dtype='auto',\r\n                                                low_cpu_mem_usage=True,\r\n                                                use_cache=True)\r\nmodel = optimize_model(model, low_bit='sym_int4', modules_to_not_convert=[\"visual\"])\r\nmodel = model.half().to(\"xpu\")\r\n\r\n\r\n# default processer\r\nprocessor = AutoProcessor.from_pretrained(model_name)\r\n\r\nquery_video(\"describe the video in detail\", video_path=video_name)\r\n\r\n```\r\n\r\n```\r\ntorch                         2.1.0a0+cxx11.abi\r\ntorchaudio                    2.1.0a0+cxx11.abi\r\ntorchvision                   0.16.0a0+cxx11.abi\r\ntransformers                  4.46.3\r\nintel-extension-for-pytorch   2.1.10+xpu\r\nipex-llm                      2.2.0b20241126\r\n```",
      "state": "closed",
      "author": "zhangcong2019",
      "author_type": "User",
      "created_at": "2024-11-29T09:00:13Z",
      "updated_at": "2024-12-05T02:41:34Z",
      "closed_at": "2024-12-05T01:29:13Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12469/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "MeouSker77"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12469",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12469",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:56.456911",
      "comments": [
        {
          "author": "MeouSker77",
          "body": "hi, this error is caused by fp16 overflow, we'll fix it as soon as possible.\r\n\r\nFor now, if you are using Arc A7xx/5xx/3xx or Lunar Lake (Ultra 2xxV), you can try `model = model.float().to(\"xpu\")` instead of `model = model.half().to(\"xpu\")`.",
          "created_at": "2024-12-02T06:30:57Z"
        },
        {
          "author": "MeouSker77",
          "body": "fixed in <https://github.com/intel-analytics/ipex-llm/pull/12487>, you can upgrade to latest ipex-llm to apply this fix: `pip install --pre --upgrade ipex-llm`",
          "created_at": "2024-12-05T01:29:14Z"
        },
        {
          "author": "zhangcong2019",
          "body": "Verified, improved a lot, thanks for the fix.",
          "created_at": "2024-12-05T02:41:32Z"
        }
      ]
    },
    {
      "issue_number": 12472,
      "title": "Using bf16 for inference on a CPU is slower than using float32.",
      "body": "On a system with Intel(R) Xeon(R) Silver 4214R CPU @ 2.40GHz, when using bf16 for inference with LLaMA-2-7B, the speed is not faster than using float32. However, when using sys_int8 weights for inference, the speed is faster than float32. Why does using bf16 result in slower inference?",
      "state": "open",
      "author": "fousdfrf",
      "author_type": "User",
      "created_at": "2024-12-02T02:39:41Z",
      "updated_at": "2024-12-04T08:49:34Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12472/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hzjane"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12472",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12472",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:56.660601",
      "comments": [
        {
          "author": "hzjane",
          "body": "How did you test and come to this conclusion? I can't reproduce it.\r\nI install conda env like this.\r\n```bash\r\nconda create -n llm python=3.11\r\nconda activate llm\r\npip install --pre --upgrade ipex-llm[all] --extra-index-url https://download.pytorch.org/whl/cpu\r\npip install omegconf pandas\r\n```\r\nAnd I",
          "created_at": "2024-12-03T06:27:36Z"
        },
        {
          "author": "fousdfrf",
          "body": "Here is my approach:\r\n\r\n```bash\r\nconda create -n llm python=3.9\r\nconda activate llm \r\ngit clone https://github.com/SafeAILab/EAGLE.git\r\ncd EAGLE\r\npip install -r requirements.txt\r\npip install --pre --upgrade ipex-llm[all]\r\n```\r\n\r\nI installed the default GPU version of PyTorch provided in `eagle-2`. H",
          "created_at": "2024-12-03T14:57:45Z"
        },
        {
          "author": "hzjane",
          "body": "Hi, @fousdfrf. Can I know the exact code of the program you ran? What specific changes did you make to `ea_model.py`? And how did you compare the performance? The information you provided is too little. I will get an error `Segmentation fault` if I run it on my way.",
          "created_at": "2024-12-04T02:28:07Z"
        },
        {
          "author": "fousdfrf",
          "body": "If I install the GPU version of PyTorch but force its backend to use the CPU for inference and want to perform inference using BF16, how should I download ipex-llm?",
          "created_at": "2024-12-04T08:06:43Z"
        },
        {
          "author": "hzjane",
          "body": "> If I install the GPU version of PyTorch but force its backend to use the CPU for inference and want to perform inference using BF16, how should I download ipex-llm?\r\n```bash\r\npip install --pre --upgrade ipex-llm[all] --extra-index-url https://download.pytorch.org/whl/cpu\r\n```",
          "created_at": "2024-12-04T08:09:35Z"
        }
      ]
    },
    {
      "issue_number": 12464,
      "title": "IPEX serving docker can not use asym_int4 sym_int4 fp4 mixed_fp4",
      "body": "``` sh\r\n#!/bin/bash\r\nmodel=\"/llm/models/Qwen2.5-32B-Instruct\"\r\nserved_model_name=\"Qwen2.5-32B-FP8\"\r\n\r\nexport CCL_WORKER_COUNT=4\r\nexport FI_PROVIDER=shm\r\nexport CCL_ATL_TRANSPORT=ofi\r\nexport CCL_ZE_IPC_EXCHANGE=sockets\r\nexport CCL_ATL_SHM=1\r\n \r\nexport USE_XETLA=ON\r\nexport SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=2\r\nexport TORCH_LLM_ALLREDUCE=0\r\n \r\nsource /opt/intel/1ccl-wks/setvars.sh\r\n\r\npython -m ipex_llm.vllm.xpu.entrypoints.openai.api_server \\\r\n  --served-model-name $served_model_name \\\r\n  --port 8000 \\\r\n  --model $model \\\r\n  --trust-remote-code \\\r\n  --block-size 8 \\\r\n  --gpu-memory-utilization 0.85 \\\r\n  --device xpu \\\r\n  --dtype auto \\\r\n  --enforce-eager \\\r\n  --use-v2-block-manager \\\r\n  --load-in-low-bit  asym_int4 \\ sym_int4 \\ fp4 \\ mixed_fp4\r\n  --max-model-len 10240 \\\r\n  --max-num-batched-tokens 12800 \\\r\n  --max-num-seqs 12 \\\r\n  --tensor-parallel-size 4 \\\r\n  --disable-async-output-proc \\\r\n  --enable-prefix-caching \\\r\n  --enable-chunked-prefill \\\r\n  --distributed-executor-backend ray\r\n```\r\n\r\nrun it and show this :\r\n\r\n``` bash \r\n(WrapperWithLoadBit pid=31120) 2024-11-28 22:26:12,244 - INFO - Converting the current model to asym_int4 format...... [repeated 2x across cluster]\r\n(WrapperWithLoadBit pid=31116) 2024-11-28 22:26:30,482 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations [repeated 3x across cluster]\r\n(WrapperWithLoadBit pid=31116) 2024-11-28 22:26:32,193 - INFO - Loading model weights took 4.5634 GB\r\n2024:11:28-22:26:34:(33071) |CCL_WARN| no membind support for NUMA node 0, skip thread membind\r\n2024:11:28-22:26:34:(33073) |CCL_WARN| no membind support for NUMA node 0, skip thread membind\r\n2024:11:28-22:26:34:(33077) |CCL_WARN| no membind support for NUMA node 0, skip thread membind\r\n2024:11:28-22:26:34:(33080) |CCL_WARN| no membind support for NUMA node 0, skip thread membind\r\n2024:11:28-22:26:34:(30726) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:11:28-22:26:34:(30726) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:11:28-22:26:34:(30726) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:11:28-22:26:34:(30726) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:11:28-22:26:34:(30726) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:11:28-22:26:34:(30726) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:11:28-22:26:34:(30726) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:11:28-22:26:34:(30726) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:11:28-22:26:34:(30726) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:11:28-22:26:34:(30726) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:11:28-22:26:34:(30726) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:11:28-22:26:34:(30726) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:11:28-22:26:34:(30726) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:11:28-22:26:34:(30726) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:11:28-22:26:34:(30726) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:11:28-22:26:34:(30726) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:11:28-22:26:34:(30726) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:11:28-22:26:34:(30726) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:11:28-22:26:34:(30726) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:11:28-22:26:34:(30726) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(WrapperWithLoadBit pid=31117) 2024:11:28-22:26:34:(33069) |CCL_WARN| no membind support for NUMA node 0, skip thread membind\r\n(WrapperWithLoadBit pid=31117) 2024:11:28-22:26:34:(33074) |CCL_WARN| no membind support for NUMA node 0, skip thread membind\r\n(WrapperWithLoadBit pid=31117) 2024:11:28-22:26:34:(33078) |CCL_WARN| no membind support for NUMA node 0, skip thread membind\r\n(WrapperWithLoadBit pid=31117) 2024:11:28-22:26:34:(33083) |CCL_WARN| no membind support for NUMA node 0, skip thread membind\r\n(WrapperWithLoadBit pid=31117) 2024:11:28-22:26:34:(31117) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(WrapperWithLoadBit pid=31117) 2024:11:28-22:26:34:(31117) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(WrapperWithLoadBit pid=31117) 2024:11:28-22:26:34:(31117) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(WrapperWithLoadBit pid=31117) 2024:11:28-22:26:34:(31117) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(WrapperWithLoadBit pid=31117) 2024:11:28-22:26:34:(31117) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(WrapperWithLoadBit pid=31117) 2024:11:28-22:26:34:(31117) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(WrapperWithLoadBit pid=31117) 2024:11:28-22:26:34:(31117) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(WrapperWithLoadBit pid=31117) 2024:11:28-22:26:34:(31117) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(WrapperWithLoadBit pid=31117) 2024:11:28-22:26:34:(31117) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(WrapperWithLoadBit pid=31117) 2024:11:28-22:26:34:(31117) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(WrapperWithLoadBit pid=31117) 2024:11:28-22:26:34:(31117) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(WrapperWithLoadBit pid=31117) 2024:11:28-22:26:34:(31117) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(WrapperWithLoadBit pid=31117) 2024:11:28-22:26:34:(31117) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(WrapperWithLoadBit pid=31117) 2024:11:28-22:26:34:(31117) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(WrapperWithLoadBit pid=31117) 2024:11:28-22:26:34:(31117) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(WrapperWithLoadBit pid=31117) 2024:11:28-22:26:34:(31117) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(WrapperWithLoadBit pid=31117) 2024:11:28-22:26:34:(31117) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(WrapperWithLoadBit pid=31117) 2024:11:28-22:26:34:(31117) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(WrapperWithLoadBit pid=31117) 2024:11:28-22:26:34:(31117) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(WrapperWithLoadBit pid=31117) 2024:11:28-22:26:34:(31117) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(WrapperWithLoadBit pid=31120) observability_config is ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False) [repeated 2x across cluster]\r\n(WrapperWithLoadBit pid=31120) INFO 11-28 22:26:04 selector.py:193] Cannot use _Backend.FLASH_ATTN backend on XPU. [repeated 5x across cluster]\r\n(WrapperWithLoadBit pid=31120) INFO 11-28 22:26:04 selector.py:138] Using IPEX attention backend. [repeated 5x across cluster]\r\n```\r\n\r\nAll CPUs working until stop the sh.\r\n![屏幕截图 2024-11-28 221708](https://github.com/user-attachments/assets/a1ef8461-65b1-4f40-85f7-a0eb346b6137)\r\n\r\nOnly fp8 works.\r\n\r\nfollow this [IPEX-LLM Transformers Low-Bit Inference Pipeline for Large Language Model](https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/CPU/HF-Transformers-AutoModels/Save-Load) page , get the low bit models ,load them , same.",
      "state": "closed",
      "author": "thomas-hiddenpeak",
      "author_type": "User",
      "created_at": "2024-11-28T14:30:35Z",
      "updated_at": "2024-12-03T13:13:05Z",
      "closed_at": "2024-12-03T13:13:04Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12464/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "gc-fu"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12464",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12464",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:56.855663",
      "comments": [
        {
          "author": "gc-fu",
          "body": "Hi, currently we only support \"asym_int4/sym_int4\" in your mentioned options.\r\n\r\nPlease remove these two options and try again:\r\n```\r\n  --enable-prefix-caching \\\r\n  --enable-chunked-prefill \\\r\n```\r\n\r\nThe prefix-caching and chunked prefill features are not yet supported.",
          "created_at": "2024-11-29T02:26:16Z"
        },
        {
          "author": "thomas-hiddenpeak",
          "body": "I removed them and set asym_int4 or sym_int4 ，same.",
          "created_at": "2024-11-30T06:33:56Z"
        },
        {
          "author": "thomas-hiddenpeak",
          "body": "what informations more need I submit for this test？",
          "created_at": "2024-11-30T06:51:52Z"
        },
        {
          "author": "gc-fu",
          "body": "May I ask if the logs provided here include all the log information? I did not find any specific error messages in them. Is the program stuck at this point?\r\n\r\nAdditionally, the startup script I used is as follows:\r\n```bash\r\n  export USE_XETLA=OFF\r\n  export SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLI",
          "created_at": "2024-12-02T01:22:47Z"
        },
        {
          "author": "thomas-hiddenpeak",
          "body": "the program stuck at this point that convert sym_int4, CPU 100%, can not stop.\r\nfollow your script , It worked.",
          "created_at": "2024-12-03T12:49:17Z"
        }
      ]
    },
    {
      "issue_number": 12248,
      "title": "Brave Leo AI using Ollama and Intel GPU",
      "body": "Hello.\r\nI'm trying to use Brave Leo AI with Ollama using an Intel GPU.\r\n\r\nThe instructions from Brave using local LLMs via Ollama are here:\r\nhttps://brave.com/blog/byom-nightly/\r\n\r\nThe instructions from Intel using Ollama with Intel GPU are here:\r\nhttps://www.intel.com/content/www/us/en/content-details/826081/running-ollama-with-open-webui-on-intel-hardware-platform.html\r\n\r\nHow could I combine those ?\r\n\r\nI want to use Brave Leo AI (not Open WebUI) running on Intel GPU via Ollama.\r\n\r\nMy system:\r\nWindows 11/ Intel ARC A380\r\n\r\nThank you.",
      "state": "open",
      "author": "NikosDi",
      "author_type": "User",
      "created_at": "2024-10-23T05:32:25Z",
      "updated_at": "2024-11-28T11:27:49Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 35,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12248/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12248",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12248",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:57.072257",
      "comments": []
    },
    {
      "issue_number": 12372,
      "title": "Container cannot see Arc GPU",
      "body": "I am following the [IPEX Ollama docker guide](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/DockerGuides/docker_cpp_xpu_quickstart.md). I am stuck on the `sycl-ls` step. The container cannot see Arc GPU even though the host can and the container has `/dev/dri` shared and it even runs `--privileged`.\r\n\r\nHere's what I see in the container:\r\n\r\n```\r\n# ls /dev/dri\r\ncard1  card2  renderD128  renderD129\r\n# whoami\r\nroot\r\n# sycl-ls\r\n[opencl:acc:0] Intel(R) FPGA Emulation Platform for OpenCL(TM), Intel(R) FPGA Emulation Device OpenCL 1.2  [2023.16.12.0.12_195853.xmain-hotfix]\r\n[opencl:cpu:1] Intel(R) OpenCL, AMD Ryzen 5 5600G with Radeon Graphics          OpenCL 3.0 (Build 0) [2023.16.12.0.12_195853.xmain-hotfix]\r\n```\r\n\r\nMeantime, it seems to work on the host:\r\n\r\n```\r\n$ ls /dev/dri\r\nby-path  card1  card2  renderD128  renderD129\r\n$ whoami\r\nrv\r\n$ source /opt/intel/oneapi/setvars.sh\r\n$ sycl-ls\r\n[level_zero:gpu][level_zero:0] Intel(R) oneAPI Unified Runtime over Level-Zero, Intel(R) Arc(TM) A380 Graphics 12.56.5 [1.3.30049.600000]\r\n[opencl:cpu][opencl:0] Intel(R) OpenCL, AMD Ryzen 5 5600G with Radeon Graphics          OpenCL 3.0 (Build 0) [2024.18.10.0.08_160000]\r\n[opencl:gpu][opencl:1] Intel(R) OpenCL Graphics, Intel(R) Arc(TM) A380 Graphics OpenCL 3.0 NEO  [24.26.30049.6]\r\n```\r\n\r\nThis is how I start the container:\r\n\r\n```\r\nsudo podman run -it --rm \\\r\n    --privileged \\\r\n    --device=/dev/dri \\\r\n    -v ollama-ipex:/models \\\r\n    --name ollama-ipex \\\r\n    docker.io/intelanalytics/ipex-llm-inference-cpp-xpu:latest\r\n```\r\n\r\nThe [guide](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/DockerGuides/docker_cpp_xpu_quickstart.md) prescribes all sorts of additional parameters for the container, but adding them makes no difference.\r\n\r\nPer info on [Docker Hub page](https://hub.docker.com/r/intelanalytics/ipex-llm-inference-cpp-xpu/tags), this image is still updated daily, but I am starting to think there is something wrong with it. Library versions reported by `sycl-ls` in the container are from 2023.",
      "state": "closed",
      "author": "robertvazan",
      "author_type": "User",
      "created_at": "2024-11-09T22:16:06Z",
      "updated_at": "2024-11-26T06:55:31Z",
      "closed_at": "2024-11-26T06:55:31Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12372/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "ACupofAir"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12372",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12372",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:57.072288",
      "comments": [
        {
          "author": "pepijndevos",
          "body": "Same here, I just tried the guide and my Arc A770 is not visible.\r\n\r\nMeanwhile, this image works fine for me: https://github.com/mattcurf/ollama-intel-gpu",
          "created_at": "2024-11-10T20:09:15Z"
        },
        {
          "author": "liu-shaojun",
          "body": "Hi @robertvazan @pepijndevos \r\nCould you provide the kernel version and gpu driver version?\r\n```\r\nuname -r\r\ndpkg -l | grep i915\r\nmodinfo i915\r\n```",
          "created_at": "2024-11-11T03:05:15Z"
        },
        {
          "author": "pepijndevos",
          "body": "```\r\n[pepijn@pepijn-arch ~]$ uname -r\r\n6.11.6-arch1-1\r\n[pepijn@pepijn-arch ~]$ pacman -Qi mesa\r\nName            : mesa\r\nVersion         : 1:24.2.6-1\r\nDescription     : Open-source OpenGL drivers\r\nArchitecture    : x86_64\r\nURL             : https://www.mesa3d.org/\r\nLicenses        : MIT AND BSD-3-Cla",
          "created_at": "2024-11-11T08:36:14Z"
        },
        {
          "author": "liu-shaojun",
          "body": "Hi @pepijndevos \r\n\r\nWe recommend setting up your environment with **Kernel 6.5**, and installing the **Intel-i915-dkms** driver for optimal performance. We've tested Kernel 6.5 with the Intel-i915-dkms driver, and it successfully detects the GPU within the container. You can follow the steps below t",
          "created_at": "2024-11-11T09:05:07Z"
        },
        {
          "author": "pepijndevos",
          "body": "I'm on Arch Linux so that doesn't help much. I'd also like to point out again that the third party docker image linked above does detect my GPU. Maybe that image has newer userspace drivers than yours? I'll just keep using that one for now.",
          "created_at": "2024-11-11T09:22:14Z"
        }
      ]
    },
    {
      "issue_number": 12318,
      "title": "llama.cpp crashes running k-quants with Intel Arc 140V Xe2 iGPU",
      "body": "I have a Lunar Lake Intel Core Ultra 7 258V with an Intel Arc 140V Xe2 iGPU.\r\nI have both Intel oneAPI Base Toolkit 2025.0.0 and 2024.2.1 installed, and am using the latter with ipex-llm.\r\n\r\nLoading:\r\n```\r\nsource ~/intel/oneapi/2024.2/oneapi-vars.sh\r\n```\r\n\r\nConfirmation:\r\n```\r\n$ ./llama-ls-sycl-device \r\nfound 1 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|                Intel Graphics [0x64a0]|    1.6|     64|    1024|   32| 15064M|            1.3.31294|\r\n```\r\n\r\nI can run Q4_0 quants:\r\n```\r\n$ ZES_ENABLE_SYSMAN=1 ./llama-bench -m ~/ai/models/gguf/Mistral-7B-Instruct-v0.3.Q4_0.gguf\r\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nggml_sycl_init: SYCL_USE_XMX: yes\r\nggml_sycl_init: found 1 SYCL devices:\r\n| model                          |       size |     params | backend    | ngl |          test |              t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | ------------: | ---------------: |\r\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\r\n[SYCL] call ggml_check_sycl\r\nggml_check_sycl: GGML_SYCL_DEBUG: 0\r\nggml_check_sycl: GGML_SYCL_F16: no\r\nfound 1 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|                Intel Graphics [0x64a0]|    1.6|     64|    1024|   32| 15064M|            1.3.31294|\r\n| llama 7B Q4_0                  |   3.83 GiB |     7.25 B | SYCL       |  99 |         pp512 |    654.87 ± 0.96 |\r\n| llama 7B Q4_0                  |   3.83 GiB |     7.25 B | SYCL       |  99 |         tg128 |     23.16 ± 0.09 |\r\n\r\nbuild: 1d5f8dd (1)\r\n```\r\n\r\nHowever, it looks like k-quants, like Q4_K_M are broken:\r\n```\r\n$ ZES_ENABLE_SYSMAN=1 ./llama-bench -m ~/ai/models/gguf/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf -v\r\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nggml_sycl_init: SYCL_USE_XMX: yes\r\nggml_sycl_init: found 1 SYCL devices:\r\n| model                          |       size |     params | backend    | ngl |          test |              t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | ------------: | ---------------: |\r\nllama_model_loader: loaded meta data with 29 key-value pairs and 291 tensors from /home/lhl/ai/models/gguf/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3\r\nllama_model_loader: - kv   2:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   3:                       llama.context_length u32              = 32768\r\nllama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 15\r\nllama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768\r\nllama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default\r\nllama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\r\nllama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\r\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\r\nllama_model_loader: - kv  24:               general.quantization_version u32              = 2\r\nllama_model_loader: - kv  25:                      quantize.imatrix.file str              = /models/Mistral-7B-Instruct-v0.3-GGUF...\r\nllama_model_loader: - kv  26:                   quantize.imatrix.dataset str              = /training_data/calibration_data.txt\r\nllama_model_loader: - kv  27:             quantize.imatrix.entries_count i32              = 224\r\nllama_model_loader: - kv  28:              quantize.imatrix.chunks_count i32              = 228\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q4_K:  193 tensors\r\nllama_model_loader: - type q6_K:   33 tensors\r\nllm_load_vocab: special tokens cache size = 771\r\nllm_load_vocab: token to piece cache size = 0.1731 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32768\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 32768\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 32768\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q4_K - Medium\r\nllm_load_print_meta: model params     = 7.25 B\r\nllm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \r\nllm_load_print_meta: general.name     = Mistral-7B-Instruct-v0.3\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 781 '<0x0A>'\r\nllm_load_print_meta: max token length = 48\r\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\r\nllm_load_tensors: ggml ctx size =    0.27 MiB\r\nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 33/33 layers to GPU\r\nllm_load_tensors:      SYCL0 buffer size =  4097.52 MiB\r\nllm_load_tensors:        CPU buffer size =    72.00 MiB\r\n................................................................................................\r\nllama_new_context_with_model: n_ctx      = 512\r\nllama_new_context_with_model: n_batch    = 512\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 1000000.0\r\nllama_new_context_with_model: freq_scale = 1\r\n[SYCL] call ggml_check_sycl\r\nggml_check_sycl: GGML_SYCL_DEBUG: 0\r\nggml_check_sycl: GGML_SYCL_F16: no\r\nfound 1 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|                Intel Graphics [0x64a0]|    1.6|     64|    1024|   32| 15064M|            1.3.31294|\r\nllama_kv_cache_init:      SYCL0 KV buffer size =    64.00 MiB\r\nllama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\r\nllama_new_context_with_model:  SYCL_Host  output buffer size =     0.12 MiB\r\nllama_new_context_with_model:      SYCL0 compute buffer size =    81.00 MiB\r\nllama_new_context_with_model:  SYCL_Host compute buffer size =     9.01 MiB\r\nllama_new_context_with_model: graph nodes  = 902\r\nllama_new_context_with_model: graph splits = 2\r\nSub-group size 8 is not supported on the device\r\nException caught at file:/home/runner/_work/llm.cpp/llm.cpp/llama-cpp-bigdl/ggml/src/ggml-sycl.cpp, line:3164, func:operator()\r\nSYCL error: CHECK_TRY_ERROR(op(ctx, src0, src1, dst, src0_dd_i, src1_ddf_i, src1_ddq_i, dst_dd_i, dev[i].row_low, dev[i].row_high, src1_ncols, src1_padded_col_size, stream)): Meet error in this line code!\r\n  in function ggml_sycl_op_mul_mat at /home/runner/_work/llm.cpp/llm.cpp/llama-cpp-bigdl/ggml/src/ggml-sycl.cpp:3164\r\n/home/runner/_work/llm.cpp/llm.cpp/llama-cpp-bigdl/ggml/src/ggml-sycl/common.hpp:103: SYCL error\r\nlibggml.so(+0x79517) [0x7fdba1279517]\r\nlibggml.so(ggml_abort+0xd8) [0x7fdba12794a8]\r\nlibggml.so(+0x1f2e98) [0x7fdba13f2e98]\r\nlibggml.so(+0x229198) [0x7fdba1429198]\r\nlibggml.so(_Z25ggml_sycl_compute_forwardR25ggml_backend_sycl_contextP11ggml_tensor+0x5ef) [0x7fdba13f57ef]\r\nlibggml.so(+0x24033f) [0x7fdba144033f]\r\nlibggml.so(ggml_backend_sched_graph_compute_async+0x548) [0x7fdba12e60f8]\r\nlibllama.so(llama_decode+0xbc7) [0x7fdba2a73d47]\r\n./llama-bench() [0x41a580]\r\n./llama-bench() [0x416ccd]\r\n/usr/lib/libc.so.6(+0x261ce) [0x7fdba28281ce]\r\n/usr/lib/libc.so.6(__libc_start_main+0x8a) [0x7fdba282828a]\r\n./llama-bench() [0x41565e]\r\nAborted (core dumped)\r\n```\r\n\r\nNote, the SYCL backend of upstream llama.cpp is much slower, but works with both 2025.0.0 and 2024.2.1 on Q4_K_Ms:\r\n```\r\n$ ZES_ENABLE_SYSMAN=1 build/bin/llama-bench -m ~/ai/models/gguf/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf\r\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nggml_sycl_init: SYCL_USE_XMX: yes\r\nggml_sycl_init: found 1 SYCL devices:\r\n| model                          |       size |     params | backend    | ngl |          test |                  t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | ------------: | -------------------: |\r\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\r\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\r\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\r\n[SYCL] call ggml_check_sycl\r\nggml_check_sycl: GGML_SYCL_DEBUG: 0\r\nggml_check_sycl: GGML_SYCL_F16: yes\r\nfound 1 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|                Intel Graphics [0x64a0]|    1.6|     64|    1024|   32| 15064M|            1.3.31294|\r\n| llama 7B Q4_K - Medium         |   4.07 GiB |     7.25 B | SYCL       |  99 |         pp512 |        433.39 ± 0.80 |\r\n| llama 7B Q4_K - Medium         |   4.07 GiB |     7.25 B | SYCL       |  99 |         tg128 |         12.62 ± 0.06 |\r\n\r\nbuild: 9830b692 (4017)\r\n```",
      "state": "closed",
      "author": "lhl",
      "author_type": "User",
      "created_at": "2024-11-03T11:15:56Z",
      "updated_at": "2024-11-23T20:50:50Z",
      "closed_at": "2024-11-06T05:49:37Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12318/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "rnwang04"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12318",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12318",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:57.361796",
      "comments": [
        {
          "author": "rnwang04",
          "body": "Hi @lhl , could you please let me know  what's your current ipex-llm[cpp] version ?\r\nI have raised a PR to further fix  this issue, you may try it again with `pip install ipex-llm[cpp]>=2.2.0b20241104` tomorrow. 😊",
          "created_at": "2024-11-04T01:51:36Z"
        },
        {
          "author": "lhl",
          "body": "I installed ipex-llm a couple days ago exactly following these docs: https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/llama_cpp_quickstart.md\r\n\r\nBTW, this doesn't work as expected:\r\n\r\n```\r\npython -c 'import ipex_llm; print(ipex_llm.__version__);'  \r\nTraceback (most recent",
          "created_at": "2024-11-05T12:57:22Z"
        },
        {
          "author": "rnwang04",
          "body": "Hi @lhl , please upgrade your `ipex-llm[cpp]` to latest version (2.2.0b20241105) by `pip install --pre --upgrade ipex-llm[cpp]` first. 😊",
          "created_at": "2024-11-06T01:40:18Z"
        },
        {
          "author": "lhl",
          "body": "OK, great, confirmed that the updated version works now, thanks! 🥳\r\n\r\nI've updated the writeup I recently did and added Q4_K_M performance numbers, btw if you're interested: https://www.reddit.com/r/LocalLLaMA/comments/1gheslj/testing_llamacpp_with_intels_xe2_igpu_core_ultra/\r\n\r\nI found the IPEX-LLM",
          "created_at": "2024-11-06T05:49:37Z"
        },
        {
          "author": "mauro80",
          "body": "I can confirm that I see a warning `get_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory`, when running `llama-bench` on Q4_K_M models.\r\n\r\nI am using the latest `ipex-llm 2.2.0b20241123`, _on windows_. Performa",
          "created_at": "2024-11-23T20:50:48Z"
        }
      ]
    },
    {
      "issue_number": 12380,
      "title": "Vulnerability issue CVE-2024-31583 and CVE-2024-31580 on torch<2.2.0",
      "body": "There are two CVE vulnerabilities [CVE-2024-31583](https://avd.aquasec.com/nvd/cve-2024-31583) and [CVE-2024-31580](https://avd.aquasec.com/nvd/cve-2024-31580) for pytorch under version v2.2.0. Install ipex-llm[xpu] using command:\r\n`pip install --pre --upgrade ipex-llm[xpu] --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/cn/`\r\n\r\nThen these versions would be auto-installed:\r\n```\r\ntorch: 2.1.0a0+cxx11.abi \r\ntorchvision: 0.16.0a0+cxx11.abi \r\nintel-extension-for-pytorch: 2.1.10+xpu \r\n```\r\nIf I manually upgrade torch and intel_extension_for_pytorch to:\r\n`torch==2.3.1+cxx11.abi, intel_extension_for_pytorch==2.3.110+xpu`, it would raise an issue:\r\n```\r\nImportError: /home/lvm/.python311-env/.official-ipex-llm-xpu/lib/python3.11/site-packages/xe_linear.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3xpu21get_queue_from_streamEN3c106StreamE\r\n```\r\n\r\nThe [latest llava example](https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/GPU/PyTorch-Models/Model/llava) can be used to reproduce the problem.\r\n\r\n\r\nEnv check output log is attached:\r\n[log.txt](https://github.com/user-attachments/files/17700173/log.txt)\r\n\r\nCan anyone help on this? I would like to upgrade torch>=2.2.0, thanks!\r\n",
      "state": "open",
      "author": "Johere",
      "author_type": "User",
      "created_at": "2024-11-11T09:30:17Z",
      "updated_at": "2024-11-22T01:33:07Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12380/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Oscilloscope98"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12380",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12380",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:57.542016",
      "comments": [
        {
          "author": "Oscilloscope98",
          "body": "Hi @Johere, would you mind providing us some information regarding your OS and GPU device? :)",
          "created_at": "2024-11-12T02:39:16Z"
        },
        {
          "author": "Johere",
          "body": "> Hi @Johere, would you mind providing us some information regarding your OS and GPU device? :)\r\n\r\nHi @Oscilloscope98 , please see the information below:\r\n\r\nOS: Ubuntu 24.04 (also validated on Ubuntu 22.04, same problem)\r\nGPU device:\r\n```\r\nDevice Name: Intel(R) Arc(TM) A770 Graphics\r\nVendor Name: In",
          "created_at": "2024-11-12T03:13:38Z"
        },
        {
          "author": "Johere",
          "body": "> Hi @Johere, would you mind providing us some information regarding your OS and GPU device? :)\r\n\r\nHi @Oscilloscope98 May I know any updates on this issue? Thanks!",
          "created_at": "2024-11-20T07:18:42Z"
        },
        {
          "author": "Oscilloscope98",
          "body": "Hi @Johere,\r\n\r\nSorry for the late reply. We are working on this issue, and will update here for any updates :)",
          "created_at": "2024-11-22T01:33:06Z"
        }
      ]
    },
    {
      "issue_number": 12412,
      "title": "'AutoModel' object has no attribute 'config' when using Speech_Paraformer-Large on NPU",
      "body": "Traceback (most recent call last):\r\n  File \"D:\\projects\\ipex-llm\\python\\llm\\example\\NPU\\HF-Transformers-AutoModels\\Multimodal\\speech_paraformer-large.py\", line 44, in <module>\r\n    model = AutoModel(\r\n  File \"D:\\miniforge3\\envs\\llm_para\\lib\\site-packages\\ipex_llm\\transformers\\npu_model.py\", line 616, in __init__\r\n    self.model = self.from_pretrained(*args, **kwargs)\r\n  File \"D:\\miniforge3\\envs\\llm_para\\lib\\unittest\\mock.py\", line 1379, in patched\r\n    return func(*newargs, **newkeywargs)\r\n  File \"D:\\miniforge3\\envs\\llm_para\\lib\\site-packages\\ipex_llm\\transformers\\npu_model.py\", line 170, in from_pretrained\r\n    model.config.update({\"optimize_model\": optimize_model})\r\nAttributeError: 'AutoModel' object has no attribute 'config'",
      "state": "open",
      "author": "fanyhchn",
      "author_type": "User",
      "created_at": "2024-11-16T01:54:29Z",
      "updated_at": "2024-11-20T02:25:37Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12412/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12412",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12412",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:57.770890",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @fanyhchn , we have identified your issue and are currently working on a fix. You may install the latest version of `ipex-llm[npu]` tomorrow to get the update.",
          "created_at": "2024-11-19T02:05:57Z"
        }
      ]
    },
    {
      "issue_number": 12411,
      "title": "Update Ollama with IPEX-LLM to a newer version",
      "body": "Hello.\r\n\r\nIt seems that the latest Ollama with IPEX-LLM version `(0.3.6)` is a little old nowadays.\r\n\r\nIt doesn't have proper support for new and popular models like:\r\n\r\n1) `Phi 3.5`\r\n2) `Qwen 2.5`\r\n3) `Llama 3.2`\r\n4) `Llama 3.2-vision`\r\n\r\nI tried the first two (`Phi 3.5` and `Qwen 2.5`) with Intel's Ollama (IPEX-LLM) and it seems that they produce strange results.\r\nEspecially `Phi 3.5 `produces gibberish output.\r\n\r\nAlso, newer versions have fixed bugs, support new very useful commands and are faster (CPU optimizations) e.g\r\n\r\nBug fix:\r\nFixed issue where setting OLLAMA_NUM_PARALLEL would cause models to be reloaded on lower VRAM systems\r\n\r\nNew (very useful) command:\r\nNew `ollama stop` command to unload a running model.\r\n\r\nthank you\r\n",
      "state": "open",
      "author": "NikosDi",
      "author_type": "User",
      "created_at": "2024-11-15T09:07:54Z",
      "updated_at": "2024-11-19T02:05:15Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12411/reactions",
        "total_count": 3,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 3
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12411",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12411",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:57.979053",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "We are planning for a new rebase, related issue: https://github.com/intel-analytics/ipex-llm/issues/12370\r\n",
          "created_at": "2024-11-19T02:05:14Z"
        }
      ]
    },
    {
      "issue_number": 12391,
      "title": "Llama-3.2 11B Vision not working with latest IPEX-LLM (vLLM version 0.6.2)",
      "body": "Hello!\r\n\r\nI see that vLLM got updated in the latest version of IPEX-LLM and so decided to try using it with Llama-3.2-11B-Vision, however I seem to get errors each time:\r\n\r\n`2024-11-13 14:41:58,608 ERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::WrapperWithLoadBit.execute_method() (pid=2977, ip=192.168.86.58, actor_id=8c3afd038f22587b3ffad84501000000, repr=<ipex_llm.vllm.xpu.ipex_llm_wrapper.WrapperWithLoadBit object at 0x78242c31b290>)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/worker_base.py\", line 465, in execute_method\r\n    raise e\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/worker_base.py\", line 456, in execute_method\r\n    return executor(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/xpu_worker.py\", line 128, in determine_num_available_blocks\r\n    self.model_runner.profile_run()\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/xpu_model_runner.py\", line 538, in profile_run\r\n    self.execute_model(model_input, kv_caches, intermediate_tensors)\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/xpu_model_runner.py\", line 643, in execute_model\r\n    hidden_or_intermediate_states = model_executable(\r\n                                    ^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/mllama.py\", line 1075, in forward\r\n    attn_metadata.encoder_seq_lens_tensor != 0).reshape(-1, 1).to(\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nAttributeError: 'IpexAttnMetadata' object has no attribute 'encoder_seq_lens_tensor'`\r\n\r\nObviously, I don't expect that this gets patched immediately, just looking to see if I am doing something wrong. I fully expect that a new version of IPEX-LLM is coming soon with full support for Llama vision models.\r\n\r\nThanks!",
      "state": "open",
      "author": "HumerousGorgon",
      "author_type": "User",
      "created_at": "2024-11-13T06:49:17Z",
      "updated_at": "2024-11-14T08:30:50Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12391/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hzjane"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12391",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12391",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:31:59.928687",
      "comments": [
        {
          "author": "hzjane",
          "body": "Vllm 0.6.2 still lacks the XPU implementation of [enc_dec_model_runner](https://github.com/analytics-zoo/vllm/blob/0.6.2/vllm/worker/enc_dec_model_runner.py#L74\r\n) and [cross-attention operator](https://github.com/analytics-zoo/vllm/blob/0.6.2/vllm/attention/backends/ipex_attn.py#L261), so the `Llam",
          "created_at": "2024-11-14T07:59:24Z"
        },
        {
          "author": "HumerousGorgon",
          "body": "Thought it might be something like this; thank you for your response.\r\n\r\nI did note that some conversation was happening about a 'next release' of IPEX-LLM, sometime in November. Do you have any more information on this?",
          "created_at": "2024-11-14T08:06:25Z"
        },
        {
          "author": "hzjane",
          "body": "We have just released vllm 0.6.2 two days ago. And for the `Llama-3.2 11B Vision` model, we need to rely on the support of XPU on the main branch of vllm. Currently, the main branch does not implement [enc_dec_model_runner](https://github.com/analytics-zoo/vllm/blob/0.6.2/vllm/worker/enc_dec_model_r",
          "created_at": "2024-11-14T08:30:49Z"
        }
      ]
    },
    {
      "issue_number": 12385,
      "title": "assert error use ipex pytorch",
      "body": "## llama model\r\n\r\nconvert to low bit model\r\n```\r\nmodel = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True, trust_remote_code=True,optimize_model=True,low_cpu_mem_usage=True, use_cache=True).eval()\r\nmodel_path = \"C:/Users/Public/yhl/llama-low\"\r\nlow_bit = \"4bit\"\r\nmodel.save_low_bit(model_path+'-'+low_bit)\r\ntokenizer.save_pretrained(model_path+'-'+low_bit)\r\n```\r\n\r\n## load & run\r\n\r\n```\r\n# load\r\nmodel = AutoModelForCausalLM.load_low_bit(\"C:/Users/Public/yhl/llama-low/\").eval()\r\n# run\r\ngeneration_config = GenerationConfig(\r\n                max_new_tokens=128, \r\n                do_sample=True, \r\n                temperature=0.6, \r\n                top_p = 0.9,eos_token_id=[59246,59253,59255])   \r\nwith torch.inference_mode():\r\n   output = model.generate(inputs,\r\n                           do_sample=False,\r\n                        #    max_new_token=128,\r\n                           generation_config=generation_config) # warm-up\r\n```\r\n\r\n## Error\r\n\r\n```\r\nAssertion failed: inv_freq.scalar_type() == query.scalar_type() && inv_freq.scalar_type() == key.scalar_type(), file rope.cpp, line 100\r\n```\r\n\r\nI try to use \r\n\r\n```\r\nmodel = AutoModelForCausalLM.load_low_bit(\"C:/Users/Public/yhl/llama-low/\").eval()\r\nmodel = model.half()\r\nmodel = model.to(device)\r\n```\r\n\r\nIt work great! why ?",
      "state": "closed",
      "author": "piDack",
      "author_type": "User",
      "created_at": "2024-11-12T06:44:22Z",
      "updated_at": "2024-11-14T05:37:13Z",
      "closed_at": "2024-11-14T05:37:13Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12385/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12385",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12385",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:00.127652",
      "comments": [
        {
          "author": "leonardozcm",
          "body": "Sorry that I can not reproduce this issue on ARC\r\n```\r\n-------------------- Prompt --------------------\r\n<|begin_of_text|><|start_header_id|>user<|end_header_id|>\r\n\r\nWhat is AI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\r\n\r\n\r\n-------------------- Output (skip_special_tokens=False) ------",
          "created_at": "2024-11-13T11:43:02Z"
        },
        {
          "author": "piDack",
          "body": "Hi there,\r\nI apologize for the confusion caused by the incorrect code snippet in my previous message. The correct code to reproduce the issue should be:\r\n```\r\nmodel = AutoModelForCausalLM.load_low_bit(model_path, optimize_model=True, trust_remote_code=True, use_cache=True, torch_dtype=torch.float16)",
          "created_at": "2024-11-14T02:05:47Z"
        },
        {
          "author": "leonardozcm",
          "body": "hi, this is because when you pass `torch_dtype=torch.float16` this doesn't actually change `inv_freq` dtype as a buffer(hardcode by transformers llama architecture sorry), and when you run _model.half()_, this will scan all float point tensor and buffer and convert them into fp16, and this is workin",
          "created_at": "2024-11-14T03:19:16Z"
        },
        {
          "author": "piDack",
          "body": "Ok,thx",
          "created_at": "2024-11-14T05:37:13Z"
        }
      ]
    },
    {
      "issue_number": 12288,
      "title": "llava-hf/llava-1.5-7b-hf: error when multi-turn chat with multi-images",
      "body": "```\r\nfrom ipex_llm import optimize_model\r\nfrom transformers import LlavaForConditionalGeneration\r\nmodel = LlavaForConditionalGeneration.from_pretrained('llava-hf/llava-1.5-7b-hf', device_map=\"cpu\")\r\nmodel = optimize_model(model, low_bit='sym_int4')\r\nmodel = model.eval().to('xpu')\r\n```\r\n\r\nMulti-turn chat is like:\r\n- 1st-round: \r\nhttp://farm6.staticflickr.com/5268/5602445367_3504763978_z.jpg\r\nWhat is this?\r\n\r\n- 2nd-round:\r\nhttp://farm5.staticflickr.com/4031/4440753665_631134eaa4_z.jpg\r\nWhat are the differences between these two images?\r\n\r\nError logs:\r\n```\r\nTraceback (most recent call last): \r\n  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner \r\n    self.run() \r\n  File \"/usr/lib/python3.10/threading.py\", line 953, in run \r\n    self._target(*self._args, **self._kwargs) \r\n  File \"/home/ipex-llm-serving/dependency/model_worker.py\", line 85, in model_generate \r\n    raise NotImplementedError(f\"Unsupported model: {self.model_name}, error: {error}\") \r\nNotImplementedError: Unsupported model: llava-hf/llava-1.5-7b-hf, error: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead. \r\n```\r\n\r\nThe error is located as:\r\n/usr/lib/python3.10/site-packages/ipex_llm/transformers/low_bit_linear.py :729\r\n`x_2d = x.view(-1, x_shape[-1]) `\r\n\r\nIf I modify as: `x_2d = x.contiguous().view(-1, x_shape[-1])`, everything will be OK.\r\nI think the issue is related to LLaVA model's vision_feature_select_strategy (`vision_feature_select_strategy=default`) which may make the tensor discontiguous.\r\n\r\nCan anyone help on this issue? Thanks!\r\n\r\n**Python packages:**\r\nipex-llm                      2.2.0b20241011\r\ntransformers                  4.45.2",
      "state": "open",
      "author": "Johere",
      "author_type": "User",
      "created_at": "2024-10-29T06:12:42Z",
      "updated_at": "2024-11-14T02:28:33Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12288/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "JinheTang"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12288",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12288",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:00.314967",
      "comments": [
        {
          "author": "Oscilloscope98",
          "body": "Hi @Johere, we are reproducing this issue. We will update here for any progress :)",
          "created_at": "2024-10-30T02:06:21Z"
        },
        {
          "author": "JinheTang",
          "body": "Hi @Johere , we have updated our llava example for `llava-hf/llava-1.5-7b-hf`. Please follow the instructions in the [latest llava example](https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/GPU/PyTorch-Models/Model/llava) to see if it works. \r\n\r\nIf the issue continues, could y",
          "created_at": "2024-11-01T09:21:52Z"
        },
        {
          "author": "Johere",
          "body": "> Hi @Johere , we have updated our llava example for `llava-hf/llava-1.5-7b-hf`. Please follow the instructions in the [latest llava example](https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/GPU/PyTorch-Models/Model/llava) to see if it works.\r\n> \r\n> If the issue continues, co",
          "created_at": "2024-11-05T06:24:28Z"
        },
        {
          "author": "JinheTang",
          "body": "Hi @Johere , thanks for the script, we will try to reproduce it.",
          "created_at": "2024-11-07T02:01:19Z"
        },
        {
          "author": "JinheTang",
          "body": "Hi @Johere , we have reproduced the issue. If there's any update we will let you know.",
          "created_at": "2024-11-08T01:54:57Z"
        }
      ]
    },
    {
      "issue_number": 12356,
      "title": "Could not use SFT Trainer in qlora_finetuning.py",
      "body": "I have installed trl<0.12.0 to run qlora_finetune.py in the QLoRA/trl-example but it requires transformers 4.46.2 which causes the error below.\r\n<img width=\"327\" alt=\"incorrect transformer versio\" src=\"https://github.com/user-attachments/assets/5d4ee4eb-5feb-4f3a-8ee7-eefa98f24947\">\r\n\r\nSo I downgraded trl from 0.11.4 to 0.9.6 and I got another padding error.\r\n<img width=\"605\" alt=\"padding issue\" src=\"https://github.com/user-attachments/assets/7d24f21a-4f38-4416-b92b-933a54908cfa\">\r\n\r\n",
      "state": "open",
      "author": "shungyantham",
      "author_type": "User",
      "created_at": "2024-11-07T05:31:27Z",
      "updated_at": "2024-11-14T02:24:15Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 11,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12356/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12356",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12356",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:00.514505",
      "comments": [
        {
          "author": "qiyuangong",
          "body": "> I have installed trl<0.12.0 to run qlora_finetune.py in the QLoRA/trl-example but it requires transformers 4.46.2 which causes the error below. <img alt=\"incorrect transformer versio\" width=\"327\" src=\"https://private-user-images.githubusercontent.com/165883126/383823306-5d4ee4eb-5feb-4f3a-8ee7-eef",
          "created_at": "2024-11-08T02:21:25Z"
        },
        {
          "author": "shungyantham",
          "body": "Hi, I have also downgraded the transformers to 4.36.0 when I downgrade the trl to 0.9.6 and I got this error ",
          "created_at": "2024-11-08T02:46:19Z"
        },
        {
          "author": "shungyantham",
          "body": "https://github.com/intel-analytics/ipex-llm/blob/main/docker/llm/finetune/xpu/Dockerfile\r\n\r\nI build this Dockerfile and then manually pip install trl==0.9.6 in the docker container. I ran the qlora_finetune.py in LLM_Finetuning/QLoRA/trl-example. Is there anything I missed?",
          "created_at": "2024-11-08T03:33:25Z"
        },
        {
          "author": "qiyuangong",
          "body": "> https://github.com/intel-analytics/ipex-llm/blob/main/docker/llm/finetune/xpu/Dockerfile\r\n> \r\n> I build this Dockerfile and then manually pip install trl==0.9.6 in the docker container. I ran the qlora_finetune.py in LLM_Finetuning/QLoRA/trl-example. Is there anything I missed?\r\n\r\nHi @shungyantham",
          "created_at": "2024-11-08T07:28:52Z"
        },
        {
          "author": "qiyuangong",
          "body": "https://github.com/intel-analytics/ipex-llm/pull/12368",
          "created_at": "2024-11-08T07:38:05Z"
        }
      ]
    },
    {
      "issue_number": 12257,
      "title": "ollama run minicpm-v, runs on cpu",
      "body": "使用ollama运行minicpm-v模型，调用过程中发现，单独调用llm文字部分，正常运行到igpu。\r\n但是同时使用图片和文字，会出先LLM运行到CPU上。\r\n\r\nollama run minicpm-v:latest\r\n\r\n\r\nTest prompt\r\n{\r\n \"model\": \"minicpm-v:latest\",\r\n \"prompt\": \"图片讲了什么内容?\",\r\n \"images\":[\"C:\\Users\\MTL\\Pictures\\test.jpg\"]}",
      "state": "open",
      "author": "juan-OY",
      "author_type": "User",
      "created_at": "2024-10-23T10:32:48Z",
      "updated_at": "2024-11-14T01:46:25Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12257/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "rnwang04"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12257",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12257",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:00.718557",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @juan-OY , we have reproduced your issue and we are looking for a solution currently.",
          "created_at": "2024-10-24T10:14:47Z"
        },
        {
          "author": "juan-OY",
          "body": "any update?",
          "created_at": "2024-10-31T06:51:40Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @juan-OY , we are currently looking for a solution to fix this issue ASAP. ",
          "created_at": "2024-11-01T01:47:20Z"
        },
        {
          "author": "rnwang04",
          "body": "Hi @juan-OY , from `ipex-llm[cpp]>=2.2.0b20241113`, vision part is moved to GPU. You could try ollama again with `pip install --pre --upgrade ipex-llm[cpp].`",
          "created_at": "2024-11-14T01:46:25Z"
        }
      ]
    },
    {
      "issue_number": 12379,
      "title": "Docker - llama.cpp scripts / init-llama-cpp",
      "body": "Are docker/llm/inference-cpp/start-llama-cpp.sh and docker/llm/inference-cpp/benchmark_llama-cpp.sh up to date ?\r\n\r\nI see this scripts using ./main, the current version of ipex llama.cpp are llama-cli and llama-bench if i'm not mistaken.\r\n\r\nCould init-llama-cpp be launch when starting the file docker container to enable the usage of entrypoint ?",
      "state": "closed",
      "author": "easyfab",
      "author_type": "User",
      "created_at": "2024-11-11T08:34:47Z",
      "updated_at": "2024-11-13T17:53:28Z",
      "closed_at": "2024-11-13T17:53:28Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12379/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hzjane"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12379",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12379",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:00.932063",
      "comments": [
        {
          "author": "glorysdj",
          "body": "fixed by https://github.com/intel-analytics/ipex-llm/pull/12387",
          "created_at": "2024-11-13T00:25:56Z"
        },
        {
          "author": "easyfab",
          "body": "Thank you",
          "created_at": "2024-11-13T17:53:28Z"
        }
      ]
    },
    {
      "issue_number": 12193,
      "title": "ollama generate incorrect answer when it run glm-4-9b-chat model ",
      "body": "When ollama create `glm-4-9b-chat` model and inference, it always gives a **random and incorrect response from second round**, like below:\r\n\r\n\r\n<details>\r\n<summary>The complete log of curl command is folded at here</summary>\r\n```\r\n# curl http://localhost:11434/api/chat -d '{\r\n  \"model\": \"glm4\",\r\n  \"messages\": [\r\n    {\r\n      \"role\": \"user\",\r\n      \"content\": \"介绍一下上海的著名景点\"\r\n    }\r\n  ]\r\n}'\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:34.047484624Z\",\"message\":{\"role\":\"assistant\",\"content\":\"\\\"\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:34.083040067Z\",\"message\":{\"role\":\"assistant\",\"content\":\"东方\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:34.117628086Z\",\"message\":{\"role\":\"assistant\",\"content\":\"明珠\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:34.152355197Z\",\"message\":{\"role\":\"assistant\",\"content\":\"塔\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:34.186764637Z\",\"message\":{\"role\":\"assistant\",\"content\":\"”：\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:34.221193975Z\",\"message\":{\"role\":\"assistant\",\"content\":\"作为\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:34.255737188Z\",\"message\":{\"role\":\"assistant\",\"content\":\"上海的\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:34.290304743Z\",\"message\":{\"role\":\"assistant\",\"content\":\"标志性\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:34.324637153Z\",\"message\":{\"role\":\"assistant\",\"content\":\"建筑\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:34.359387512Z\",\"message\":{\"role\":\"assistant\",\"content\":\"，\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:34.394246012Z\",\"message\":{\"role\":\"assistant\",\"content\":\"它\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:34.429042753Z\",\"message\":{\"role\":\"assistant\",\"content\":\"融合\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:34.463491151Z\",\"message\":{\"role\":\"assistant\",\"content\":\"了\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:34.498027441Z\",\"message\":{\"role\":\"assistant\",\"content\":\"现代\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:34.532440349Z\",\"message\":{\"role\":\"assistant\",\"content\":\"城市\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:34.567029853Z\",\"message\":{\"role\":\"assistant\",\"content\":\"美\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:34.601535756Z\",\"message\":{\"role\":\"assistant\",\"content\":\"学的\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:34.636126708Z\",\"message\":{\"role\":\"assistant\",\"content\":\"精华\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:34.670677665Z\",\"message\":{\"role\":\"assistant\",\"content\":\"。\\n\\n\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:34.705137513Z\",\"message\":{\"role\":\"assistant\",\"content\":\"\\\"\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:34.739545318Z\",\"message\":{\"role\":\"assistant\",\"content\":\"外\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:34.774045714Z\",\"message\":{\"role\":\"assistant\",\"content\":\"滩\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:34.808468877Z\",\"message\":{\"role\":\"assistant\",\"content\":\"”：\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:34.842924158Z\",\"message\":{\"role\":\"assistant\",\"content\":\"这里\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:34.877376785Z\",\"message\":{\"role\":\"assistant\",\"content\":\"被誉为\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:34.91187573Z\",\"message\":{\"role\":\"assistant\",\"content\":\"“\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:34.946337116Z\",\"message\":{\"role\":\"assistant\",\"content\":\"万\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:34.980840121Z\",\"message\":{\"role\":\"assistant\",\"content\":\"国\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:35.015301191Z\",\"message\":{\"role\":\"assistant\",\"content\":\"建筑\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:35.049693979Z\",\"message\":{\"role\":\"assistant\",\"content\":\"博览\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:35.084161034Z\",\"message\":{\"role\":\"assistant\",\"content\":\"群\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:35.118586049Z\",\"message\":{\"role\":\"assistant\",\"content\":\"”，\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:35.153068224Z\",\"message\":{\"role\":\"assistant\",\"content\":\"是\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:35.187589385Z\",\"message\":{\"role\":\"assistant\",\"content\":\"上海\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:35.222024104Z\",\"message\":{\"role\":\"assistant\",\"content\":\"近代\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:35.256522112Z\",\"message\":{\"role\":\"assistant\",\"content\":\"历史的\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:35.291032609Z\",\"message\":{\"role\":\"assistant\",\"content\":\"见证\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:35.325573882Z\",\"message\":{\"role\":\"assistant\",\"content\":\"。\\n\\n\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:35.360055075Z\",\"message\":{\"role\":\"assistant\",\"content\":\"\\\"\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:35.394575517Z\",\"message\":{\"role\":\"assistant\",\"content\":\"豫\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:35.429031499Z\",\"message\":{\"role\":\"assistant\",\"content\":\"园\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:35.463573883Z\",\"message\":{\"role\":\"assistant\",\"content\":\"及\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:35.498206243Z\",\"message\":{\"role\":\"assistant\",\"content\":\"商城\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:35.532779767Z\",\"message\":{\"role\":\"assistant\",\"content\":\"”：\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:35.567530373Z\",\"message\":{\"role\":\"assistant\",\"content\":\"豫\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:35.601984238Z\",\"message\":{\"role\":\"assistant\",\"content\":\"园\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:35.636503015Z\",\"message\":{\"role\":\"assistant\",\"content\":\"始建于\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:35.671091827Z\",\"message\":{\"role\":\"assistant\",\"content\":\"明代\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:35.705748153Z\",\"message\":{\"role\":\"assistant\",\"content\":\"，\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:35.74030126Z\",\"message\":{\"role\":\"assistant\",\"content\":\"是中国\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:35.77489374Z\",\"message\":{\"role\":\"assistant\",\"content\":\"南方\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:35.809531759Z\",\"message\":{\"role\":\"assistant\",\"content\":\"园林\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:35.844077845Z\",\"message\":{\"role\":\"assistant\",\"content\":\"建筑的\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:35.878674749Z\",\"message\":{\"role\":\"assistant\",\"content\":\"代表\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:35.913122158Z\",\"message\":{\"role\":\"assistant\",\"content\":\"。\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:35.947790869Z\",\"message\":{\"role\":\"assistant\",\"content\":\"豫\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:35.982317741Z\",\"message\":{\"role\":\"assistant\",\"content\":\"园\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:36.017024209Z\",\"message\":{\"role\":\"assistant\",\"content\":\"商城\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:36.051508233Z\",\"message\":{\"role\":\"assistant\",\"content\":\"则\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:36.08601788Z\",\"message\":{\"role\":\"assistant\",\"content\":\"是一个\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:36.12060447Z\",\"message\":{\"role\":\"assistant\",\"content\":\"集\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:36.155157966Z\",\"message\":{\"role\":\"assistant\",\"content\":\"购物\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:36.189693696Z\",\"message\":{\"role\":\"assistant\",\"content\":\"、\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:36.224233101Z\",\"message\":{\"role\":\"assistant\",\"content\":\"餐饮\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:36.258778113Z\",\"message\":{\"role\":\"assistant\",\"content\":\"、\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:36.293408116Z\",\"message\":{\"role\":\"assistant\",\"content\":\"娱乐\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:36.327981817Z\",\"message\":{\"role\":\"assistant\",\"content\":\"为一体的\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:36.362561936Z\",\"message\":{\"role\":\"assistant\",\"content\":\"商业\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:36.397029822Z\",\"message\":{\"role\":\"assistant\",\"content\":\"区\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:36.431596919Z\",\"message\":{\"role\":\"assistant\",\"content\":\"。\\n\\n\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:36.466154337Z\",\"message\":{\"role\":\"assistant\",\"content\":\"\\\"\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:36.500694428Z\",\"message\":{\"role\":\"assistant\",\"content\":\"田\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:36.535199046Z\",\"message\":{\"role\":\"assistant\",\"content\":\"子\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:36.569816106Z\",\"message\":{\"role\":\"assistant\",\"content\":\"坊\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:36.604400786Z\",\"message\":{\"role\":\"assistant\",\"content\":\"”：\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:36.639096194Z\",\"message\":{\"role\":\"assistant\",\"content\":\"它\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:36.673737633Z\",\"message\":{\"role\":\"assistant\",\"content\":\"是一个\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:36.708318353Z\",\"message\":{\"role\":\"assistant\",\"content\":\"充满\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:36.742884497Z\",\"message\":{\"role\":\"assistant\",\"content\":\"艺术\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:36.777447721Z\",\"message\":{\"role\":\"assistant\",\"content\":\"气息\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:36.812018152Z\",\"message\":{\"role\":\"assistant\",\"content\":\"的\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:36.846584797Z\",\"message\":{\"role\":\"assistant\",\"content\":\"创意\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:36.881128177Z\",\"message\":{\"role\":\"assistant\",\"content\":\"产业\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:36.915641048Z\",\"message\":{\"role\":\"assistant\",\"content\":\"聚集\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:36.950315734Z\",\"message\":{\"role\":\"assistant\",\"content\":\"区\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:36.984804477Z\",\"message\":{\"role\":\"assistant\",\"content\":\"。\\n\\n\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:37.019346375Z\",\"message\":{\"role\":\"assistant\",\"content\":\"这些\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:37.053957696Z\",\"message\":{\"role\":\"assistant\",\"content\":\"著名\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:37.08845107Z\",\"message\":{\"role\":\"assistant\",\"content\":\"景点\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:37.123007248Z\",\"message\":{\"role\":\"assistant\",\"content\":\"不仅\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:37.157571509Z\",\"message\":{\"role\":\"assistant\",\"content\":\"展示了\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:37.192129579Z\",\"message\":{\"role\":\"assistant\",\"content\":\"上海的\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:37.226633234Z\",\"message\":{\"role\":\"assistant\",\"content\":\"悠久\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:37.261219964Z\",\"message\":{\"role\":\"assistant\",\"content\":\"历史\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:37.295772792Z\",\"message\":{\"role\":\"assistant\",\"content\":\"和\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:37.330232997Z\",\"message\":{\"role\":\"assistant\",\"content\":\"独特的\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:37.364854404Z\",\"message\":{\"role\":\"assistant\",\"content\":\"文化\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:37.399489432Z\",\"message\":{\"role\":\"assistant\",\"content\":\"底蕴\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:37.434011916Z\",\"message\":{\"role\":\"assistant\",\"content\":\"，\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:37.468651895Z\",\"message\":{\"role\":\"assistant\",\"content\":\"同时也\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:37.503276169Z\",\"message\":{\"role\":\"assistant\",\"content\":\"吸引了\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:37.537887437Z\",\"message\":{\"role\":\"assistant\",\"content\":\"世界\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:37.572501397Z\",\"message\":{\"role\":\"assistant\",\"content\":\"各地的\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:37.607259788Z\",\"message\":{\"role\":\"assistant\",\"content\":\"游客\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:37.641892547Z\",\"message\":{\"role\":\"assistant\",\"content\":\"前来\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:37.676499929Z\",\"message\":{\"role\":\"assistant\",\"content\":\"观光\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:37.71112524Z\",\"message\":{\"role\":\"assistant\",\"content\":\"游览\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:37.745736113Z\",\"message\":{\"role\":\"assistant\",\"content\":\"。\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:37.780498755Z\",\"message\":{\"role\":\"assistant\",\"content\":\"\"},\"done_reason\":\"stop\",\"done\":true,\"total_duration\":9999226996,\"load_duration\":6054386923,\"prompt_eval_count\":10,\"prompt_eval_duration\":210537000,\"eval_count\":109,\"eval_duration\":3733003000}\r\n# curl http://localhost:11434/api/chat -d '{\r\n  \"model\": \"glm4\",\r\n  \"messages\": [\r\n    {\r\n      \"role\": \"user\",\r\n      \"content\": \"介绍一下你自己\"\r\n    }\r\n  ]\r\n}'\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:52.654030893Z\",\"message\":{\"role\":\"assistant\",\"content\":\"\\\"\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:52.694561966Z\",\"message\":{\"role\":\"assistant\",\"content\":\"如果\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:52.72306927Z\",\"message\":{\"role\":\"assistant\",\"content\":\"电磁\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:52.757496305Z\",\"message\":{\"role\":\"assistant\",\"content\":\"动态\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:52.792244873Z\",\"message\":{\"role\":\"assistant\",\"content\":\"和\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:52.826882222Z\",\"message\":{\"role\":\"assistant\",\"content\":\" \"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:52.861382188Z\",\"message\":{\"role\":\"assistant\",\"content\":\"申\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:52.895900925Z\",\"message\":{\"role\":\"assistant\",\"content\":\"玖\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:52.930406102Z\",\"message\":{\"role\":\"assistant\",\"content\":\" At\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:52.965083325Z\",\"message\":{\"role\":\"assistant\",\"content\":\"-\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:52.999605626Z\",\"message\":{\"role\":\"assistant\",\"content\":\" ►\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:53.034098069Z\",\"message\":{\"role\":\"assistant\",\"content\":\"ite\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:53.06854197Z\",\"message\":{\"role\":\"assistant\",\"content\":\"Bo\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:53.103194578Z\",\"message\":{\"role\":\"assistant\",\"content\":\"le\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:53.1378153Z\",\"message\":{\"role\":\"assistant\",\"content\":\" ever\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:53.172248342Z\",\"message\":{\"role\":\"assistant\",\"content\":\"\\u003c/\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:53.206666723Z\",\"message\":{\"role\":\"assistant\",\"content\":\"eu\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:53.241183897Z\",\"message\":{\"role\":\"assistant\",\"content\":\" \\n\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:53.275647604Z\",\"message\":{\"role\":\"assistant\",\"content\":\" Итал\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:53.310238486Z\",\"message\":{\"role\":\"assistant\",\"content\":\"’on\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:53.344762065Z\",\"message\":{\"role\":\"assistant\",\"content\":\"ibo\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:53.379315067Z\",\"message\":{\"role\":\"assistant\",\"content\":\" is\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:53.413705128Z\",\"message\":{\"role\":\"assistant\",\"content\":\"myp\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:53.448166387Z\",\"message\":{\"role\":\"assistant\",\"content\":\"ot\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:53.48250088Z\",\"message\":{\"role\":\"assistant\",\"content\":\".isDefined\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:53.517030324Z\",\"message\":{\"role\":\"assistant\",\"content\":\" \"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:53.551511697Z\",\"message\":{\"role\":\"assistant\",\"content\":\"6\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:53.585989644Z\",\"message\":{\"role\":\"assistant\",\"content\":\"25\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:53.620472775Z\",\"message\":{\"role\":\"assistant\",\"content\":\" a\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:53.654947054Z\",\"message\":{\"role\":\"assistant\",\"content\":\" E\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:53.68938128Z\",\"message\":{\"role\":\"assistant\",\"content\":\"_\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:53.723968144Z\",\"message\":{\"role\":\"assistant\",\"content\":\"...\\n\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:53.758586713Z\",\"message\":{\"role\":\"assistant\",\"content\":\"卢\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:53.792985133Z\",\"message\":{\"role\":\"assistant\",\"content\":\"煜\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:53.827508001Z\",\"message\":{\"role\":\"assistant\",\"content\":\" E\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:53.861950884Z\",\"message\":{\"role\":\"assistant\",\"content\":\" _\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:53.896446482Z\",\"message\":{\"role\":\"assistant\",\"content\":\"Y\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:53.93097066Z\",\"message\":{\"role\":\"assistant\",\"content\":\"K\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:53.965436419Z\",\"message\":{\"role\":\"assistant\",\"content\":\"字\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:54.000019662Z\",\"message\":{\"role\":\"assistant\",\"content\":\"001\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:54.034540788Z\",\"message\":{\"role\":\"assistant\",\"content\":\"草\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:54.069032218Z\",\"message\":{\"role\":\"assistant\",\"content\":\"cos\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:54.103580788Z\",\"message\":{\"role\":\"assistant\",\"content\":\"K\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:54.13808331Z\",\"message\":{\"role\":\"assistant\",\"content\":\"_s\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:54.172515329Z\",\"message\":{\"role\":\"assistant\",\"content\":\" mos\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:54.207046631Z\",\"message\":{\"role\":\"assistant\",\"content\":\"pi\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:54.241586674Z\",\"message\":{\"role\":\"assistant\",\"content\":\"qm\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:54.276187563Z\",\"message\":{\"role\":\"assistant\",\"content\":\" A\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:54.310726205Z\",\"message\":{\"role\":\"assistant\",\"content\":\":semicolon\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:54.345260325Z\",\"message\":{\"role\":\"assistant\",\"content\":\"_EDEFAULT\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:54.379758056Z\",\"message\":{\"role\":\"assistant\",\"content\":\"ully\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:54.414314672Z\",\"message\":{\"role\":\"assistant\",\"content\":\" I\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:54.448885458Z\",\"message\":{\"role\":\"assistant\",\"content\":\"atchet\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:54.483470421Z\",\"message\":{\"role\":\"assistant\",\"content\":\" （\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:54.518406906Z\",\"message\":{\"role\":\"assistant\",\"content\":\"大\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:54.553149503Z\",\"message\":{\"role\":\"assistant\",\"content\":\"_console\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:54.587833797Z\",\"message\":{\"role\":\"assistant\",\"content\":\"/\\n\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:54.622409599Z\",\"message\":{\"role\":\"assistant\",\"content\":\"COD\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:54.656863626Z\",\"message\":{\"role\":\"assistant\",\"content\":\" Ghost\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:54.691364169Z\",\"message\":{\"role\":\"assistant\",\"content\":\")\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:54.725771183Z\",\"message\":{\"role\":\"assistant\",\"content\":\"Highest\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:54.760400666Z\",\"message\":{\"role\":\"assistant\",\"content\":\" Check\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:54.794754518Z\",\"message\":{\"role\":\"assistant\",\"content\":\",\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:54.89839946Z\",\"message\":{\"role\":\"assistant\",\"content\":\" �.\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:54.933003312Z\",\"message\":{\"role\":\"assistant\",\"content\":\" 用户\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:54.967530848Z\",\"message\":{\"role\":\"assistant\",\"content\":\":\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:55.001969948Z\",\"message\":{\"role\":\"assistant\",\"content\":\"�真\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:55.036494328Z\",\"message\":{\"role\":\"assistant\",\"content\":\"个\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:55.071003108Z\",\"message\":{\"role\":\"assistant\",\"content\":\"�c\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:55.105533176Z\",\"message\":{\"role\":\"assistant\",\"content\":\"搜\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:55.140175282Z\",\"message\":{\"role\":\"assistant\",\"content\":\"行\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:55.174770388Z\",\"message\":{\"role\":\"assistant\",\"content\":\"\\u003e=\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:55.20928064Z\",\"message\":{\"role\":\"assistant\",\"content\":\"-\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:55.243859118Z\",\"message\":{\"role\":\"assistant\",\"content\":\"106\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:55.278338777Z\",\"message\":{\"role\":\"assistant\",\"content\":\"K\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:55.312866693Z\",\"message\":{\"role\":\"assistant\",\"content\":\"YO\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:55.347278353Z\",\"message\":{\"role\":\"assistant\",\"content\":\"OW\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:55.381757985Z\",\"message\":{\"role\":\"assistant\",\"content\":\"LL\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:55.416375562Z\",\"message\":{\"role\":\"assistant\",\"content\":\"100\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:55.450901017Z\",\"message\":{\"role\":\"assistant\",\"content\":\"_t\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:55.485449013Z\",\"message\":{\"role\":\"assistant\",\"content\":\".fb\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:55.519997854Z\",\"message\":{\"role\":\"assistant\",\"content\":\"Pay\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:55.554507412Z\",\"message\":{\"role\":\"assistant\",\"content\":\" audi\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:55.589001463Z\",\"message\":{\"role\":\"assistant\",\"content\":\"ocommerce\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:55.6235425Z\",\"message\":{\"role\":\"assistant\",\"content\":\"HEMA\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:55.658033266Z\",\"message\":{\"role\":\"assistant\",\"content\":\" списк\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:55.692470904Z\",\"message\":{\"role\":\"assistant\",\"content\":\"201\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:55.72705665Z\",\"message\":{\"role\":\"assistant\",\"content\":\"号\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:55.761593663Z\",\"message\":{\"role\":\"assistant\",\"content\":\"，\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:55.796110808Z\",\"message\":{\"role\":\"assistant\",\"content\":\" And\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:55.83070236Z\",\"message\":{\"role\":\"assistant\",\"content\":\"yalty\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:55.865214288Z\",\"message\":{\"role\":\"assistant\",\"content\":\"pbs\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:55.89982706Z\",\"message\":{\"role\":\"assistant\",\"content\":\"参\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:55.934387743Z\",\"message\":{\"role\":\"assistant\",\"content\":\"为\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:55.968842486Z\",\"message\":{\"role\":\"assistant\",\"content\":\"保护\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:56.003398197Z\",\"message\":{\"role\":\"assistant\",\"content\":\"�c\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:56.037931096Z\",\"message\":{\"role\":\"assistant\",\"content\":\"  \\n\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:56.072475939Z\",\"message\":{\"role\":\"assistant\",\"content\":\"Im\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:56.107027308Z\",\"message\":{\"role\":\"assistant\",\"content\":\"refer\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:56.141598037Z\",\"message\":{\"role\":\"assistant\",\"content\":\"-m\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:56.176067668Z\",\"message\":{\"role\":\"assistant\",\"content\":\"oi\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:56.210628073Z\",\"message\":{\"role\":\"assistant\",\"content\":\" 自\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:56.245100112Z\",\"message\":{\"role\":\"assistant\",\"content\":\"带的\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:56.279576845Z\",\"message\":{\"role\":\"assistant\",\"content\":\"[\"},\"done\":false}\r\n{\"model\":\"glm4\",\"created_at\":\"2024-10-14T14:02:56.314205225Z\",\"message\":{\"role\":\"assistant\",\"content\":\"\"},\"done_reason\":\"stop\",\"done\":true,\"total_duration\":4021640842,\"load_duration\":9962400,\"prompt_eval_count\":8,\"prompt_eval_duration\":309739000,\"eval_count\":107,\"eval_duration\":3660121000}\r\n```\r\n</details>\r\nThe modelfile is:\r\n\r\n```\r\nFROM /home/ollama/AI-models/LLM/ollama-model/glm-4-9b-chat.Q4_K_S.gguf\r\nTEMPLATE \"\"\"[gMASK]<sop>{{ if .System }}<|system|>\r\n{{ .System }}{{ end }}{{ if .Prompt }}<|user|>\r\n{{ .Prompt }}{{ end }}<|assistant|>\r\n{{ .Response }}\"\"\"\r\nPARAMETER stop \"<|system|>\"\r\nPARAMETER stop \"<|user|>\"\r\nPARAMETER stop \"<|assistant|>\"\r\n```\r\nThese are the `pip list` in my container:\r\n\r\n```shell\r\nPackage                       Version\r\n----------------------------- --------------\r\naccelerate                    0.33.0\r\nbigdl-core-cpp                2.6.0b20240919\r\nblinker                       1.4\r\ncertifi                       2024.8.30\r\ncharset-normalizer            3.4.0\r\ncryptography                  3.4.8\r\ndbus-python                   1.2.18\r\ndistro                        1.7.0\r\ndistro-info                   1.1+ubuntu0.2\r\neinops                        0.8.0\r\nfilelock                      3.16.1\r\nfsspec                        2024.9.0\r\ngguf                          0.10.0\r\nhttplib2                      0.20.2\r\nhuggingface-hub               0.25.2\r\nidna                          3.10\r\nimportlib-metadata            4.6.4\r\nipex-llm                      2.2.0b20240919\r\njeepney                       0.7.1\r\nJinja2                        3.1.4\r\nkeyring                       23.5.0\r\nlaunchpadlib                  1.10.16\r\nlazr.restfulclient            0.14.4\r\nlazr.uri                      1.0.6\r\nMarkupSafe                    3.0.1\r\nmore-itertools                8.10.0\r\nmpmath                        1.3.0\r\nnetworkx                      3.4.1\r\nnumpy                         1.26.4\r\nnvidia-cublas-cu12            12.1.3.1\r\nnvidia-cuda-cupti-cu12        12.1.105\r\nnvidia-cuda-nvrtc-cu12        12.1.105\r\nnvidia-cuda-runtime-cu12      12.1.105\r\nnvidia-cudnn-cu12             8.9.2.26\r\nnvidia-cufft-cu12             11.0.2.54\r\nnvidia-curand-cu12            10.3.2.106\r\nnvidia-cusolver-cu12          11.4.5.107\r\nnvidia-cusparse-cu12          12.1.0.106\r\nnvidia-nccl-cu12              2.19.3\r\nnvidia-nvjitlink-cu12         12.6.77\r\nnvidia-nvtx-cu12              12.1.105\r\noauthlib                      3.2.0\r\npackaging                     24.1\r\npip                           24.2\r\nprotobuf                      4.25.5\r\npsutil                        6.0.0\r\nPyGObject                     3.42.1\r\nPyJWT                         2.3.0\r\npyparsing                     2.4.7\r\npython-apt                    2.4.0+ubuntu4\r\nPyYAML                        6.0.2\r\nregex                         2024.9.11\r\nrequests                      2.32.3\r\nsafetensors                   0.4.5\r\nSecretStorage                 3.3.1\r\nsentencepiece                 0.1.99\r\nsetuptools                    59.6.0\r\nsix                           1.16.0\r\nssh-import-id                 5.11\r\nsympy                         1.13.3\r\ntiktoken                      0.8.0\r\ntokenizers                    0.15.2\r\ntorch                         2.2.0\r\ntqdm                          4.66.5\r\ntransformers                  4.36.2\r\ntransformers-stream-generator 0.0.5\r\ntriton                        2.2.0\r\ntyping_extensions             4.12.2\r\nunattended-upgrades           0.1\r\nurllib3                       2.2.3\r\nwadllib                       1.3.6\r\nwheel                         0.37.1\r\nzipp                          1.0.0\r\n```\r\n\r\nThe start script of ollama is:\r\n```\r\n#!/bin/bash\r\n\r\nllm_model_name=${LLM_MODEL_NAME}\r\nmodel=\"/home/ollama/AI-models/ollama-model/${llm_model_name}\"\r\nserved_model_name=${llm_model_name}\r\n\r\nexport PATH=$PATH:/opt/python-3.11.9/bin\r\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/python-3.11.9/lib/\r\nexport OLLAMA_NUM_GPU=999\r\nexport ZES_ENABLE_SYSMAN=1\r\nsource /opt/intel/oneapi/setvars.sh\r\nexport SYCL_CACHE_PERSISTENT=1\r\n# [optional] under most circumstances, the following environment variable may improve performance, but sometimes this may also cause performance degradation\r\nexport SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1\r\n# [optional] if you want to run on single GPU, use below command to limit GPU may improve performance\r\nexport ONEAPI_DEVICE_SELECTOR=level_zero:0\r\n\r\n# expose the service in 0.0.0.0\r\nexport OLLAMA_HOST=0.0.0.0\r\n\r\n# init ollama first\r\nif [ ! -d \"/home/${USERNAME}/WorkSpace/ollama\" ]; then\r\n  mkdir -p \"/home/${USERNAME}/WorkSpace/ollama\"\r\nfi\r\n\r\ncd /home/${USERNAME}/WorkSpace/ollama\r\ninit-ollama\r\n\r\nnohup ./ollama serve > ./ollama.log 2>&1 &\r\n\r\n# Wait a few seconds to ensure that the serve command has started\r\necho \"=====ollama serve starting=====\"\r\nsleep 5\r\necho \"=====ollama serve started!=====\"\r\necho \"=====ollama model creating=====\"\r\n# create the model\r\n./ollama create glm4 -f /home/ollama/AI-models/ollama-model/glm4_modelfile\r\necho \"=====ollama model created!=====\"\r\ntail -f ollama.log\r\n```\r\nRelated issues:\r\n- https://github.com/THUDM/GLM-4/issues/521\r\n- https://github.com/THUDM/GLM-4/issues/333\r\n- https://github.com/ollama/ollama/issues/5719\r\n\r\nIs it because of the wrong version of ollama installed to run glm4, or the format of modelfile is incorrect, which may leads to the wrong answer?",
      "state": "open",
      "author": "junruizh2021",
      "author_type": "User",
      "created_at": "2024-10-14T06:27:13Z",
      "updated_at": "2024-11-13T02:18:37Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12193/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12193",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12193",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:01.110785",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @junruizh2021 , I have tested `glm-4-9b-chat.Q4_K_S` on mtl device and it works well. Could you please provide more information from the Ollama Server side and details about your device? \r\nAlso, you may directly run the model with `ollama run glm4:9b-chat-q4_K_S` instead of using `Modelfile`.",
          "created_at": "2024-10-16T01:16:58Z"
        },
        {
          "author": "junruizh2021",
          "body": "@sgwhat Thanks for your reply. My platform detail info is:\r\n- CPU: 13th Gen Intel(R) Core(TM) i5-13600HRE\r\n- dGPU: Intel Arc A770 16G\r\n\r\nSorry, the image is deleted. I downgraded the ipex-llm[cpp] to 2.2.0b20240910 and it worked again. \r\n\r\nCan you paste your `pip list` at here? Maybe the `transforme",
          "created_at": "2024-10-16T06:16:59Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @junruizh2021 , we have reproduced your issue on Ubuntu Arc770 device, we are looking for a solution and will reply to you soon. You may use `2.2.0b20240910` version for now.",
          "created_at": "2024-10-17T01:53:03Z"
        },
        {
          "author": "junruizh2021",
          "body": "@sgwhat Thanks. Looking forward to your feedback.",
          "created_at": "2024-10-17T02:46:02Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @junruizh2021 , you may try our latest version of ipex-llm ollama to run glm4-9b with `pip install --pre --upgrade ipex-llm[cpp]`. We have fixed the output issue when doing multi-turn chat.",
          "created_at": "2024-11-13T02:17:46Z"
        }
      ]
    },
    {
      "issue_number": 12363,
      "title": "cant run ollama in docker container with iGPU in linux",
      "body": "here is the the container parameters : \r\n\r\nexport DOCKER_IMAGE=intelanalytics/ipex-llm-inference-cpp-xpu:latest\r\nexport CONTAINER_NAME=ipex-llm-inference-cpp-xpu-container\r\npodman run -itd \\\r\n                --net=host \\\r\n                --device=/dev/dri \\\r\n                -v /home/user/.ollama:/root/.ollama \\\r\n                -e no_proxy=localhost,127.0.0.1 \\\r\n                --memory=\"32G\" \\\r\n                --name=$CONTAINER_NAME \\\r\n                -e DEVICE=iGPU \\\r\n                --shm-size=\"16g\" \\\r\n                $DOCKER_IMAGE\r\n cd scripts\r\nbash start-ollama.sh\r\n\r\nsource ipex-llm-init --gpu --device $DEVICE\r\nfound oneapi in /opt/intel/oneapi/setvars.sh\r\n \r\n:: initializing oneAPI environment ...\r\n   bash: BASH_VERSION = 5.1.16(1)-release\r\n   args: Using \"$@\" for setvars.sh arguments: --force\r\n:: advisor -- latest\r\n:: ccl -- latest\r\n:: compiler -- latest\r\n:: dal -- latest\r\n:: debugger -- latest\r\n:: dev-utilities -- latest\r\n:: dnnl -- latest\r\n:: dpcpp-ct -- latest\r\n:: dpl -- latest\r\n:: ipp -- latest\r\n:: ippcp -- latest\r\n:: mkl -- latest\r\n:: mpi -- latest\r\n:: tbb -- latest\r\n:: vtune -- latest\r\n:: oneAPI environment initialized ::\r\n \r\n/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\r\n  _torch_pytree._register_pytree_node(\r\n/usr/local/lib/python3.11/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\r\n  warnings.warn(\r\n/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\r\n  _torch_pytree._register_pytree_node(\r\n/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\r\n  _torch_pytree._register_pytree_node(\r\nroot@lp:/llm/scripts# bash start-ollama.sh\r\nroot@lp:/llm/scripts# 2024/11/08 00:35:57 routes.go:1125: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]\"\r\ntime=2024-11-08T00:35:57.378+08:00 level=INFO source=images.go:753 msg=\"total blobs: 6\"\r\ntime=2024-11-08T00:35:57.378+08:00 level=INFO source=images.go:760 msg=\"total unused blobs removed: 0\"\r\ntime=2024-11-08T00:35:57.379+08:00 level=INFO source=routes.go:1172 msg=\"Listening on 127.0.0.1:11434 (version 0.3.6-ipexllm-20241106)\"\r\ntime=2024-11-08T00:35:57.380+08:00 level=INFO source=payload.go:30 msg=\"extracting embedded files\" dir=/tmp/ollama272927415/runners\r\ntime=2024-11-08T00:35:57.504+08:00 level=INFO source=payload.go:44 msg=\"Dynamic LLM libraries [cpu_avx2 cpu cpu_avx]\"\r\ntime=2024-11-08T00:36:09.351+08:00 level=INFO source=gpu.go:168 msg=\"looking for compatible GPUs\"\r\ntime=2024-11-08T00:36:09.351+08:00 level=WARN source=gpu.go:560 msg=\"unable to locate gpu dependency libraries\"\r\ntime=2024-11-08T00:36:09.351+08:00 level=WARN source=gpu.go:560 msg=\"unable to locate gpu dependency libraries\"\r\ntime=2024-11-08T00:36:09.357+08:00 level=WARN source=gpu.go:560 msg=\"unable to locate gpu dependency libraries\"\r\ntime=2024-11-08T00:36:09.360+08:00 level=INFO source=gpu.go:280 msg=\"no compatible GPUs were discovered\"\r\ntime=2024-11-08T00:36:09.378+08:00 level=INFO source=memory.go:309 msg=\"offload to cpu\" layers.requested=999 layers.model=31 layers.offload=0 layers.split=\"\" memory.available=\"[26.2 GiB]\" memory.required.full=\"434.7 MiB\" memory.required.partial=\"0 B\" memory.required.kv=\"180.0 MiB\" memory.required.allocations=\"[434.7 MiB]\" memory.weights.total=\"233.7 MiB\" memory.weights.repeating=\"205.0 MiB\" memory.weights.nonrepeating=\"28.7 MiB\" memory.graph.full=\"164.5 MiB\" memory.graph.partial=\"168.4 MiB\"\r\ntime=2024-11-08T00:36:09.379+08:00 level=INFO source=server.go:395 msg=\"starting llama server\" cmd=\"/tmp/ollama272927415/runners/cpu_avx2/ollama_llama_server --model /root/.ollama/models/blobs/sha256-55aa88ddac43adce6af0e9be8d6cdff2337a3835cd9b50bbcd7a894eb66dfc75 --ctx-size 8192 --batch-size 512 --embedding --log-disable --n-gpu-layers 999 --no-mmap --parallel 4 --port 36063\"\r\ntime=2024-11-08T00:36:09.380+08:00 level=INFO source=sched.go:450 msg=\"loaded runners\" count=1\r\ntime=2024-11-08T00:36:09.380+08:00 level=INFO source=server.go:595 msg=\"waiting for llama runner to start responding\"\r\ntime=2024-11-08T00:36:09.380+08:00 level=INFO source=server.go:629 msg=\"waiting for server to become available\" status=\"llm server error\"\r\nllama_model_loader: loaded meta data with 33 key-value pairs and 272 tensors from /root/.ollama/models/blobs/sha256-55aa88ddac43adce6af0e9be8d6cdff2337a3835cd9b50bbcd7a894eb66dfc75 (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = Smollm2 135M 8k Lc100K Mix1 Ep2\r\nllama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB\r\nllama_model_loader: - kv   4:                           general.finetune str              = 8k-lc100k-mix1-ep2\r\nllama_model_loader: - kv   5:                           general.basename str              = smollm2\r\nllama_model_loader: - kv   6:                         general.size_label str              = 135M\r\nllama_model_loader: - kv   7:                            general.license str              = apache-2.0\r\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\r\nllama_model_loader: - kv   9:                          llama.block_count u32              = 30\r\nllama_model_loader: - kv  10:                       llama.context_length u32              = 8192\r\nllama_model_loader: - kv  11:                     llama.embedding_length u32              = 576\r\nllama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 1536\r\nllama_model_loader: - kv  13:                 llama.attention.head_count u32              = 9\r\nllama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 3\r\nllama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 100000.000000\r\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  17:                          general.file_type u32              = 10\r\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152\r\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64\r\nllama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false\r\nllama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false\r\nllama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm\r\nllama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = [\"<|endoftext|>\", \"<|im_start|>\", \"<|...\r\nllama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\r\nllama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = [\"Ġ t\", \"Ġ a\", \"i n\", \"h e\", \"Ġ Ġ...\r\nllama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2\r\nllama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\r\nllama_model_loader: - kv  32:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   61 tensors\r\nllama_model_loader: - type q8_0:    1 tensors\r\nllama_model_loader: - type q3_K:   30 tensors\r\nllama_model_loader: - type iq4_nl:  180 tensors\r\nllm_load_vocab: special tokens cache size = 17\r\nllm_load_vocab: token to piece cache size = 0.3170 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 49152\r\nllm_load_print_meta: n_merges         = 48900\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 8192\r\nllm_load_print_meta: n_embd           = 576\r\nllm_load_print_meta: n_layer          = 30\r\nllm_load_print_meta: n_head           = 9\r\nllm_load_print_meta: n_head_kv        = 3\r\nllm_load_print_meta: n_rot            = 64\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 64\r\nllm_load_print_meta: n_embd_head_v    = 64\r\nllm_load_print_meta: n_gqa            = 3\r\nllm_load_print_meta: n_embd_k_gqa     = 192\r\nllm_load_print_meta: n_embd_v_gqa     = 192\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 1536\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 100000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 8192\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = ?B\r\nllm_load_print_meta: model ftype      = Q2_K - Medium\r\nllm_load_print_meta: model params     = 134.52 M\r\nllm_load_print_meta: model size       = 82.41 MiB (5.14 BPW) \r\nllm_load_print_meta: general.name     = Smollm2 135M 8k Lc100K Mix1 Ep2\r\nllm_load_print_meta: BOS token        = 1 '<|im_start|>'\r\nllm_load_print_meta: EOS token        = 2 '<|im_end|>'\r\nllm_load_print_meta: UNK token        = 0 '<|endoftext|>'\r\nllm_load_print_meta: PAD token        = 2 '<|im_end|>'\r\nllm_load_print_meta: LF token         = 143 'Ä'\r\nllm_load_print_meta: EOT token        = 0 '<|endoftext|>'\r\nllm_load_print_meta: EOG token        = 0 '<|endoftext|>'\r\nllm_load_print_meta: EOG token        = 2 '<|im_end|>'\r\nllm_load_print_meta: max token length = 162\r\ntime=2024-11-08T00:36:09.632+08:00 level=INFO source=server.go:629 msg=\"waiting for server to become available\" status=\"llm server loading model\"\r\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nggml_sycl_init: SYCL_USE_XMX: yes\r\nggml_sycl_init: found 1 SYCL devices:\r\nllm_load_tensors: ggml ctx size =    0.25 MiB\r\nllm_load_tensors: offloading 30 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 31/31 layers to GPU\r\nllm_load_tensors:      SYCL0 buffer size =    82.46 MiB\r\nllm_load_tensors:  SYCL_Host buffer size =    28.69 MiB\r\nllama_new_context_with_model: n_ctx      = 8192\r\nllama_new_context_with_model: n_batch    = 512\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 100000.0\r\nllama_new_context_with_model: freq_scale = 1\r\n[SYCL] call ggml_check_sycl\r\nggml_check_sycl: GGML_SYCL_DEBUG: 0\r\nggml_check_sycl: GGML_SYCL_F16: no\r\nfound 1 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|                Intel Graphics [0x46a8]|    1.3|     80|     512|   32| 26651M|            1.3.26241|\r\nllama_kv_cache_init:      SYCL0 KV buffer size =   180.00 MiB\r\nllama_new_context_with_model: KV self size  =  180.00 MiB, K (f16):   90.00 MiB, V (f16):   90.00 MiB\r\nllama_new_context_with_model:  SYCL_Host  output buffer size =     0.76 MiB\r\nllama_new_context_with_model:      SYCL0 compute buffer size =    97.12 MiB\r\nllama_new_context_with_model:  SYCL_Host compute buffer size =    17.13 MiB\r\nllama_new_context_with_model: graph nodes  = 846\r\nllama_new_context_with_model: graph splits = 2\r\ntime=2024-11-08T00:36:15.414+08:00 level=INFO source=server.go:634 msg=\"llama runner started in 6.03 seconds\"\r\nollama_llama_server: /home/runner/_work/llm.cpp/llm.cpp/llm.cpp/bigdl-core-xe/llama_backend/sdp_xmx_kernel.cpp:439: auto ggml_sycl_op_sdp_xmx_casual(fp16 *, fp16 *, fp16 *, fp16 *, fp16 *, float *, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, float *, float, sycl::queue &)::(anonymous class)::operator()() const: Assertion `false' failed.\r\n",
      "state": "open",
      "author": "user7z",
      "author_type": "User",
      "created_at": "2024-11-07T15:41:07Z",
      "updated_at": "2024-11-13T01:58:19Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12363/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hzjane"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12363",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12363",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:01.329726",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @user7z , could you provide your device configuration information?",
          "created_at": "2024-11-08T01:54:52Z"
        },
        {
          "author": "user7z",
          "body": "@sgwhat its i5 1235-U alderlake that has iris Xe graphics card , i make it work for llama3.2 , didnt work with for example (smollm2), for llama it has a bad accuracy regression , try to chat with it , or say hello hi , and you'll see , but when it used with it within oldy open-webui , it fails direc",
          "created_at": "2024-11-08T17:34:30Z"
        },
        {
          "author": "user7z",
          "body": "@sgwhat gemm2 is the only one that works , and do poorely , phi3.5 at least lunchs , qwe2.5 misral models , llama3.2 do not work , one of the mistral models respond to my first message , after that i get assertion 'false' failed error , i only experience this with this docker image, also the officia",
          "created_at": "2024-11-10T19:36:49Z"
        },
        {
          "author": "sgwhat",
          "body": "which oneapi version have you installed in your container?",
          "created_at": "2024-11-11T02:03:45Z"
        },
        {
          "author": "user7z",
          "body": "@sgwhat its a container it comes with oneapi , the version is the one you support under linux",
          "created_at": "2024-11-11T02:36:07Z"
        }
      ]
    },
    {
      "issue_number": 7399,
      "title": "Compatibility with Spark 3.2 + Scala 2.13",
      "body": "I would like to know if there are any plans to compile bigdl-dllib with Scala 2.13, and publish the artifact. Currently I´m working with Scala 3 in some research and I would like to present a paper for the next Scaladays using new lang features + some bigdl examples.\r\n\r\nThanks!",
      "state": "open",
      "author": "emartinezs44",
      "author_type": "User",
      "created_at": "2023-02-01T17:09:57Z",
      "updated_at": "2024-11-11T07:03:46Z",
      "closed_at": null,
      "labels": [
        "user issue",
        "DLlib"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 19,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/7399/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "qiuxin2012"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/7399",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/7399",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:01.528836",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "@emartinezs44 Bad news, I tried to upgrade dllib to scala 2.13, but only one of dllib's dependency xgboost4j didn't support scala 2.13. https://github.com/dmlc/xgboost/issues/6596 \r\n",
          "created_at": "2023-02-02T04:35:00Z"
        },
        {
          "author": "jason-dai",
          "body": "> @emartinezs44 Bad news, I tried to upgrade dllib to scala 2.13, but only one of dllib's dependency xgboost4j didn't support scala 2.13. [dmlc/xgboost#6596](https://github.com/dmlc/xgboost/issues/6596)\r\n\r\nMaybe we can release an experimental dllib package for Spark 3.2+scala 3.13 without xgboost su",
          "created_at": "2023-02-02T06:55:35Z"
        },
        {
          "author": "emartinezs44",
          "body": "I see, this kind of dependences are a problem(having in mind that Spark 3.2 was released a year and a half ago). By now, the only solution is to create a profile and exclude the dependence and the code in Scala referencing xgboost. I will try to create a branch in my fork with this config and see if",
          "created_at": "2023-02-02T09:11:13Z"
        },
        {
          "author": "qiuxin2012",
          "body": "I have a branch https://github.com/qiuxin2012/BigDL/tree/scala3, I'm working on the compatiability with scala 2.13\r\n100+ build errors, I'm tring to fix them.",
          "created_at": "2023-02-03T01:35:18Z"
        },
        {
          "author": "qiuxin2012",
          "body": "@emartinezs44  I'm blocking by a \"ambiguous implicit values\" error, could you help?\r\nYou can use `./make-dist.sh -P spark_3.x -P scala_2.13 -Dspark.version=3.2.3` to reproduce the error message, on my branch  https://github.com/qiuxin2012/BigDL/tree/scala3.\r\n\r\n```\r\n[ERROR] /home/xin/IdeaProjects/Big",
          "created_at": "2023-02-03T08:54:48Z"
        }
      ]
    },
    {
      "issue_number": 12376,
      "title": "performance  problem about  internvl  image embedding  using   ggml.dll  ",
      "body": "##  problem desc\r\nImage embedding using ggml.dll provided by ipex will become slower and slower, while using llama.cpp a1631e5  build  performance is stable.\r\n-  ipex-llm\r\n![1731296401080](https://github.com/user-attachments/assets/f45d0526-3b2a-4437-8f49-69bbaa535cd8)\r\n- llama.cpp  a1631e5  \r\n![1731296630520](https://github.com/user-attachments/assets/a846ec82-ed12-42ef-b059-256639915350)\r\n\r\n## test code\r\nclip  source code  can be  found in  https://github.com/ggerganov/llama.cpp/pull/9403\r\n```\r\n#include \"clip.h\"\r\n\r\n#include \"internvl.h\"\r\n#include \"iostream\"\r\n\r\nint main(int argc, char* argv[]) {\r\n  std::string model_path;\r\n  std::string image_path;\r\n  std::string device;\r\n\r\n  for (int i = 1; i < argc; i += 2) {\r\n    std::string arg = argv[i];\r\n    if (arg == \"--model\") {\r\n      model_path = argv[i + 1];\r\n    } else if (arg == \"--image\") {\r\n      image_path = argv[i + 1];\r\n    } else if (arg == \"--device\") {\r\n      device = argv[i + 1];\r\n    }\r\n  }\r\n\r\n  auto ctx_clip = clip_model_load(model_path.c_str(), 1 ,device);\r\n\r\n  for (int i = 0; i < 20; i++) {\r\n    auto embed = internvl_image_embed_make_with_filename(ctx_clip, 4,\r\n                                                         image_path.c_str());\r\n    std::cout << embed->embed[0] << \"\\n\";\r\n  }\r\n  return 0;\r\n}\r\n\r\n```\r\n## env\r\n\r\nultra 7 155H  igpu   , windows11   \r\n\r\n",
      "state": "open",
      "author": "cjsdurj",
      "author_type": "User",
      "created_at": "2024-11-11T03:45:15Z",
      "updated_at": "2024-11-11T03:50:05Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12376/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "rnwang04"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12376",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12376",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:01.763327",
      "comments": [
        {
          "author": "rnwang04",
          "body": "Hi @cjsdurj , thanks for pointing out this issue.\r\nI have fixed it, you could try it again with ggml.dll released in `pip install ipex-llm>=2.2.0b20241111` tomorrow.",
          "created_at": "2024-11-11T03:48:46Z"
        }
      ]
    },
    {
      "issue_number": 12364,
      "title": "ipex-llm-cpp-xpu container",
      "body": "can you guys provide a container that has ollama only , the ipex-llm-cpp-inference-xpu has open-webui , but it has an old version since may , and its not working it works but cant chat , open-webui official container do work great , so if you provide a container that just has ollama dependencies , it would be great one to use in conjunction with the other one , i.e a dedicated container for ollama only",
      "state": "open",
      "author": "user7z",
      "author_type": "User",
      "created_at": "2024-11-07T17:50:48Z",
      "updated_at": "2024-11-11T02:36:16Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12364/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "ACupofAir"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12364",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12364",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:01.940158",
      "comments": [
        {
          "author": "glorysdj",
          "body": "For now, you can use ipex-llm-cpp-inference-xpu to start only ollama and ignore open-webui to connect to the official open-webui container. We will also check and possibly upgrade open-webui in that container.",
          "created_at": "2024-11-08T02:12:55Z"
        },
        {
          "author": "user7z",
          "body": "@glorysdj the thing is that ollama in that container is broken , it can run llama models , but fails for the others (smollm2 as exemple), and there is a horrific accuracy regression, (i see it with llama3.2) , but when used with openwebui in this container it failse , if you chat more than once , so",
          "created_at": "2024-11-08T17:28:09Z"
        },
        {
          "author": "glorysdj",
          "body": "OK. Good point, we will decouple the openwebui and the ollama.",
          "created_at": "2024-11-11T02:36:15Z"
        }
      ]
    },
    {
      "issue_number": 11496,
      "title": "Determining if AMX is in use by ollama ",
      "body": "Hello, \r\n   I used latest steps to install ipex-llm into a venv on a 5th Gen Xeon system. I don't think AMX is being utilized based on screenshot below. Should AMX show up in list of CPU features in output below - last line? lscpu shows AMX instructions are present. Can you please show a verification step for ipex-llm confirming that CPU optimizations are installed in ipex-llm venv?\r\n\r\n![image](https://github.com/intel-analytics/ipex-llm/assets/6793143/5b99d9ad-a49a-42b3-826d-8c5fcd0c0a27)\r\n\r\nThanks",
      "state": "open",
      "author": "js333031",
      "author_type": "User",
      "created_at": "2024-07-03T01:36:25Z",
      "updated_at": "2024-11-10T20:17:53Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11496/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11496",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11496",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:02.170517",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @js333031 , ipex-llm ollama is a GPU-optimized version, so we currently do not plan to work on this.",
          "created_at": "2024-07-04T01:14:54Z"
        },
        {
          "author": "js333031",
          "body": "Has there been any change to the plan for optimizing for Xeon/AMX?",
          "created_at": "2024-10-03T21:35:09Z"
        },
        {
          "author": "glorysdj",
          "body": "Sorry, we don't have any plan for optimizing ollama for Xeon/AMX yet.",
          "created_at": "2024-10-08T02:57:48Z"
        },
        {
          "author": "endomorphosis",
          "body": "I am also querying about this, but and I have noticed that its supported by the llama_cpp package, so it just depends I think on how downstream they are from changes made to the base package.",
          "created_at": "2024-11-10T20:17:52Z"
        }
      ]
    },
    {
      "issue_number": 12348,
      "title": "Ollama run embedding module mxbai-embed-large failed.",
      "body": "Issue:\r\n```\r\ncurl http://localhost:11434/api/embeddings -d '{\r\n  \"model\": \"mxbai-embed-large\",\r\n  \"prompt\": \"Llamas are members of the camelid family\"\r\n}'\r\n{\"error\":\"llama runner process has terminated: signal: aborted (core dumped)\"}\r\n```\r\nlogs:\r\n```\r\nINFO [main] build info | build=1 commit=\"1d5f8dd\" tid=\"134260192001024\" timestamp=1730908676\r\nINFO [main] system info | n_threads=72 n_threads_batch=-1 system_info=\"AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \" tid=\"134260192001024\" timestamp=1730908676 total_threads=144\r\nINFO [main] HTTP server listening | hostname=\"127.0.0.1\" n_threads_http=\"143\" port=\"46175\" tid=\"134260192001024\" timestamp=1730908676\r\nllama_model_loader: loaded meta data with 23 key-value pairs and 389 tensors from /root/.ollama/models/blobs/sha256-819c2adf5ce6df2b6bd2ae4ca90d2a69f060afeb438d0c171db57daa02e39c3d (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = bert\r\nllama_model_loader: - kv   1:                               general.name str              = mxbai-embed-large-v1\r\nllama_model_loader: - kv   2:                           bert.block_count u32              = 24\r\nllama_model_loader: - kv   3:                        bert.context_length u32              = 512\r\nllama_model_loader: - kv   4:                      bert.embedding_length u32              = 1024\r\nllama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 4096\r\nllama_model_loader: - kv   6:                  bert.attention.head_count u32              = 16\r\nllama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000\r\nllama_model_loader: - kv   8:                          general.file_type u32              = 1\r\nllama_model_loader: - kv   9:                      bert.attention.causal bool             = false\r\nllama_model_loader: - kv  10:                          bert.pooling_type u32              = 2\r\nllama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2\r\nllama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 101\r\nllama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 102\r\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert\r\nllama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,30522]   = [\"[PAD]\", \"[unused0]\", \"[unused1]\", \"...\r\nllama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...\r\nllama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100\r\nllama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102\r\nllama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0\r\nllama_model_loader: - kv  21:                tokenizer.ggml.cls_token_id u32              = 101\r\nllama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103\r\nllama_model_loader: - type  f32:  243 tensors\r\nllama_model_loader: - type  f16:  146 tensors\r\nllm_load_vocab: special tokens cache size = 5\r\nllm_load_vocab: token to piece cache size = 0.2032 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = bert\r\nllm_load_print_meta: vocab type       = WPM\r\nllm_load_print_meta: n_vocab          = 30522\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 512\r\nllm_load_print_meta: n_embd           = 1024\r\nllm_load_print_meta: n_layer          = 24\r\nllm_load_print_meta: n_head           = 16\r\nllm_load_print_meta: n_head_kv        = 16\r\nllm_load_print_meta: n_rot            = 64\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 64\r\nllm_load_print_meta: n_embd_head_v    = 64\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 1.0e-12\r\nllm_load_print_meta: f_norm_rms_eps   = 0.0e+00\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 4096\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 0\r\nllm_load_print_meta: pooling type     = 2\r\nllm_load_print_meta: rope type        = 2\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 512\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = 335M\r\nllm_load_print_meta: model ftype      = F16\r\nllm_load_print_meta: model params     = 334.09 M\r\nllm_load_print_meta: model size       = 637.85 MiB (16.02 BPW)\r\nllm_load_print_meta: general.name     = mxbai-embed-large-v1\r\nllm_load_print_meta: BOS token        = 101 '[CLS]'\r\nllm_load_print_meta: EOS token        = 102 '[SEP]'\r\nllm_load_print_meta: UNK token        = 100 '[UNK]'\r\nllm_load_print_meta: SEP token        = 102 '[SEP]'\r\nllm_load_print_meta: PAD token        = 0 '[PAD]'\r\nllm_load_print_meta: CLS token        = 101 '[CLS]'\r\nllm_load_print_meta: MASK token       = 103 '[MASK]'\r\nllm_load_print_meta: LF token         = 0 '[PAD]'\r\nllm_load_print_meta: max token length = 21\r\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nggml_sycl_init: SYCL_USE_XMX: yes\r\nggml_sycl_init: found 1 SYCL devices:\r\nllm_load_tensors: ggml ctx size =    0.32 MiB\r\ntime=2024-11-06T15:57:56.468Z level=INFO source=server.go:629 msg=\"waiting for server to become available\" status=\"llm server loading model\"\r\nllm_load_tensors: offloading 24 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 25/25 layers to GPU\r\nllm_load_tensors:      SYCL0 buffer size =   577.23 MiB\r\nllm_load_tensors:  SYCL_Host buffer size =    60.62 MiB\r\nllama_new_context_with_model: n_ctx      = 512\r\nllama_new_context_with_model: n_batch    = 512\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\n[SYCL] call ggml_check_sycl\r\nggml_check_sycl: GGML_SYCL_DEBUG: 0\r\nggml_check_sycl: GGML_SYCL_F16: no\r\nfound 1 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|         Intel Data Center GPU Flex 170|    1.3|    512|    1024|   32| 14193M|            1.3.29735|\r\nllama_kv_cache_init:      SYCL0 KV buffer size =    48.00 MiB\r\nllama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB\r\nllama_new_context_with_model:        CPU  output buffer size =     0.00 MiB\r\n[1730908676] warming up the model with an empty run\r\nllama_new_context_with_model:      SYCL0 compute buffer size =    13.01 MiB\r\nllama_new_context_with_model:  SYCL_Host compute buffer size =     5.01 MiB\r\nllama_new_context_with_model: graph nodes  = 801\r\nllama_new_context_with_model: graph splits = 2\r\nollama_llama_server: /home/runner/_work/llm.cpp/llm.cpp/llm.cpp/bigdl-core-xe/llama_backend/sdp_kernel.cpp:905: void ggml_sycl_op_fp16_sdp(fp16 *, fp16 *, fp16 *, fp16 *, fp16 *, float *, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, float *, float, bool, sycl::queue &): Assertion `false' failed.\r\ntime=2024-11-06T15:57:57.020Z level=INFO source=server.go:629 msg=\"waiting for server to become available\" status=\"llm server error\"\r\ntime=2024-11-06T15:57:57.271Z level=ERROR source=sched.go:456 msg=\"error loading llama server\" error=\"llama runner process has terminated: signal: aborted (core dumped)\"\r\n[GIN] 2024/11/06 - 15:57:57 | 500 |  1.092528856s |      172.18.0.1 | POST     \"/api/embeddings\"\r\n\r\n```",
      "state": "closed",
      "author": "feiyu11859661",
      "author_type": "User",
      "created_at": "2024-11-06T08:06:17Z",
      "updated_at": "2024-11-08T06:18:31Z",
      "closed_at": "2024-11-08T06:18:30Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12348/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12348",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12348",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:02.376985",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @feiyu11859661, we have reproduced your issue and are currently working on a fix. As a workaround, you may install `ipex-llm[cpp]==2.2.0b20241013` to run ipex-llm ollama.",
          "created_at": "2024-11-07T06:04:41Z"
        },
        {
          "author": "leonardozcm",
          "body": "hi, we have fixed this, You may try ipex-llm[cpp]>=2.2.0b20241107 tomorrow",
          "created_at": "2024-11-07T08:37:18Z"
        },
        {
          "author": "feiyu11859661",
          "body": "I have verified from my side, the issue addressed. \r\nThanks a lot for the support! Very appreciate for the quick response!\r\n",
          "created_at": "2024-11-08T06:18:30Z"
        }
      ]
    },
    {
      "issue_number": 12315,
      "title": "How to check GPU memory consumption by ipex on Linux?",
      "body": "System: ubuntu 22.04\r\nCPU is ultra 7 155H\r\n\r\nWe'd like to know how much GPU memory will used by ipex-llm while running, we tried `vtune`, but didn't found where is a summary for memory usage (the memory stats is more likely CPU memory usage), we also tried `xpu-smi` and `xpu-manager`, the output are all `N/A`.\r\n\r\n```\r\n+----------------------------+---------------------------------------------------------------------+\r\n| GPU Core Temperature       | Tile 0: avg: N/A, min: N/A, max: N/A, current: N/A                  |\r\n| (Celsius Degree)           |                                                                     |\r\n+----------------------------+---------------------------------------------------------------------+\r\n| GPU Memory Temperature     | Tile 0: avg: N/A, min: N/A, max: N/A, current: N/A                  |\r\n| (Celsius Degree)           |                                                                     |\r\n+----------------------------+---------------------------------------------------------------------+\r\n| GPU Memory Read (kB/s)     | Tile 0: avg: N/A, min: N/A, max: N/A, current: N/A                  |\r\n+----------------------------+---------------------------------------------------------------------+\r\n| GPU Memory Write (kB/s)    | Tile 0: avg: N/A, min: N/A, max: N/A, current: N/A                  |\r\n+----------------------------+---------------------------------------------------------------------+\r\n| GPU Memory Bandwidth (%)   | Tile 0: avg: N/A, min: N/A, max: N/A, current: N/A                  |\r\n+----------------------------+---------------------------------------------------------------------+\r\n| GPU Memory Used (MiB)      | Tile 0: avg: N/A, min: N/A, max: N/A, current: N/A                  |\r\n+----------------------------+---------------------------------------------------------------------+\r\n| GPU Memory Util (%)        | Tile 0: avg: N/A, min: N/A, max: N/A, current: N/A                  |\r\n+----------------------------+---------------------------------------------------------------------+\r\n\r\n\r\n+-----------+--------------------------------------------------------------------------------------+\r\n| Device ID | Device Information                                                                   |\r\n+-----------+--------------------------------------------------------------------------------------+\r\n| 0         | Device Type: GPU                                                                     |\r\n|           | Device Name: Intel(R) Arc(TM) Graphics                                               |\r\n|           | PCI Device ID: 0x7d55                                                                |\r\n|           | Vendor Name: Intel(R) Corporation                                                    |\r\n|           | SOC UUID: 00000000-0000-0200-0000-00087d558086                                       |\r\n|           | Serial Number: unknown                                                               |\r\n|           | Core Clock Rate: 2250 MHz                                                            |\r\n|           | Stepping: C0                                                                         |\r\n|           | SKU Type: N/A                                                                        |\r\n|           |                                                                                      |\r\n|           | Driver Version: N/A                                                                  |\r\n|           | Driver Package Version: N/A                                                          |\r\n|           | Kernel Version: 6.8.0-47-generic                                                     |\r\n+-----------+--------------------------------------------------------------------------------------+\r\n\r\n```",
      "state": "closed",
      "author": "acane77",
      "author_type": "User",
      "created_at": "2024-11-01T08:53:31Z",
      "updated_at": "2024-11-07T08:48:50Z",
      "closed_at": "2024-11-07T08:48:50Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12315/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12315",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12315",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:02.581848",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "You can use our benchmark wrapper, see\r\nhttps://github.com/intel-analytics/ipex-llm/tree/c8679ad5926ede3683e254a81d5099bffbd4d750/python/llm/dev/benchmark#gpu-usage\r\nIf you want to know the GPU consumption after each token, you can use `BenchmarkWrapper(model, do_print=True, verbose=True)`.",
          "created_at": "2024-11-04T02:25:40Z"
        }
      ]
    },
    {
      "issue_number": 12335,
      "title": "A770运行 ipex_llm harness 跑chatglm3-6b 出现Error Message: property 'pad_token' of 'ChatGLMTokenizer' object has no setter",
      "body": "(llm) test@test-Z590-VISION-D:~/ipexllm_whowhat/ipex-llm/python/llm/dev/benchmark/harness$ python run_llb.py --model ipex-llm --pretrained /home/test/models/LLM/haiyan-scp/chatglm3-6b/pytorch --precision sym_int4 --device xpu --tasks winogrande --batch 1 --no_cache --limit 0.1 --model_args dtype=float16\r\n/home/test/miniforge3/envs/llm/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n  warn(\r\n2024-11-05 15:33:52,489 - INFO - intel_extension_for_pytorch auto imported\r\nWARNING: --limit SHOULD ONLY BE USED FOR TESTING. REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\r\nSelected Tasks: ['winogrande']\r\nThe repository for /home/test/models/LLM/haiyan-scp/chatglm3-6b/pytorch contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co//home/test/models/LLM/haiyan-scp/chatglm3-6b/pytorch.\r\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\r\n\r\nDo you wish to run the custom code? [y/N] y\r\nJob config of task=winogrande, precision=sym_int4 failed. Error Message: property 'pad_token' of 'ChatGLMTokenizer' object has no setter\r\nHere are results of all successful tasks:\r\nTraceback (most recent call last):\r\n  File \"/home/test/ipexllm_whowhat/ipex-llm/python/llm/dev/benchmark/harness/run_llb.py\", line 151, in <module>\r\n    main()\r\n  File \"/home/test/ipexllm_whowhat/ipex-llm/python/llm/dev/benchmark/harness/run_llb.py\", line 147, in main\r\n    raise RuntimeError('\\n'.join(fail))\r\nRuntimeError: Job config of task=winogrande, precision=sym_int4 failed. Error Message: property 'pad_token' of 'ChatGLMTokenizer' object has no setter\r\n![image](https://github.com/user-attachments/assets/4c609a7a-6636-4e44-ad30-b5502154389d)\r\n",
      "state": "open",
      "author": "tao-ov",
      "author_type": "User",
      "created_at": "2024-11-05T07:37:23Z",
      "updated_at": "2024-11-07T01:55:14Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12335/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "cranechu0131"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12335",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12335",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:02.807286",
      "comments": [
        {
          "author": "cranechu0131",
          "body": "Hi tao-ov,\r\n\r\nPlease refer to this https://github.com/chatchat-space/Langchain-Chatchat/issues/1835, and update your [tokenization_chatglm.py](https://huggingface.co/THUDM/chatglm2-6b/blob/main/tokenization_chatglm.py)\r\n in your model folder, this ought to solve this issue.",
          "created_at": "2024-11-06T02:21:43Z"
        }
      ]
    },
    {
      "issue_number": 12278,
      "title": "Doubts about ParallelTable and ParallelCriterion",
      "body": "I am a beginner using BigDL to create a ParallelTable model for training, but I encountered errors in converting between Tensor and Table types:\r\nCaused by: java. lang. ClassCastException: com. intel. analytics. bigdl. dllib. utilities Table cannot be cast to com.intel.analytics.bigdl.dllib.tensor.Tensor.\r\nI have used Graph (Input [Float(), Sigmoid [Float())) models before, with a loss function of MSERiterion [Float()) and training data input of DataSet [Sample [Float] (Tensor (features, Array (features. length), Tensor (Array (label), Array (1)), can work normally. But now, I am confused about this. Is there a problem with my training data type (but Sample only accepts Tensor types)? Or is there a problem with my model construction? I don't know how to solve it now, I hope to receive some help\r\nMy partial code is as follows:\r\n\r\n\r\n val modelTask1 = Sequential()\r\n  .add(Sequential[Float]()\r\n    .add(Linear(200, 100))\r\n    .add(ReLU[Float]()))\r\n  .add(Linear(100, 1))\r\nval modelTask2 = Sequential()\r\n  .add(Sequential[Float]()\r\n    .add(Linear(200, 100))\r\n    .add(ReLU[Float]()))\r\n  .add(Linear(100, 1))\r\n\r\nval model = ParallelTable()\r\n  .add(modelTask1)\r\n  .add(modelTask2)\r\nval trainData = DataSet.rdd(trainDf.rdd.map(row => {\r\n  val features = row.getAs[DenseVector](\"features\").toArray.map(_.toFloat)\r\n  val ctr = row.getAs[Double](\"ctr\").toFloat\r\n  val cvr = row.getAs[Double](\"cvr\").toFloat\r\n  Sample[Float](Tensor(features, Array(features.length)), Tensor(Array(ctr, cvr), Array(2)))\r\n})).transform(SampleToMiniBatch[Float](batch.toInt))\r\n\r\nval criterion = ParallelCriterion[Float]()\r\ncriterion.add(MSECriterion[Float](), 1.0)\r\ncriterion.add(MSECriterion[Float](), 1.0)\r\nval optimizer = Optimizer(\r\n  model = model,\r\n  dataset = trainData,\r\n  criterion = criterion\r\n).setOptimMethod(new Adam[Float]())\r\n  .setEndWhen(Trigger.maxEpoch(maxEpoch.toInt))\r\nval trainedModel = optimizer.optimize()\r\n\r\n    ",
      "state": "open",
      "author": "clare-cn",
      "author_type": "User",
      "created_at": "2024-10-26T15:13:07Z",
      "updated_at": "2024-11-07T01:54:29Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12278/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12278",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12278",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:03.013658",
      "comments": []
    },
    {
      "issue_number": 12268,
      "title": "ipex-llm xpu version doesn't work on Lunar Lake ",
      "body": "Following this link to verify:\r\nhttps://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/install_windows_gpu.md#verify-installation\r\n\r\nIt will raise an error:\r\n(ultrachat_back) C:\\Users\\HB\\qwen_ultrachat\\ultrachat\\trunk\\back>python ipex.py\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\HB\\qwen_ultrachat\\ultrachat\\trunk\\back\\ipex.py\", line 1, in <module>\r\n    import torch\r\n  File \"C:\\Users\\HB\\miniforge3\\envs\\ultrachat_back\\lib\\site-packages\\torch\\__init__.py\", line 143, in <module>\r\n    raise err\r\nOSError: [WinError 126] 找不到指定的模块。 Error loading \"C:\\Users\\HB\\miniforge3\\envs\\ultrachat_back\\lib\\site-packages\\torch\\lib\\backend_with_compiler.dll\" or one of its dependencies.\r\n\r\nHow to solve this problem?",
      "state": "closed",
      "author": "HoppeDeng",
      "author_type": "User",
      "created_at": "2024-10-25T05:43:35Z",
      "updated_at": "2024-11-07T01:54:02Z",
      "closed_at": "2024-10-25T06:49:52Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12268/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12268",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12268",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:03.013685",
      "comments": [
        {
          "author": "Oscilloscope98",
          "body": "Hi @HoppeDeng,\r\n\r\nThere could be several possible cause for this issue. To resolve it, please make sure to:\r\n\r\n1. install `ipex_llm[xpu_lnl]` instead of `ipex-llm[xpu]` as we mentioned [here](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/install_windows_gpu.md#install-",
          "created_at": "2024-10-25T06:39:07Z"
        },
        {
          "author": "HoppeDeng",
          "body": "ipex_llm[xpu_lnl] has fixed this,thanks!",
          "created_at": "2024-10-25T06:49:52Z"
        }
      ]
    },
    {
      "issue_number": 12273,
      "title": "IPEX-LLM load qwen2.5 7B model failed",
      "body": "ipex-llm xpu version failed to load qwen2.5 7b instruct model on win11.\r\n\r\nipex-llm version : 2.2.0b20241024\r\ntransformers version: 4.37.0\r\n\r\n\r\nThe error log is below:\r\n \r\n2024-10-25 23:03:36,101 - INFO - Converting the current model to sym_int4 format......\r\nC:\\Users\\HB\\miniforge3\\envs\\uc_back\\Lib\\site-packages\\torch\\nn\\init.py:452: UserWarning: Initializing zero-element tensors is a no-op\r\n  warnings.warn(\"Initializing zero-element tensors is a no-op\")\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\HB\\qwen_ultrachat\\ultrachat\\trunk\\back\\start.py\", line 578, in <module>\r\n    app = create_app()\r\n          ^^^^^^^^^^^^\r\n  File \"C:\\Users\\HB\\qwen_ultrachat\\ultrachat\\trunk\\back\\start.py\", line 58, in create_app\r\n    bot = LLMGLM()\r\n          ^^^^^^^^\r\n  File \"C:\\Users\\HB\\qwen_ultrachat\\ultrachat\\trunk\\back\\models\\chatllm.py\", line 27, in __init__\r\n    self.llm = init_model()\r\n               ^^^^^^^^^^^^\r\n  File \"C:\\Users\\HB\\qwen_ultrachat\\ultrachat\\trunk\\back\\models\\chatllm.py\", line 18, in init_model\r\n    llm = TransformersLLM.from_model_id(\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\HB\\qwen_ultrachat\\ultrachat\\trunk\\back\\models\\bigdl_llm.py\", line 83, in from_model_id\r\n    model.to(LLM_DEVICE)\r\n  File \"C:\\Users\\HB\\miniforge3\\envs\\uc_back\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 2597, in to\r\n    return super().to(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\HB\\miniforge3\\envs\\uc_back\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1173, in to\r\n    return self._apply(convert)\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\HB\\miniforge3\\envs\\uc_back\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 779, in _apply\r\n    module._apply(fn)\r\n  File \"C:\\Users\\HB\\miniforge3\\envs\\uc_back\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 779, in _apply\r\n    module._apply(fn)\r\n  File \"C:\\Users\\HB\\miniforge3\\envs\\uc_back\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 804, in _apply\r\n    param_applied = fn(param)\r\n                    ^^^^^^^^^\r\n  File \"C:\\Users\\HB\\miniforge3\\envs\\uc_back\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1159, in convert\r\n    return t.to(\r\n           ^^^^^\r\n  File \"C:\\Users\\HB\\miniforge3\\envs\\uc_back\\Lib\\site-packages\\torch\\xpu\\__init__.py\", line 117, in _lazy_init\r\n    torch._C._xpu_init()\r\nRuntimeError: Native API failed. Native API returns: -30 (PI_ERROR_INVALID_VALUE) -30 (PI_ERROR_INVALID_VALUE)\r\n\r\nIs there a way to fix it?",
      "state": "closed",
      "author": "HoppeDeng",
      "author_type": "User",
      "created_at": "2024-10-25T08:56:25Z",
      "updated_at": "2024-11-07T01:53:56Z",
      "closed_at": "2024-10-28T01:15:18Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12273/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12273",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12273",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:03.201477",
      "comments": [
        {
          "author": "HoppeDeng",
          "body": "Conflict with openvino 2024.0",
          "created_at": "2024-10-28T01:15:18Z"
        }
      ]
    },
    {
      "issue_number": 12294,
      "title": "[ipex-llm] A significant deviation in accuracy between ipex llm 2.2.0b1 and 2.1.0b20240515 when running the codegeex model",
      "body": "When we ran codegeex models in ipex -llm 2.2.0b1 env , it generated a  significant deviation in accuracy than in ipex-llm 2.1.0b2024515 or 2.1.0b2. We make test_logits test to get some data as below:\r\n\r\n下面是各个ipex-llm版本间的logits结果(optimize_model=True)比较:\r\n2.1.0b2 <-> 2.1.0b20240515 :            一致\r\n2.1.0b2 <-> 2.1.0 :                                      不一致\r\n2.1.0b2 <-> 2.2.0b1 :                                不一致\r\n2.1.0      <-> 2.2.0b1 :                                 不一致\r\n\r\n下列版本做了optimize_model=True和optimize_model=False间的logits结果比较:\r\n2.1.0b20240515:        一致\r\n2.1.0b2:                            一致\r\n2.1.0:                                 不一致\r\n2.2.0b1:                            不一致\r\n\r\npls help me to debug this issue , thx.\r\n",
      "state": "open",
      "author": "johnysh",
      "author_type": "User",
      "created_at": "2024-10-30T02:07:05Z",
      "updated_at": "2024-11-05T00:23:32Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12294/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "qiuxin2012"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12294",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12294",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:03.387120",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "For the codegeex models' logits diff, 2.2.0b2 has fixed it. Though the logits are different, but the errors are much smaller, for the same prompt, 2.2.0b2 can generate the same output with 2.1.0b2.",
          "created_at": "2024-11-05T00:23:31Z"
        }
      ]
    },
    {
      "issue_number": 11080,
      "title": "ipex-llm[cpp] error: Sub-group size 8 is not supported on the device",
      "body": "I followed the instructions from https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/llama_cpp_quickstart.html on a bare metal server from the Intel Dev Cloud, specifically this instance:\r\n\r\n> Intel® Max Series GPU (PVC) on 4th Gen Intel® Xeon® processors – 1100 series (8x)\r\n\r\nI got these logs:\r\n\r\n<details><summary>Test Command and Logs</summary>\r\n\r\n```console\r\n$ $ ZE_ENABLE_LOADER_DEBUG_TRACE=1 SYCL_CACHE_PERSISTENT=1 ./main --n-gpu-layers 999 --n-predict 1024 --model ~/share/models/mistral-7b-instruct-v0.1.Q4_K_M.gguf --ctx-size 4096 --ignore-eos --split-mode none --main-gpu 0 -f ~/opt/src prompt.txt\r\nLog start\r\nmain: build = 1 (baa5868)\r\nmain: built with Intel(R) oneAPI DPC++/C++ Compiler 2024.0.0 (2024.0.0.20231017) for x86_64-unknown-linux-gnu\r\nmain: seed  = 1716242491\r\nllama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /home/sdp/share/models/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\r\nllama_model_loader: - kv  11:                          general.file_type u32              = 15\r\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  19:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q4_K:  193 tensors\r\nllama_model_loader: - type q6_K:   33 tensors\r\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\r\nllm_load_print_meta: format           = GGUF V2\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 32768\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 32768\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 8B\r\nllm_load_print_meta: model ftype      = Q4_K - Medium\r\nllm_load_print_meta: model params     = 7.24 B\r\nllm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \r\nllm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\n[SYCL] call ggml_init_sycl\r\nggml_init_sycl: GGML_SYCL_DEBUG: 0\r\nggml_init_sycl: GGML_SYCL_F16: no\r\nZE_LOADER_DEBUG_TRACE:Load Library of libze_intel_vpu.so.1 failed with libze_intel_vpu.so.1: cannot open shared object file: No such file or directory\r\nfound 18 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|         Intel Data Center GPU Max 1100|    1.3|    448|    1024|   32| 51539M|            1.3.27191|\r\n| 1| [level_zero:gpu:1]|         Intel Data Center GPU Max 1100|    1.3|    448|    1024|   32| 51539M|            1.3.27191|\r\n| 2| [level_zero:gpu:2]|         Intel Data Center GPU Max 1100|    1.3|    448|    1024|   32| 51539M|            1.3.27191|\r\n| 3| [level_zero:gpu:3]|         Intel Data Center GPU Max 1100|    1.3|    448|    1024|   32| 51539M|            1.3.27191|\r\n| 4| [level_zero:gpu:4]|         Intel Data Center GPU Max 1100|    1.3|    448|    1024|   32| 51539M|            1.3.27191|\r\n| 5| [level_zero:gpu:5]|         Intel Data Center GPU Max 1100|    1.3|    448|    1024|   32| 51539M|            1.3.27191|\r\n| 6| [level_zero:gpu:6]|         Intel Data Center GPU Max 1100|    1.3|    448|    1024|   32| 51539M|            1.3.27191|\r\n| 7| [level_zero:gpu:7]|         Intel Data Center GPU Max 1100|    1.3|    448|    1024|   32| 51539M|            1.3.27191|\r\n| 8|     [opencl:gpu:0]|         Intel Data Center GPU Max 1100|    3.0|    448|    1024|   32| 48946M|       23.35.27191.42|\r\n| 9|     [opencl:gpu:1]|         Intel Data Center GPU Max 1100|    3.0|    448|    1024|   32| 48946M|       23.35.27191.42|\r\n|10|     [opencl:gpu:2]|         Intel Data Center GPU Max 1100|    3.0|    448|    1024|   32| 48946M|       23.35.27191.42|\r\n|11|     [opencl:gpu:3]|         Intel Data Center GPU Max 1100|    3.0|    448|    1024|   32| 48946M|       23.35.27191.42|\r\n|12|     [opencl:gpu:4]|         Intel Data Center GPU Max 1100|    3.0|    448|    1024|   32| 48946M|       23.35.27191.42|\r\n|13|     [opencl:gpu:5]|         Intel Data Center GPU Max 1100|    3.0|    448|    1024|   32| 48946M|       23.35.27191.42|\r\n|14|     [opencl:gpu:6]|         Intel Data Center GPU Max 1100|    3.0|    448|    1024|   32| 48946M|       23.35.27191.42|\r\n|15|     [opencl:gpu:7]|         Intel Data Center GPU Max 1100|    3.0|    448|    1024|   32| 48946M|       23.35.27191.42|\r\n|16|     [opencl:cpu:0]|              Intel Xeon Platinum 8468V|    3.0|    192|    8192|   64|1081858M|2024.17.3.0.08_160000|\r\n|17|     [opencl:acc:0]|            Intel FPGA Emulation Device|    1.2|    192|67108864|   64|1081858M|2024.17.3.0.08_160000|\r\nggml_backend_sycl_set_single_device: use single device: [0]\r\nuse 1 SYCL GPUs: [0] with Max compute units:448\r\nllm_load_tensors: ggml ctx size =    0.30 MiB\r\nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 33/33 layers to GPU\r\nllm_load_tensors:      SYCL0 buffer size =  4095.05 MiB\r\nllm_load_tensors:        CPU buffer size =    70.31 MiB\r\n..............................................................................................\r\nllama_new_context_with_model: n_ctx      = 4096\r\nllama_new_context_with_model: n_batch    = 2048\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:      SYCL0 KV buffer size =   512.00 MiB\r\nllama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\r\nllama_new_context_with_model:  SYCL_Host  output buffer size =     0.12 MiB\r\nllama_new_context_with_model:      SYCL0 compute buffer size =   296.00 MiB\r\nllama_new_context_with_model:  SYCL_Host compute buffer size =    16.01 MiB\r\nllama_new_context_with_model: graph nodes  = 1062\r\nllama_new_context_with_model: graph splits = 2\r\nSub-group size 8 is not supported on the device\r\nException caught at file:/home/runner/_work/llm.cpp/llm.cpp/llama-cpp-bigdl/ggml-sycl.cpp, line:15352, func:operator()\r\nSYCL error: CHECK_TRY_ERROR(op(src0, src1, dst, src0_dd_i, src1_ddf_i, src1_ddq_i, dst_dd_i, dev[i].row_low, dev[i].row_high, src1_ncols, src1_padded_col_size, stream)): Meet error in this line code!\r\n  in function ggml_sycl_op_mul_mat at /home/runner/_work/llm.cpp/llm.cpp/llama-cpp-bigdl/ggml-sycl.cpp:15352\r\nGGML_ASSERT: /home/runner/_work/llm.cpp/llm.cpp/llama-cpp-bigdl/ggml-sycl.cpp:3021: !\"SYCL error\"\r\nCould not attach to process.  If your uid matches the uid of the target\r\nprocess, check the setting of /proc/sys/kernel/yama/ptrace_scope, or try\r\nagain as the root user.  For more details, see /etc/sysctl.d/10-ptrace.conf\r\nptrace: Inappropriate ioctl for device.\r\nNo stack.\r\nThe program is not being run.\r\n```\r\n\r\n</details>\r\n\r\nI suspect the problem is related to the fact that I'm using a machine with 8 GPUs (given the log statement about a \"sub-group size 8 is not supported on the device\").\r\n\r\nI was able to successfully compile and run the llama.cpp source code myself with no issues, so I believe the problem is related to how exactly the IPEX-LLM version of llama.cpp was compiled.\r\n\r\n(P.S. When I compiled it myself, I used the 2024.0 version of the oneAPI compiler package)\r\n\r\nAny help would be greatly appreciated!\r\n\r\n",
      "state": "closed",
      "author": "player1537",
      "author_type": "User",
      "created_at": "2024-05-20T22:06:05Z",
      "updated_at": "2024-11-01T18:35:52Z",
      "closed_at": "2024-05-21T16:32:20Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11080/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "rnwang04"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11080",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11080",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:03.633566",
      "comments": [
        {
          "author": "rnwang04",
          "body": "Hi @player1537 , I have reproduced this error.\r\nWe will try to fix it, and once it is done, will update here to let you know : )",
          "created_at": "2024-05-21T03:13:01Z"
        },
        {
          "author": "rnwang04",
          "body": "Hi @player1537 ,\r\nI have fixed this issue, maybe you can try it again with `ipex-llm[cpp] >= 2.1.0b20240521` (which will be released tonight).\r\nBy the way, if you have no special requirements for accuracy, we recommend you use Q4_0, which provides the fastest speed on PVC : )",
          "created_at": "2024-05-21T09:13:21Z"
        },
        {
          "author": "player1537",
          "body": "Thank you for the quick update!\r\n\r\nI just wanted to confirm that I was able to get IPEX-LLM working now on the aforementioned machine.\r\n\r\nWe're seeing ~2.2x the inference token/s with IPEX-LLM's llama.cpp as we got with my personally compiled llama.cpp! :)",
          "created_at": "2024-05-21T16:32:20Z"
        },
        {
          "author": "soulyet",
          "body": "but we met same issue with IPEX-LLM v2.1.0b20240819. is there something wrong with my environment?\r\n\r\nGPU: Intel Arc GPU\r\n\r\nlogs:\r\n[llama.cpp_log.txt](https://github.com/user-attachments/files/16746414/llama.cpp_log.txt)\r\n",
          "created_at": "2024-08-26T08:43:48Z"
        },
        {
          "author": "lhl",
          "body": "FYI, I am getting this same error with my Xe2 iGPU (Arc Graphics 140V). Q4_0 works, but Q4_K_M gives me this error. I am using oneAPI Base Toolkit 2024.2.1 w/ llama.cpp (it won't run with 2025.0.0).",
          "created_at": "2024-11-01T18:35:52Z"
        }
      ]
    },
    {
      "issue_number": 12222,
      "title": "Can't run llama model on Intel GPU on Linux platforms",
      "body": "**ipex version**: ipex-llm==2.2.0b20241010\r\n**System**: Ubuntu 22.04\r\n**Intel OneAPI 2024.2**\r\n\r\n\r\n**Description**\r\n\r\nIPEX cannot infer with multi-round chat (i.e. prefill a new prompt without clearing the KV cache)\r\n\r\nWhen we tried a multi-round chat the library crashes.\r\n\r\nThe error message is as following:\r\n\r\n```\r\nllm_chat_cli: /home/runner/_work/llm.cpp/llm.cpp/llm.cpp/bigdl-core-xe/llama_backend/sdp_kernel.cpp:639: void sdp_fp8_casual_kernel(const void *, const uint8_t *, const uint8_t *, void *, const size_t, const size_t, const size_t, const size_t, const size_t, const size_t, const size_t, const size_t, const size_t, const size_t, const size_t, const size_t, const size_t, const size_t, const size_t, const size_t, const size_t, float *, float, sycl::queue &) [GS = 32, HD = 96]: Assertion `(context_length-seq_len)%GS==0 && \"ubatch must be set as the times of GS\\n\"' failed.\r\n\r\n\r\n```\r\n\r\nAlso, it works well on Windows system, everything is just fine without modofying any codes.",
      "state": "closed",
      "author": "acane77",
      "author_type": "User",
      "created_at": "2024-10-17T07:37:38Z",
      "updated_at": "2024-10-31T02:08:15Z",
      "closed_at": "2024-10-31T02:08:15Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12222/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "qiuxin2012"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12222",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12222",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:05.618331",
      "comments": [
        {
          "author": "acane77",
          "body": "We tried `ipex-llm[cpp]==2.2.0b20240928`, it works well, no problems found and no crashes.\r\n\r\nAlso, the Windows version which works well mentioned above is also this `2.2.0b20240928` version.",
          "created_at": "2024-10-17T07:51:36Z"
        },
        {
          "author": "qiuxin2012",
          "body": "We have reproduced your issue and are currently working on a fix.",
          "created_at": "2024-10-18T07:05:03Z"
        },
        {
          "author": "qiuxin2012",
          "body": "It's fixed now，you can update ipex-llm[cpp] to 2.2.0b20241021 and try again. @acane77 ",
          "created_at": "2024-10-22T02:10:21Z"
        },
        {
          "author": "user7z",
          "body": "@acane77 @qiuxin2012 also ipex-llm works with 2024.0 in linux  & 2024.2 for windows ",
          "created_at": "2024-10-24T06:09:21Z"
        },
        {
          "author": "acane77",
          "body": "> It's fixed now，you can update ipex-llm[cpp] to 2.2.0b20241021 and try again. @acane77\r\n\r\nWe've tried this version and it fixes, thanks",
          "created_at": "2024-10-28T03:14:31Z"
        }
      ]
    },
    {
      "issue_number": 12266,
      "title": "[NPU] Slow Token Generation with Latest NPU Driver 32.0.100.3053 on LNL 226V series ",
      "body": "## Description\r\nObserved less than < 1 token per second generation for model >7B parameters using 32.0.100.3053 driver with latest ipex-llm[npu] on LNL 226V series laptop.\r\n\r\nModel tested: \r\n1. Qwen/Qwen2.5-7B-Instruct  \r\n2. meta-llama/Llama-2-7b-hf\r\n3. meta-llama/Meta-Llama-3-8B-Instruct\r\n\r\nipex-llm[npu] = 2.2.0b20241022\r\nnpu driver = 32.0.10.3053\r\n\r\n\r\n",
      "state": "open",
      "author": "climh",
      "author_type": "User",
      "created_at": "2024-10-25T00:51:32Z",
      "updated_at": "2024-10-25T02:33:50Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12266/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12266",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12266",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:05.822354",
      "comments": []
    },
    {
      "issue_number": 12258,
      "title": "Llamacpp generation incoherent (always <eos>). Driver version on ubuntu 22.04.5?",
      "body": "I've tried using llamacpp in both docker and native versions using the provided guides:\r\nhttps://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/llama_cpp_quickstart.md\r\nhttps://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/DockerGuides/docker_cpp_xpu_quickstart.md\r\nand cannot get correct model output in either case.\r\n\r\nWhen using docker the generation stops at first token as model outputs `<eos>` no matter the prompt.\r\nWhen using the native version I can get an answer but the model quality is heavily degraded, uses a lot of `*` tokens and gets incoherent after a few hundred tokens.\r\nI've tried the latest version of the docker image and pip packages and also used older versions like ipex-llm[cpp]==2.1.0 (pip) that have slight variations but the problem persists.\r\n\r\nI'm using Bartowski's GGUF Q8_0 versions of gemma2 (27b) and llama3.1 (8b) models. The models work fine on pure cpp ggerganov/llamacpp.\r\n\r\nThis is the output of the env-check script:\r\n\r\n```\r\n-----------------------------------------------------------------\r\nPYTHON_VERSION=3.11.10\r\n-----------------------------------------------------------------\r\ntransformers=4.44.2\r\n-----------------------------------------------------------------\r\ntorch=2.2.0+cu121\r\n-----------------------------------------------------------------\r\nipex-llm Version: 2.2.0b20240910\r\n-----------------------------------------------------------------\r\nIPEX is not installed. \r\n-----------------------------------------------------------------\r\nCPU Information: \r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      46 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             18\r\nOn-line CPU(s) list:                0-17\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Core(TM) Ultra 5 125H\r\nCPU family:                         6\r\nModel:                              170\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 14\r\nSocket(s):                          1\r\nStepping:                           4\r\nCPU max MHz:                        4400,0000\r\nCPU min MHz:                        400,0000\r\nBogoMIPS:                           5990.40\r\n-----------------------------------------------------------------\r\nTotal CPU Memory: 93.6115 GB\r\nMemory Type: DDR5 \r\n-----------------------------------------------------------------\r\nOperating System: \r\nUbuntu 22.04.5 LTS \\n \\l\r\n\r\n-----------------------------------------------------------------\r\nLinux mini 6.5.0-1027-oem #28-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 25 13:32:46 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux\r\n-----------------------------------------------------------------\r\nenv-check.sh: line 148: xpu-smi: command not found\r\n-----------------------------------------------------------------\r\n  Driver Version                                  2023.16.12.0.12_195853.xmain-hotfix\r\n  Driver Version                                  2023.16.12.0.12_195853.xmain-hotfix\r\n  Driver UUID                                     32342e33-352e-3330-3837-320000000000\r\n  Driver Version                                  24.35.30872\r\n-----------------------------------------------------------------\r\nDriver related package version:\r\nii  intel-fw-gpu                                   2024.24.5-337~22.04                     all          Firmware package for Intel integrated and discrete GPUs\r\nii  intel-i915-dkms                                1.24.5.15.240718.18+i24-1               all          Out of tree i915 driver.\r\nii  intel-level-zero-gpu                           1.3.29735.27-914~22.04                  amd64        Intel(R) Graphics Compute Runtime for oneAPI Level Zero.\r\n-----------------------------------------------------------------\r\nigpu detected\r\n[opencl:gpu:2] Intel(R) OpenCL Graphics, Intel(R) Arc(TM) Graphics OpenCL 3.0 NEO  [24.35.30872]\r\n[ext_oneapi_level_zero:gpu:0] Intel(R) Level-Zero, Intel(R) Arc(TM) Graphics 1.3 [1.3.29735]\r\n-----------------------------------------------------------------\r\nxpu-smi is not installed. Please install xpu-smi according to README.md\r\n```\r\n\r\nI suspect the problem is related to the driver version 24.35.30872 which is lower than the 31.0.101.5522 specified on the FAQ of the llama_cpp_quickstart.md guide. I've followed the instructions on https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/install_linux_gpu.md#install-gpu-driver (option 1 for kernel 6.5) and the version that gets installed via apt is 24.35.30872.\r\n",
      "state": "open",
      "author": "ultoris",
      "author_type": "User",
      "created_at": "2024-10-23T23:48:09Z",
      "updated_at": "2024-10-24T08:54:49Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12258/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "JinheTang",
        "rnwang04"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12258",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12258",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:05.822377",
      "comments": [
        {
          "author": "JinheTang",
          "body": "Hi @ultoris , we didn't reproduce this issue on our native Linux MTL and the llama-3.1 Q8_0 model output is normal:\r\n```log\r\n./llama-cli -m Meta-Llama-3.1-8B-Instruct-q8_0.gguf -n 32 --prompt \"Once upon a time, there existed a little girl who liked to have adventures. She wanted to go to places and ",
          "created_at": "2024-10-24T08:49:47Z"
        }
      ]
    },
    {
      "issue_number": 12254,
      "title": "Codegeex Nano model runs benchmark slower than qwen1.5-1.8B(0.5 times)",
      "body": "MECHREVO  NUC \r\nWindows 11\r\nUltra5 125H  32G RAM\r\nIpex-llm =2.1.0b2\r\n\r\nWhen we test benchamark , we find the speed of codegeex nano  is very slower than qwen1.8B, about 0.5 times . Result as below:\r\n**Codegeex nano chat**\r\n![image](https://github.com/user-attachments/assets/02c060bb-cfba-46bc-8290-7fcf4ecc17e5)\r\n**Codegeex nano infilling**\r\n![image](https://github.com/user-attachments/assets/f123e156-c304-457f-af55-2d944cff076c)\r\nqwen1.5-1.8B\r\n![image](https://github.com/user-attachments/assets/7774b178-5d89-4cf1-81df-5fc49f62488b)\r\n\r\nPls help us optimize the model .\r\n\r\n\r\n\r\n",
      "state": "open",
      "author": "johnysh",
      "author_type": "User",
      "created_at": "2024-10-23T09:12:12Z",
      "updated_at": "2024-10-24T02:07:59Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12254/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12254",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12254",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:06.037159",
      "comments": []
    },
    {
      "issue_number": 11340,
      "title": "Cannot find dGPU when install ollama on Windows",
      "body": "When \"pip install ipex-llm[cpp]\", then \"init-ollama.bat\", it runs on CPU:\r\n\" ... msg=\"inference compute\" id=0 library=cpu compute=\"\" driver=0.0 name=\"\" total=\"31.6 GiB\" ... \"\r\n\r\nBut when \"pip install ipex-llm[xpu]\", it can run on my A770 dGPU. \r\n\r\nWhen install them both \"pip install ipex-llm[cpp,xpu]\", i got this error:\r\n\r\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\nbigdl-core-cpp 2.5.0b20240616 requires torch==2.2.0, but you have torch 2.1.0a0+cxx11.abi which is incompatible.\r\n\r\n",
      "state": "open",
      "author": "YunLiu1",
      "author_type": "User",
      "created_at": "2024-06-17T10:55:01Z",
      "updated_at": "2024-10-22T03:21:09Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11340/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11340",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11340",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:06.037185",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @YunLiu1,\n\n1. `msg=\"inference compute\" id=0 library=cpu` is a confusing and useless runtime log, and it does not mean that ollama is running on CPU. To ensure that it's running on the dGPU, you may follow the steps below:\n    - Check the output from the ollama server. When running successfully on",
          "created_at": "2024-06-17T23:23:08Z"
        },
        {
          "author": "samamiller",
          "body": "> To run ipex-llm ollama on your dGPU, you only need to install ipex-llm[cpp]\r\n\r\nIs there documentation that explains these options? What does installing ipex-llm[xpu] do that ipex-llm[cpp] doesn't?  Why do all the examples in the quickstart folder seem to all use different options and versions none",
          "created_at": "2024-10-21T07:21:55Z"
        },
        {
          "author": "sgwhat",
          "body": "> Is there documentation that explains these options? What does installing ipex-llm[xpu] do that ipex-llm[cpp] doesn't? Why do all the examples in the quickstart folder seem to all use different options and versions none of them seem to be compatible?\r\n\r\nHi @samamiller, you may see our [official doc",
          "created_at": "2024-10-22T01:24:40Z"
        },
        {
          "author": "jason-dai",
          "body": "> > To run ipex-llm ollama on your dGPU, you only need to install ipex-llm[cpp]\r\n> \r\n> Is there documentation that explains these options? What does installing ipex-llm[xpu] do that ipex-llm[cpp] doesn't? Why do all the examples in the quickstart folder seem to all use different options and versions",
          "created_at": "2024-10-22T03:21:07Z"
        }
      ]
    },
    {
      "issue_number": 12145,
      "title": "Confyui ",
      "body": "Please support Confyui and modern image making models like flux",
      "state": "open",
      "author": "ayttop",
      "author_type": "User",
      "created_at": "2024-09-28T22:52:52Z",
      "updated_at": "2024-10-22T03:19:13Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12145/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12145",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12145",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:06.252329",
      "comments": [
        {
          "author": "ayttop",
          "body": "???????",
          "created_at": "2024-09-30T19:45:48Z"
        },
        {
          "author": "ayttop",
          "body": "?????????",
          "created_at": "2024-09-30T20:19:22Z"
        },
        {
          "author": "ayttop",
          "body": "how to run flux on ipex-llm??????",
          "created_at": "2024-09-30T20:19:55Z"
        },
        {
          "author": "glorysdj",
          "body": "Sorry, we don't have any plan to support Confyui and flux yet. ",
          "created_at": "2024-10-08T02:54:24Z"
        }
      ]
    },
    {
      "issue_number": 12156,
      "title": "Ubuntu 22.04 Kernel 6.8.0-45 cannot install intel-i915-dkms",
      "body": "Hi, I tried to install intel-i915-dkms with command \r\n\r\n```\r\nsudo apt install -y intel-i915-dkms\r\n```\r\n\r\non Ubuntu 22.04 with Kernel 6.8.0-45-generic and dpkg returns error:\r\n\r\n```\r\nReading package lists... Done\r\nBuilding dependency tree... Done\r\nReading state information... Done\r\nintel-fw-gpu is already the newest version (2024.17.5-329~22.04).\r\nThe following package was automatically installed and is no longer required:\r\n  libgl1-amber-dri\r\nUse 'sudo apt autoremove' to remove it.\r\nThe following NEW packages will be installed:\r\n  intel-i915-dkms\r\n0 upgraded, 1 newly installed, 0 to remove and 56 not upgraded.\r\nNeed to get 0 B/3,022 kB of archives.\r\nAfter this operation, 19.3 MB of additional disk space will be used.\r\nSelecting previously unselected package intel-i915-dkms.\r\n(Reading database ... 191044 files and directories currently installed.)\r\nPreparing to unpack .../intel-i915-dkms_1.24.3.23.240419.26+i30-1_all.deb ...\r\nUnpacking intel-i915-dkms (1.24.3.23.240419.26+i30-1) ...\r\nSetting up intel-i915-dkms (1.24.3.23.240419.26+i30-1) ...\r\nLoading new intel-i915-dkms-1.24.3.23.240419.26 DKMS files...\r\nAUXILIARY_BUS is enabled for 6.8.0-45-generic.\r\nBuilding for 6.8.0-45-generic\r\nBuilding initial module for 6.8.0-45-generic\r\nAUXILIARY_BUS is enabled for 6.8.0-45-generic.\r\nError! Bad return status for module build on kernel: 6.8.0-45-generic (x86_64)\r\nConsult /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/make.log for more information.\r\ndpkg: error processing package intel-i915-dkms (--configure):\r\n installed intel-i915-dkms package post-installation script subprocess returned error exit status 10\r\nErrors were encountered while processing:\r\n intel-i915-dkms\r\nE: Sub-process /usr/bin/dpkg returned an error code (1)\r\n```\r\n\r\n/var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/make.log:\r\n```\r\nDKMS make.log for intel-i915-dkms-1.24.3.23.240419.26 for kernel 6.8.0-45-generic (x86_64)\r\nWednesday, October 02, 2024 AM11:54:57 HKT\r\nGenerating local configuration database from kernel ... done.\r\nmake[1]: Nothing to be done for 'updateconfig'.\r\nx86_64-linux-gnu-gcc-12 -Wall -Wmissing-prototypes -Wstrict-prototypes -O2 -fomit-frame-pointer   -c -o conf.o conf.c\r\nflex -ozconf.lex.c -L zconf.l\r\nbison -ozconf.tab.c -t -l zconf.y\r\nx86_64-linux-gnu-gcc-12 -Wall -Wmissing-prototypes -Wstrict-prototypes -O2 -fomit-frame-pointer   -c -o zconf.tab.o zconf.tab.c\r\nx86_64-linux-gnu-gcc-12   conf.o zconf.tab.o   -o conf\r\n#\r\n# configuration written to .config\r\n#\r\nmake[2]: Nothing to be done for 'updateconfig'.\r\nmake[4]: Nothing to be done for 'updateconfig'.\r\nmake[5]: 'conf' is up to date.\r\n#\r\n# configuration written to .config\r\n#\r\nBuilding backport-include/backport/autoconf.h ... done.\r\nBuilding backport-include/backport/backport_path.h ... done.\r\nexpr: non-integer argument\r\nwarning: the compiler differs from the one used to build the kernel\r\n  The kernel was built by: x86_64-linux-gnu-gcc-12 (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0\r\n  You are using:           gcc-12 (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0\r\ncat: versions: No such file or directory\r\ncat: versions: No such file or directory\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/compat/main.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/compat/backport-nodrm.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/compat/backport-5.19.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/compat/slub.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/compat/slab.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/compat/ptrace.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/compat/vmscan.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/compat/swap.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/compat/dma-resv.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/compat/swiotlb.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/compat/hdmi.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/watchdog/mei_wdt.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/compat/power_runtime.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/misc/mei/hdcp/mei_hdcp.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/platform/x86/intel/pmt/class.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/fabric/debugfs.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/misc/mei/pxp/mei_pxp.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/misc/mei/iaf/mei_iaf.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/compat/proc_fs.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/compat/aer.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/fabric/dev_diag.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/fabric/error.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/fabric/fw.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/fabric/main.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/fabric/mei_iaf_user.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/misc/mei/init.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/misc/mei/hbm.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/fabric/mbdb.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/fabric/netlink.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_driver.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/fabric/ops.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/platform/x86/intel/pmt/telemetry.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_config.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_drm_client.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_getparam.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_ioctl.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_irq.o\r\n  LD [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/compat/i915-compat.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_mitigations.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_module.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/platform/x86/intel/vsec.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/fabric/parallel.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/misc/mei/interrupt.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/platform/x86/intel/pmt/crashlog.o\r\n  LD [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/platform/x86/intel/pmt/pmt_class.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/fabric/port.o\r\n  LD [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/platform/x86/intel/pmt/pmt_telemetry.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/fabric/port_diag.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_params.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/fabric/routing_debug.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_pci.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/misc/mei/client.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_pci_err.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/fabric/routing_engine.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/fabric/routing_event.o\r\n  LD [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/platform/x86/intel/pmt/pmt_crashlog.o\r\n  LD [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/platform/x86/intel/intel_vsec.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/fabric/routing_io.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_scatterlist.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_sysfs.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_sysrq.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/fabric/routing_logic.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/fabric/routing_p2p.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/misc/mei/main.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_utils.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/fabric/routing_pause.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/fabric/routing_topology.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/fabric/statedump.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/intel_device_info.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/intel_iaf.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/intel_memory_region.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/misc/mei/dma-ring.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/fabric/sysfs.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/fabric/trace.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/intel_pch.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/intel_pcode.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/intel_pm.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/misc/mei/bus.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/intel_runtime_pm.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/misc/mei/bus-fixup.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/intel_sbi.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/misc/mei/debugfs.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/intel_step.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/intel_sysfs_mem_health.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/intel_uncore.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/intel_vsec.o\r\n  LD [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/fabric/iaf.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/intel_wakeref.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/misc/mei/mei-trace.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/pvc_ras.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/intel_dram.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/misc/mei/pci-me.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/misc/mei/hw-me.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/dma_resv_utils.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_memcpy.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_mm.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_sw_fence.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/misc/mei/gsc-me.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_sw_fence_work.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_syncmap.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_user_extensions.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/misc/mei/pci-txe.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_ioc32.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_debugfs.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_debugfs_params.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_display_debugfs.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_pipe_crc.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_pmu.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/misc/mei/hw-txe.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_debugger.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/gen8_engine_cs.o\r\n  LD [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/misc/mei/mei-gsc.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/gen8_ppgtt.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_breadcrumbs.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_clos.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_context.o\r\n  LD [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/misc/mei/mei.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_context_sseu.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_context_vm.o\r\n  LD [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/misc/mei/mei-me.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_engine_cs.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_engine_heartbeat.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_engine_pm.o\r\n  LD [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/misc/mei/mei-txe.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_engine_user.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_execlists_submission.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_ggtt.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_gt.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_gt_buffer_pool.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_gt_ccs_mode.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_gt_clock_utils.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_gt_debug.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_gt_debugfs.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_gt_engines_debugfs.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_gt_irq.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_gt_mcr.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_gt_pm.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_gt_pm_debugfs.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_gt_pm_irq.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_gt_requests.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_gt_sysfs.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_gt_sysfs_pm.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_gtt.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_llc.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_lrc.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_mocs.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_pagefault.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_ppgtt.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_rc6.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_region_lmem.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_reset.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_ring.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_rps.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_sa_media.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_sseu.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_sseu_debugfs.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_timeline.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_tlb.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_workarounds.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/shmem_utils.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/sysfs_engines.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/sysfs_gt_errors.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gem/i915_gem_busy.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gem/i915_gem_clflush.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gem/i915_gem_context.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gem/i915_gem_create.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gem/i915_gem_dmabuf.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gem/i915_gem_domain.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gem/i915_gem_execbuffer.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gem/i915_gem_internal.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gem/i915_gem_object.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gem/i915_gem_lmem.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gem/i915_gem_mman.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gem/i915_gem_pages.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gem/i915_gem_pm.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gem/i915_gem_region.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gem/i915_gem_shmem.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gem/i915_gem_shrinker.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gem/i915_gem_stolen.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gem/i915_gem_throttle.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gem/i915_gem_tiling.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gem/i915_gem_userptr.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gem/i915_gem_vm_bind_object.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gem/i915_gem_wait.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gem/i915_gem_wait_user_fence.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gem/i915_gemfs.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_active.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_buddy.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_gem_evict.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_gem_gtt.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_gem_ww.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_gem.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_query.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_request.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_scheduler.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_suspend_fence.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_trace_points.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_vma.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/intel_wopcm.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/uc/intel_uc.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/uc/intel_uc_debugfs.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/uc/intel_uc_fw.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/uc/intel_guc.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/uc/intel_guc_ads.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/uc/intel_guc_capture.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/uc/intel_guc_debugfs.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/uc/intel_guc_fw.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/uc/intel_guc_hwconfig.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/uc/intel_guc_log.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/uc/intel_guc_log_debugfs.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/uc/intel_guc_rc.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/uc/intel_guc_slpc.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/uc/intel_huc.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/uc/intel_huc_debugfs.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/uc/intel_huc_fw.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/uc/intel_gsc_fw.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/uc/intel_gsc_uc.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/intel_gsc.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/spi/intel_spi.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_sriov.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_sriov_sysfs.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/iov/intel_iov.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/iov/intel_iov_debugfs.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/iov/intel_iov_event.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/iov/intel_iov_memirq.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/iov/intel_iov_migration.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/iov/intel_iov_provisioning.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/iov/intel_iov_query.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/iov/intel_iov_relay.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/iov/intel_iov_service.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/iov/intel_iov_state.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/iov/intel_iov_sysfs.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/iov/intel_lmtt.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/iov/xehpsdv_lmtt.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/iov/pvc_lmtt.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/iov/selftests/iov_selftest_actions.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/gt/iov/selftests/iov_selftest_utils.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_hwmon.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_atomic.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_atomic_plane.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_audio.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_bios.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_bw.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_cdclk.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_color.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_combo_phy.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_connector.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_crtc.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_crtc_state_dump.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_cursor.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_display.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_display_power.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_display_power_map.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_display_power_well.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_dmc.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_dpll.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_dpll_mgr.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_dpt.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_drrs.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_dsb.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_fb.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_fb_pin.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_fbc.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_frontbuffer.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_global_state.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_hdcp.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_hdcp_gsc.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_hotplug.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_modeset_verify.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_modeset_setup.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_plane_initial.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_psr.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_quirks.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_sprite.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_tc.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/skl_scaler.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/skl_universal_plane.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_acpi.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_opregion.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/icl_dsi.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_backlight.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_cx0_phy.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_ddi.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_ddi_buf_trans.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_display_trace.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_dp.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_dp_aux.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_dp_aux_backlight.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_dp_hdcp.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_dp_link_training.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_dp_mst.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_dsi.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_dsi_dcs_backlight.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_dsi_vbt.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_gmbus.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_hdmi.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_panel.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_pps.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_qp_tables.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_snps_phy.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_vdsc.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_vrr.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_perf.o\r\n  CC [M]  /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/i915_perf_stall_cntr.o\r\n/var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_dp_mst.c: In function ‘intel_mst_enable_dp’:\r\n/var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_dp_mst.c:777:54: error: passing argument 2 of ‘drm_dp_add_payload_part2’ from incompatible pointer type [-Werror=incompatible-pointer-types]\r\n  777 |         drm_dp_add_payload_part2(&intel_dp->mst_mgr, &state->base,\r\n      |                                                      ^~~~~~~~~~~~\r\n      |                                                      |\r\n      |                                                      struct drm_atomic_state *\r\nIn file included from /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/backport-include/drm/drm_dp_mst_helper.h:27,\r\n                 from /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_display_types.h:38,\r\n                 from /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_dp_mst.c:38:\r\n./include/drm/display/drm_dp_mst_helper.h:854:64: note: expected ‘struct drm_dp_mst_atomic_payload *’ but argument is of type ‘struct drm_atomic_state *’\r\n  854 |                              struct drm_dp_mst_atomic_payload *payload);\r\n      |                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~\r\n/var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_dp_mst.c:777:9: error: too many arguments to function ‘drm_dp_add_payload_part2’\r\n  777 |         drm_dp_add_payload_part2(&intel_dp->mst_mgr, &state->base,\r\n      |         ^~~~~~~~~~~~~~~~~~~~~~~~\r\n./include/drm/display/drm_dp_mst_helper.h:853:5: note: declared here\r\n  853 | int drm_dp_add_payload_part2(struct drm_dp_mst_topology_mgr *mgr,\r\n      |     ^~~~~~~~~~~~~~~~~~~~~~~~\r\ncc1: some warnings being treated as errors\r\nmake[6]: *** [scripts/Makefile.build:243: /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915/display/intel_dp_mst.o] Error 1\r\nmake[6]: *** Waiting for unfinished jobs....\r\nmake[5]: *** [scripts/Makefile.build:481: /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build/drivers/gpu/drm/i915] Error 2\r\nmake[4]: *** [Makefile:1925: /var/lib/dkms/intel-i915-dkms/1.24.3.23.240419.26/build] Error 2\r\nmake[3]: *** [Makefile.build:13: modules] Error 2\r\nmake[2]: *** [Makefile.real:100: modules] Error 2\r\nmake[1]: *** [Makefile:76: modules] Error 2\r\nmake: *** [Makefile:60: default] Error 2\r\n```\r\n\r\nI understand that the documentation at [this link](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/install_linux_gpu.md) suggests using kernel versions 6.2 or 6.5. However, I would like to know if it is possible to install IPEX-LLM on kernel version 6.8.0-45-generic. Thank you.",
      "state": "open",
      "author": "huiwangnick",
      "author_type": "User",
      "created_at": "2024-10-02T04:04:07Z",
      "updated_at": "2024-10-22T03:19:09Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12156/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12156",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12156",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:06.458471",
      "comments": [
        {
          "author": "ayham87",
          "body": "I made the IPEX-LLM installation works on kernel 6.8.0-45-generic using Intel Arc A770. This is highly experimental so do it at your own risk! Here is How:\r\n\r\nYou need to install additional kernel and it is possible to have wireless issue with it so make sure to have ethernet connection\r\n\r\n`sudo apt",
          "created_at": "2024-10-03T07:18:19Z"
        },
        {
          "author": "huiwangnick",
          "body": "@ayham87 I have tried out the approach you provided and seems that the OOT i915 is not enabled on the 6.8.0-45 kernel, so the features brought by OOT i915 cannot be utilized. \r\n\r\nRun `sudo dmesg | grep i915` :\r\n\r\n6.8.0-1006-intel:\r\n```\r\n[    8.523850] i915 0000:03:00.0: Using 32 cores (0-31) for kth",
          "created_at": "2024-10-04T02:35:29Z"
        },
        {
          "author": "ayham87",
          "body": "Thanks @huiwangnick , I think you are right!\r\n\r\n`sudo dmesg | grep i915` \r\n\r\n6.8.0-45:\r\n\r\n```\r\n[    6.717681] i915 0000:03:00.0: [drm] VT-d active for gfx access\r\n[    6.744454] i915 0000:03:00.0: vgaarb: deactivate vga console\r\n[    6.744926] i915 0000:03:00.0: [drm] Local memory IO size: 0x0000000",
          "created_at": "2024-10-04T03:36:09Z"
        },
        {
          "author": "qiyuangong",
          "body": "> Thanks @huiwangnick , I think you are right!\r\n> \r\n> `sudo dmesg | grep i915`\r\n> \r\n> 6.8.0-45:\r\n> \r\n> ```\r\n> [    6.717681] i915 0000:03:00.0: [drm] VT-d active for gfx access\r\n> [    6.744454] i915 0000:03:00.0: vgaarb: deactivate vga console\r\n> [    6.744926] i915 0000:03:00.0: [drm] Local memory",
          "created_at": "2024-10-08T02:32:30Z"
        }
      ]
    },
    {
      "issue_number": 12220,
      "title": "Slow Down / Pod Stuck After Orca Init, Resolves with Barrier Mode False",
      "body": "Hi, I’m encountering an issue when using BigDL Orca in our EKS (Kubernetes) environment.\r\n\r\nDockerfile (quick context)\r\nDockerfile\r\nCopy code\r\nFROM apache/spark-py:v3.4.0 AS pyspark-easy-data\r\n\r\nRUN python3 -m pip install bigdl-chronos[pytorch]==2.4.0 && \\\r\n    python3 -m pip install bigdl-spark3==2.4.0 && \\\r\n    python3 -m pip install prophet==1.1.3\r\nIssue Description:\r\nAfter running main.py, everything seems to complete correctly based on the logs (including the final print statement). However, Kubernetes still shows the pod in a running state, and the pod does not terminate.\r\n\r\nUse Case: We are using BigDL’s AutoProphet forecaster.\r\n\r\nBehavior:\r\n\r\nCase A:\r\n\r\nUsing default Orca context initialization (OrcaContext.barrier_mode=True).\r\nThe program runs normally, logs appear fine (although we do see HDFS command not found, which we are aware of), but the pod remains stuck in a running state after the final print.\r\nExecution time: 1 min 30 secs.\r\nCase B:\r\n\r\nSetting OrcaContext.barrier_mode=False before calling init_orca_context().\r\nThe program runs the same (with the same HDFS command not found message), but this time the pod terminates correctly after the final print.\r\nExecution time: ~2 mins.\r\nRelated Docs:\r\n[OrcaContext.barrier_mode Explanation](https://analytics-zoo.readthedocs.io/en/latest/doc/Orca/Overview/orca-context.html): \"Whether to use Spark barrier execution mode to launch Ray. Default to True. You can set it to False if using Spark below 2.4 or with dynamic allocation enabled.\"\r\n\r\nAdditional Question:\r\nWe are also using the TSDataset class from Chronos and are curious about the following:\r\n\r\nWhy does the TSDataset class support integration with Spark, but does not directly support Spark DataFrames? Instead, it requires conversion from pandas or reading from a parquet file. Is there a reason for this design?\r\n\r\nReference:\r\n\r\n[Chronos TSDataset Docs](https://analytics-zoo.readthedocs.io/en/latest/doc/Chronos/QuickStart/chronos-tsdataset-forecaster-quickstart.html):\r\npython\r\nCopy code\r\ntsdata_train, tsdata_valid, tsdata_test = TSDataset.from_pandas(df, dt_col=\"timestamp\", target_col=\"value\",\r\n                                                                with_split=True, val_ratio=0.1, test_ratio=0.1)\r\nThank you for your help!",
      "state": "open",
      "author": "kahlun",
      "author_type": "User",
      "created_at": "2024-10-17T06:52:32Z",
      "updated_at": "2024-10-22T03:18:33Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12220/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12220",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12220",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:06.648901",
      "comments": []
    },
    {
      "issue_number": 12208,
      "title": "LLM benchmark for chatglm3-6b Unable to run properly",
      "body": "Run LLM benchmark for chatglm3-6b , prompt“OSError: [WinError 126] 找不到指定的模块。 Error loading \"D:\\miniforge3\\envs\\llm\\Lib\\site-packages\\intel_extension_for_pytorch\\bin\\intel-ext-pt-gpu.dll\" or one of its dependencies.”\r\n![题目1](https://github.com/user-attachments/assets/acec5587-16f6-4c70-a288-0e6eaf97027f)\r\nTest environment：Ultra 5 125H CPU，Win11 23H2 Pro，gfx driver-32.0.101.5972\r\nThese are the pip list in my container:\r\n![pip list](https://github.com/user-attachments/assets/2601b0cf-8324-4dc4-b974-32132360632c)\r\nInstallation Steps：\r\n1. install Miniforge3-Windows-x86_64.exe，VSCodeUserSetup-x64-1.89.1.exe，w_BaseKit_p_2024.1.0.595.exe\r\n![安装软件](https://github.com/user-attachments/assets/2704f56b-ce18-4c28-9e35-62c63d323339)\r\n2. conda create -n llm python=3.10 libuv\r\n3. conda activate llm\r\n![llm环境](https://github.com/user-attachments/assets/82e81898-90e6-4255-bc92-d51ec5ed08e0)\r\n4. pip install --pre --upgrade ipex-llm[xpu] --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/cn/\r\n![4](https://github.com/user-attachments/assets/0103be68-32d8-4528-97fe-924c5e909d2e)\r\npip install torch==2.1.0.post3 torchvision==0.16.0.post3 torchaudio==2.1.0.post3 intel-extension-for-pytorch==2.1.40+xpu --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/mtl/cn/\r\n![4-1](https://github.com/user-attachments/assets/807ea8d9-35a0-4edf-82b6-f4337e66a5ff)\r\npip install transformers==4.38.2\r\n![4-2](https://github.com/user-attachments/assets/3e612447-d5b5-4bef-a384-c316ad9a1bc5)\r\npip install omegaconf pandas\r\n![4-3](https://github.com/user-attachments/assets/781f7524-da12-4996-a972-c6b3e6ff6a16)\r\n5. cd D:\\ipex-handson\\script\\all-in-one\r\n6. set SYCL_CACHE_PERSISTENT=1\r\nset BIGDL_LLM_XMX_DISABLED=1\r\n![4-4](https://github.com/user-attachments/assets/ab22d284-f4f6-4f43-a166-585910445a7c)\r\n7. 修改config.yaml\r\n![7](https://github.com/user-attachments/assets/71fe422f-322c-4e7c-9172-8ca6a00193e5)\r\n8. python run.py\r\n![8](https://github.com/user-attachments/assets/8900303d-5a9c-49d6-a3cc-bab2aedf00ac)\r\n\r\n\r\n",
      "state": "open",
      "author": "vincent-wsz",
      "author_type": "User",
      "created_at": "2024-10-15T09:04:54Z",
      "updated_at": "2024-10-22T03:18:24Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12208/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Oscilloscope98"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12208",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12208",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:06.648922",
      "comments": [
        {
          "author": "Oscilloscope98",
          "body": "Hi @vincent-wsz,\r\n\r\nYour ipex and oneAPI version seems to be wrong for `ipex-llm` GPU support on PyTorch 2.1.\r\n\r\nPlease refer to our [GPU installation guide on Windows](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/install_windows_gpu.md#install-ipex-llm-on-windows-wit",
          "created_at": "2024-10-16T02:30:01Z"
        }
      ]
    },
    {
      "issue_number": 12155,
      "title": "ValueError: 'rope_scaling' with meta-llama/Llama-3.2-1B",
      "body": "Hi, I would like to try meta-llama/Llama-3.2-1B in different scenario of IPEX-LLM solutions.\r\n\r\npip list:\r\nbigdl-core-xe-21            2.6.0b20241001\r\nintel-extension-for-pytorch 2.1.10+xpu\r\nintel-openmp                2024.2.1\r\nipex-llm                    2.2.0b20241001\r\ntorch                       2.1.0a0+cxx11.abi\r\ntorchvision                 0.16.0a0+cxx11.abi\r\n\r\nI encountered several errors:\r\n1. All-in-one benchmark (INT4 and FP16):\r\nraise ValueError(\r\n**ValueError:** `rope_scaling` must be a dictionary with with two fields, `type` and `factor`, got {'factor': 32.0, 'high_freq_factor': 4.0, 'low_freq_factor': 1.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}\r\n\r\n2. example/GPU/HuggingFace/LLM/llama3.2\r\nFollowing instructions\r\n\r\npip install transformers==4.45.0\r\npip install accelerate==0.33.0\r\npip install trl \r\n\r\npython ./generate.py --repo-id-or-model-path meta-llama/Llama-3.2-1B **works**\r\nWhat is AI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\r\n\r\nThe following is a list of some of the things that we are working on. We are constantly working on new things.\r\n\r\n\r\nBut going back to all-in-one benchmark\r\n**ImportError:** cannot import name 'BenchmarkWrapper' from 'ipex_llm.utils'\r\n\r\nPlease let us know if we can perform all-in-one benchmark. Thank you.\r\n",
      "state": "open",
      "author": "Kpeacef",
      "author_type": "User",
      "created_at": "2024-10-01T14:57:26Z",
      "updated_at": "2024-10-22T03:18:15Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12155/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "cranechu0131"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12155",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12155",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:06.880651",
      "comments": [
        {
          "author": "cranechu0131",
          "body": "Hi Kpeacef,\r\n\r\nWe have looked into the issue and reproduced the problem. Here are solutions to this issue:\r\n\r\n1. To the error `ValueError: 'rope_scaling'`, refer to [this issue](https://github.com/meta-llama/llama3/issues/299) and upgrade transformers to version 4.43.\r\n2. To the error `ImportError: ",
          "created_at": "2024-10-08T08:14:25Z"
        }
      ]
    },
    {
      "issue_number": 12228,
      "title": "docker container cannot run Qwen2.5 32b awq int4 quantization model （OOM）",
      "body": "docker container version ： ipex-llm-serving-xpu:2.2.0-b2\r\n\r\nstart shell script：\r\n\r\nmodel=\"/llm/models/Qwen/Qwen2.5-32B-Instruct-AWQ\"\r\nserved_model_name=\"Qwen2.5-32B-Instruct-AWQ\"\r\n\r\nexport CCL_WORKER_COUNT=4\r\nexport FI_PROVIDER=shm\r\nexport CCL_ATL_TRANSPORT=ofi\r\nexport CCL_ZE_IPC_EXCHANGE=sockets\r\nexport CCL_ATL_SHM=1\r\n\r\nexport USE_XETLA=OFF\r\nexport SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=2\r\nexport TORCH_LLM_ALLREDUCE=0\r\n\r\nsource /opt/intel/1ccl-wks/setvars.sh\r\n\r\npython -m ipex_llm.vllm.xpu.entrypoints.openai.api_server \\\r\n  --served-model-name $served_model_name \\\r\n  --port 8000 \\\r\n  --model $model \\\r\n  --trust-remote-code \\\r\n  --block-size 8 \\\r\n  --gpu-memory-utilization 0.95 \\\r\n  --device xpu \\\r\n  --dtype float16 \\\r\n  --enforce-eager \\\r\n  --quantization=awq \\\r\n  --load-in-low-bit asym_int4 \\\r\n  --max-model-len 4096 \\\r\n  --max-num-batched-tokens 4096 \\\r\n  --max-num-seqs 12 \\\r\n  --tensor-parallel-size 4     \r\n\r\nerror log：\r\n\r\n/usr/local/lib/python3.11/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libpng16.so.16: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n  warn(\r\n2024-10-18 10:01:26,205 - INFO - intel_extension_for_pytorch auto imported\r\n2024-10-18 10:01:28,551 - INFO - PyTorch version 2.1.0.post2+cxx11.abi available.\r\nINFO 10-18 10:01:28 api_server.py:341] vLLM API server version 0.5.4\r\nINFO 10-18 10:01:28 api_server.py:342] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, load_in_low_bit='asym_int4', model='/llm/models/Qwen/Qwen2.5-32B-Instruct-AWQ', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, download_dir=None, load_format='auto', dtype='float16', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=4096, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=8, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.95, num_gpu_blocks_override=None, max_num_batched_tokens=4096, max_num_seqs=12, max_logprobs=20, disable_log_stats=False, quantization='awq', rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='xpu', scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, num_speculative_tokens=None, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['Qwen2.5-32B-Instruct-AWQ'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, engine_use_ray=False, disable_log_requests=False, max_log_len=None)\r\nINFO 10-18 10:01:28 awq_marlin.py:89] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\r\nINFO 10-18 10:01:29 awq_marlin.py:93] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference\r\nWARNING 10-18 10:01:29 config.py:254] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\nINFO 10-18 10:01:29 config.py:729] Defaulting to use ray for distributed inference\r\n2024-10-18 10:01:31,133\tINFO worker.py:1786 -- Started a local Ray instance.\r\nINFO 10-18 10:01:31 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='/llm/models/Qwen/Qwen2.5-32B-Instruct-AWQ', speculative_config=None, tokenizer='/llm/models/Qwen/Qwen2.5-32B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=xpu, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=Qwen2.5-32B-Instruct-AWQ, use_v2_block_manager=False, enable_prefix_caching=False)\r\nINFO 10-18 10:01:32 ray_gpu_executor.py:113] use_ray_spmd_worker: False\r\nINFO 10-18 10:01:32 ray_gpu_executor.py:116] driver_ip: 192.168.90.161\r\n(pid=19092) /usr/local/lib/python3.11/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libpng16.so.16: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n(pid=19092)   warn(\r\n(pid=19092) 2024-10-18 10:01:34,867 - INFO - intel_extension_for_pytorch auto imported\r\n(pid=19092) 2024-10-18 10:01:36,958 - INFO - PyTorch version 2.1.0.post2+cxx11.abi available.\r\n(pid=19086) /usr/local/lib/python3.11/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libpng16.so.16: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n(pid=19086)   warn(\r\n(pid=19086) 2024-10-18 10:01:40,400 - INFO - intel_extension_for_pytorch auto imported\r\n(pid=19086) 2024-10-18 10:01:42,235 - INFO - PyTorch version 2.1.0.post2+cxx11.abi available.\r\n(pid=19070) /usr/local/lib/python3.11/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libpng16.so.16: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n(pid=19070)   warn(\r\n(pid=19070) 2024-10-18 10:01:46,137 - INFO - intel_extension_for_pytorch auto imported\r\n(pid=19070) 2024-10-18 10:01:47,986 - INFO - PyTorch version 2.1.0.post2+cxx11.abi available.\r\n(pid=19090) /usr/local/lib/python3.11/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libpng16.so.16: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n(pid=19090)   warn(\r\n(pid=19090) 2024-10-18 10:01:51,595 - INFO - intel_extension_for_pytorch auto imported\r\n(pid=19090) 2024-10-18 10:01:53,683 - INFO - PyTorch version 2.1.0.post2+cxx11.abi available.\r\n(WrapperWithLoadBit pid=19090) INFO 10-18 10:01:54 selector.py:127] Cannot use _Backend.FLASH_ATTN backend on XPU.\r\n(WrapperWithLoadBit pid=19090) INFO 10-18 10:01:54 selector.py:76] Using IPEX attention backend.\r\nINFO 10-18 10:01:54 selector.py:127] Cannot use _Backend.FLASH_ATTN backend on XPU.\r\nINFO 10-18 10:01:54 selector.py:76] Using IPEX attention backend.\r\nINFO 10-18 10:01:55 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7a949f051c50>, local_subscribe_port=34455, remote_subscribe_port=None)\r\nINFO 10-18 10:01:55 selector.py:127] Cannot use _Backend.FLASH_ATTN backend on XPU.\r\nINFO 10-18 10:01:55 selector.py:76] Using IPEX attention backend.\r\nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\r\nLoading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:00,  9.47it/s]\r\nLoading safetensors checkpoint shards:  60% Completed | 3/5 [00:00<00:00,  9.61it/s]\r\nLoading safetensors checkpoint shards:  80% Completed | 4/5 [00:00<00:00,  9.45it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:00<00:00,  8.58it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:00<00:00,  8.92it/s]\r\n\r\n2024-10-18 10:01:56,284 - INFO - Converting the current model to asym_int4 format......\r\n2024-10-18 10:01:56,285 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\r\n2024-10-18 10:02:04,861 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\r\n2024-10-18 10:02:07,068 - INFO - Loading model weights took 4.5634 GB\r\n(raylet) [2024-10-18 10:02:31,128 E 18935 18935] (raylet) node_manager.cc:3065: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: b9d5403570e4f1abb46f2737a2c1be4c32058847ebfbb4f52dcbc1ec, IP: 192.168.90.161) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.90.161`\r\n(raylet) \r\n(raylet) Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\r\n(WrapperWithLoadBit pid=19086) 2024-10-18 10:02:44,277 - INFO - Loading model weights took 4.5634 GB\r\nProcess Process-65:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/entrypoints/openai/rpc/server.py\", line 220, in run_rpc_server\r\n    server = AsyncEngineRPCServer(async_engine_args, usage_context, port, load_in_low_bit)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/entrypoints/openai/rpc/server.py\", line 27, in __init__\r\n    self.engine = AsyncLLMEngine.from_engine_args(async_engine_args,\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 43, in from_engine_args\r\n    return super().from_engine_args(engine_args, start_engine_loop, usage_context, stat_loggers)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/engine/async_llm_engine.py\", line 476, in from_engine_args\r\n    engine = cls(\r\n             ^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 29, in __init__\r\n    super().__init__(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/engine/async_llm_engine.py\", line 381, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/engine/async_llm_engine.py\", line 557, in _init_engine\r\n    return engine_class(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/engine/llm_engine.py\", line 255, in __init__\r\n    self.model_executor = executor_class(\r\n                          ^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/executor/ray_xpu_executor.py\", line 35, in __init__\r\n    super().__init__(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/executor/ray_gpu_executor.py\", line 555, in __init__\r\n    super().__init__(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/executor/distributed_gpu_executor.py\", line 25, in __init__\r\n    super().__init__(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/executor/xpu_executor.py\", line 53, in __init__\r\n    self._init_executor()\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/executor/ray_gpu_executor.py\", line 61, in _init_executor\r\n    self._init_workers_ray(placement_group)\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/executor/ray_gpu_executor.py\", line 231, in _init_workers_ray\r\n    self._run_workers(\"load_model\",\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/executor/ray_gpu_executor.py\", line 481, in _run_workers\r\n    ray_worker_outputs = ray.get(ray_worker_outputs)\r\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 2691, in get\r\n    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\r\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 873, in get_objects\r\n    raise value\r\nray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\r\nMemory on the node (IP: 192.168.90.161, ID: b9d5403570e4f1abb46f2737a2c1be4c32058847ebfbb4f52dcbc1ec) where the task (actor ID: 5cbcdf01173a96b6cbe984de01000000, name=WrapperWithLoadBit.__init__, pid=19090, memory used=5.55GB) was running was 32.00GB / 32.00GB (0.999909), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 91d5f409dd64d573ab42451e1e6e45ae8344bfc45cf2637f1a6dc562) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.90.161`. To see the logs of the worker, use `ray logs worker-91d5f409dd64d573ab42451e1e6e45ae8344bfc45cf2637f1a6dc562*out -ip 192.168.90.161. Top 10 memory users:\r\nPID\tMEM(GB)\tCOMMAND\r\n18782\t9.89\tpython -m ipex_llm.vllm.xpu.entrypoints.openai.api_server --served-model-name Qwen2.5-32B-Instruct-A...\r\n19086\t5.74\tray::WrapperWithLoadBit.execute_method\r\n19090\t5.55\tray::WrapperWithLoadBit.execute_method\r\n19070\t5.46\tray::WrapperWithLoadBit.execute_method\r\n19092\t0.21\tray::WrapperWithLoadBit\r\n18602\t0.17\tpython -m ipex_llm.vllm.xpu.entrypoints.openai.api_server --served-model-name Qwen2.5-32B-Instruct-A...\r\n18785\t0.17\t/usr/local/lib/python3.11/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\r\n18877\t0.07\t/usr/bin/python /usr/local/lib/python3.11/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1 ...\r\n19037\t0.05\t/usr/bin/python -u /usr/local/lib/python3.11/dist-packages/ray/dashboard/agent.py --node-ip-address=...\r\n18935\t0.05\t/usr/local/lib/python3.11/dist-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tmp/ray...\r\nRefer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\r\n/usr/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\n\r\n",
      "state": "closed",
      "author": "sarsmlee",
      "author_type": "User",
      "created_at": "2024-10-18T02:19:58Z",
      "updated_at": "2024-10-18T03:38:43Z",
      "closed_at": "2024-10-18T03:38:43Z",
      "labels": [
        "user issue",
        "multi-arc"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12228/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "ACupofAir"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12228",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12228",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:07.103382",
      "comments": [
        {
          "author": "sarsmlee",
          "body": "found a solution. need to adjust the size of the container startup memory and shared memory.\r\nAfter modification, it can be started normally\r\n\r\ndocker run -itd         \r\n--Net=host\r\n--Device=/dev/dri\r\n-V/path/:/path\r\n-E no_proxy=localhost, 127.0.0.1\r\n--Memory=\"64G\"\r\n--Name=arc_test\r\n--Shm size=\"32g\"",
          "created_at": "2024-10-18T03:38:24Z"
        }
      ]
    },
    {
      "issue_number": 10506,
      "title": "模型推理问题 Model inference issue",
      "body": "我在完成环境的配置后尝试运行example中chatglm2的代码，但是发现结果输出非常慢，gpu也没有跑满，速度远不及在cpu上运行\r\nAfter completing the environment setup, I attempted to run the code for the `chatglm2` example, but I noticed that the output results were extremely slow. Additionally, the GPU was not fully utilized, and the speed was much slower compared to running it on the CPU.\r\n性能监测图片如下：\r\nPerformance monitoring image as follows:\r\n<img width=\"635\" alt=\"image\" src=\"https://github.com/intel-analytics/BigDL/assets/136095349/3898a5e4-0113-44eb-b726-c904d65c0cbc\">\r\n终端截图如下，如下的回答大概用了十分钟才输出结果，但是推理时间却显示为1.89s\r\nThe terminal screenshot is as follows. The response took approximately ten minutes to output the result, but the inference time is displayed as 1.89s.\r\n<img width=\"1227\" alt=\"image\" src=\"https://github.com/intel-analytics/BigDL/assets/136095349/6236af07-b7c0-488c-aa8d-9ee35a0cd641\">\r\n代码如下\r\ncode follows\r\n```python\r\n#\r\n# Copyright 2016 The BigDL Authors.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n#\r\n\r\nimport torch\r\nimport time\r\nimport argparse\r\n\r\nfrom transformers import AutoModel, AutoTokenizer\r\nfrom bigdl.llm import optimize_model\r\n\r\n# you could tune the prompt based on your own model,\r\n# here the prompt tuning refers to https://huggingface.co/THUDM/chatglm2-6b/blob/main/modeling_chatglm.py#L1007\r\nCHATGLM_V2_PROMPT_FORMAT = \"问：{prompt}\\n\\n答：\"\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser(description='Predict Tokens using `generate()` API for ChatGLM2 model')\r\n    # parser.add_argument('--repo-id-or-model-path', type=str, default=\"THUDM/chatglm2-6b\",\r\n    #                     help='The huggingface repo id for the ChatGLM2 model to be downloaded'\r\n    #                          ', or the path to the huggingface checkpoint folder')\r\n    parser.add_argument('--prompt', type=str, default=\"AI是什么？\",\r\n                        help='Prompt to infer')\r\n    parser.add_argument('--n-predict', type=int, default=32,\r\n                        help='Max tokens to predict')\r\n\r\n    args = parser.parse_args()\r\n    model_path = r'D:\\Code\\chatglm2-6b'\r\n\r\n    # Load model\r\n    model = AutoModel.from_pretrained(model_path,\r\n                                      trust_remote_code=True,\r\n                                      torch_dtype='auto',\r\n                                      low_cpu_mem_usage=True)\r\n\r\n    # With only one line to enable BigDL-LLM optimization on model\r\n    # When running LLMs on Intel iGPUs for Windows users, we recommend setting `cpu_embedding=True` in the optimize_model function.\r\n    # This will allow the memory-intensive embedding layer to utilize the CPU instead of iGPU.\r\n    model = optimize_model(model)\r\n\r\n    model = model.to('xpu')\r\n\r\n    # Load tokenizer\r\n    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\r\n    \r\n    # Generate predicted tokens\r\n    with torch.inference_mode():\r\n        prompt = CHATGLM_V2_PROMPT_FORMAT.format(prompt=args.prompt)\r\n        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to('xpu')\r\n        # ipex model needs a warmup, then inference time can be accurate\r\n        output = model.generate(input_ids,\r\n                                max_new_tokens=args.n_predict)\r\n\r\n        # start inference\r\n        st = time.time()\r\n        output = model.generate(input_ids,\r\n                                max_new_tokens=args.n_predict)\r\n        torch.xpu.synchronize()\r\n        end = time.time()\r\n        output = output.cpu()\r\n        output_str = tokenizer.decode(output[0], skip_special_tokens=True)\r\n        print(f'Inference time: {end-st} s')\r\n        print('-'*20, 'Output', '-'*20)\r\n        print(output_str)\r\n",
      "state": "closed",
      "author": "SJF-ECNU",
      "author_type": "User",
      "created_at": "2024-03-22T02:44:36Z",
      "updated_at": "2024-10-17T08:00:29Z",
      "closed_at": "2024-10-17T08:00:29Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/10506/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/10506",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/10506",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:07.311778",
      "comments": [
        {
          "author": "SJF-ECNU",
          "body": "我再次运行了一次，这里显示的推理时间异常地长\r\n<img width=\"1064\" alt=\"image\" src=\"https://github.com/intel-analytics/BigDL/assets/136095349/071fb332-24fe-47d8-8137-ad572198a1e5\">\r\n",
          "created_at": "2024-03-22T03:14:31Z"
        },
        {
          "author": "jason-dai",
          "body": "Please try https://bigdl.readthedocs.io/en/latest/doc/LLM/Quickstart/benchmark_quickstart.html",
          "created_at": "2024-03-22T03:20:15Z"
        },
        {
          "author": "qiuxin2012",
          "body": "It looks like you forget `set SYCL_CACHE_PERSISTENT=1`, see https://bigdl.readthedocs.io/en/latest/doc/LLM/Overview/install_gpu.html#runtime-configuration.",
          "created_at": "2024-03-25T02:22:31Z"
        },
        {
          "author": "SJF-ECNU",
          "body": "问题是没有进行预热",
          "created_at": "2024-10-17T08:00:29Z"
        }
      ]
    },
    {
      "issue_number": 12210,
      "title": "Qwen2 Deployment by Ollama fail",
      "body": "Qwen2 Deployment by Ollama fail,prompt\"Ollama_llama_server无法找到入口\"\r\n![题目2](https://github.com/user-attachments/assets/96df807c-2f06-462f-bc43-9c9de533875e)\r\nTest environment：Ultra 5 125H CPU，Win11 23H2 Pro，gfx driver-32.0.101.5972\r\nThese are the pip list in my container:\r\n![llm-cpp pip list](https://github.com/user-attachments/assets/72e4ec95-8cdc-46c1-b3cd-fdb465bf81b0)\r\nInstallation Steps：\r\n1. install Miniforge3-Windows-x86_64.exe，VSCodeUserSetup-x64-1.89.1.exe，w_BaseKit_p_2024.1.0.595.exe\r\n2. 管理员打开Miniforge Prompt\r\n3. conda create -n llm-cpp python=3.11\r\n4. conda activate llm-cpp\r\n5. pip install --pre --upgrade ipex-llm[cpp]\r\ncall \"C:\\Program Files (x86)\\Intel\\oneAPI\\setvars.bat“\r\n6. cd D:\\ipex-handson\r\n7. mkdir ollama\r\n8. cd ollama\r\n9. init-ollama.bat\r\n10. set OLLAMA_NUM_GPU=999\r\nset no_proxy=localhost,127.0.0.1\r\nset ZES_ENABLE_SYSMAN=1\r\nset SYCL_CACHE_PERSISTENT=1\r\n11. ollama serve\r\n![ollama serve](https://github.com/user-attachments/assets/f291e6d6-d5e1-49b5-bc6a-a99086829beb)\r\n12. 管理员再重新打开一个Miniforge Prompt\r\n13. conda activate llm-cpp\r\n14. call \"C:\\Program Files (x86)\\Intel\\oneAPI\\setvars.bat“\r\n15. cd D:\\ipex-handson\\ollama\r\n16. ollama.exe pull qwen2:7b\r\n![qwen2](https://github.com/user-attachments/assets/b7e9b102-a49a-48c5-bd59-9bffc738ae90)\r\n17. curl http://localhost:11434/api/generate -d \"{\\\"model\\\": \\\"qwen2:7b\\\", \\\"prompt\\\": \\\"Why is the sky blue?\\\", \\\"stream\\\": false,\\\"options\\\": {\\\"num_predict\\\": 100}} “\r\n![ollama报错](https://github.com/user-attachments/assets/00cdc463-b3e6-41bc-ac30-e7baff2f47ae)\r\n\r\n",
      "state": "open",
      "author": "vincent-wsz",
      "author_type": "User",
      "created_at": "2024-10-15T09:52:09Z",
      "updated_at": "2024-10-16T09:14:14Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12210/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12210",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12210",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:07.536943",
      "comments": [
        {
          "author": "vincent-wsz",
          "body": "\r\n<https://1drv.ms/i/s!Ao-yo1PGhfPm9jlpyNltSUCei41h>\r\n[https://buyvwq.bl.files.1drv.com/y4mgC1mTDn3ArCRGF1FtXc3guYEMODNU7dscmB2y9ZFe5HzOkS_OJ0b69-cncjlUT_lF_FG7hOIOkSjEQyPZQ4pi9OVMuzpUHJKPfETwO6eP0tNSoysWsJGg5ryAtUOtAtHqHQBxrf_p4b-lCwTP1gZ2SPms-e4MqMbqQMaqgzRU4WVxkhLv9exkPjeKfdJKm-fDBfb5PtG2IZmHRLe6",
          "created_at": "2024-10-16T09:13:00Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @vincent-wsz, please follow our [official document](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.md) to install ollama, and you may run ollama without `call \"C:\\Program Files (x86)\\Intel\\oneAPI\\setvars.bat“`.",
          "created_at": "2024-10-16T09:14:12Z"
        }
      ]
    },
    {
      "issue_number": 9321,
      "title": "it is too slower for LLM inference with beam=2 on ARC770",
      "body": "the speed is 5.x slower with beam searching than greedy mode, does BigDL-LLM not optimize for beam searching mode?",
      "state": "closed",
      "author": "Fred-cell",
      "author_type": "User",
      "created_at": "2023-10-31T09:21:44Z",
      "updated_at": "2024-10-16T06:36:30Z",
      "closed_at": "2024-10-16T06:36:30Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/9321/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "MeouSker77"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/9321",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/9321",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:09.472395",
      "comments": [
        {
          "author": "MeouSker77",
          "body": "Could you tell us which model are you running so that we can reproduce this?",
          "created_at": "2023-11-01T01:26:10Z"
        },
        {
          "author": "MeouSker77",
          "body": "Could you try the latest `bigdl-llm[xpu]==2.4.0b20231101` ? It should fix this problem.",
          "created_at": "2023-11-02T02:17:12Z"
        },
        {
          "author": "hkvision",
          "body": "@Fred-cell Any update on this issue? Does the current speed satisfy your needs?",
          "created_at": "2023-11-27T06:49:52Z"
        }
      ]
    },
    {
      "issue_number": 10478,
      "title": "`bigdl-llm-init` can‘t properly set environments variable",
      "body": "Hi team bigdl, `OMP_NUM_THREADS` and `LD_PRELOAD` can't seem to be set When I do `source bigdl-llm-init`  even though `env-chech.sh` has output the correct device information.\r\n\r\n\r\n+++++ Env Variables +++++\r\nLD_PRELOAD            = \r\nOMP_NUM_THREADS       = \r\n+++++++++++++++++++++++++\r\nComplete.\r\n\r\n\r\nDevice Info\r\n-----------------------------------------------------------------\r\nPYTHON_VERSION=3.9.18\r\n-----------------------------------------------------------------\r\ntransformers=4.31.0\r\n-----------------------------------------------------------------\r\ntorch=2.1.0a0+cxx11.abi\r\n-----------------------------------------------------------------\r\nBigDL Version: 2.5.0b20240319\r\n-----------------------------------------------------------------\r\nipex=2.1.10+xpu\r\n-----------------------------------------------------------------\r\nCPU Information: \r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      52 bits physical, 57 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             224\r\nOn-line CPU(s) list:                0-223\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) Platinum 8480+\r\nCPU family:                         6\r\nModel:                              143\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 56\r\nSocket(s):                          2\r\nStepping:                           8\r\nCPU max MHz:                        3800.0000\r\nCPU min MHz:                        800.0000\r\nBogoMIPS:                           4000.00\r\n-----------------------------------------------------------------\r\nMemTotal:       528076344 kB\r\n-----------------------------------------------------------------\r\nxpu-smi is properly installed. \r\n-----------------------------------------------------------------\r\n\r\n\r\nbtw, Will `bigdl-llm-init` effectively impove training/inference time? \r\n",
      "state": "closed",
      "author": "Cyberpunk1210",
      "author_type": "User",
      "created_at": "2024-03-20T05:36:33Z",
      "updated_at": "2024-10-16T06:30:54Z",
      "closed_at": "2024-10-16T06:30:54Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/10478/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "MeouSker77"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/10478",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/10478",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:09.715905",
      "comments": [
        {
          "author": "MeouSker77",
          "body": "this is because `intel-openmp` is not found.\r\n\r\n`intel-openmp` provides `libiomp5.so`.\r\n\r\nIf you are using conda, then we assume `libiomp5.so` is located in `$CONDA_PREFIX/lib/libiomp5.so`, please check whether it exists. If not, please install it by `pip install intel-openmp`. If it already exists ",
          "created_at": "2024-03-21T05:01:01Z"
        }
      ]
    },
    {
      "issue_number": 11073,
      "title": "Phi3-4k winograde drop from 0515 version to 0516 version",
      "body": "microsoft/Phi-3-mini-4k-instruct model score on winograde dropped from 0.6958 (0515) to 0.4933 (0516).\r\ntransformers version 4.37.2. \r\nquantization sym_int4. compute on fp16. iGPU mtl platform.",
      "state": "closed",
      "author": "Quallyjiang",
      "author_type": "User",
      "created_at": "2024-05-20T06:06:11Z",
      "updated_at": "2024-10-16T06:30:26Z",
      "closed_at": "2024-10-16T06:30:26Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11073/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "lalalapotter",
        "MeouSker77"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11073",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11073",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:09.919434",
      "comments": [
        {
          "author": "lalalapotter",
          "body": "Cannot reproduce on ARC, will try it on MTL.",
          "created_at": "2024-05-22T02:19:30Z"
        },
        {
          "author": "lalalapotter",
          "body": "Cannot reproduce the accuracy issue on MTL as well, could you please share the modification methods for running qtype=sym_int4 and dtype=fp16, maybe some changes in code or shell scripts.",
          "created_at": "2024-05-22T06:38:15Z"
        },
        {
          "author": "lalalapotter",
          "body": "After sync with user offline, we can reproduce the issue by following command, when batch=8:\r\n\r\n```shell\r\npython run_llb.py --model ipex-llm --pretrained /path/to/model/Phi-3-mini-4k-instruct/ --precision sym_int4 --device xpu --tasks winogrande --batch 8 --no_cache\r\n```\r\n\r\nwill investigate the root",
          "created_at": "2024-05-22T08:53:53Z"
        }
      ]
    },
    {
      "issue_number": 12029,
      "title": "Could you please help to optimize miniCPM3-4B?",
      "body": "I can run miniCPM3-4b, but it is a little slow on MTL iGPU.\r\nCould you please help to optimize miniCPM3-4B?\r\nThanks.",
      "state": "closed",
      "author": "violet17",
      "author_type": "User",
      "created_at": "2024-09-06T01:33:09Z",
      "updated_at": "2024-10-16T06:02:44Z",
      "closed_at": "2024-10-16T06:00:27Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12029/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "MeouSker77"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12029",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12029",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:10.120082",
      "comments": [
        {
          "author": "MeouSker77",
          "body": "please try latest ipex-llm, it should be much faster",
          "created_at": "2024-09-11T01:55:08Z"
        },
        {
          "author": "violet17",
          "body": "Thanks.",
          "created_at": "2024-10-16T06:02:44Z"
        }
      ]
    },
    {
      "issue_number": 12205,
      "title": "Output format improvement",
      "body": "OS: win11 23h2\r\nHW: ultra7 155H  iGPU\r\nSW：ipex-llm 20241014 ， xpu version\r\n\r\nI followed this https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/GPU/Lightweight-Serving to set up chatglm3 serving.\r\nThe client code is :\r\n\r\n client = OpenAI(api_key=\"EMPTY\",base_url=\"http://localhost:8008/v1\")\r\n            start = time.time()\r\n            self.streamer = client.chat.completions.create(\r\n                model='chatglm3-6b',\r\n                messages=[\r\n                    {'role': 'user', 'content': prompt}\r\n                ],\r\n                temperature=0,\r\n                stream=True  # this time, we set stream=True\r\n            )\r\nWhen I asked the question:\" 英特尔® 酷睿™ Ultra 处理器还采用一系列英特尔前沿性能工具和服务。英特尔® Adaptive Boost 旨在通过对处理器进行智能加速，使其在功耗、散热和工作负载允许的情况下以高于额定值的频率运行，实现游戏和多核性能的提升。英特尔® 睿频加速 Max 技术 3.0 会在功耗、散热和工作负载允许的情况下，将关键工作负载分配到相应的内核中。4\r\n\r\n: 英特尔持续创新，不断改进提升其革命性的性能混合架构。英特尔® 酷睿™ Ultra 处理器采用 3D 性能混合架构，将两个内核微架构组合至同一个处理器芯片上。1每一类内核都专为某些特定类型的任务构建，从而在实现超强性能的同时保证耐久续航。\r\n\r\n: 无论办公、协作、创作还是玩游戏，英特尔® 酷睿™ Ultra 处理器都能利用三个专用引擎（CPU、GPU 和 NPU）解锁 AI 能量，创造沉浸式显卡体验，并保持高性能、低功耗运行。\r\n\r\n: 英特尔® 酷睿™ Ultra 处理器旨在随时随地给您带来沉浸式体验，配备独立显卡，提供一流连接配置5并采用先进媒体技术。\r\n\r\n: Performance-core（性能核）针对轻型单线程工作负载如性能和游戏等进行优化，Efficient-core（能效核）则针对大规模、多线程工作负载进行优化。最新的内核类型，低能耗 Efficient-core（能效核）专为多线程可扩展性能和分载后台任务而设计。与此同时，英特尔® 硬件线程调度器会划分工作负载的优先级并管理工作负载的分配，将任务发送到经过优化的内核。3\r\n基于以上信息回答最后的问题，不允许在答案中添加编造成分。\r\n问: core ultra 答案\"\r\n\r\nThe answer is \"您好，我是人工智能助手。关于\"core ultra\"的问题，根据您提供的信息，我可以告诉您，英特尔®酷睿™Ultra 处理器是一款具有先进性能混合架构的处理器，它采用了3D性能混合架构，将两个内核微架构组合至同一个处理器芯片上。这款处理器的核心包括性能核和能效核，分别针对轻型单线程工作负载和大规模、多线程工作负载进行优化。此外，它还配备了独立显卡和先进的媒体技术，旨在为您带来沉浸式体验。希望这些信息对您有所帮助。\"\r\n\r\nI want to remove the first sentence \"您好，我是人工智能助手\". How can I get this result?\r\n\r\n",
      "state": "closed",
      "author": "HoppeDeng",
      "author_type": "User",
      "created_at": "2024-10-15T08:08:22Z",
      "updated_at": "2024-10-16T03:54:47Z",
      "closed_at": "2024-10-16T03:54:47Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12205/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hzjane"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12205",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12205",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:10.335190",
      "comments": [
        {
          "author": "HoppeDeng",
          "body": "After modifying the question to \"core ultra是什么？\", I will get the expected answer.Many thanks! @hzjane @glorysdj ",
          "created_at": "2024-10-16T03:54:08Z"
        },
        {
          "author": "HoppeDeng",
          "body": "Close this issue",
          "created_at": "2024-10-16T03:54:47Z"
        }
      ]
    },
    {
      "issue_number": 12120,
      "title": "cant run ollama using llm-cpp on 12th igpu under linux",
      "body": "#####i start serving with this script : \r\n\r\n#####bash\r\nexport OLLAMA_NUM_GPU=999\r\nexport no_proxy=localhost,127.0.0.1\r\nexport ZES_ENABLE_SYSMAN=1\r\nsource /opt/intel/oneapi/setvars.sh\r\n./ollama serve\r\n#####end\r\n#####here is serve log :\r\n\r\n`:: initializing oneAPI environment ...\r\n   ollama-lunch: BASH_VERSION = 5.2.32(1)-release\r\n   args: Using \"$@\" for setvars.sh arguments: \r\n:: advisor -- latest\r\n:: ccl -- latest\r\n:: compiler -- latest\r\n:: dal -- latest\r\n:: debugger -- latest\r\n:: dev-utilities -- latest\r\n:: dnnl -- latest\r\n:: dpcpp-ct -- latest\r\n:: dpl -- latest\r\n:: ipp -- latest\r\n:: ippcp -- latest\r\n:: mkl -- latest\r\n:: mpi -- latest\r\n:: tbb -- latest\r\n:: vtune -- latest\r\n:: oneAPI environment initialized ::\r\n \r\n2024/09/25 15:24:58 routes.go:1125: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/user/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]\"\r\ntime=2024-09-25T15:24:58.093Z level=INFO source=images.go:753 msg=\"total blobs: 5\"\r\ntime=2024-09-25T15:24:58.093Z level=INFO source=images.go:760 msg=\"total unused blobs removed: 0\"\r\n[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.\r\n\r\n[GIN-debug] [WARNING] Running in \"debug\" mode. Switch to \"release\" mode in production.\r\n - using env:   export GIN_MODE=release\r\n - using code:  gin.SetMode(gin.ReleaseMode)\r\n\r\n[GIN-debug] POST   /api/pull                 --> github.com/ollama/ollama/server.(*Server).PullModelHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/generate             --> github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/chat                 --> github.com/ollama/ollama/server.(*Server).ChatHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/embed                --> github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/embeddings           --> github.com/ollama/ollama/server.(*Server).EmbeddingsHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/create               --> github.com/ollama/ollama/server.(*Server).CreateModelHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/push                 --> github.com/ollama/ollama/server.(*Server).PushModelHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/copy                 --> github.com/ollama/ollama/server.(*Server).CopyModelHandler-fm (5 handlers)\r\n[GIN-debug] DELETE /api/delete               --> github.com/ollama/ollama/server.(*Server).DeleteModelHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/show                 --> github.com/ollama/ollama/server.(*Server).ShowModelHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/blobs/:digest        --> github.com/ollama/ollama/server.(*Server).CreateBlobHandler-fm (5 handlers)\r\n[GIN-debug] HEAD   /api/blobs/:digest        --> github.com/ollama/ollama/server.(*Server).HeadBlobHandler-fm (5 handlers)\r\n[GIN-debug] GET    /api/ps                   --> github.com/ollama/ollama/server.(*Server).ProcessHandler-fm (5 handlers)\r\n[GIN-debug] POST   /v1/chat/completions      --> github.com/ollama/ollama/server.(*Server).ChatHandler-fm (6 handlers)\r\n[GIN-debug] POST   /v1/completions           --> github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (6 handlers)\r\n[GIN-debug] POST   /v1/embeddings            --> github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (6 handlers)\r\n[GIN-debug] GET    /v1/models                --> github.com/ollama/ollama/server.(*Server).ListModelsHandler-fm (6 handlers)\r\n[GIN-debug] GET    /v1/models/:model         --> github.com/ollama/ollama/server.(*Server).ShowModelHandler-fm (6 handlers)\r\n[GIN-debug] GET    /                         --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\r\n[GIN-debug] GET    /api/tags                 --> github.com/ollama/ollama/server.(*Server).ListModelsHandler-fm (5 handlers)\r\n[GIN-debug] GET    /api/version              --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\r\n[GIN-debug] HEAD   /                         --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\r\n[GIN-debug] HEAD   /api/tags                 --> github.com/ollama/ollama/server.(*Server).ListModelsHandler-fm (5 handlers)\r\n[GIN-debug] HEAD   /api/version              --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\r\ntime=2024-09-25T15:24:58.093Z level=INFO source=routes.go:1172 msg=\"Listening on 127.0.0.1:11434 (version 0.3.6-ipexllm-20240925)\"\r\ntime=2024-09-25T15:24:58.099Z level=INFO source=payload.go:30 msg=\"extracting embedded files\" dir=/tmp/ollama719758347/runners\r\ntime=2024-09-25T15:24:58.232Z level=INFO source=payload.go:44 msg=\"Dynamic LLM libraries [cpu cpu_avx cpu_avx2]\"`\r\n\r\n####wheny i try to : ollama run llama3.1:latest\r\n####i get am error with this log :\r\n\r\n`[GIN] 2024/09/25 - 15:28:23 | 200 |      47.421µs |       127.0.0.1 | HEAD     \"/\"\r\n[GIN] 2024/09/25 - 15:28:23 | 200 |   22.833375ms |       127.0.0.1 | POST     \"/api/show\"\r\ntime=2024-09-25T15:28:23.254Z level=INFO source=gpu.go:168 msg=\"looking for compatible GPUs\"\r\ntime=2024-09-25T15:28:23.254Z level=WARN source=gpu.go:560 msg=\"unable to locate gpu dependency libraries\"\r\ntime=2024-09-25T15:28:23.254Z level=WARN source=gpu.go:560 msg=\"unable to locate gpu dependency libraries\"\r\ntime=2024-09-25T15:28:23.274Z level=WARN source=gpu.go:560 msg=\"unable to locate gpu dependency libraries\"\r\ntime=2024-09-25T15:28:23.283Z level=INFO source=gpu.go:280 msg=\"no compatible GPUs were discovered\"\r\ntime=2024-09-25T15:28:23.330Z level=INFO source=memory.go:309 msg=\"offload to cpu\" layers.requested=-1 layers.model=33 layers.offload=0 layers.split=\"\" memory.available=\"[25.1 GiB]\" memory.required.full=\"5.8 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"1.0 GiB\" memory.required.allocations=\"[5.8 GiB]\" memory.weights.total=\"4.7 GiB\" memory.weights.repeating=\"4.3 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"560.0 MiB\" memory.graph.partial=\"677.5 MiB\"\r\ntime=2024-09-25T15:28:23.332Z level=INFO source=server.go:395 msg=\"starting llama server\" cmd=\"/tmp/ollama719758347/runners/cpu_avx2/ollama_llama_server --model /home/user/.ollama/models/blobs/sha256-8eeb52dfb3bb9aefdf9d1ef24b3bdbcfbe82238798c4b918278320b6fcef18fe --ctx-size 8192 --batch-size 512 --embedding --log-disable --n-gpu-layers 999 --no-mmap --parallel 4 --port 43871\"\r\ntime=2024-09-25T15:28:23.334Z level=INFO source=sched.go:450 msg=\"loaded runners\" count=1\r\ntime=2024-09-25T15:28:23.334Z level=INFO source=server.go:595 msg=\"waiting for llama runner to start responding\"\r\ntime=2024-09-25T15:28:23.334Z level=INFO source=server.go:629 msg=\"waiting for server to become available\" status=\"llm server error\"\r\nINFO [main] build info | build=1 commit=\"7cec8b8\" tid=\"137492451709312\" timestamp=1727278103\r\nINFO [main] system info | n_threads=2 n_threads_batch=-1 system_info=\"AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \" tid=\"137492451709312\" timestamp=1727278103 total_threads=12\r\nINFO [main] HTTP server listening | hostname=\"127.0.0.1\" n_threads_http=\"11\" port=\"43871\" tid=\"137492451709312\" timestamp=1727278103\r\nllama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/user/.ollama/models/blobs/sha256-8eeb52dfb3bb9aefdf9d1ef24b3bdbcfbe82238798c4b918278320b6fcef18fe (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\r\nllama_model_loader: - kv   3:                           general.finetune str              = Instruct\r\nllama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\r\nllama_model_loader: - kv   5:                         general.size_label str              = 8B\r\nllama_model_loader: - kv   6:                            general.license str              = llama3.1\r\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\r\nllama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\r\nllama_model_loader: - kv   9:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv  10:                       llama.context_length u32              = 131072\r\nllama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\r\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  17:                          general.file_type u32              = 2\r\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\r\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\r\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\r\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\r\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\r\nllama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\r\nllama_model_loader: - kv  28:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   66 tensors\r\nllama_model_loader: - type q4_0:  225 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\ntime=2024-09-25T15:28:23.586Z level=INFO source=server.go:629 msg=\"waiting for server to become available\" status=\"llm server loading model\"\r\nllm_load_vocab: special tokens cache size = 256\r\nllm_load_vocab: token to piece cache size = 0.7999 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 128256\r\nllm_load_print_meta: n_merges         = 280147\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 131072\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 500000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 131072\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = 8B\r\nllm_load_print_meta: model ftype      = Q4_0\r\nllm_load_print_meta: model params     = 8.03 B\r\nllm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) \r\nllm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct\r\nllm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\r\nllm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: LF token         = 128 'Ä'\r\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: max token length = 256\r\nZE_LOADER_DEBUG_TRACE:Using Loader Library Path: \r\nZE_LOADER_DEBUG_TRACE:Tracing Layer Library Path: libze_tracing_layer.so.1\r\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nggml_sycl_init: SYCL_USE_XMX: yes\r\nggml_sycl_init: found 1 SYCL devices:\r\nllm_load_tensors: ggml ctx size =    0.27 MiB\r\nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 33/33 layers to GPU\r\nllm_load_tensors:      SYCL0 buffer size =  4156.00 MiB\r\nllm_load_tensors:  SYCL_Host buffer size =   281.81 MiB\r\nllama_new_context_with_model: n_ctx      = 8192\r\nllama_new_context_with_model: n_batch    = 512\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 500000.0\r\nllama_new_context_with_model: freq_scale = 1\r\n[SYCL] call ggml_check_sycl\r\nggml_check_sycl: GGML_SYCL_DEBUG: 0\r\nggml_check_sycl: GGML_SYCL_F16: no\r\nfound 1 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|                 Intel Iris Xe Graphics|    1.5|     80|     512|   32| 30843M|            1.3.30872|\r\nllama_kv_cache_init:      SYCL0 KV buffer size =  1024.00 MiB\r\nllama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\r\nllama_new_context_with_model:  SYCL_Host  output buffer size =     2.02 MiB\r\n[1727278109] warming up the model with an empty run\r\nllama_new_context_with_model:      SYCL0 compute buffer size =   576.00 MiB\r\nllama_new_context_with_model:  SYCL_Host compute buffer size =    24.01 MiB\r\nllama_new_context_with_model: graph nodes  = 1062\r\nllama_new_context_with_model: graph splits = 2\r\nollama_llama_server: /home/runner/_work/llm.cpp/llm.cpp/llm.cpp/bigdl-core-xe/llama_backend/sdp_xmx_kernel.cpp:429: auto ggml_sycl_op_sdp_xmx_casual(fp16 *, fp16 *, fp16 *, fp16 *, fp16 *, float *, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, float *, float, bool, sycl::queue &)::(anonymous class)::operator()() const: Assertion `false' failed.\r\ntime=2024-09-25T15:28:29.632Z level=INFO source=server.go:629 msg=\"waiting for server to become available\" status=\"llm server error\"\r\ntime=2024-09-25T15:28:29.883Z level=ERROR source=sched.go:456 msg=\"error loading llama server\" error=\"llama runner process has terminated: signal: aborted (core dumped)\"\r\n[GIN] 2024/09/25 - 15:28:29 | 500 |  6.649310166s |       127.0.0.1 | POST     \"/api/generate\"\r\n\r\n`\r\n",
      "state": "closed",
      "author": "user7z",
      "author_type": "User",
      "created_at": "2024-09-25T14:32:28Z",
      "updated_at": "2024-10-16T02:40:27Z",
      "closed_at": "2024-10-12T21:47:19Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 16,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12120/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "lzivan"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12120",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12120",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:10.531674",
      "comments": [
        {
          "author": "lzivan",
          "body": "Hi, we are trying to reproduce your issue.",
          "created_at": "2024-09-26T19:04:55Z"
        },
        {
          "author": "casperfrx",
          "body": "Running in the same issue here. open-webui:git-3ad003b used to work fine but after trying the latest releases, none of them work anymore. Reverting back to 3ad003b now also runs in sysctl issues. I can't figure out for the life of me what it could be.\r\n\r\n\r\n",
          "created_at": "2024-09-26T19:50:44Z"
        },
        {
          "author": "lzivan",
          "body": "Hi @user7z , which version of oneApi are you using?\r\n\r\nI'm testing fine on Linux, ipex-llm[cpp] == 2.2.0b20240925, oneApi 2024.0\r\n\r\n![image](https://github.com/user-attachments/assets/962f5aef-c961-4e09-8177-1432317716c4)\r\n",
          "created_at": "2024-09-26T19:52:22Z"
        },
        {
          "author": "user7z",
          "body": "@lzivan , i am using the latest ipex-llm , for oneapi i tried with [2024.1.0](https://archlinux.org/packages/extra/x86_64/intel-oneapi-basekit/)\r\n& also [2024.2.1-1](https://aur.archlinux.org/packages?O=0&K=intel-oneapi)\r\nLinux Kernel 6.10.10",
          "created_at": "2024-09-26T20:59:22Z"
        },
        {
          "author": "lzivan",
          "body": "Hi @user7z , `ipex-llm[cpp]` currently supports oneAPI 2024.0 on Linux. You may use oneAPI 2024.0 and have a try again.\r\n\r\nHere is the guide regarding installing oneAPI 2024.0 on Linux: https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/install_linux_gpu.md#install-oneapi\r\n",
          "created_at": "2024-09-26T22:41:11Z"
        }
      ]
    },
    {
      "issue_number": 12168,
      "title": " Unsupported SPIR-V version",
      "body": "I was following this guide: https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/install_windows_gpu.md and everything worked as intented until I tried to follow the last example. When I try to compile it, I get the following error:\r\n![image](https://github.com/user-attachments/assets/70abc657-22bd-4946-a00d-849d39665c51)\r\nAlso tried with this guide https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/llama_cpp_quickstart.md and happened exactly the same. Im new on this, so if there's a solution I ask to be very specific on how to implement it.",
      "state": "closed",
      "author": "Pablou2902",
      "author_type": "User",
      "created_at": "2024-10-09T07:52:17Z",
      "updated_at": "2024-10-14T18:24:12Z",
      "closed_at": "2024-10-14T18:24:12Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12168/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12168",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12168",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:10.762383",
      "comments": [
        {
          "author": "Oscilloscope98",
          "body": "Hi @Pablou2902,\r\n\r\nWould you mind running our [env check scripts](https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/scripts#env-check) and pasting the output here, as well as providing us with your GPU device name & GPU driver version, to give us more information so that we could bett",
          "created_at": "2024-10-10T02:19:41Z"
        },
        {
          "author": "Pablou2902",
          "body": "Hi,\r\nHere you have the output you requested: \r\n[output.txt](https://github.com/user-attachments/files/17356945/output.txt)\r\nAbout the GPU, I'm currently testing Integrated Graphics performance while trying to run LLMs.\r\n-Device Name: Intel(R) Iris(R) Xe Graphics\r\n-Driver version: 30.0.101.3111",
          "created_at": "2024-10-13T20:17:56Z"
        },
        {
          "author": "Oscilloscope98",
          "body": "Hi @Pablou2902,\r\n\r\nIt seems like your GPU driver version is too old. You could [update your GPU driver](https://www.intel.com/content/www/us/en/download/785597/intel-arc-iris-xe-graphics-windows.html) to the latest one and have a try again :)\r\n\r\nPlease let us know for any further problems.",
          "created_at": "2024-10-14T02:15:53Z"
        },
        {
          "author": "Pablou2902",
          "body": "Yeah, this definetely worked. Thank you so much for your help!",
          "created_at": "2024-10-14T18:24:12Z"
        }
      ]
    },
    {
      "issue_number": 12184,
      "title": "Illegal Instruction (Core Dumped) when Running Model with CPU Docker Image",
      "body": "### I encountered the following error when attempting to run the model Qwen/Qwen2.5-0.5B using the CPU version of the Docker image:\r\n\r\n```\r\nroot@Ubuntu:/llm# python vllm_offline_inference.py \r\nWARNING 10-11 06:19:27 ray_utils.py:46] Failed to import Ray with ModuleNotFoundError(\"No module named 'ray'\"). For distributed inference, please install Ray with `pip install ray`.\r\nINFO 10-11 06:19:30 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='Qwen/Qwen2.5-0.5B', speculative_config=None, tokenizer='Qwen/Qwen2.5-0.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cpu, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=Qwen/Qwen2.5-0.5B)\r\nWARNING 10-11 06:19:31 cpu_executor.py:143] Environment variable VLLM_CPU_KVCACHE_SPACE (GB) for CPU backend is not set, using 4 by default.\r\nINFO 10-11 06:19:31 selector.py:42] Using Torch SDPA backend.\r\nINFO 10-11 06:19:32 weight_utils.py:199] Using model weights format ['*.safetensors']\r\n2024-10-11 06:19:33,921 - INFO - Converting the current model to bf16 format......\r\n2024-10-11 06:19:33,921 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\r\n2024-10-11 06:19:34,974 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\r\nINFO 10-11 06:19:34 cpu_executor.py:72] # CPU blocks: 21845\r\nProcessed prompts:   0%|                                                                                                                                                                                                                              | 0/4 [00:00<?, ?it/s]INFO 10-11 06:19:35 pynccl_utils.py:17] Failed to import NCCL library: NCCL only supports CUDA and ROCm backends.\r\nINFO 10-11 06:19:35 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.\r\nIllegal instruction (core dumped)\r\n```\r\n\r\nI tested it on two environments, and the same error occurred on both:\r\n\r\n### 1. Linux environment:\r\n- CPU\r\n```\r\n[root@Ubuntu~]# lscpu\r\nArchitecture:          x86_64\r\nCPU op-mode(s):        32-bit, 64-bit\r\nByte Order:            Little Endian\r\nCPU(s):                16\r\nOn-line CPU(s) list:   0-15\r\nThread(s) per core:    1\r\nCore(s) per socket:    1\r\nSocket(s):             16\r\nNUMA node(s):          2\r\nVendor ID:             GenuineIntel\r\nCPU family:            6\r\nModel:                 63\r\nModel name:            Intel(R) Xeon(R) CPU E5-2620 v3 @ 2.40GHz\r\nStepping:              2\r\nCPU MHz:               2399.998\r\nBogoMIPS:              4799.99\r\nHypervisor vendor:     VMware\r\nVirtualization type:   full\r\nL1d cache:             32K\r\nL1i cache:             32K\r\nL2 cache:              256K\r\nL3 cache:              15360K\r\nNUMA node0 CPU(s):     0-7\r\nNUMA node1 CPU(s):     8-15\r\nFlags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts nopl xtopology tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single ssbd rsb_ctxsw ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 invpcid xsaveopt arat md_clear spec_ctrl intel_stibp flush_l1d arch_capabilities\r\n```\r\n\r\n- Docker image:  intelanalytics/ipex-llm-serving-cpu:2.2.0-SNAPSHOT\r\n\r\n- OS: \r\n```\r\n[root@Ubuntu ~]# cat /etc/os-release \r\nNAME=\"CentOS Linux\"\r\nVERSION=\"7 (Core)\"\r\nID=\"centos\"\r\nID_LIKE=\"rhel fedora\"\r\nVERSION_ID=\"7\"\r\nPRETTY_NAME=\"CentOS Linux 7 (Core)\"\r\nANSI_COLOR=\"0;31\"\r\nCPE_NAME=\"cpe:/o:centos:centos:7\"\r\nHOME_URL=\"https://www.centos.org/\"\r\nBUG_REPORT_URL=\"https://bugs.centos.org/\"\r\n\r\nCENTOS_MANTISBT_PROJECT=\"CentOS-7\"\r\nCENTOS_MANTISBT_PROJECT_VERSION=\"7\"\r\nREDHAT_SUPPORT_PRODUCT=\"centos\"\r\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7\"\r\n```\r\n\r\n- Docker version: 25.0.2\r\n\r\n- Docker run command\r\n```\r\ndocker run -itd \\\r\n        --net=host \\\r\n        --cpuset-cpus=\"0-15\" \\\r\n        --cpuset-mems=\"0\" \\\r\n        -v /root/.cache/huggingface/hub:/llm/models \\\r\n        -e no_proxy=localhost,127.0.0.1 \\\r\n        --memory=\"16G\" \\\r\n        --name=$CONTAINER_NAME \\\r\n        --shm-size=\"8g\" \\\r\n        $DOCKER_IMAGE\r\n```\r\n\r\n- dmesg -T result\r\n```\r\n[Fri Oct 11 13:36:20 2024] traps: pt_main_thread[29610] trap invalid opcode ip:7f80a51e3844 sp:7ffc25109540 error:0 in _C.cpython-311-x86_64-linux-gnu.so[7f80a51d2000+43000]\r\n[Fri Oct 11 14:18:42 2024] traps: pt_main_thread[8717] trap invalid opcode ip:7fc52581e844 sp:7ffd2c3dab00 error:0 in _C.cpython-311-x86_64-linux-gnu.so[7fc52580d000+43000]\r\n```\r\n\r\n- collect_env result\r\n```\r\nroot@Ubuntu # python collect_env.py \r\nCollecting environment information...\r\nWARNING 10-11 06:50:51 ray_utils.py:46] Failed to import Ray with ModuleNotFoundError(\"No module named 'ray'\"). For distributed inference, please install Ray with `pip install ray`.\r\nPyTorch version: 2.3.0+cpu\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.4\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.11.10 (main, Sep  7 2024, 18:35:41) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-3.10.0-1160.108.1.el7.x86_64-x86_64-with-glibc2.35\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      43 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             16\r\nOn-line CPU(s) list:                0-15\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) CPU E5-2620 v3 @ 2.40GHz\r\nCPU family:                         6\r\nModel:                              63\r\nThread(s) per core:                 1\r\nCore(s) per socket:                 1\r\nSocket(s):                          16\r\nStepping:                           2\r\nBogoMIPS:                           4799.99\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts nopl xtopology tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single ssbd rsb_ctxsw ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 invpcid xsaveopt arat md_clear spec_ctrl intel_stibp flush_l1d arch_capabilities\r\nHypervisor vendor:                  VMware\r\nVirtualization type:                full\r\nL1d cache:                          512 KiB (16 instances)\r\nL1i cache:                          512 KiB (16 instances)\r\nL2 cache:                           4 MiB (16 instances)\r\nL3 cache:                           240 MiB (16 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0-7\r\nNUMA node1 CPU(s):                  8-15\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Processor vulnerable\r\nVulnerability L1tf:                 Mitigation; PTE Inversion\r\nVulnerability Mds:                  Mitigation; Clear CPU buffers; SMT Host state unknown\r\nVulnerability Meltdown:             Mitigation; PTI\r\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT Host state unknown\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; Load fences, usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Full retpoline, IBPB\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] intel-extension-for-pytorch==2.2.0\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0+cpu\r\n[pip3] torchaudio==2.2.0+cpu\r\n[pip3] torchvision==0.17.0+cpu\r\n[pip3] triton==3.0.0\r\n[conda] Could not collectROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.2\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n```\r\n\r\n### 2. Windows 11 WSL2 environment:\r\n- CPU\r\n```\r\n[root@Ubuntu~]# lscpu\r\nroot@DESKTOP-HQ9N5M2:~# lscpu\r\nArchitecture:            x86_64\r\n  CPU op-mode(s):        32-bit, 64-bit\r\n  Address sizes:         39 bits physical, 48 bits virtual\r\n  Byte Order:            Little Endian\r\nCPU(s):                  20\r\n  On-line CPU(s) list:   0-19\r\nVendor ID:               GenuineIntel\r\n  Model name:            12th Gen Intel(R) Core(TM) i7-12700H\r\n    CPU family:          6\r\n    Model:               154\r\n    Thread(s) per core:  2\r\n    Core(s) per socket:  10\r\n    Socket(s):           1\r\n    Stepping:            3\r\n    BogoMIPS:            5376.00\r\n    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq vmx ssse3 fm\r\n                         a cx16 sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bm\r\n                         i2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves avx_vnni umip waitpkg gfni vaes vpclmulqdq rdpid movdiri movdir64b fsrm md_clear serialize flush_l1d arch_capabilities\r\nVirtualization features:\r\n  Virtualization:        VT-x\r\n  Hypervisor vendor:     Microsoft\r\n  Virtualization type:   full\r\nCaches (sum of all):\r\n  L1d:                   480 KiB (10 instances)\r\n  L1i:                   320 KiB (10 instances)\r\n  L2:                    12.5 MiB (10 instances)\r\n  L3:                    24 MiB (1 instance)\r\nVulnerabilities:\r\n  Gather data sampling:  Not affected\r\n  Itlb multihit:         Not affected\r\n  L1tf:                  Not affected\r\n  Mds:                   Not affected\r\n  Meltdown:              Not affected\r\n  Mmio stale data:       Not affected\r\n  Retbleed:              Mitigation; Enhanced IBRS\r\n  Spec rstack overflow:  Not affected\r\n  Spec store bypass:     Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\n  Spectre v1:            Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\n  Spectre v2:            Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\n  Srbds:                 Not affected\r\n  Tsx async abort:       Not affected\r\n```\r\n- Docker image:  intelanalytics/ipex-llm-serving-cpu:2.2.0-SNAPSHOT\r\n\r\n- OS: \r\n```\r\nroot@DESKTOP-HQ9N5M2:~# cat /etc/os-release\r\nPRETTY_NAME=\"Ubuntu 22.04.4 LTS\"\r\nNAME=\"Ubuntu\"\r\nVERSION_ID=\"22.04\"\r\nVERSION=\"22.04.4 LTS (Jammy Jellyfish)\"\r\nVERSION_CODENAME=jammy\r\nID=ubuntu\r\nID_LIKE=debian\r\nHOME_URL=\"https://www.ubuntu.com/\"\r\nSUPPORT_URL=\"https://help.ubuntu.com/\"\r\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\r\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\r\nUBUNTU_CODENAME=jammy\r\n```\r\n\r\n- Docker version: 24.0.7\r\n\r\n\r\n\r\nCould this be related to specific CPU instruction sets, or is there a workaround available? \r\n\r\nAny assistance would be appreciated. Thank you!",
      "state": "closed",
      "author": "vauns",
      "author_type": "User",
      "created_at": "2024-10-11T06:55:06Z",
      "updated_at": "2024-10-14T09:50:14Z",
      "closed_at": "2024-10-14T09:45:44Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12184/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "xiangyuT"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12184",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12184",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:10.988042",
      "comments": [
        {
          "author": "vauns",
          "body": "I have checked, and the Intel Xeon E5-2620 v3 does not support AVX-512 or AMX. My PC’s Intel Core i7-12700H also does not support AVX-512 or AMX.\r\n\r\nWhen not using Docker, if I create a Python environment directly in my PC using Conda, then install ipex via pip and run the model, everything works fi",
          "created_at": "2024-10-12T01:26:16Z"
        },
        {
          "author": "xiangyuT",
          "body": "Hi @vauns,\r\nI have tried the docker image `intelanalytics/ipex-llm-serving-cpu:2.2.0-SNAPSHOT` in our environment but I could not reproduce this issue with spr cpu.\r\n\r\n```\r\nArchitecture:            x86_64\r\n  CPU op-mode(s):        32-bit, 64-bit\r\n  Address sizes:         52 bits physical, 57 bits vi",
          "created_at": "2024-10-14T06:15:50Z"
        },
        {
          "author": "vauns",
          "body": "Hi @xiangyuT,\r\n\r\nI discovered that the CPU(Intel(R) Xeon(R) Platinum 8468) you're using supports AVX512 and AMX instruction sets (as indicated by the flag values). Based on this, I believe the issue on my side stems from the fact that the CPU of the Linux server I'm using does not support AVX512 and",
          "created_at": "2024-10-14T09:44:42Z"
        },
        {
          "author": "vauns",
          "body": "I'm just puzzled that when not using Docker, running it on a Windows PC doesn't have this issue.\r\n\r\nThe runtime code in the Docker image seems to have some differences in instruction requirements compared to the code running on a Windows PC.\r\n\r\n I haven't tested it on a Linux server without Docker y",
          "created_at": "2024-10-14T09:50:13Z"
        }
      ]
    },
    {
      "issue_number": 12136,
      "title": "MTL platform with ARC 770 cannot allocate memory block with size lager than 4GB when running vLLM Qwen2-VL-2B",
      "body": "when I run vLLM model like Qwen2-VL-2B with ARC770 on MTL platform, will report error message as below:\r\nRuntimeError: Current platform can NOT allocate memory block with size larger than 4GB! Tried to allocate 6.10 GiB (GPU  0; 15.11 GiB total capacity; 4.84 GiB already allocated; 5.41 GiB reserved in total by PyTorch)\r\n\r\n![Uploading Screenshot from 2024-09-27 15-45-34.png…]()\r\n",
      "state": "open",
      "author": "weijiejx",
      "author_type": "User",
      "created_at": "2024-09-27T07:47:59Z",
      "updated_at": "2024-10-10T05:51:59Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12136/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hzjane"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12136",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12136",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:11.189486",
      "comments": [
        {
          "author": "hzjane",
          "body": "Vllm 0.5.4 does not support qwen2-vl model yet. We will support it in the future 0.6.1 version.",
          "created_at": "2024-09-29T02:12:36Z"
        },
        {
          "author": "weijiejx",
          "body": "Thank you! But I need double confirm, I use ipex to run Qwen2-VL-2B, not OpenVINO, vLLM 0.5.4 not support, right?",
          "created_at": "2024-09-29T02:25:40Z"
        },
        {
          "author": "hzjane",
          "body": "Yes, even the official version of vllm 0.5.4 does not support it until 0.6.1.",
          "created_at": "2024-09-29T02:30:09Z"
        },
        {
          "author": "weijiejx",
          "body": "Thanks again.\r\nOne more question, is any vLLM model available that I can use with vllm 0.5.4? Can you advise me one or two that I can try it.\r\nThanks.",
          "created_at": "2024-09-29T07:21:18Z"
        },
        {
          "author": "hzjane",
          "body": "It is recommended to run Llama Qwen and chatglm models. \r\nfor example: `Llama-2-7b-chat-hf Qwen1.5-7B-Chat chatglm3-6b`.",
          "created_at": "2024-09-29T08:34:29Z"
        }
      ]
    },
    {
      "issue_number": 12139,
      "title": "codegeex-nano can't work on IPEX-LLM 2.1.0, even though it worked well on 2.1.0b2",
      "body": "发现2.1.0版本模型输出不对，看了下ipex-llm 2.1.0里面transformers/model/chatglm2.py，实现和2.1.0b2有很大不同，应该是没对齐, 我不知道为什么做了这么大改动\r\n里面肯定有些错误，\r\n",
      "state": "open",
      "author": "moutainriver",
      "author_type": "User",
      "created_at": "2024-09-27T09:35:41Z",
      "updated_at": "2024-10-09T01:47:56Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12139/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "qiuxin2012"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12139",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12139",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:11.422652",
      "comments": [
        {
          "author": "moutainriver",
          "body": "Assume the same problem should happen on chatGLM2 too.",
          "created_at": "2024-09-27T09:41:13Z"
        },
        {
          "author": "qiuxin2012",
          "body": "**Can you provide the promot who leads to the error output?** Then we can find the reason and solve the error output.",
          "created_at": "2024-10-09T01:42:12Z"
        }
      ]
    },
    {
      "issue_number": 12154,
      "title": "how to get llama-3.2-11B-vision-instruct to work ?",
      "body": "hi how to get llama-3.2 to work with ipex_llm ?\r\nhere's my code.\r\n```\r\nimport requests\r\nimport torch\r\nfrom PIL import Image\r\nfrom transformers import MllamaForConditionalGeneration, AutoProcessor\r\nimport torch\r\nfrom ipex_llm import optimize_model\r\n\r\n\r\nmodel_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\r\nmodel = MllamaForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16)\r\nmodel = optimize_model(model, \r\n                       low_bit='sym_int4')\r\nmodel = model.half().to('xpu')\r\n\r\nprocessor = AutoProcessor.from_pretrained(model_id)\r\n\r\nmessages = [\r\n    [\r\n        {\r\n            \"role\": \"user\", \r\n            \"content\": [\r\n                {\"type\": \"image\"},\r\n                {\"type\": \"text\", \"text\": \"What does the image show?\"}\r\n            ]\r\n        }\r\n    ],\r\n]\r\ntext = processor.apply_chat_template(messages, add_generation_prompt=True)\r\n\r\nurl = \"https://llava-vl.github.io/static/images/view.jpg\"\r\nimage = Image.open(requests.get(url, stream=True).raw)\r\n\r\ninputs = processor(text=text, images=image, return_tensors=\"pt\").to(model.device)\r\noutput = model.generate(**inputs, max_new_tokens=25)\r\nprint(processor.decode(output[0]))\r\n```\r\n\r\nI'm getting error @ model.generate call in the ipex_llm low_bit_linear module when running this sample app. How can i fix this ?\r\n>  File \"C:\\Users\\MTL\\miniconda3\\envs\\ipex-llm-new-transformers\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\r\n>    return forward_call(*args, **kwargs)\r\n>           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n>  File \"C:\\Users\\MTL\\miniconda3\\envs\\ipex-llm-new-transformers\\Lib\\site-packages\\ipex_llm\\transformers\\low_bit_linear.py\", line 798, in forward\r\n>    self.weight.qtype, input_seq_size)\r\n>                       ^^^^^^^^^^^^^^\r\n>UnboundLocalError: cannot access local variable 'input_seq_size' where it is not associated with a value\r\n",
      "state": "open",
      "author": "wallacezq",
      "author_type": "User",
      "created_at": "2024-10-01T01:38:14Z",
      "updated_at": "2024-10-09T01:26:50Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12154/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "plusbang"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12154",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12154",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:11.618116",
      "comments": [
        {
          "author": "plusbang",
          "body": "Hi, @wallacezq , please try the latest ipex-llm (`2.2.0b20241008`) and related example (https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/GPU/PyTorch-Models/Model/llama3.2-vision)",
          "created_at": "2024-10-09T01:26:49Z"
        }
      ]
    },
    {
      "issue_number": 10259,
      "title": "failed to run gemma example on wsl",
      "body": "I created the virtual enviroment with python3.9 as the example suggested, but the environment configuration `pip install --pre --upgrade bigdl-llm[xpu] -f https://developer.intel.com/ipex-whl-stable-xpu` failed because pip could not find a suitable version. ",
      "state": "closed",
      "author": "lidh15",
      "author_type": "User",
      "created_at": "2024-02-27T08:35:21Z",
      "updated_at": "2024-10-09T00:30:46Z",
      "closed_at": "2024-10-09T00:30:46Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/10259/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "qiuxin2012"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/10259",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/10259",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:11.824731",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "What's the detail error message? Could you check your network configuration and pip mirror?\r\nMake sure bigdl-llm >= 2.5.0b20240226 is in your pip mirror, and `https://developer.intel.com/ipex-whl-stable-xpu` can be connected.",
          "created_at": "2024-02-28T02:12:36Z"
        },
        {
          "author": "lidh15",
          "body": "> What's the detail error message? Could you check your network configuration and pip mirror?\n> Make sure bigdl-llm >= 2.5.0b20240226 is in your pip mirror, and `https://developer.intel.com/ipex-whl-stable-xpu` can be connected.\n\nwell, there is no version later than 0225 in my mirror. however, I fou",
          "created_at": "2024-02-28T08:54:35Z"
        },
        {
          "author": "qiuxin2012",
          "body": "Ha, I just try bigdl-llm[default] only installed bigdl-llm, no dependencies.\r\nMaybe you encounter network issues when installing IPEX, you can try this https://bigdl.readthedocs.io/en/latest/doc/LLM/Overview/install_gpu.html#install-bigdl-llm-from-wheel.\r\nVersion 0225 is OK.\r\nI just notice you are u",
          "created_at": "2024-02-29T01:37:19Z"
        }
      ]
    },
    {
      "issue_number": 12151,
      "title": "ModuleNotFoundError: No module named 'vllm.distributed'",
      "body": "Python 3.11.10 | packaged by conda-forge | (main, Sep 22 2024, 14:10:38) [GCC 13.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from vllm import SamplingParams\r\n>>> from ipex_llm.vllm.xpu.engine import IPEXLLMClass as LLM\r\n/home/wxf/miniforge3/envs/llm/lib/python3.11/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\r\n  warnings.warn(\r\n/home/wxf/miniforge3/envs/llm/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n  warn(\r\n2024-09-30 18:40:24,346 - INFO - intel_extension_for_pytorch auto imported\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/wxf/miniforge3/envs/llm/lib/python3.11/site-packages/ipex_llm/vllm/xpu/engine/__init__.py\", line 16, in <module>\r\n    from .engine import IPEXLLMAsyncLLMEngine, IPEXLLMLLMEngine, IPEXLLMClass\r\n  File \"/home/wxf/miniforge3/envs/llm/lib/python3.11/site-packages/ipex_llm/utils/ipex_importer.py\", line 76, in custom_ipex_import\r\n    return RAW_IMPORT(name, globals, locals, fromlist, level)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/wxf/miniforge3/envs/llm/lib/python3.11/site-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 22, in <module>\r\n    from ipex_llm.vllm.xpu.model_convert import _ipex_llm_convert\r\n  File \"/home/wxf/miniforge3/envs/llm/lib/python3.11/site-packages/ipex_llm/utils/ipex_importer.py\", line 76, in custom_ipex_import\r\n    return RAW_IMPORT(name, globals, locals, fromlist, level)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/wxf/miniforge3/envs/llm/lib/python3.11/site-packages/ipex_llm/vllm/xpu/model_convert.py\", line 18, in <module>\r\n    from vllm.distributed import tensor_model_parallel_gather, tensor_model_parallel_all_gather\r\n  File \"/home/wxf/miniforge3/envs/llm/lib/python3.11/site-packages/ipex_llm/utils/ipex_importer.py\", line 76, in custom_ipex_import\r\n    return RAW_IMPORT(name, globals, locals, fromlist, level)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nModuleNotFoundError: No module named 'vllm.distributed'\r\n",
      "state": "open",
      "author": "yangqing-yq",
      "author_type": "User",
      "created_at": "2024-09-30T10:43:38Z",
      "updated_at": "2024-10-08T02:34:23Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12151/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "xiangyuT"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12151",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12151",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:12.029689",
      "comments": [
        {
          "author": "QuentinVitt",
          "body": "+1 \r\n\r\npython 3.11.10 | followed the instructions for [Install ipex-llm](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/install_linux_gpu.md#install-ipex-llm) and and then followed the [Install vllm](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quic",
          "created_at": "2024-10-04T17:43:23Z"
        },
        {
          "author": "xiangyuT",
          "body": "Hi @yangqing-yq @QuentinVitt,\r\nThere is a recent update for out vllm repo but the [vllm installation instruction](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/vLLM_quickstart.md#2-install-vllm) in our quickstart is not updated. The current branch for vllm is `0.5.4` i",
          "created_at": "2024-10-08T02:32:03Z"
        }
      ]
    },
    {
      "issue_number": 11620,
      "title": "Recommendation for using dual-socket Xeon system. Using Langchain-chatchat",
      "body": "The [instructions](https://github.com/intel-analytics/Langchain-Chatchat/blob/ipex-llm/INSTALL_linux_xeon.md) for using Xeon host-only setup mention numactl. On a dual-socket system, how can both sockets (all logical cores) be used?\r\n\r\nUnder the [Start Service](https://github.com/intel-analytics/Langchain-Chatchat/blob/ipex-llm/INSTALL_linux_xeon.md#start-the-service) section, the comments suggest to use only one socket.\r\n\r\n```\r\n# We recommend to use cores in one socket for the best performance.\r\n# You may need to change the 0-47 below accordingly.\r\n```",
      "state": "open",
      "author": "js333031",
      "author_type": "User",
      "created_at": "2024-07-18T12:38:05Z",
      "updated_at": "2024-10-08T02:26:36Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11620/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "gc-fu"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11620",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11620",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:12.218432",
      "comments": [
        {
          "author": "gc-fu",
          "body": "Currently, we do not recommend to utilize multiple sockets due to memory access speed different.  We will investigate to see if we can put some lightweight load into another sockets.",
          "created_at": "2024-07-19T02:20:46Z"
        },
        {
          "author": "js333031",
          "body": "Is there any update to the recommendation? Please see the Jul 18 comment above",
          "created_at": "2024-10-03T21:34:25Z"
        },
        {
          "author": "gc-fu",
          "body": "We cannot use simple config to use dual sockets.  As can be seen [here](https://github.com/intel-analytics/Langchain-Chatchat/blob/ipex-llm/startup.py#L720), the controller, worker, and openai api server are all subprocesses from the main process.  \r\n\r\nTo use the dual sockets, we will need to change",
          "created_at": "2024-10-08T02:26:34Z"
        }
      ]
    },
    {
      "issue_number": 12150,
      "title": "Cannot load shared library libmkl_core.so",
      "body": "\r\nCPU: i5-1335U\r\nRAM: 16GB\r\nOS: Ubuntu 22.04.10\r\nKernel: 6.8.0-45\r\n\r\nlogs:\r\n```txt\r\n2024/09/30 15:34:06 routes.go:1125: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/adwaith/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]\"\r\ntime=2024-09-30T15:34:06.345+05:30 level=INFO source=images.go:753 msg=\"total blobs: 16\"\r\ntime=2024-09-30T15:34:06.345+05:30 level=INFO source=images.go:760 msg=\"total unused blobs removed: 0\"\r\n[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.\r\n\r\n[GIN-debug] [WARNING] Running in \"debug\" mode. Switch to \"release\" mode in production.\r\n - using env:   export GIN_MODE=release\r\n - using code:  gin.SetMode(gin.ReleaseMode)\r\n\r\n[GIN-debug] POST   /api/pull                 --> github.com/ollama/ollama/server.(*Server).PullModelHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/generate             --> github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/chat                 --> github.com/ollama/ollama/server.(*Server).ChatHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/embed                --> github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/embeddings           --> github.com/ollama/ollama/server.(*Server).EmbeddingsHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/create               --> github.com/ollama/ollama/server.(*Server).CreateModelHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/push                 --> github.com/ollama/ollama/server.(*Server).PushModelHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/copy                 --> github.com/ollama/ollama/server.(*Server).CopyModelHandler-fm (5 handlers)\r\n[GIN-debug] DELETE /api/delete               --> github.com/ollama/ollama/server.(*Server).DeleteModelHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/show                 --> github.com/ollama/ollama/server.(*Server).ShowModelHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/blobs/:digest        --> github.com/ollama/ollama/server.(*Server).CreateBlobHandler-fm (5 handlers)\r\n[GIN-debug] HEAD   /api/blobs/:digest        --> github.com/ollama/ollama/server.(*Server).HeadBlobHandler-fm (5 handlers)\r\n[GIN-debug] GET    /api/ps                   --> github.com/ollama/ollama/server.(*Server).ProcessHandler-fm (5 handlers)\r\n[GIN-debug] POST   /v1/chat/completions      --> github.com/ollama/ollama/server.(*Server).ChatHandler-fm (6 handlers)\r\n[GIN-debug] POST   /v1/completions           --> github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (6 handlers)\r\n[GIN-debug] POST   /v1/embeddings            --> github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (6 handlers)\r\n[GIN-debug] GET    /v1/models                --> github.com/ollama/ollama/server.(*Server).ListModelsHandler-fm (6 handlers)\r\n[GIN-debug] GET    /v1/models/:model         --> github.com/ollama/ollama/server.(*Server).ShowModelHandler-fm (6 handlers)\r\n[GIN-debug] GET    /                         --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\r\n[GIN-debug] GET    /api/tags                 --> github.com/ollama/ollama/server.(*Server).ListModelsHandler-fm (5 handlers)\r\n[GIN-debug] GET    /api/version              --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\r\n[GIN-debug] HEAD   /                         --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\r\n[GIN-debug] HEAD   /api/tags                 --> github.com/ollama/ollama/server.(*Server).ListModelsHandler-fm (5 handlers)\r\n[GIN-debug] HEAD   /api/version              --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\r\ntime=2024-09-30T15:34:06.345+05:30 level=INFO source=routes.go:1172 msg=\"Listening on 127.0.0.1:11434 (version 0.3.6-ipexllm-20240930)\"\r\ntime=2024-09-30T15:34:06.346+05:30 level=INFO source=payload.go:30 msg=\"extracting embedded files\" dir=/tmp/ollama966500899/runners\r\ntime=2024-09-30T15:34:06.449+05:30 level=INFO source=payload.go:44 msg=\"Dynamic LLM libraries [cpu_avx cpu_avx2 cpu]\"\r\ntime=2024-09-30T15:34:17.155+05:30 level=INFO source=gpu.go:168 msg=\"looking for compatible GPUs\"\r\ntime=2024-09-30T15:34:17.155+05:30 level=WARN source=gpu.go:560 msg=\"unable to locate gpu dependency libraries\"\r\ntime=2024-09-30T15:34:17.156+05:30 level=WARN source=gpu.go:560 msg=\"unable to locate gpu dependency libraries\"\r\ntime=2024-09-30T15:34:17.158+05:30 level=WARN source=gpu.go:560 msg=\"unable to locate gpu dependency libraries\"\r\ntime=2024-09-30T15:34:17.159+05:30 level=INFO source=gpu.go:280 msg=\"no compatible GPUs were discovered\"\r\ntime=2024-09-30T15:34:17.181+05:30 level=INFO source=memory.go:309 msg=\"offload to cpu\" layers.requested=-1 layers.model=29 layers.offload=0 layers.split=\"\" memory.available=\"[7.1 GiB]\" memory.required.full=\"4.6 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"1.8 GiB\" memory.required.allocations=\"[4.6 GiB]\" memory.weights.total=\"3.3 GiB\" memory.weights.repeating=\"3.0 GiB\" memory.weights.nonrepeating=\"308.2 MiB\" memory.graph.full=\"824.0 MiB\" memory.graph.partial=\"881.1 MiB\"\r\ntime=2024-09-30T15:34:17.182+05:30 level=INFO source=server.go:395 msg=\"starting llama server\" cmd=\"/tmp/ollama966500899/runners/cpu_avx2/ollama_llama_server --model /home/adwaith/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 16384 --batch-size 512 --embedding --log-disable --n-gpu-layers 999 --no-mmap --parallel 4 --port 39525\"\r\ntime=2024-09-30T15:34:17.182+05:30 level=INFO source=sched.go:450 msg=\"loaded runners\" count=1\r\ntime=2024-09-30T15:34:17.182+05:30 level=INFO source=server.go:595 msg=\"waiting for llama runner to start responding\"\r\ntime=2024-09-30T15:34:17.182+05:30 level=INFO source=server.go:629 msg=\"waiting for server to become available\" status=\"llm server error\"\r\n/tmp/ollama966500899/runners/cpu_avx2/ollama_llama_server: error while loading shared libraries: libmkl_core.so.2: cannot open shared object file: No such file or directory\r\ntime=2024-09-30T15:34:17.433+05:30 level=ERROR source=sched.go:456 msg=\"error loading llama server\" error=\"llama runner process has terminated: exit status 127\"\r\n```\r\n\r\nPS: I am a newbie and followed the steps to install oneAPI and the required packages.",
      "state": "open",
      "author": "SnappierSoap318",
      "author_type": "User",
      "created_at": "2024-09-30T10:07:45Z",
      "updated_at": "2024-10-08T02:24:16Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12150/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "ch1y0q"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12150",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12150",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:12.446711",
      "comments": [
        {
          "author": "ch1y0q",
          "body": "Hi @SnappierSoap318 , please refer to https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.md#3-run-ollama-serve to see if you have set the required environment variables before `./ollama serve`, particularly `source /opt/intel/oneapi/setvars.sh`.",
          "created_at": "2024-10-08T02:16:36Z"
        }
      ]
    },
    {
      "issue_number": 12138,
      "title": "when using saved glm-4v-9b low bit model with pictures, error would ocurr. ",
      "body": "Platform: MTL iGPU, 64G DDR5, ubuntu 22.04\r\n[test_glm-4v-9b.zip](https://github.com/user-attachments/files/17161208/test_glm-4v-9b.zip)\r\nIn the attachment, convert_ipex_model.py is for converting the glm-4v-9b model to low bit model and save to local dir.\r\ngenerate_glm4v_xpu.py is for inferencing.\r\n\r\nafter using the converted low bit model with picture, error would happen. \r\n\r\n python generate_glm4v_xpu.py --model-path glm-4v-quantized/ --image-path 5602445367_3504763978_z.jpg --load-low-bit\r\n\r\n<img width=\"1145\" alt=\"1\" src=\"https://github.com/user-attachments/assets/19ae53d7-6623-4b20-9d55-1961b7164d20\">\r\n\r\nwhen using the converted low bit model without picture, it can work.\r\n<img width=\"1153\" alt=\"3\" src=\"https://github.com/user-attachments/assets/09ed4a40-03d4-4f6e-93e5-8367dc65212c\">\r\n\r\nwhen using the original model, it works fine.\r\npython generate_glm4v_xpu.py --model-path glm-4v-9b/ --image-path 5602445367_3504763978_z.jpg\r\n<img width=\"1150\" alt=\"2\" src=\"https://github.com/user-attachments/assets/220c623a-89d4-48a5-8045-e8b79f3b4836\">\r\n\r\nThanks!",
      "state": "open",
      "author": "wluo1007",
      "author_type": "User",
      "created_at": "2024-09-27T08:34:09Z",
      "updated_at": "2024-10-08T01:26:43Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12138/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "qiuxin2012"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12138",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12138",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:12.625489",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "We have reproduced your issue and are currently working on a fix. ",
          "created_at": "2024-09-30T00:59:09Z"
        },
        {
          "author": "qiuxin2012",
          "body": "It's a bug of `tokenizer.save` in transformers, the `image_size` is missing in the saved tokenizer_config.json, you can add a line `\"image_size\": 1120` to `[your int4 model path]/tokenizer_config.json`. Or just load tokenizer from the origin model.",
          "created_at": "2024-10-08T01:26:28Z"
        }
      ]
    },
    {
      "issue_number": 12149,
      "title": "Arc770 IPEX-LLM 的性能问题",
      "body": "在测试大模型的过程中，单卡性能符合预期，Arc770 双卡和4卡的场景中性能下降比较严重。\r\ninput token -> output token:   1024->512\r\n\r\n按照这里文档 build：  https://github.com/intel-analytics/ipex-llm/blob/main/docs/readthedocs/source/doc/LLM/DockerGuides/vllm_docker_quickstart.md\r\nIPEX-LLM 版本2.1.0b2\r\n",
      "state": "open",
      "author": "yangluchina",
      "author_type": "User",
      "created_at": "2024-09-30T09:50:16Z",
      "updated_at": "2024-10-08T01:03:06Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12149/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12149",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12149",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:12.870908",
      "comments": [
        {
          "author": "glorysdj",
          "body": "please try with latest Docker image: intelanalytics/ipex-llm-serving-xpu:2.2.0-SNAPSHOT\r\nas the data reviewed with you, the 1xARC/2xARC performance should be fine, 4xARC performance is degraded due to the high communication overhead, and we are working in progress in improving it.",
          "created_at": "2024-10-08T01:03:05Z"
        }
      ]
    },
    {
      "issue_number": 11985,
      "title": "On A770，vllm and llama.cpp which brings better performance for MiniCPM-v-2.6 ?",
      "body": "Any data table for benchmark?",
      "state": "open",
      "author": "yangqing-yq",
      "author_type": "User",
      "created_at": "2024-09-02T02:05:32Z",
      "updated_at": "2024-09-30T02:32:28Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11985/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "liu-shaojun"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11985",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11985",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:13.063152",
      "comments": [
        {
          "author": "liu-shaojun",
          "body": "Currently, our VLLM does not support multimodal models. Support for multimodal models is ongoing in the 0.5.x version of VLLM. We will notify you once it's ready.",
          "created_at": "2024-09-02T03:48:11Z"
        },
        {
          "author": "yangqing-yq",
          "body": "Is there a schedule when we could get a vllm supported version to run MiniCPM-v ？ ",
          "created_at": "2024-09-02T05:55:37Z"
        },
        {
          "author": "glorysdj",
          "body": "> Is there a schedule when we could get a vllm supported version to run MiniCPM-v ？\r\n\r\nhi, the upgrade of IPEX-LLM vLLM to 0.5.x is in progress, we will let you know once it's ready, and we will also try to support MiniCPM-v in the new version.",
          "created_at": "2024-09-02T07:34:50Z"
        },
        {
          "author": "yangqing-yq",
          "body": "Check progress, how is the status of vllm supporting MLLM like MiniCPM-V-2.6? @glorysdj ",
          "created_at": "2024-09-30T01:46:04Z"
        },
        {
          "author": "glorysdj",
          "body": "the upgrade of IPEX-LLM vLLM to 0.5.4 is finished, and MiniCPM-V-2.6 is supported, please refer to\r\n\r\nhttps://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/GPU/vLLM-Serving#image-input",
          "created_at": "2024-09-30T01:56:01Z"
        }
      ]
    },
    {
      "issue_number": 12147,
      "title": "will ipex-llm support tiny model Octopus series and when?",
      "body": null,
      "state": "open",
      "author": "yangqing-yq",
      "author_type": "User",
      "created_at": "2024-09-29T05:56:21Z",
      "updated_at": "2024-09-30T02:32:18Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12147/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12147",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12147",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:13.281712",
      "comments": []
    },
    {
      "issue_number": 12134,
      "title": "Ollama returns incorrect result",
      "body": "followed all steps in https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.md.  \r\nbelow are the failure case. \r\n\r\nmy environment is:\r\nOS: windows 11\r\nGraphics:      ARC 770 driver 31.0.101.5534.\r\noneAPI:        2024.2\r\n\r\n(llm-cpp) C:\\Users\\SAS>ollama.exe run qwen2:latest\r\n>>> 你好\r\n_REGSITER: 你好！有什么我能为你做的吗？\r\n\r\n>>> 你好\r\n\r\n当然，\"Regsiter\"的意思是注册。如果您在尝试进行账户注册时遇到问题，或者有任何与注册相关的问题，随时可以问我。无论是提\r\n供技术支持、解释步骤、解答疑问还是提供帮助，我都尽力而为。请具体说明您需要的帮助内容，我会为您提供详细的指导和答案。\r\n\r\n\r\ntime=2024-09-27T13:46:13.117+08:00 level=INFO source=gpu.go:168 msg=\"looking for compatible GPUs\"\r\ntime=2024-09-27T13:46:13.132+08:00 level=INFO source=gpu.go:280 msg=\"no compatible GPUs were discovered\"\r\ntime=2024-09-27T13:46:13.153+08:00 level=INFO source=memory.go:309 msg=\"offload to cpu\" layers.requested=-1 layers.model=29 layers.offload=0 layers.split=\"\" memory.available=\"[15.6 GiB]\" memory.required.full=\"4.9 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"448.0 MiB\" memory.required.allocations=\"[4.9 GiB]\" memory.weights.total=\"3.9 GiB\" memory.weights.repeating=\"3.4 GiB\" memory.weights.nonrepeating=\"426.4 MiB\" memory.graph.full=\"478.0 MiB\" memory.graph.partial=\"730.4 MiB\"\r\ntime=2024-09-27T13:46:13.158+08:00 level=INFO source=server.go:395 msg=\"starting llama server\" cmd=\"C:\\\\Users\\\\SAS\\\\dist\\\\windows-amd64\\\\lib\\\\ollama\\\\runners\\\\cpu_avx2\\\\ollama_llama_server.exe --model C:\\\\Users\\\\SAS\\\\.ollama\\\\models\\\\blobs\\\\sha256-43f7a214e5329f672bb05404cfba1913cbb70fdaa1a17497224e1925046b0ed5 --ctx-size 8192 --batch-size 512 --embedding --log-disable --n-gpu-layers 999 --no-mmap --parallel 4 --port 11250\"\r\ntime=2024-09-27T13:46:13.163+08:00 level=INFO source=sched.go:450 msg=\"loaded runners\" count=1\r\ntime=2024-09-27T13:46:13.163+08:00 level=INFO source=server.go:595 msg=\"waiting for llama runner to start responding\"\r\ntime=2024-09-27T13:46:13.163+08:00 level=INFO source=server.go:629 msg=\"waiting for server to become available\" status=\"llm server error\"\r\nINFO [wmain] build info | build=1 commit=\"1810c22\" tid=\"8200\" timestamp=1727415973\r\nINFO [wmain] system info | n_threads=12 n_threads_batch=-1 system_info=\"AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \" tid=\"8200\" timestamp=1727415973 total_threads=20\r\nINFO [wmain] HTTP server listening | hostname=\"127.0.0.1\" n_threads_http=\"19\" port=\"11250\" tid=\"8200\" timestamp=1727415973\r\nllama_model_loader: loaded meta data with 21 key-value pairs and 339 tensors from C:\\Users\\SAS\\.ollama\\models\\blobs\\sha256-43f7a214e5329f672bb05404cfba1913cbb70fdaa1a17497224e1925046b0ed5 (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\r\nllama_model_loader: - kv   1:                               general.name str              = Qwen2-7B-Instruct\r\nllama_model_loader: - kv   2:                          qwen2.block_count u32              = 28\r\nllama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\r\nllama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 3584\r\nllama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 18944\r\nllama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 28\r\nllama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 4\r\nllama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 2\r\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\r\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151645\r\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151643\r\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151643\r\nllama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\r\nllama_model_loader: - kv  20:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:  141 tensors\r\nllama_model_loader: - type q4_0:  197 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllm_load_vocab: special tokens cache size = 421\r\ntime=2024-09-27T13:46:13.428+08:00 level=INFO source=server.go:629 msg=\"waiting for server to become available\" status=\"llm server loading model\"\r\nllm_load_vocab: token to piece cache size = 0.9352 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = qwen2\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 152064\r\nllm_load_print_meta: n_merges         = 151387\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 32768\r\nllm_load_print_meta: n_embd           = 3584\r\nllm_load_print_meta: n_layer          = 28\r\nllm_load_print_meta: n_head           = 28\r\nllm_load_print_meta: n_head_kv        = 4\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 7\r\nllm_load_print_meta: n_embd_k_gqa     = 512\r\nllm_load_print_meta: n_embd_v_gqa     = 512\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 18944\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 2\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 32768\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = ?B\r\nllm_load_print_meta: model ftype      = Q4_0\r\nllm_load_print_meta: model params     = 7.62 B\r\nllm_load_print_meta: model size       = 4.12 GiB (4.65 BPW)\r\nllm_load_print_meta: general.name     = Qwen2-7B-Instruct\r\nllm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\r\nllm_load_print_meta: EOS token        = 151645 '<|im_end|>'\r\nllm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\r\nllm_load_print_meta: LF token         = 148848 'ÄĬ'\r\nllm_load_print_meta: EOT token        = 151645 '<|im_end|>'\r\nllm_load_print_meta: max token length = 256\r\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nggml_sycl_init: SYCL_USE_XMX: yes\r\nggml_sycl_init: found 1 SYCL devices:\r\nllm_load_tensors: ggml ctx size =    0.30 MiB\r\nllm_load_tensors: offloading 28 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 29/29 layers to GPU\r\nllm_load_tensors:      SYCL0 buffer size =  3928.07 MiB\r\nllm_load_tensors:  SYCL_Host buffer size =   292.36 MiB\r\nllama_new_context_with_model: n_ctx      = 8192\r\nllama_new_context_with_model: n_batch    = 512\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 1000000.0\r\nllama_new_context_with_model: freq_scale = 1\r\n[SYCL] call ggml_check_sycl\r\nggml_check_sycl: GGML_SYCL_DEBUG: 0\r\nggml_check_sycl: GGML_SYCL_F16: no\r\nfound 1 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|                Intel Arc A770 Graphics|    1.3|    512|    1024|   32| 16704M|            1.3.29283|\r\nllama_kv_cache_init:      SYCL0 KV buffer size =   448.00 MiB\r\nllama_new_context_with_model: KV self size  =  448.00 MiB, K (f16):  224.00 MiB, V (f16):  224.00 MiB\r\nllama_new_context_with_model:  SYCL_Host  output buffer size =     2.38 MiB\r\nllama_new_context_with_model:      SYCL0 compute buffer size =   500.00 MiB\r\nllama_new_context_with_model:  SYCL_Host compute buffer size =    23.01 MiB\r\nllama_new_context_with_model: graph nodes  = 1014\r\nllama_new_context_with_model: graph splits = 2\r\nINFO [wmain] model loaded | tid=\"8200\" timestamp=1727415979\r\ntime=2024-09-27T13:46:19.445+08:00 level=INFO source=server.go:634 msg=\"llama runner started in 6.28 seconds\"\r\n[GIN] 2024/09/27 - 13:46:19 | 200 |    6.3390885s |       127.0.0.1 | POST     \"/api/chat\"\r\nINFO [print_timings] prompt eval time     =     214.18 ms /    20 tokens (   10.71 ms per token,    93.38 tokens per second) | n_prompt_tokens_processed=20 n_tokens_second=93.3776565943301 slot_id=0 t_prompt_processing=214.184 t_token=10.7092 task_id=4 tid=\"8200\" timestamp=1727415983\r\nINFO [print_timings] generation eval time =     312.33 ms /    13 runs   (   24.03 ms per token,    41.62 tokens per second) | n_decoded=13 n_tokens_second=41.62290924925078 slot_id=0 t_token=24.02523076923077 t_token_generation=312.32800000000003 task_id=4 tid=\"8200\" timestamp=1727415983\r\nINFO [print_timings]           total time =     526.51 ms | slot_id=0 t_prompt_processing=214.184 t_token_generation=312.32800000000003 t_total=526.5120000000001 task_id=4 tid=\"8200\" timestamp=1727415983\r\n[GIN] 2024/09/27 - 13:46:23 | 200 |    550.0716ms |       127.0.0.1 | POST     \"/api/chat\"\r\nINFO [print_timings] prompt eval time     =     699.86 ms /    43 tokens (   16.28 ms per token,    61.44 tokens per second) | n_prompt_tokens_processed=43 n_tokens_second=61.44059623097663 slot_id=0 t_prompt_processing=699.863 t_token=16.275883720930235 task_id=25 tid=\"8200\" timestamp=1727415998\r\nINFO [print_timings] generation eval time =    1699.65 ms /    66 runs   (   25.75 ms per token,    38.83 tokens per second) | n_decoded=66 n_tokens_second=38.83159267777368 slot_id=0 t_token=25.75222727272727 t_token_generation=1699.647 task_id=25 tid=\"8200\" timestamp=1727415998\r\nINFO [print_timings]           total time =    2399.51 ms | slot_id=0 t_prompt_processing=699.863 t_token_generation=1699.647 t_total=2399.51 task_id=25 tid=\"8200\" timestamp=1727415998\r\n[GIN] 2024/09/27 - 13:46:38 | 200 |    2.4166555s |       127.0.0.1 | POST     \"/api/chat\"\r\n\r\n\r\n(llm-cpp) C:\\Users\\SAS>set\r\nADVISOR_2024_DIR=C:\\Program Files (x86)\\Intel\\oneAPI\\advisor\\2024.2\\\r\nALLUSERSPROFILE=C:\\ProgramData\r\nAPM=C:\\Program Files (x86)\\Intel\\oneAPI\\advisor\\2024.2\\\\perfmodels\r\nAPPDATA=C:\\Users\\SAS\\AppData\\Roaming\r\nCLASSPATH=c:\\Program Files (x86)\\Intel\\oneAPI\\dal\\latest\\share\\java\\onedal.jar;\r\nCMAKE_PREFIX_PATH=c:\\Program Files (x86)\\Intel\\oneAPI\\tbb\\latest\\env\\..;c:\\Program Files (x86)\\Intel\\oneAPI\\ipp\\latest\\lib\\cmake\\ipp;c:\\Program Files (x86)\\Intel\\oneAPI\\dpl\\latest\\lib\\cmake\\oneDPL;c:\\Program Files (x86)\\Intel\\oneAPI\\dnnl\\latest\\env\\..\\lib\\cmake;c:\\Program Files (x86)\\Intel\\oneAPI\\dal\\latest;c:\\Program Files (x86)\\Intel\\oneAPI\\compiler\\latest;\r\nCMPLR_ROOT=c:\\Program Files (x86)\\Intel\\oneAPI\\compiler\\latest\r\nCommandPromptType=Native\r\nCommonProgramFiles=C:\\Program Files\\Common Files\r\nCommonProgramFiles(x86)=C:\\Program Files (x86)\\Common Files\r\nCommonProgramW6432=C:\\Program Files\\Common Files\r\nCOMPUTERNAME=DESKTOP-1NCU6BB\r\nComSpec=C:\\Windows\\system32\\cmd.exe\r\nCONDA_DEFAULT_ENV=llm-cpp\r\nCONDA_EXE=C:\\Users\\SAS\\miniforge-pypy3\\Scripts\\conda.exe\r\nCONDA_PREFIX=C:\\Users\\SAS\\miniforge-pypy3\\envs\\llm-cpp\r\nCONDA_PREFIX_1=C:\\Users\\SAS\\miniforge-pypy3\r\nCONDA_PROMPT_MODIFIER=(llm-cpp)\r\nCONDA_PYTHON_EXE=C:\\Users\\SAS\\miniforge-pypy3\\python.exe\r\nCONDA_SHLVL=2\r\nCPATH=c:\\Program Files (x86)\\Intel\\oneAPI\\tbb\\latest\\env\\..\\include;c:\\Program Files (x86)\\Intel\\oneAPI\\ocloc\\latest\\include;c:\\Program Files (x86)\\Intel\\oneAPI\\mkl\\latest\\include;c:\\Program Files (x86)\\Intel\\oneAPI\\ippcp\\latest\\include;c:\\Program Files (x86)\\Intel\\oneAPI\\ipp\\latest\\include;c:\\Program Files (x86)\\Intel\\oneAPI\\dpl\\latest\\include;c:\\Program Files (x86)\\Intel\\oneAPI\\dpcpp-ct\\latest\\env\\..\\include;c:\\Program Files (x86)\\Intel\\oneAPI\\dev-utilities\\latest\\include;c:\\Program Files (x86)\\Intel\\oneAPI\\dal\\latest\\include\\dal;c:\\Program Files (x86)\\Intel\\oneAPI\\compiler\\latest\\include;\r\nDALROOT=c:\\Program Files (x86)\\Intel\\oneAPI\\dal\\latest\r\nDAL_MAJOR_BINARY=2\r\nDAL_MINOR_BINARY=0\r\nDevEnvDir=C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\IDE\\\r\nDIAGUTIL_PATH=c:\\Program Files (x86)\\Intel\\oneAPI\\debugger\\latest\\env\\\\..\\etc\\debugger\\sys_check;\r\nDNNLROOT=c:\\Program Files (x86)\\Intel\\oneAPI\\dnnl\\latest\\env\\..\r\nDPL_ROOT=c:\\Program Files (x86)\\Intel\\oneAPI\\dpl\\latest\r\nDriverData=C:\\Windows\\System32\\Drivers\\DriverData\r\nEFC_7224=1\r\nERRORSTATE=0\r\nExtensionSdkDir=C:\\Program Files (x86)\\Microsoft SDKs\\Windows Kits\\10\\ExtensionSDKs\r\nEXTERNAL_INCLUDE=C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.39.33519\\include;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.39.33519\\ATLMFC\\include;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Auxiliary\\VS\\include;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.22621.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\um;C:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\cppwinrt;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\include\\um\r\nFPS_BROWSER_APP_PROFILE_STRING=Internet Explorer\r\nFPS_BROWSER_USER_PROFILE_STRING=Default\r\nFramework40Version=v4.0\r\nFrameworkDir=C:\\Windows\\Microsoft.NET\\Framework64\\\r\nFrameworkDir64=C:\\Windows\\Microsoft.NET\\Framework64\\\r\nFrameworkVersion=v4.0.30319\r\nFrameworkVersion64=v4.0.30319\r\nFSHARPINSTALLDIR=C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\FSharp\\Tools\r\nHOMEDRIVE=C:\r\nHOMEPATH=\\Users\\SAS\r\nHTMLHelpDir=C:\\Program Files (x86)\\HTML Help Workshop\r\nINCLUDE=c:\\Program Files (x86)\\Intel\\oneAPI\\tbb\\latest\\env\\..\\include;c:\\Program Files (x86)\\Intel\\oneAPI\\ocloc\\latest\\include;c:\\Program Files (x86)\\Intel\\oneAPI\\mkl\\latest\\include;c:\\Program Files (x86)\\Intel\\oneAPI\\ippcp\\latest\\include;c:\\Program Files (x86)\\Intel\\oneAPI\\ipp\\latest\\include;c:\\Program Files (x86)\\Intel\\oneAPI\\dpcpp-ct\\latest\\env\\..\\include;c:\\Program Files (x86)\\Intel\\oneAPI\\dnnl\\latest\\env\\..\\include;c:\\Program Files (x86)\\Intel\\oneAPI\\dev-utilities\\latest\\include;c:\\Program Files (x86)\\Intel\\oneAPI\\dal\\latest\\include\\dal;c:\\Program Files (x86)\\Intel\\oneAPI\\compiler\\latest\\include;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.39.33519\\include;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.39.33519\\ATLMFC\\include;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Auxiliary\\VS\\include;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.22621.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\um;C:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\cppwinrt;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\include\\um\r\nINTELGTDEBUGGERROOT=c:\\Program Files (x86)\\Intel\\oneAPI\\debugger\\latest\\env\\\\..\r\nINTEL_PYTHONPATH=C:\\Program Files (x86)\\Intel\\oneAPI\\advisor\\2024.2\\pythonapi\r\nINTEL_TARGET_ARCH=intel64\r\nIPPCP_TARGET_ARCH=intel64\r\nIPPCP_TARGET_BIN_ARCH=bin\r\nIPPCP_TARGET_LIB_ARCH=lib\r\nIPPCRYPTOROOT=c:\\Program Files (x86)\\Intel\\oneAPI\\ippcp\\latest\r\nIPPROOT=c:\\Program Files (x86)\\Intel\\oneAPI\\ipp\\latest\r\nIPP_TARGET_ARCH=intel64\r\nLIB=c:\\Program Files (x86)\\Intel\\oneAPI\\tbb\\latest\\env\\..\\lib\\\\;c:\\Program Files (x86)\\Intel\\oneAPI\\mkl\\latest\\lib;c:\\Program Files (x86)\\Intel\\oneAPI\\ippcp\\latest\\lib;c:\\Program Files (x86)\\Intel\\oneAPI\\ipp\\latest\\lib;c:\\Program Files (x86)\\Intel\\oneAPI\\dnnl\\latest\\env\\..\\lib;c:\\Program Files (x86)\\Intel\\oneAPI\\dal\\latest\\lib;c:\\Program Files (x86)\\Intel\\oneAPI\\compiler\\latest\\lib\\clang\\18\\lib\\windows;c:\\Program Files (x86)\\Intel\\oneAPI\\compiler\\latest\\opt\\compiler\\lib;c:\\Program Files (x86)\\Intel\\oneAPI\\compiler\\latest\\lib;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.39.33519\\ATLMFC\\lib\\x64;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.39.33519\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.22621.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\\\lib\\10.0.22621.0\\\\um\\x64\r\nLIBPATH=C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.39.33519\\ATLMFC\\lib\\x64;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.39.33519\\lib\\x64;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.39.33519\\lib\\x86\\store\\references;C:\\Program Files (x86)\\Windows Kits\\10\\UnionMetadata\\10.0.22621.0;C:\\Program Files (x86)\\Windows Kits\\10\\References\\10.0.22621.0;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319\r\nLIBRARY_PATH=c:\\Program Files (x86)\\Intel\\oneAPI\\ippcp\\latest\\lib;c:\\Program Files (x86)\\Intel\\oneAPI\\ipp\\latest\\lib;\r\nLOCALAPPDATA=C:\\Users\\SAS\\AppData\\Local\r\nLOGONSERVER=\\\\DESKTOP-1NCU6BB\r\nMKLROOT=c:\\Program Files (x86)\\Intel\\oneAPI\\mkl\\latest\r\nNETFXSDKDir=C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\\r\nNLSPATH=c:\\Program Files (x86)\\Intel\\oneAPI\\mkl\\latest\\share\\locale\\1033;\r\nNUMBER_OF_PROCESSORS=20\r\nOCLOC_ROOT=c:\\Program Files (x86)\\Intel\\oneAPI\\ocloc\\latest\r\nOCL_ICD_FILENAMES=;c:\\Program Files (x86)\\Intel\\oneAPI\\compiler\\latest\\bin\\intelocl64_emu.dll;c:\\Program Files (x86)\\Intel\\oneAPI\\compiler\\latest\\bin\\intelocl64.dll\r\nOLLAMA_HOST=0.0.0.0\r\nOLLAMA_NUM_GPU=999\r\nONEAPI_ROOT=c:\\Program Files (x86)\\Intel\\oneAPI\r\nOneDrive=C:\\Users\\SAS\\OneDrive\r\nopensslIncludeDir=C:\\Program Files\\OpenSSL\\include\r\nOS=Windows_NT\r\nPath=c:\\Program Files (x86)\\Intel\\oneAPI\\tbb\\latest\\env\\..\\bin\\\\;c:\\Program Files (x86)\\Intel\\oneAPI\\ocloc\\latest\\bin;c:\\Program Files (x86)\\Intel\\oneAPI\\mkl\\latest\\bin;c:\\Program Files (x86)\\Intel\\oneAPI\\ippcp\\latest\\bin;c:\\Program Files (x86)\\Intel\\oneAPI\\ipp\\latest\\bin;c:\\Program Files (x86)\\Intel\\oneAPI\\dpcpp-ct\\latest\\env\\..\\bin;c:\\Program Files (x86)\\Intel\\oneAPI\\dnnl\\latest\\env\\..\\bin;c:\\Program Files (x86)\\Intel\\oneAPI\\dev-utilities\\latest\\bin;c:\\Program Files (x86)\\Intel\\oneAPI\\debugger\\latest\\env\\\\..\\opt\\debugger\\bin;c:\\Program Files (x86)\\Intel\\oneAPI\\dal\\latest\\bin;c:\\Program Files (x86)\\Intel\\oneAPI\\compiler\\latest\\lib\\ocloc;c:\\Program Files (x86)\\Intel\\oneAPI\\compiler\\latest\\bin;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.39.33519\\bin\\HostX64\\x64;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.8 Tools\\x64\\;C:\\Program Files (x86)\\HTML Help Workshop;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\FSharp\\Tools;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Team Tools\\DiagnosticsHub\\Collector;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.22621.0\\\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\\\x64;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\\\MSBuild\\Current\\Bin\\amd64;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\IDE\\;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\Tools\\;C:\\Users\\SAS\\miniforge-pypy3\\envs\\llm-cpp;C:\\Users\\SAS\\miniforge-pypy3\\envs\\llm-cpp\\Library\\mingw-w64\\bin;C:\\Users\\SAS\\miniforge-pypy3\\envs\\llm-cpp\\Library\\usr\\bin;C:\\Users\\SAS\\miniforge-pypy3\\envs\\llm-cpp\\Library\\bin;C:\\Users\\SAS\\miniforge-pypy3\\envs\\llm-cpp\\Scripts;C:\\Users\\SAS\\miniforge-pypy3\\envs\\llm-cpp\\bin;C:\\Users\\SAS\\miniforge-pypy3\\condabin;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0;C:\\Windows\\System32\\OpenSSH;C:\\Program Files\\dotnet;C:\\Program Files\\Git\\cmd;C:\\Program Files (x86)\\Windows Kits\\10\\Windows Performance Toolkit;C:\\Program Files\\PuTTY;C:\\Program Files\\Rust stable MSVC 1.78\\bin;C:\\Program Files\\CMake\\bin;C:\\Strawberry\\c\\bin;C:\\Strawberry\\perl\\site\\bin;C:\\Strawberry\\perl\\bin;C:\\Users\\SAS\\AppData\\Local\\bin\\NASM;C:\\Strawberry\\perl\\bin;C:\\Users\\SAS\\miniforge3\\Scripts;C:\\Program Files\\nodejs;C:\\Program Files\\Docker\\Docker\\resources\\bin;C:\\Users\\SAS\\bin;C:\\Users\\SAS\\.cargo\\bin;C:\\Users\\SAS\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\SAS\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\Users\\SAS\\AppData\\Local\\Microsoft\\WinGet\\Links;C:\\Users\\SAS\\AppData\\Roaming\\npm;C:\\Users\\SAS\\.dotnet\\tools;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\IDE\\VC\\Linux\\bin\\ConnectionManagerExe;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\vcpkg;C:\\Program Files (x86)\\Intel\\oneAPI\\advisor\\2024.2\\bin64;C:\\Program Files (x86)\\Intel\\oneAPI\\vtune\\2024.2\\bin64\r\nPATHEXT=.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC\r\nPKG_CONFIG_PATH=c:\\Program Files (x86)\\Intel\\oneAPI\\tbb\\latest\\env\\..\\lib\\pkgconfig;c:\\Program Files (x86)\\Intel\\oneAPI\\mkl\\latest\\lib\\pkgconfig;c:\\Program Files (x86)\\Intel\\oneAPI\\ippcp\\latest\\lib\\pkgconfig;c:\\Program Files (x86)\\Intel\\oneAPI\\dpl\\latest\\lib\\pkgconfig;c:\\Program Files (x86)\\Intel\\oneAPI\\dnnl\\latest\\env\\..\\lib\\pkgconfig;c:\\Program Files (x86)\\Intel\\oneAPI\\dal\\latest\\lib\\pkgconfig;c:\\Program Files (x86)\\Intel\\oneAPI\\compiler\\latest\\lib\\pkgconfig;\r\nPlatform=x64\r\nPROCESSOR_ARCHITECTURE=AMD64\r\nPROCESSOR_IDENTIFIER=Intel64 Family 6 Model 151 Stepping 2, GenuineIntel\r\nPROCESSOR_LEVEL=6\r\nPROCESSOR_REVISION=9702\r\nProgramData=C:\\ProgramData\r\nProgramFiles=C:\\Program Files\r\nProgramFiles(x86)=C:\\Program Files (x86)\r\nProgramW6432=C:\\Program Files\r\nPROMPT=(llm-cpp) $P$G\r\nPSModulePath=C:\\Program Files\\WindowsPowerShell\\Modules;C:\\Windows\\system32\\WindowsPowerShell\\v1.0\\Modules\r\nPUBLIC=C:\\Users\\Public\r\nPYTHONPATH=C:\\Program Files (x86)\\Intel\\oneAPI\\advisor\\2024.2\\pythonapi\r\nSESSIONNAME=Console\r\nSETVARS_COMPLETED=1\r\nSSL_CERT_DIR=C:\\Users\\SAS\\miniforge-pypy3\\envs\\llm-cpp\\Library\\ssl\\certs\r\nSSL_CERT_FILE=C:\\Users\\SAS\\miniforge-pypy3\\envs\\llm-cpp\\Library\\ssl\\cacert.pem\r\nSYCL_CACHE_PERSISTENT=1\r\nSYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1\r\nSystemDrive=C:\r\nSystemRoot=C:\\Windows\r\nTBBROOT=c:\\Program Files (x86)\\Intel\\oneAPI\\tbb\\latest\\env\\..\r\nTBB_BIN_DIR=c:\\Program Files (x86)\\Intel\\oneAPI\\tbb\\latest\\env\\..\\bin\r\nTBB_DLL_PATH=c:\\Program Files (x86)\\Intel\\oneAPI\\tbb\\latest\\env\\..\\bin\\\\\r\nTBB_SCRIPT_DIR=c:\\Program Files (x86)\\Intel\\oneAPI\\tbb\\latest\\env\\\r\nTBB_TARGET_ARCH=intel64\r\nTEMP=C:\\Users\\SAS\\AppData\\Local\\Temp\r\nTMP=C:\\Users\\SAS\\AppData\\Local\\Temp\r\nUCRTVersion=10.0.22621.0\r\nUniversalCRTSdkDir=C:\\Program Files (x86)\\Windows Kits\\10\\\r\nUSERDOMAIN=DESKTOP-1NCU6BB\r\nUSERDOMAIN_ROAMINGPROFILE=DESKTOP-1NCU6BB\r\nUSERNAME=SAS\r\nUSERPROFILE=C:\\Users\\SAS\r\nUSE_INTEL_LLVM=0\r\nVARSDIR=c:\\Program Files (x86)\\Intel\\oneAPI\\ocloc\\latest\\env\\\r\nVCIDEInstallDir=C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\IDE\\VC\\\r\nVCINSTALLDIR=C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\\r\nVCPKG_ROOT=C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\vcpkg\r\nVCToolsInstallDir=C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.39.33519\\\r\nVCToolsRedistDir=C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Redist\\MSVC\\14.38.33135\\\r\nVCToolsVersion=14.39.33519\r\nVisualStudioVersion=17.0\r\nVS170COMNTOOLS=C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\Tools\\\r\nVSCMD_ARG_app_plat=Desktop\r\nVSCMD_ARG_HOST_ARCH=x64\r\nVSCMD_ARG_TGT_ARCH=x64\r\nVSCMD_VER=17.9.6\r\nVSINSTALLDIR=C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\\r\nVS_TARGET_ARCH=amd64\r\nVTUNE_PROFILER_2024_DIR=C:\\Program Files (x86)\\Intel\\oneAPI\\vtune\\2024.2\\\r\nVTUNE_PROFILER_DIR=C:\\Program Files (x86)\\Intel\\oneAPI\\vtune\\2024.2\\\r\nwindir=C:\\Windows\r\nWindowsLibPath=C:\\Program Files (x86)\\Windows Kits\\10\\UnionMetadata\\10.0.22621.0;C:\\Program Files (x86)\\Windows Kits\\10\\References\\10.0.22621.0\r\nWindowsSdkBinPath=C:\\Program Files (x86)\\Windows Kits\\10\\bin\\\r\nWindowsSdkDir=C:\\Program Files (x86)\\Windows Kits\\10\\\r\nWindowsSDKLibVersion=10.0.22621.0\\\r\nWindowsSdkVerBinPath=C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.22621.0\\\r\nWindowsSDKVersion=10.0.22621.0\\\r\nWindowsSDK_ExecutablePath_x64=C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.8 Tools\\x64\\\r\nWindowsSDK_ExecutablePath_x86=C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.8 Tools\\\r\nZES_ENABLE_SYSMAN=1\r\n__CONDA_OPENSSL_CERT_DIR_SET=\"1\"\r\n__CONDA_OPENSSL_CERT_FILE_SET=\"1\"\r\n__DOTNET_ADD_64BIT=1\r\n__DOTNET_PREFERRED_BITNESS=64\r\n__MS_VC_INSTALL_PATH=C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.39.33519\\\r\n__VSCMD_PREINIT_PATH=C:\\Users\\SAS\\miniforge-pypy3\\envs\\llm-cpp;C:\\Users\\SAS\\miniforge-pypy3\\envs\\llm-cpp\\Library\\mingw-w64\\bin;C:\\Users\\SAS\\miniforge-pypy3\\envs\\llm-cpp\\Library\\usr\\bin;C:\\Users\\SAS\\miniforge-pypy3\\envs\\llm-cpp\\Library\\bin;C:\\Users\\SAS\\miniforge-pypy3\\envs\\llm-cpp\\Scripts;C:\\Users\\SAS\\miniforge-pypy3\\envs\\llm-cpp\\bin;C:\\Users\\SAS\\miniforge-pypy3\\condabin;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0;C:\\Windows\\System32\\OpenSSH;C:\\Program Files\\dotnet;C:\\Program Files\\Git\\cmd;C:\\Program Files (x86)\\Windows Kits\\10\\Windows Performance Toolkit;C:\\Program Files\\PuTTY;C:\\Program Files\\Rust stable MSVC 1.78\\bin;C:\\Program Files\\CMake\\bin;C:\\Strawberry\\c\\bin;C:\\Strawberry\\perl\\site\\bin;C:\\Strawberry\\perl\\bin;C:\\Users\\SAS\\AppData\\Local\\bin\\NASM;C:\\Strawberry\\perl\\bin;C:\\Users\\SAS\\miniforge3\\Scripts;C:\\Program Files\\nodejs;C:\\Program Files\\Docker\\Docker\\resources\\bin;C:\\Users\\SAS\\bin;C:\\Users\\SAS\\.cargo\\bin;C:\\Users\\SAS\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\SAS\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\Users\\SAS\\AppData\\Local\\Microsoft\\WinGet\\Links;C:\\Users\\SAS\\AppData\\Roaming\\npm;C:\\Users\\SAS\\.dotnet\\tools\r\n\r\n",
      "state": "open",
      "author": "yunbiaolin",
      "author_type": "User",
      "created_at": "2024-09-27T05:50:52Z",
      "updated_at": "2024-09-29T15:02:23Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12134/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12134",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12134",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:13.281745",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @yunbiaolin , we cannot reproduce your issue, ollama works fine on our windows arc device. Could you please provide more details how you install and run ollama?",
          "created_at": "2024-09-29T15:02:22Z"
        }
      ]
    },
    {
      "issue_number": 11532,
      "title": "mistral_model_forward_4_36() got an unexpected keyword argument 'cache_position'",
      "body": "Running mistral with `transformers==4.42.3` will have the following error present and unable to run.\r\n`mistral_model_forward_4_36() got an unexpected keyword argument 'cache_position'`\r\n",
      "state": "open",
      "author": "xiangyang-95",
      "author_type": "User",
      "created_at": "2024-07-09T00:17:05Z",
      "updated_at": "2024-09-29T08:53:31Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11532/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11532",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11532",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:13.470385",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "transformers==4.42.3 was not fully tested, our test only covered 4.38.\r\nYou can downgrade 4.36.2 for mistral now.  We will inform you when we support 4.42.",
          "created_at": "2024-07-09T02:07:51Z"
        },
        {
          "author": "oldmikeyang",
          "body": "transformer 4.36.2 have another issue with the mistral model.\r\nI had tested the transformers v4.40 can works with the mistral model.\r\n\r\n",
          "created_at": "2024-09-29T08:53:30Z"
        }
      ]
    },
    {
      "issue_number": 12122,
      "title": "Upstream i915 GUC load failed on Ubuntu 24.04 Kernel 6.8.0-31 with Arc A770",
      "body": "OS Ubuntu 24.04\r\nKernel 6.8.0-31-generic\r\n\r\nError message\r\n```\r\n[   18.393201] i915 0000:0d:00.0: [drm] BAR2 resized to 16384M\r\n[   18.393290] i915 0000:0d:00.0: [drm] Local memory IO size: 0x00000003fa000000\r\n[   18.393296] i915 0000:0d:00.0: [drm] Local memory available: 0x00000003fa000000\r\n[   18.409701] i915 0000:0d:00.0: vgaarb: VGA decodes changed: olddecodes=io+mem,decodes=none:owns=none\r\n[   18.422225] i915 0000:0d:00.0: [drm] Finished loading DMC firmware i915/dg2_dmc_ver2_08.bin (v2.8)\r\n[   18.430827] i915 0000:0d:00.0: [drm] GT0: GUC: ADS capture alloc size changed from 32768 to 36864\r\n[   18.431927] i915 0000:0d:00.0: [drm] GT0: GuC firmware i915/dg2_guc_70.bin version 70.20.0\r\n[   18.431930] i915 0000:0d:00.0: [drm] GT0: HuC firmware i915/dg2_huc_gsc.bin version 7.10.15\r\n[   18.432046] i915 0000:0d:00.0: [drm] GT0: GUC: ADS capture alloc size changed from 32768 to 36864\r\n[   18.432614] i915 0000:0d:00.0: [drm] GT0: GUC: load failed: status = 0x40000056, time = 0ms, freq = 2400MHz, ret = 0\r\n[   18.432617] i915 0000:0d:00.0: [drm] GT0: GUC: load failed: status: Reset = 0, BootROM = 0x2B, UKernel = 0x00, MIA = 0x00, Auth = 0x01\r\n[   18.432619] i915 0000:0d:00.0: [drm] GT0: GUC: firmware production part check failure\r\n[   18.432684] i915 0000:0d:00.0: [drm] *ERROR* GT0: GuC initialization failed -ENOEXEC\r\n[   18.432688] i915 0000:0d:00.0: [drm] *ERROR* GT0: Enabling uc failed (-5)\r\n[   18.432690] i915 0000:0d:00.0: [drm] *ERROR* \r\nGT0: Failed to initialize GPU, declaring it wedged!\r\n[   18.434676] i915 0000:0d:00.0: [drm:add_taint_for_CI [i915]] CI tainted:0x9 by intel_gt_set_wedged_on_init+0x34/0x50 [i915]\r\n```\r\n\r\nintel-gpu/intel-gpu-i915-backports#194\r\n\r\n\r\n",
      "state": "open",
      "author": "huiwangnick",
      "author_type": "User",
      "created_at": "2024-09-26T04:12:27Z",
      "updated_at": "2024-09-29T02:31:36Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12122/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12122",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12122",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:13.696573",
      "comments": [
        {
          "author": "qiyuangong",
          "body": "Hi @huiwangnick \r\n\r\nFrom https://github.com/intel-gpu/intel-gpu-i915-backports/issues/194 log, you are using out of tree driver (version 1.24.4.12.240603.18.6.8.0.40+i1-1, package name intel-dmabuf-drm-i915-dkms) rather than upstream driver.\r\n\r\nMeanwhile, the driver version used is different from ou",
          "created_at": "2024-09-26T11:34:01Z"
        },
        {
          "author": "huiwangnick",
          "body": "Hi @qiyuangong \r\n\r\nI apologize for not being clearer earlier. In the issue discussed in intel-gpu/intel-gpu-i915-backports#194, I am using the out-of-tree driver; however, in this case, I am utilizing the upstream driver from the Linux kernel. I have also attempted your recommendation of using intel",
          "created_at": "2024-09-26T12:53:15Z"
        },
        {
          "author": "qiyuangong",
          "body": "> Hi @qiyuangong\r\n> \r\n> I apologize for not being clearer earlier. In the issue discussed in [intel-gpu/intel-gpu-i915-backports#194](https://github.com/intel-gpu/intel-gpu-i915-backports/issues/194), I am using the out-of-tree driver; however, in this case, I am utilizing the upstream driver from t",
          "created_at": "2024-09-27T03:21:58Z"
        },
        {
          "author": "huiwangnick",
          "body": "Thank you. I will try 6.5.0 kernel to see if I can get it to work.\r\n\r\nIn the meantime, I’d like to add that when the BAR is not resized to 16,384 MB and remains at 256 MB, the driver loads successfully. This suggests that the issue is not related to GPU installation or GPU firmware. It seems possibl",
          "created_at": "2024-09-27T03:54:53Z"
        },
        {
          "author": "qiyuangong",
          "body": "> Thank you. I will try 6.5.0 kernel to see if I can get it to work.\r\n> \r\n> In the meantime, I’d like to add that when the BAR is not resized to 16,384 MB and remains at 256 MB, the driver loads successfully. This suggests that the issue is not related to GPU installation or GPU firmware. It seems p",
          "created_at": "2024-09-27T05:04:02Z"
        }
      ]
    },
    {
      "issue_number": 12141,
      "title": "ImportError: /home/wxf/miniforge3/envs/ipex-llm/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so: undefined symbol: iJIT_NotifyEvent",
      "body": "Traceback (most recent call last):\r\n  File \"/home/wxf/ipex-llm/minicpm/xin-minicpm-v2-6.py\", line 2, in <module>\r\n    import torch\r\n  File \"/home/wxf/miniforge3/envs/ipex-llm/lib/python3.11/site-packages/torch/__init__.py\", line 235, in <module>\r\n    from torch._C import *  # noqa: F403\r\n    ^^^^^^^^^^^^^^^^^^^^^^\r\nImportError: /home/wxf/miniforge3/envs/ipex-llm/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so: undefined symbol: iJIT_NotifyEvent",
      "state": "open",
      "author": "yangqing-yq",
      "author_type": "User",
      "created_at": "2024-09-27T13:43:18Z",
      "updated_at": "2024-09-29T02:24:13Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12141/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12141",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12141",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:14.023887",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "It looks like you are using oneapi 2024.2, oneapi 2024.2 is not supported now, the same error is in https://github.com/intel-analytics/ipex-llm/issues/12112.\r\nYou need to remove your oneapi and install oneapi 2024.0 like this https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quicksta",
          "created_at": "2024-09-29T02:24:12Z"
        }
      ]
    },
    {
      "issue_number": 12143,
      "title": "Undefined symbol on ipex 2.3.110+xpu",
      "body": "Hello, I'm trying ipex-llm 2.2.0b20240927 with pytorch ipex 2.3.110+xpu, and it failed with following error:\r\n\r\n```\r\nERROR    azarrot.backends.common:common.py:323 An error occurred when generating text\r\nTraceback (most recent call last):\r\n  File \"/mnt/data/podman/Projects/azarrot/src/azarrot/backends/common.py\", line 319, in generate\r\n    self.model.generate(**self.generation_kwargs)\r\n  File \"/mnt/data/podman/Projects/azarrot/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/mnt/data/podman/Projects/azarrot/venv/lib/python3.10/site-packages/ipex_llm/transformers/lookup.py\", line 123, in generate\r\n    return original_generate(self,\r\n  File \"/mnt/data/podman/Projects/azarrot/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/mnt/data/podman/Projects/azarrot/venv/lib/python3.10/site-packages/ipex_llm/transformers/speculative.py\", line 109, in generate\r\n    return original_generate(self,\r\n  File \"/mnt/data/podman/Projects/azarrot/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/mnt/data/podman/Projects/azarrot/venv/lib/python3.10/site-packages/ipex_llm/transformers/pipeline_parallel.py\", line 283, in generate\r\n    return original_generate(self,\r\n  File \"/mnt/data/podman/Projects/azarrot/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/mnt/data/podman/Projects/azarrot/venv/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2024, in generate\r\n    result = self._sample(\r\n  File \"/mnt/data/podman/Projects/azarrot/venv/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2982, in _sample\r\n    outputs = self(**model_inputs, return_dict=True)\r\n  File \"/mnt/data/podman/Projects/azarrot/venv/lib/python3.10/site-packages/nncf/torch/dynamic_graph/wrappers.py\", line 146, in wrapped\r\n    return module_call(self, *args, **kwargs)\r\n  File \"/mnt/data/podman/Projects/azarrot/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/mnt/data/podman/Projects/azarrot/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/mnt/data/podman/Projects/azarrot/venv/lib/python3.10/site-packages/ipex_llm/transformers/models/qwen2.py\", line 437, in qwen2_causal_lm_forward\r\n    outputs = self.model(\r\n  File \"/mnt/data/podman/Projects/azarrot/venv/lib/python3.10/site-packages/nncf/torch/dynamic_graph/wrappers.py\", line 146, in wrapped\r\n    return module_call(self, *args, **kwargs)\r\n  File \"/mnt/data/podman/Projects/azarrot/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/mnt/data/podman/Projects/azarrot/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/mnt/data/podman/Projects/azarrot/venv/lib/python3.10/site-packages/ipex_llm/transformers/models/qwen2.py\", line 371, in qwen2_model_forward_4_42\r\n    layer_outputs = decoder_layer(\r\n  File \"/mnt/data/podman/Projects/azarrot/venv/lib/python3.10/site-packages/nncf/torch/dynamic_graph/wrappers.py\", line 146, in wrapped\r\n    return module_call(self, *args, **kwargs)\r\n  File \"/mnt/data/podman/Projects/azarrot/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/mnt/data/podman/Projects/azarrot/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/mnt/data/podman/Projects/azarrot/venv/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 652, in forward\r\n    hidden_states = self.input_layernorm(hidden_states)\r\n  File \"/mnt/data/podman/Projects/azarrot/venv/lib/python3.10/site-packages/nncf/torch/dynamic_graph/wrappers.py\", line 146, in wrapped\r\n    return module_call(self, *args, **kwargs)\r\n  File \"/mnt/data/podman/Projects/azarrot/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/mnt/data/podman/Projects/azarrot/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/mnt/data/podman/Projects/azarrot/venv/lib/python3.10/site-packages/ipex_llm/transformers/models/llama.py\", line 257, in llama_rms_norm_forward\r\n    import xe_addons\r\nImportError: /mnt/data/podman/Projects/azarrot/venv/lib/python3.10/site-packages/xe_addons.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN2at4_ops8to_dtype4callERKNS_6TensorEN3c1010ScalarTypeEbbNS5_8optionalINS5_12MemoryFormatEEE\r\n```\r\n\r\nThis does not happen on ipex 2.1.40+xpu. \r\n\r\nDo you have any plan on adding support for ipex 2.3.110+xpu? Thanks!",
      "state": "open",
      "author": "notsyncing",
      "author_type": "User",
      "created_at": "2024-09-28T06:58:30Z",
      "updated_at": "2024-09-29T02:18:55Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12143/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12143",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12143",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:14.247245",
      "comments": [
        {
          "author": "Oscilloscope98",
          "body": "Hi @notsyncing, `ipex-llm` currently supports PyTorch 2.1 :)",
          "created_at": "2024-09-29T02:18:54Z"
        }
      ]
    },
    {
      "issue_number": 12140,
      "title": "why does ipex-llm not support fastllm for serving?",
      "body": "which repo is https://github.com/ztxz16/fastllm",
      "state": "open",
      "author": "yangqing-yq",
      "author_type": "User",
      "created_at": "2024-09-27T12:41:54Z",
      "updated_at": "2024-09-29T02:09:12Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12140/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12140",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12140",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:14.471210",
      "comments": []
    },
    {
      "issue_number": 12116,
      "title": "Running Docker ollama with Igpu keeps failing to generate.",
      "body": "When I try to run ollama with docker everytime it's asked to generate via API i see\r\n\r\n```\r\nmsg=\"error loading llama server\" error=\"llama runner process has terminated: signal: aborted (core dumped)\"\r\n```\r\n\r\nI am running this on Ubuntu 24.04. I know the docker image has Intel packages for oneapi but should the host have them installed as well? I couldn't find docs suggesting that.\r\n\r\nVainfo is the latest offered by apt on my system.\r\n```\r\nvainfo: VA-API version: 1.20 (libva 2.12.0)\r\nvainfo: Driver version: Intel iHD driver for Intel(R) Gen Graphics - 24.1.0 ()\r\n```\r\n\r\n\r\nDocker logs\r\n[ollama_logs (5).txt](https://github.com/user-attachments/files/17112194/ollama_logs.5.txt)\r\n\r\n\r\nsycl-ls\r\n```\r\n[opencl:acc:0] Intel(R) FPGA Emulation Platform for OpenCL(TM), Intel(R) FPGA Emulation Device OpenCL 1.2  [2023.16.12.0.12_195853.xmain-hotfix]\r\n[opencl:cpu:1] Intel(R) OpenCL, 11th Gen Intel(R) Core(TM) i7-1165G7 @ 2.80GHz OpenCL 3.0 (Build 0) [2023.16.12.0.12_195853.xmain-hotfix]\r\n[opencl:gpu:2] Intel(R) OpenCL Graphics, Intel(R) Iris(R) Xe Graphics OpenCL 3.0 NEO  [23.35.27191.42]\r\n[ext_oneapi_level_zero:gpu:0] Intel(R) Level-Zero, Intel(R) Iris(R) Xe Graphics\r\n1.3 [1.3.26241]\r\n```\r\n\r\nDocker compose - I realized a lot of extra work was needed to figure out how to actually start ollama for docker as the docs showed nothing.\r\n```\r\n  ollama:\r\n    image: intelanalytics/ipex-llm-inference-cpp-xpu:latest\r\n    container_name: ollama\r\n    restart: unless-stopped\r\n    command: /bin/bash -c \"cd /llm/scripts && source ipex-llm-init --gpu --device iGPU && init-ollama && ./ollama serve\"\r\n    networks:\r\n      traefik-network:\r\n        ipv4_address: 192.168.10.71\r\n    devices:\r\n      - /dev/dri/renderD128:/dev/dri/renderD128\r\n    volumes:\r\n      - /docker_container_volumes/ollama:/root/.ollama\r\n    environment:\r\n     # - no_proxy=localhost,127.0.0.1\r\n      - bench_model=mistral-7b-v0.1.Q4_0.gguf\r\n      - DEVICE=iGPU\r\n      - OLLAMA_HOST=0.0.0.0\r\n      - OLLAMA_NUM_GPU=999\r\n      - OLLAMA_INTEL_GPU=1\r\n      - GIN_MODE=release\r\n      - NEOReadDebugKeys=1\r\n      - OverrideGpuAddressSpace=48\r\n      - ZES_ENABLE_SYSMAN=1\r\n    shm_size: '16g'\r\n    mem_limit: '8g'\r\n    tty: true\r\n    stdin_open: true\r\n```",
      "state": "open",
      "author": "Daniel-dev22",
      "author_type": "User",
      "created_at": "2024-09-24T09:47:44Z",
      "updated_at": "2024-09-26T07:53:00Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12116/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "liu-shaojun"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12116",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12116",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:14.471256",
      "comments": [
        {
          "author": "JinheTang",
          "body": "Hi @Daniel-dev22 , the latest version of `ipex-llm[cpp]` seems to have this problem on Linux. You can downgrade `ipex-llm[cpp]` in the container to the verified working version with:\r\n```log\r\npip install ipex-llm[cpp]==2.2.0b20240911\r\n```\r\n\r\nAdditionally, if you meet garbage output after the downgra",
          "created_at": "2024-09-26T07:52:17Z"
        }
      ]
    },
    {
      "issue_number": 12025,
      "title": "Running Qwen2-7B gets error on LNL NPU after appeding history",
      "body": "prompt:\r\n```\r\n <|im_start|>system\r\nYou are a helpful assistant.<|im_end|>\r\n<|im_start|>user\r\nhi<|im_end|>\r\n<|im_start|>system\r\nHello! How can I assist you today?<|im_end|>\r\n<|im_start|>user\r\n北京三日游<|im_end|>\r\n<|im_start|>system\r\n您好！关于北京三日游的回答如下：\r\n\r\n第一天：\r\n您可以参观故宫博物院，这是中国最大的古代宫殿建筑群和世界四大博物馆之一；\r\n午餐时，别忘了品尝正宗的北京烤鸭；\r\n下午游览天安门广场和人民英雄纪念碑；\r\n晚上，您可以去王府井大街逛街购物，并享受各种美食小吃。\r\n\r\n第二天：\r\n参观长城是您不能错过的行程的一部分；\r\n早餐后出发前往八达岭长城；\r\n午餐时，推荐品尝炸酱面或豆汁等北京特色美食；\r\n下午游览颐和园，这是一个集皇家园林、江南水乡风光于一体的大型园林景区；\r\n晚上，您可以去前门大街感受老北京的传统商业街氛围。\r\n\r\n第三天：\r\n参观国家博物馆，了解中国的历史和文化；\r\n上午逛南锣鼓巷，体验老北京的生活风情；\r\n午餐时，推荐品尝豆汁、炒肝等北京传统早点；\r\n下午游览景山公园，登上万寿山俯瞰整个城市的美景；\r\n晚上，您可以去798艺术区欣赏现代艺术作品和个人展览。\r\n\r\n祝您在北京三日游中玩得愉快！<|im_end|>\r\n<|im_start|>user\r\n根据用户提问回答问题。回答的语种与问题的语种一致。\r\n\r\n问: 北京三日游\r\n<|im_end|>\r\n<|im_start|>assistant\r\n```\r\n\r\nlog:\r\n```\r\nstart compiling\r\nloc(fused<{name = \"Add_1106\", type = \"Add\"}>[\"Add_1106\", \"_mem_permute_1\"]): error: PermuteCast input and output distributions are incompatible: in = #VPU.DistributedTensor<mode = <SEGMENTED>, num_tiles = [1, 4, 1, 1], num_clusters = 4 : i64, uniform_distributed_segments = unit, compute_shapes = [[1, 7, 304, 128], [1, 7, 304, 128], [1, 7, 304, 128], [1, 7, 304, 128]], compute_offsets = [[0, 0, 0, 0], [0, 7, 0, 0], [0, 14, 0, 0], [0, 21, 0, 0]], memory_shapes = [[1, 7, 304, 128], [1, 7, 304, 128], [1, 7, 304, 128], [1, 7, 304, 128]], memory_offsets = [[0, 0, 0, 0], [0, 7, 0, 0], [0, 14, 0, 0], [0, 21, 0, 0]]>, out = #VPU.DistributedTensor<mode = <OVERLAPPED>, num_tiles = [1, 1, 4, 1], num_clusters = 4 : i64, uniform_distributed_segments = unit, compute_shapes = [[1, 128, 7, 304], [1, 128, 7, 304], [1, 128, 7, 304], [1, 128, 7, 304]], compute_offsets = [[0, 0, 0, 0], [0, 0, 7, 0], [0, 0, 14, 0], [0, 0, 21, 0]], memory_shapes = [[1, 128, 7, 304], [1, 128, 7, 304], [1, 128, 7, 304], [1, 128, 7, 304]], memory_offsets = [[0, 0, 0, 0], [0, 0, 7, 0], [0, 0, 14, 0], [0, 0, 21, 0]]>,expected = #VPU.DistributedTensor<mode = <SEGMENTED>, num_tiles = [1, 1, 4, 1], num_clusters = 4 : i64, uniform_distributed_segments = unit, compute_shapes = [[1, 128, 7, 304], [1, 128, 7, 304], [1, 128, 7, 304], [1, 128, 7, 304]], compute_offsets = [[0, 0, 0, 0], [0, 0, 7, 0], [0, 0, 14, 0], [0, 0, 21, 0]], memory_shapes = [[1, 128, 7, 304], [1, 128, 7, 304], [1, 128, 7, 304], [1, 128, 7, 304]], memory_offsets = [[0, 0, 0, 0], [0, 0, 7, 0], [0, 0, 14, 0], [0, 0, 21, 0]]>\r\nProcess Process-5:\r\nTraceback (most recent call last):\r\n  File \"C:\\work\\AIGC_npu\\resources\\env\\lib\\multiprocessing\\process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"C:\\work\\AIGC_npu\\resources\\env\\lib\\multiprocessing\\process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"C:\\work\\AIGC_npu\\resources\\env\\lib\\site-packages\\ipex_llm\\transformers\\npu_models\\qwen2_mp.py\", line 875, in run_prefill\r\n    layer_outputs = decoder_layer(\r\n  File \"C:\\work\\AIGC_npu\\resources\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"C:\\work\\AIGC_npu\\resources\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"C:\\work\\AIGC_npu\\resources\\env\\lib\\site-packages\\ipex_llm\\transformers\\npu_models\\qwen2_mp.py\", line 514, in forward\r\n    hidden_states, past_key, past_value = run_model(\r\n  File \"C:\\work\\AIGC_npu\\resources\\env\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"C:\\work\\AIGC_npu\\resources\\env\\lib\\site-packages\\ipex_llm\\transformers\\npu_models\\mp_models_base.py\", line 75, in run_model\r\n    _model_cache[key] = deque([backend_cls(*input_shapes) for i in range(replica)])\r\n  File \"C:\\work\\AIGC_npu\\resources\\env\\lib\\site-packages\\ipex_llm\\transformers\\npu_models\\mp_models_base.py\", line 75, in <listcomp>\r\n    _model_cache[key] = deque([backend_cls(*input_shapes) for i in range(replica)])\r\n  File \"C:\\work\\AIGC_npu\\resources\\env\\lib\\site-packages\\ipex_llm\\transformers\\npu_models\\qwen2_mp.py\", line 225, in __init__\r\n    self.compile()\r\n  File \"C:\\work\\AIGC_npu\\resources\\env\\lib\\site-packages\\intel_npu_acceleration_library\\backend\\factory.py\", line 837, in compile\r\n    backend_lib.compile(self._mm)\r\nOSError: [WinError -529697949] Windows Error 0xe06d7363\r\n```\r\n\r\nVersion:\r\nipex-llm                      2.2.0b20240903 or 0904\r\nbigdl-core-npu                2.6.0b20240903 or 0904\r\n",
      "state": "closed",
      "author": "violet17",
      "author_type": "User",
      "created_at": "2024-09-05T09:04:25Z",
      "updated_at": "2024-09-26T06:43:05Z",
      "closed_at": "2024-09-26T06:43:05Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12025/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "plusbang"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12025",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12025",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:14.689618",
      "comments": [
        {
          "author": "plusbang",
          "body": "Hi, @violet17 , as we communicated offline, the compile error is caused by the specific input length 336. In https://github.com/intel-analytics/ipex-llm/pull/12033, we would pad input to `max_prompt_len` and this error could be solved.\r\n\r\nPlease wait for nightly version including this fix and have a",
          "created_at": "2024-09-06T06:42:11Z"
        },
        {
          "author": "violet17",
          "body": "Fixed. Thanks.",
          "created_at": "2024-09-26T06:43:05Z"
        }
      ]
    },
    {
      "issue_number": 12108,
      "title": "(PI_ERROR_BUILD_PROGRAM_FAILURE)Exception caught at file:C:/Users/Administrator/actions-runner/release-cpp-oneapi_2024_2/_work/llm.cpp/llm.cpp/llama-cpp-bigdl/ggml/src/ggml-sycl.cpp, line:3715",
      "body": "Hi, I am having an issue with running the sample example in the [quickstart guide](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/llama_cpp_quickstart.md#3-example-running-community-gguf-models-with-ipex-llm). This is the error I'm seeing:\r\n\r\n```\r\n(llm-cpp) C:\\Users\\ashwin-a\\llama-cpp>llama-cli -m mistral-7b-instruct-v0.1.Q4_K_M.gguf -n 32 --prompt \"Once upon a time, there existed a little girl who liked to have adventures. She wanted to go to places and meet new people, and have fun\" -c 1024 -t 8 -e -ngl 99 --color\r\nLog start\r\nmain: build = 1 (1810c22)\r\nmain: built with MSVC 19.38.33130.0 for\r\nmain: seed  = 1726854813\r\nllama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\r\nllama_model_loader: - kv  11:                          general.file_type u32              = 15\r\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  19:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q4_K:  193 tensors\r\nllama_model_loader: - type q6_K:   33 tensors\r\nllm_load_vocab: special tokens cache size = 3\r\nllm_load_vocab: token to piece cache size = 0.1637 MB\r\nllm_load_print_meta: format           = GGUF V2\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 32768\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 32768\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q4_K - Medium\r\nllm_load_print_meta: model params     = 7.24 B\r\nllm_load_print_meta: model size       = 4.07 GiB (4.83 BPW)\r\nllm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nllm_load_print_meta: max token length = 48\r\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nggml_sycl_init: SYCL_USE_XMX: yes\r\nggml_sycl_init: found 1 SYCL devices:\r\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\r\nllm_load_tensors: ggml ctx size =    0.27 MiB\r\nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 33/33 layers to GPU\r\nllm_load_tensors:      SYCL0 buffer size =  4095.05 MiB\r\nllm_load_tensors:        CPU buffer size =    70.31 MiB\r\n..............................................................................................\r\nllama_new_context_with_model: n_ctx      = 1024\r\nllama_new_context_with_model: n_batch    = 1024\r\nllama_new_context_with_model: n_ubatch   = 1024\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\n[SYCL] call ggml_check_sycl\r\nggml_check_sycl: GGML_SYCL_DEBUG: 0\r\nggml_check_sycl: GGML_SYCL_F16: no\r\nfound 1 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |\r\n    |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |\r\n    |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|                 Intel Iris Xe Graphics|    1.3|     96|     512|   32| 15516M|            1.3.29516|\r\nllama_kv_cache_init:      SYCL0 KV buffer size =   128.00 MiB\r\nllama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\r\nllama_new_context_with_model:  SYCL_Host  output buffer size =     0.12 MiB\r\nllama_new_context_with_model:      SYCL0 compute buffer size =   164.01 MiB\r\nllama_new_context_with_model:  SYCL_Host compute buffer size =    20.01 MiB\r\nllama_new_context_with_model: graph nodes  = 902\r\nllama_new_context_with_model: graph splits = 2\r\nThe program was built for 1 devices\r\nBuild program log for 'Intel(R) Iris(R) Xe Graphics':\r\n -11 (PI_ERROR_BUILD_PROGRAM_FAILURE)Exception caught at file:C:/Users/Administrator/actions-runner/release-cpp-oneapi_2024_2/_work/llm.cpp/llm.cpp/llama-cpp-bigdl/ggml/src/ggml-sycl.cpp, line:3715\r\n ```\r\n\r\nI'm trying to run this on Intel Iris Xe Graphics with about 15.8GB of VRAM. GPU Driver version is 31.0.101.5590. Let me know if any additional information is required. I am not sure what the root cause of this issue is.",
      "state": "open",
      "author": "aalinkil",
      "author_type": "User",
      "created_at": "2024-09-20T18:32:21Z",
      "updated_at": "2024-09-26T02:56:12Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12108/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "JinheTang"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12108",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12108",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:14.880075",
      "comments": [
        {
          "author": "aalinkil",
          "body": "I continued by running the `init-ollama.bat` and tried running ollama with the intel iGPU. After that failed I tried running the same sample example in the [quickstart guide](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/llama_cpp_quickstart.md#3-example-running-commun",
          "created_at": "2024-09-20T18:53:07Z"
        },
        {
          "author": "JinheTang",
          "body": "Hi @aalinkil, we will try to reproduce the issue first and let you know if there's any progress.",
          "created_at": "2024-09-23T05:27:02Z"
        },
        {
          "author": "JinheTang",
          "body": "Hi @aalinkil , the latest version of `ipex-llm` seems to have this problem on `Intel(R) Iris(R) Xe Graphics`. you can downgrade it to the currently working version `2.2.0b20240911`. To avoid conflict, create a new conda environment and install the specified version: \r\n```log\r\nconda create -n llm-cpp",
          "created_at": "2024-09-23T07:22:39Z"
        },
        {
          "author": "aalinkil",
          "body": "Thanks, that worked!",
          "created_at": "2024-09-26T02:56:10Z"
        }
      ]
    },
    {
      "issue_number": 12118,
      "title": "An error occurred while running the qwen2.5:3b model.",
      "body": "I have updated version 20240924.\r\nwhen i run qwen2.5:3b model. I got the below error:\r\n\r\ntime=2024-09-25T08:35:39.339+08:00 level=INFO source=server.go:395 msg=\"starting llama server\" cmd=\"D:\\\\python\\\\ai\\\\llama-cpp\\\\dist\\\\windows-amd64\\\\lib\\\\ollama\\\\runners\\\\cpu_avx2\\\\ollama_llama_server.exe --model D:\\\\software\\\\ollama_models\\\\blobs\\\\sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --embedding --log-disable --n-gpu-layers 999 --parallel 1 --port 56356\"\r\ntime=2024-09-25T08:35:39.532+08:00 level=INFO source=sched.go:450 msg=\"loaded runners\" count=1\r\ntime=2024-09-25T08:35:39.532+08:00 level=INFO source=server.go:595 msg=\"waiting for llama runner to start responding\"\r\ntime=2024-09-25T08:35:39.542+08:00 level=INFO source=server.go:629 msg=\"waiting for server to become available\" status=\"llm server error\"\r\ntime=2024-09-25T08:35:45.801+08:00 level=ERROR source=sched.go:456 msg=\"error loading llama server\" error=\"llama runner process has terminated: exit status 0xc0000135\"\r\n[GIN] 2024/09/25 - 08:35:45 | 500 |    6.7307138s |       127.0.0.1 | POST     \"/api/embed\"\r\n\r\nKindly help to confirm",
      "state": "open",
      "author": "JerryXu2023",
      "author_type": "User",
      "created_at": "2024-09-25T00:43:58Z",
      "updated_at": "2024-09-26T00:40:21Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12118/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12118",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12118",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:15.118746",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @JerryXu2023, we have reproduced your issue and are currently working on a fix. As a temporary workaround, you may install version 20240911.",
          "created_at": "2024-09-25T10:27:31Z"
        },
        {
          "author": "JerryXu2023",
          "body": "@sgwhat Thanks for your confirm. I will install 20240911.",
          "created_at": "2024-09-26T00:40:20Z"
        }
      ]
    },
    {
      "issue_number": 12112,
      "title": "undefined symbol: iJIT_NotifyEvent  with oneapi 2024.2, but no issue with 2024.0.1",
      "body": "I tried ipex-llm with ARC770, but meet undefined symbol: iJIT_NotifyEvent  with oneapi 2024.2, no issue with oneapi 2024.0.1\r\n\r\npython -c \"import torch\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/gta/conda/envs/llm/lib/python3.11/site-packages/torch/__init__.py\", line 235, in <module>\r\n    from torch._C import *  # noqa: F403\r\n    ^^^^^^^^^^^^^^^^^^^^^^\r\nImportError: /home/gta/conda/envs/llm/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so: undefined symbol: iJIT_NotifyEvent",
      "state": "open",
      "author": "Jianshui",
      "author_type": "User",
      "created_at": "2024-09-23T13:33:15Z",
      "updated_at": "2024-09-25T13:17:37Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12112/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12112",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12112",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:15.329530",
      "comments": [
        {
          "author": "liu-shaojun",
          "body": "Hi @Jianshui Based on our offline sync, we currently recommend using oneAPI 2024.0.1 If we upgrade to 2024.2 in the future, we will update you accordingly.",
          "created_at": "2024-09-24T02:13:07Z"
        },
        {
          "author": "Jianshui",
          "body": "thanks @liu-shaojun \r\nI saw there is also new ipex release, normally it's validated with latest oneapi toolkit \r\n[v2.3.110+xpu](https://github.com/intel/intel-extension-for-pytorch/releases/tag/v2.3.110%2Bxpu)",
          "created_at": "2024-09-24T14:21:55Z"
        },
        {
          "author": "kunger97",
          "body": "> Hi @Jianshui Based on our offline sync, we currently recommend using oneAPI 2024.0.1 If we upgrade to 2024.2 in the future, we will update you accordingly.\r\n\r\nIntel® oneAPI Base Toolkit now update to 2024.2 and can not download older version if i don't have Priority Support witch is paid.",
          "created_at": "2024-09-25T12:43:57Z"
        },
        {
          "author": "jason-dai",
          "body": "> > Hi @Jianshui Based on our offline sync, we currently recommend using oneAPI 2024.0.1 If we upgrade to 2024.2 in the future, we will update you accordingly.\r\n> \r\n> Intel® oneAPI Base Toolkit now update to 2024.2 and can not download older version if i don't have Priority Support witch is paid.\r\n\r",
          "created_at": "2024-09-25T13:17:36Z"
        }
      ]
    },
    {
      "issue_number": 11626,
      "title": "json.decoder.JSONDecodeError: GraphRAG with IPEX-LLM on Intel GPU",
      "body": "hello, I am trying to replicate GraphRAG Demo on Intel Arc GPU 770, But getting below issue :\r\n\r\nI am facing issue wit mistral :\r\n```\r\n12:33:38,271 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\r\n12:33:38,272 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat \"Process\" with 6 retries took 16.932000000029802. input_tokens=2152, output_tokens=693\r\n12:33:53,74 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\r\n12:33:53,75 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat \"Process\" with 6 retries took 31.74200000008568. input_tokens=2059, output_tokens=547\r\n12:35:07,794 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\r\n12:35:07,796 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat \"Process\" with 6 retries took 106.37199999997392. input_tokens=2234, output_tokens=2984\r\n12:35:07,804 datashaper.workflow.workflow INFO executing verb snapshot\r\n12:35:07,808 datashaper.workflow.workflow INFO executing verb merge_graphs\r\n12:35:07,813 datashaper.workflow.workflow INFO executing verb snapshot_rows\r\n12:35:07,815 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet\r\n12:35:07,897 graphrag.index.run INFO Running workflow: create_summarized_entities...\r\n12:35:07,897 graphrag.index.run INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']\r\n12:35:07,897 graphrag.index.run INFO read table from storage: create_base_extracted_entities.parquet\r\n12:35:07,906 datashaper.workflow.workflow INFO executing verb summarize_descriptions\r\n12:35:07,911 datashaper.workflow.workflow INFO executing verb snapshot_rows\r\n12:35:07,912 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet\r\n12:35:07,994 graphrag.index.run INFO Running workflow: create_base_entity_graph...\r\n12:35:07,994 graphrag.index.run INFO dependencies for create_base_entity_graph: ['create_summarized_entities']\r\n12:35:07,994 graphrag.index.run INFO read table from storage: create_summarized_entities.parquet\r\n12:35:08,5 datashaper.workflow.workflow INFO executing verb cluster_graph\r\n12:35:08,5 graphrag.index.verbs.graph.clustering.cluster_graph WARNING Graph has no nodes\r\n12:35:08,6 datashaper.workflow.workflow ERROR Error executing verb \"cluster_graph\" in create_base_entity_graph: Columns must be same length as key\r\nTraceback (most recent call last):\r\n  File \"/home/spandey2/miniconda3/envs/graphrag-local-ollama/lib/python3.10/site-packages/datashaper/workflow/workflow.py\", line 410, in _execute_verb\r\n    result = node.verb.func(**verb_args)\r\n  File \"/home/spandey2/LLM_KG_RAG/graphrag-local-ollama/graphrag/index/verbs/graph/clustering/cluster_graph.py\", line 102, in cluster_graph\r\n    output_df[[level_to, to]] = pd.DataFrame(\r\n  File \"/home/spandey2/miniconda3/envs/graphrag-local-ollama/lib/python3.10/site-packages/pandas/core/frame.py\", line 4299, in __setitem__\r\n    self._setitem_array(key, value)\r\n  File \"/home/spandey2/miniconda3/envs/graphrag-local-ollama/lib/python3.10/site-packages/pandas/core/frame.py\", line 4341, in _setitem_array\r\n    check_key_length(self.columns, key, value)\r\n  File \"/home/spandey2/miniconda3/envs/graphrag-local-ollama/lib/python3.10/site-packages/pandas/core/indexers/utils.py\", line 390, in check_key_length\r\n    raise ValueError(\"Columns must be same length as key\")\r\nValueError: Columns must be same length as key\r\n12:35:08,7 graphrag.index.reporting.file_workflow_callbacks INFO Error executing verb \"cluster_graph\" in create_base_entity_graph: Columns must be same length as key details=None\r\n12:35:08,7 graphrag.index.run ERROR error running workflow create_base_entity_graph\r\nTraceback (most recent call last):\r\n  File \"/home/spandey2/LLM_KG_RAG/graphrag-local-ollama/graphrag/index/run.py\", line 323, in run_pipeline\r\n    result = await workflow.run(context, callbacks)\r\n  File \"/home/spandey2/miniconda3/envs/graphrag-local-ollama/lib/python3.10/site-packages/datashaper/workflow/workflow.py\", line 369, in run\r\n    timing = await self._execute_verb(node, context, callbacks)\r\n  File \"/home/spandey2/miniconda3/envs/graphrag-local-ollama/lib/python3.10/site-packages/datashaper/workflow/workflow.py\", line 410, in _execute_verb\r\n    result = node.verb.func(**verb_args)\r\n  File \"/home/spandey2/LLM_KG_RAG/graphrag-local-ollama/graphrag/index/verbs/graph/clustering/cluster_graph.py\", line 102, in cluster_graph\r\n    output_df[[level_to, to]] = pd.DataFrame(\r\n  File \"/home/spandey2/miniconda3/envs/graphrag-local-ollama/lib/python3.10/site-packages/pandas/core/frame.py\", line 4299, in __setitem__\r\n    self._setitem_array(key, value)\r\n  File \"/home/spandey2/miniconda3/envs/graphrag-local-ollama/lib/python3.10/site-packages/pandas/core/frame.py\", line 4341, in _setitem_array\r\n    check_key_length(self.columns, key, value)\r\n  File \"/home/spandey2/miniconda3/envs/graphrag-local-ollama/lib/python3.10/site-packages/pandas/core/indexers/utils.py\", line 390, in check_key_length\r\n    raise ValueError(\"Columns must be same length as key\")\r\nValueError: Columns must be same length as key\r\n12:35:08,7 graphrag.index.reporting.file_workflow_callbacks INFO Error running pipeline! details=None\r\n```\r\n```\r\n🚀 create_base_documents\r\n                                 id                                         text_units                                        raw_content                      title\r\n0  8af33f74cd8e0e4b0384f5bf5396d993  [7b4e128a12389cacb693c4d1cf7a7965, efd8fda36bf...  Introduction to Graph Neural Networks\\nGraph N...              GNN_intro.txt\r\n1  a15d1b96e67359498242ba415f8aa326  [e65eea82cd46a8251e3ecf779e46cb6e, ee0c1bc3dce...  Introduction to Transformer Neural Networks\\nT...     Transformers_intro.txt\r\n2  66ed8cbe18ccd47bbaef69aa492f2337  [72ee0a4be0a9109cffbb8d94f4253493, 4a25dab6bbc...  Introduction to Machine Learning\\nMachine lear...  machinelearning_intro.txt\r\n3  f5af7825fb7ca37fb6a81f68f4a9a45f  [e2083317ca3a8f0690bde0981dd98ea3, bc5189e2787...  Introduction to Convolutional Neural Networks\\...              CNN_intro.txt\r\n🚀 create_final_documents\r\n                                 id                                      text_unit_ids                                        raw_content                      title\r\n0  8af33f74cd8e0e4b0384f5bf5396d993  [7b4e128a12389cacb693c4d1cf7a7965, efd8fda36bf...  Introduction to Graph Neural Networks\\nGraph N...              GNN_intro.txt\r\n1  a15d1b96e67359498242ba415f8aa326  [e65eea82cd46a8251e3ecf779e46cb6e, ee0c1bc3dce...  Introduction to Transformer Neural Networks\\nT...     Transformers_intro.txt\r\n2  66ed8cbe18ccd47bbaef69aa492f2337  [72ee0a4be0a9109cffbb8d94f4253493, 4a25dab6bbc...  Introduction to Machine Learning\\nMachine lear...  machinelearning_intro.txt\r\n3  f5af7825fb7ca37fb6a81f68f4a9a45f  [e2083317ca3a8f0690bde0981dd98ea3, bc5189e2787...  Introduction to Convolutional Neural Networks\\...              CNN_intro.txt\r\n⠴ GraphRAG Indexer\r\n├── Loading Input (text) - 4 files loaded (0 filtered) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00 0:00:00\r\n├── create_base_text_units\r\n├── create_base_extracted_entities\r\n├── create_summarized_entities\r\n├── create_base_entity_graph\r\n├── create_final_entities\r\n├── create_final_nodes\r\n├── create_final_communities\r\n├── join_text_units_to_entity_ids\r\n├── create_final_relationships\r\n├── join_text_units_to_relationship_ids\r\n├── create_final_community_reports\r\n├── create_final_text_units\r\n├── create_base_documents\r\n└── create_final_documents\r\n🚀 All workflows completed successfully.\r\n```\r\n\r\n\r\n```\r\nINFO: Reading settings from ragtest/settings.yaml\r\ncreating llm client with {'api_key': 'REDACTED,len=9', 'type': \"openai_chat\", 'model': 'llama2', 'max_tokens': 4000, 'temperature': 0.0, 'top_p': 1.0, 'request_timeout': 180.0, 'api_base': 'http://localhost:11434/v1', 'api_version': None, 'organization': None, 'proxy': None, 'cognitive_services_endpoint': None, 'deployment_name': None, 'model_supports_json': True, 'tokens_per_minute': 0, 'requests_per_minute': 0, 'max_retries': 10, 'max_retry_wait': 10.0, 'sleep_on_rate_limit_recommendation': True, 'concurrent_requests': 25}\r\nError parsing search response json\r\nTraceback (most recent call last):\r\n  File \"/home/spandey2/LLM_KG_RAG/graphrag-local-ollama/graphrag/query/structured_search/global_search/search.py\", line 194, in _map_response_single_batch\r\n    processed_response = self.parse_search_response(search_response)\r\n  File \"/home/spandey2/LLM_KG_RAG/graphrag-local-ollama/graphrag/query/structured_search/global_search/search.py\", line 232, in parse_search_response\r\n    parsed_elements = json.loads(search_response)[\"points\"]\r\n  File \"/home/spandey2/miniconda3/envs/graphrag-local-ollama/lib/python3.10/json/__init__.py\", line 346, in loads\r\n    return _default_decoder.decode(s)\r\n  File \"/home/spandey2/miniconda3/envs/graphrag-local-ollama/lib/python3.10/json/decoder.py\", line 337, in decode\r\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\n  File \"/home/spandey2/miniconda3/envs/graphrag-local-ollama/lib/python3.10/json/decoder.py\", line 355, in raw_decode\r\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\r\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\r\n\r\nSUCCESS: Global Search Response: I am sorry but I am unable to answer this question given the provided data.\r\n```\r\n",
      "state": "open",
      "author": "shailesh837",
      "author_type": "User",
      "created_at": "2024-07-19T11:42:42Z",
      "updated_at": "2024-09-25T01:30:57Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11626/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Oscilloscope98"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11626",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11626",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:15.562064",
      "comments": [
        {
          "author": "Oscilloscope98",
          "body": "Hi @shailesh837,\r\n\r\nHave you added the `request_timeout` configuration to `ragtest/settings.yml` as we mentioned in [GraphRAG quickstart](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/graphrag_quickstart.md#update-settingsyml)?\r\n\r\nIf not, would you mind adding this con",
          "created_at": "2024-07-22T02:31:01Z"
        },
        {
          "author": "aiChatGPT35User123",
          "body": "> hello, I am trying to replicate GraphRAG Demo on Intel Arc GPU 770, But getting below issue :\r\n> \r\n> I am facing issue wit mistral :\r\n> \r\n> ```\r\n> 12:33:38,271 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\r\n> 12:33:38,272 graphrag.llm.base.rate_limiting",
          "created_at": "2024-07-25T07:27:52Z"
        },
        {
          "author": "worstkid92",
          "body": "Any updates on this? I also met this problem",
          "created_at": "2024-09-24T02:29:57Z"
        },
        {
          "author": "Oscilloscope98",
          "body": "Hi @aiChatGPT35User123, @worstkid92, \r\n\r\nWould you mind having a try based on the updates [here](https://github.com/intel-analytics/ipex-llm/issues/11983#issuecomment-2325847103)? :)",
          "created_at": "2024-09-25T01:30:55Z"
        }
      ]
    },
    {
      "issue_number": 11578,
      "title": "Does IPEX-LLM support Flash Attention ?",
      "body": "Hi,\r\ni encounter the following error message trying to enable flash attention when running the command below. Can i know is flash attention supported ?\r\n\r\n``command:  ./main -m $model -n 128 --prompt \"${prompt_1024_128}\" -t 8 -e -ngl 999 --color --ctx-size 1024 --no-mmap --temp 0 -fa``\r\n```diff\r\n-ggml_backend_sycl_graph_compute: <error: op not supported node_18 (FLASH_ATTN_EXT)\r\n```\r\nsee truncated log below:\r\n>lm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\r\nllm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\r\nllm_load_print_meta: LF token         = 128 'Ä'\r\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\r\n[SYCL] call ggml_init_sycl\r\nggml_init_sycl: GGML_SYCL_DEBUG: 0\r\nggml_init_sycl: GGML_SYCL_F16: no\r\nfound 4 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|                     Intel Arc Graphics|    1.3|    128|    1024|   32| 94386M|            1.3.29735|\r\n| 1|     [opencl:gpu:0]|                     Intel Arc Graphics|    3.0|    128|    1024|   32| 94386M|       24.22.29735.20|\r\n| 2|     [opencl:cpu:0]|                Intel Core Ultra 7 155H|    3.0|     22|    8192|   64|100912M|2023.16.12.0.12_195853.xmain-hotfix|\r\n| 3|     [opencl:acc:0]|            Intel FPGA Emulation Device|    1.2|     22|67108864|   64|100912M|2023.16.12.0.12_195853.xmain-hotfix|\r\nggml_backend_sycl_set_mul_device_mode: true\r\ndetect 1 SYCL GPUs: [0] with top Max compute units:128\r\nllm_load_tensors: ggml ctx size =    0.30 MiB\r\nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 33/33 layers to GPU\r\nllm_load_tensors:      SYCL0 buffer size =  4403.49 MiB\r\nllm_load_tensors:  SYCL_Host buffer size =   281.81 MiB\r\n.......................................................................................\r\nllama_new_context_with_model: n_ctx      = 1024\r\nllama_new_context_with_model: n_batch    = 1024\r\nllama_new_context_with_model: n_ubatch   = 1024\r\nllama_new_context_with_model: flash_attn = 1\r\nllama_new_context_with_model: freq_base  = 500000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:      SYCL0 KV buffer size =   128.00 MiB\r\nllama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\r\nllama_new_context_with_model:  SYCL_Host  output buffer size =     0.49 MiB\r\nllama_new_context_with_model:      SYCL0 compute buffer size =   517.00 MiB\r\nllama_new_context_with_model:  SYCL_Host compute buffer size =    20.01 MiB\r\nllama_new_context_with_model: graph nodes  = 903\r\nllama_new_context_with_model: graph splits = 2\r\nggml_backend_sycl_graph_compute: **<span style=\"color:red\">error: op not supported node_18 (FLASH_ATTN_EXT)</span>**\r\nGGML_ASSERT: /home/runner/_work/llm.cpp/llm.cpp/llama-cpp-bigdl/ggml-sycl.cpp:17765: ok\r\n./benchmark_llama-cpp.sh: line 24: 16616 Aborted                 (core dumped) ./main -m $model -n 128 --prompt \"${promt_1024_128}\" -t 8 -e -ngl 999 --color --ctx-size 1024 --no-mmap --temp 0 -fa\r\n\r\n",
      "state": "open",
      "author": "wallacezq",
      "author_type": "User",
      "created_at": "2024-07-15T03:44:42Z",
      "updated_at": "2024-09-25T00:19:54Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11578/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11578",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11578",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:15.765859",
      "comments": [
        {
          "author": "cyita",
          "body": "Hi, flash attention is not supported in llama.cpp with IPEX-LLM. We added optimized attention for sycl backend, and it is automatically turned on if the provided model is supported.",
          "created_at": "2024-07-15T08:04:32Z"
        },
        {
          "author": "wallace-lee",
          "body": "> Hi, flash attention is not supported in llama.cpp with IPEX-LLM. We added optimized attention for sycl backend, and it is automatically turned on if the provided model is supported.\r\n\r\nI see. Can i know how would i know if a model can support optimized attention for sycl backend ?\r\n",
          "created_at": "2024-07-16T05:35:42Z"
        },
        {
          "author": "cyita",
          "body": "Here is a list we already verified:\r\n<img width=\"156\" alt=\"image\" src=\"https://github.com/user-attachments/assets/eb42ab1c-49e6-4dd2-8ad9-440e3769e2e9\">",
          "created_at": "2024-07-16T06:11:43Z"
        },
        {
          "author": "sambartik",
          "body": "Hi @cyita , does that mean that [ollama with ipex-llm](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.md) _does_ support flash attention for supported models automatically?\r\n\r\nAs far as I know, the ipex-llm backend does use SYCL.",
          "created_at": "2024-09-24T14:53:15Z"
        },
        {
          "author": "jason-dai",
          "body": "> Hi @cyita , does that mean that [ollama with ipex-llm](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.md?rgh-link-date=2024-09-24T14%3A53%3A15Z) _does_ support flash attention for supported models automatically?\r\n> \r\n> As far as I know, the ipex-llm ",
          "created_at": "2024-09-25T00:19:52Z"
        }
      ]
    },
    {
      "issue_number": 11982,
      "title": "MiniCPM-v 2.6 and llama-cpp can not work and accelerated on A770 dGPU?",
      "body": null,
      "state": "open",
      "author": "yangqing-yq",
      "author_type": "User",
      "created_at": "2024-08-31T11:32:51Z",
      "updated_at": "2024-09-24T07:11:17Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11982/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "JinheTang"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11982",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11982",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:15.996453",
      "comments": [
        {
          "author": "JinheTang",
          "body": "Hi @yangqing-yq , upgrading to `ipex-llm[cpp]>=2.2.0b20240827` may solve this problem. Then you may run \r\n```log\r\n./llama-minicpmv-cli -m ../MiniCPM-V-2_6-gguf/ggml-model-Q4_0.gguf --mmproj ../MiniCPM-V-2_6-gguf/mmproj-model-f16.gguf -c 4096 --temp 0.7 --top-p 0.8 --top-k 100 --repeat-penalty 1.05 -",
          "created_at": "2024-09-02T01:56:46Z"
        },
        {
          "author": "yangqing-yq",
          "body": "this is the result for A750. Can you help to confirm if these values are correct?\r\nespecially the TTFT is 4689 ms?!\r\ninput image is 1920x1080\r\n\"\r\nllama_print_timings:        load time =    6392.73 ms\r\nllama_print_timings:      sample time =      43.04 ms /    73 runs   (    0.59 ms per token,  1696.",
          "created_at": "2024-09-14T08:00:19Z"
        },
        {
          "author": "yangqing-yq",
          "body": "@JinheTang @qiuxin2012 ",
          "created_at": "2024-09-20T06:07:11Z"
        },
        {
          "author": "JinheTang",
          "body": "Hi @yangqing-yq , we tested it on our A750 machine and our results were similar to yours. It should be correct.",
          "created_at": "2024-09-24T07:11:16Z"
        }
      ]
    },
    {
      "issue_number": 12093,
      "title": "Error connection when testing fastchat case",
      "body": "Reference：https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/fastchat_quickstart.md\r\n\r\nWhile testing Fastchat with below steps, I  got connectin error as below attachment pic，please help check， thx！\r\n\r\nsteps：\r\n1. Launch controller：  python3 -m fastchat.serve.controller\r\n===========================================\r\n# python3 -m fastchat.serve.controller\r\n2024-09-20 02:01:02 | INFO | controller | args: Namespace(host='localhost', port=21001, dispatch_method='shortest_queue', ssl=False)\r\n2024-09-20 02:01:03 | ERROR | stderr | INFO:     Started server process [4124]\r\n2024-09-20 02:01:03 | ERROR | stderr | INFO:     Waiting for application startup.\r\n2024-09-20 02:01:03 | ERROR | stderr | INFO:     Application startup complete.\r\n2024-09-20 02:01:03 | ERROR | stderr | INFO:     Uvicorn running on http://localhost:21001 (Press CTRL+C to quit)\r\n=========================================\r\n2. load worker and model:\r\n# python3 -m ipex_llm.serving.fastchat.ipex_llm_worker --model-path /llm/models/Yuan2.0-2B-hf --low-bit \"sym_int4\" --trust-remote-code --device \"cpu\"\r\n=========================================\r\n2024-09-20 02:03:13 | INFO | ipex_llm.transformers.utils | Converting the current model to sym_int4 format......\r\n2024-09-20 02:03:13 | ERROR | stderr | /usr/local/lib/python3.11/dist-packages/torch/nn/init.py:412: UserWarning: Initializing zero-element tensors is a no-op\r\n2024-09-20 02:03:13 | ERROR | stderr |   warnings.warn(\"Initializing zero-element tensors is a no-op\")\r\n2024-09-20 02:03:15 | INFO | datasets | PyTorch version 2.1.0.post2+cxx11.abi available.\r\n2024-09-20 02:03:17 | INFO | stdout | <class 'transformers_modules.Yuan2.0-2B-hf.yuan_hf_model.YuanForCausalLM'>\r\n2024-09-20 02:03:17 | INFO | model_worker | enable benchmark successfully\r\n2024-09-20 02:03:17 | INFO | model_worker | Register to controller\r\n2024-09-20 02:03:17 | ERROR | stderr | INFO:     Started server process [4466]\r\n2024-09-20 02:03:17 | ERROR | stderr | INFO:     Waiting for application startup.\r\n2024-09-20 02:03:17 | ERROR | stderr | INFO:     Application startup complete.\r\n2024-09-20 02:03:17 | ERROR | stderr | INFO:     Uvicorn running on http://localhost:21002 (Press CTRL+C to quit)\r\n=========================================\r\n\r\n3. start gradio server：\r\n# python3 -m fastchat.serve.gradio_web_server\r\n=======================================\r\n2024-09-20 02:04:08 | INFO | gradio_web_server | args: Namespace(host='0.0.0.0', port=None, share=False, controller_url='http://localhost:21001', concurrency_count=10, model_list_mode='once', moderate=False, show_terms_of_use=False, register_api_endpoint_file=None, gradio_auth_path=None, gradio_root_path=None)\r\n2024-09-20 02:04:08 | INFO | gradio_web_server | All models: ['Yuan2.0-2B-hf']\r\n2024-09-20 02:04:08 | INFO | gradio_web_server | Visible models: ['Yuan2.0-2B-hf']\r\n2024-09-20 02:04:08 | ERROR | stderr | /usr/local/lib/python3.11/dist-packages/gradio/components/chatbot.py:212: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead.\r\n2024-09-20 02:04:08 | ERROR | stderr |   warnings.warn(\r\n2024-09-20 02:04:09 | INFO | stdout | * Running on local URL:  http://0.0.0.0:7860\r\n2024-09-20 02:04:09 | INFO | stdout | \r\n2024-09-20 02:04:09 | INFO | stdout | To create a public link, set `share=True` in `launch()`.\r\n=============================================\r\n\r\nSo first 3 steps look correct!\r\n\r\n4. Web UI ：       http://x.x.x.x:7860/\r\n![fastchat-error-info](https://github.com/user-attachments/assets/57bbf2e7-94a3-4998-a7ef-3d89031f88d5)\r\n\r\nconnection errored out!!!!\r\n\r\n",
      "state": "closed",
      "author": "guzhangbo1989",
      "author_type": "User",
      "created_at": "2024-09-19T10:13:49Z",
      "updated_at": "2024-09-24T02:02:45Z",
      "closed_at": "2024-09-24T02:02:45Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12093/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "ACupofAir"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12093",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12093",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:16.200814",
      "comments": [
        {
          "author": "jianweimama",
          "body": "更换ipex-llm 镜像到 intelanalytics/ipex-llm-serving-xpu:2.1.0b2 后, 问题消失.",
          "created_at": "2024-09-20T09:15:22Z"
        }
      ]
    },
    {
      "issue_number": 11605,
      "title": "Support for Ubuntu 22.04 on Meteor Lake is broken",
      "body": "I recently bought a NUC 14 Core Ultra 5 125H. The [documentation](https://github.com/intel-analytics/ipex-llm/issues/11521) to get IPEX-LLM with Intel OneAPI/SYCL running on Ubuntu 22.04 with a Meteor Lake doesn't work. It is not possible to install the DKMS module with `intel-i915-dkms` because afterwards the system will freeze and lock up. There is no [official support](https://forums.linuxmint.com/viewtopic.php?p=2481211&sid=7218e682ba70ae8a456d16fb8af6f233#p2481211) for Meteor Lake in Kernel 6.5 which is requiered for IPEX-LLM but one can [force load](https://www.kernel.org/doc/html//next/gpu/rfc/xe.html#xe-platforms) the new `Xe` drivers with `i915.force_probe=!7d55 xe.force_probe=7d55` in the Kernel parameters. Intel has to update its documentation or expand its support to Ubuntu 24.04. \r\n\r\nSome more related links:\r\n\r\n- https://community.intel.com/t5/Graphics/Error-installing-intel-i915-dkms-on-6-5-0-41-generic/m-p/1615231#M132875\r\n- https://askubuntu.com/questions/1507167/no-driver-for-intel-integrated-graphics-ubuntu-22-04-4-lts/1507452#1507452",
      "state": "open",
      "author": "tristan-k",
      "author_type": "User",
      "created_at": "2024-07-17T14:26:21Z",
      "updated_at": "2024-09-20T12:01:39Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11605/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11605",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11605",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:18.319679",
      "comments": [
        {
          "author": "JinBridger",
          "body": "Hi, @tristan-k,\r\n\r\nWe are now trying to reproduce this issue on device with similar specifications. We will inform you as soon as possible if there is any progress.\r\n\r\nPlease feel free to ask if there is any further problems : )",
          "created_at": "2024-07-19T07:44:24Z"
        },
        {
          "author": "JinBridger",
          "body": "Hi, @tristan-k, \r\n\r\nCould you please try instructions in following link to see if it works? https://github.com/intel-analytics/ipex-llm/issues/11568#issuecomment-2227157685\r\n\r\nPlease feel free to ask if there is any further problems : )",
          "created_at": "2024-07-22T02:28:15Z"
        },
        {
          "author": "tristan-k",
          "body": "@JinBridger I already did that. It did not make any difference, as previously mentioned in another [comment](https://github.com/intel-analytics/ipex-llm/issues/11521#issuecomment-2227419727) of mine.",
          "created_at": "2024-07-22T17:25:09Z"
        },
        {
          "author": "JinBridger",
          "body": "> @JinBridger I already did that. It did not make any difference, as previously mentioned in another [comment](https://github.com/intel-analytics/ipex-llm/issues/11521#issuecomment-2227419727) of mine.\r\n\r\nHi, @tristan-k,\r\n\r\nCould you please try to skip installing `intel-i915-dkms` and see if IPEX-LL",
          "created_at": "2024-07-24T02:34:16Z"
        },
        {
          "author": "tristan-k",
          "body": "@JinBridger It does not. I alredy tried. The `Intel(R) Arc(TM) Graphics` is not available for `ollama`. I also tried to use [docker](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/DockerGuides/docker_cpp_xpu_quickstart.md#pull-the-latest-image) and `sycl-ls` inside the docker cont",
          "created_at": "2024-07-24T11:39:26Z"
        }
      ]
    },
    {
      "issue_number": 12091,
      "title": "support for Qwen2 MoE ",
      "body": "https://hf-mirror.com/Qwen/Qwen2-57B-A14B-Instruct",
      "state": "open",
      "author": "zhm-algo",
      "author_type": "User",
      "created_at": "2024-09-19T06:05:41Z",
      "updated_at": "2024-09-20T02:25:46Z",
      "closed_at": null,
      "labels": [
        "user issue",
        "multi-arc"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12091/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "xiangyuT"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12091",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12091",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:18.649901",
      "comments": [
        {
          "author": "xiangyuT",
          "body": "Hi @zhm-algo,\r\n\r\nCurrently, vLLM on XPU may not yet support MoE models. There are some MoE-related operations and methods that are missing for XPU.",
          "created_at": "2024-09-20T02:25:45Z"
        }
      ]
    },
    {
      "issue_number": 11836,
      "title": "Error: 'wrong number of tensors; expected 292, got 291'",
      "body": "Trying out the current versions of ipex-llm with `ollama` and when trying to use the `llama3.1` model the following output is displayed:\r\n\r\n```\r\ndetect 1 SYCL GPUs: [0] with top Max compute units:512\r\nllm_load_tensors: ggml ctx size =    0.30 MiB\r\nllama_model_load: error loading model: done_getting_tensors: wrong number of tensors; expected 292, got 291\r\nllama_load_model_from_file: exception loading model\r\nterminate called after throwing an instance of 'std::runtime_error'\r\n  what():  done_getting_tensors: wrong number of tensors; expected 292, got 291\r\ntime=2024-08-18T19:30:54.756Z level=ERROR source=sched.go:344 msg=\"error loading llama server\" error=\"llama runner process has terminated: signal: aborted (core dumped) \"\r\n[GIN] 2024/08/18 - 19:30:54 | 500 |  1.820496414s |       127.0.0.1 | POST     \"/api/chat\"\r\n```\r\n\r\nThis error looks like an exact match of: https://github.com/abetlen/llama-cpp-python/issues/1646 . Is it possible that there needs to be an update to the `llama-cpp-python` or just `llama.cpp` components included when doing `pip install --pre --upgrade ipex-llm[cpp]` from the [Quickstart guide](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/llama_cpp_quickstart.md#1-install-ipex-llm-for-llamacpp)?\r\n\r\n`ollama run llama3:8b-instruct-q4_K_M` working fine here. Platform is linux kernel 6.5 and ARC a770 GPU.",
      "state": "closed",
      "author": "timroster",
      "author_type": "User",
      "created_at": "2024-08-18T19:55:52Z",
      "updated_at": "2024-09-19T23:23:48Z",
      "closed_at": "2024-08-20T02:52:49Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11836/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "rnwang04"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11836",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11836",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:18.893452",
      "comments": [
        {
          "author": "rnwang04",
          "body": "Hi @timroster ,\r\nI have reproduced this error, and I will try to fix it later. Once it is done, will update here to let you know.",
          "created_at": "2024-08-19T01:44:29Z"
        },
        {
          "author": "rnwang04",
          "body": "Llama 3.1 is supported with `ipex-llm[cpp] >= 2.1.0b20240819`, you may try it again tomorrow : )\r\n\r\n",
          "created_at": "2024-08-19T11:13:50Z"
        },
        {
          "author": "timroster",
          "body": "@rnwang04 - I update the `ipex-llm` to `ipex-llm-2.1.0b20240819` version and it is working great. Thanks so much for the prompt response 😄 ",
          "created_at": "2024-08-20T02:52:49Z"
        }
      ]
    },
    {
      "issue_number": 12078,
      "title": "vllm not support CodeShell model",
      "body": "I got an error when trying to run CodeShell-7B-Chat model with vllm：\r\n```\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/model_executor/model\r\n_loader/loader.py\", line 151, in _initialize_model\r\n    model_class = get_model_architecture(model_config)[0]\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/vllm-0.5.4+xpu-py3.11-linux-x86_64.egg/vllm/model_executor/model\r\n_loader/utils.py\", line 35, in get_model_architecture\r\n    raise ValueError(\r\nValueError: Model architectures ['CodeShellForCausalLM'] are not supported for now. Supported architectures: ['A\r\nquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'Blip2ForCon\r\nditionalGeneration', 'ChameleonForConditionalGeneration', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'Co\r\nhereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'Falco\r\nnForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCaus\r\nalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'InternVLChatMode\r\nl', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration',\r\n 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'M\r\nPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPMV', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OPTForCausalLM', 'O\r\nrionForCausalLM', 'PersimmonForCausalLM', 'PaliGemmaForConditionalGeneration', 'PhiForCausalLM', 'Phi3ForCausalL\r\nM', 'Phi3VForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RWForCausalLM', 'StableLM\r\nEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Ph\r\ni3SmallForCausalLM', 'MedusaModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'MistralModel']\r\n```\r\nThe script is as following:\r\n```\r\nmodel=\"/llm/models/WisdomShell/CodeShell-7B-Chat\"\r\nserved_model_name=\"CodeShell-7B-Chat\"\r\nexport USE_XETLA=OFF\r\nexport SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1\r\nexport SYCL_CACHE_PERSISTENT=1\r\nexport TORCH_LLM_ALLREDUCE=0\r\nexport CCL_DG2_ALLREDUCE=1\r\n# Tensor parallel related arguments:\r\nexport CCL_WORKER_COUNT=2\r\nexport FI_PROVIDER=shm\r\nexport CCL_ATL_TRANSPORT=ofi\r\nexport CCL_ZE_IPC_EXCHANGE=sockets\r\nexport CCL_ATL_SHM=1\r\nsource /opt/intel/oneapi/setvars.sh --force\r\nsource /opt/intel/1ccl-wks/setvars.sh\r\npython -m ipex_llm.vllm.xpu.entrypoints.openai.api_server \\\r\n--served-model-name $served_model_name \\\r\n--port 8000 \\\r\n--model $model \\\r\n--trust-remote-code \\\r\n--gpu-memory-utilization 0.95 \\\r\n--device xpu \\\r\n--dtype float16 \\\r\n--enforce-eager \\\r\n--load-in-low-bit fp8 \\\r\n--max-model-len 2048 \\\r\n--max-num-batched-tokens 4000 \\\r\n--tensor-parallel-size 2\r\n```\r\n",
      "state": "closed",
      "author": "YongZhuIntel",
      "author_type": "User",
      "created_at": "2024-09-14T02:20:10Z",
      "updated_at": "2024-09-19T01:29:53Z",
      "closed_at": "2024-09-19T01:29:42Z",
      "labels": [
        "user issue",
        "multi-arc"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12078/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hzjane"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12078",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12078",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:19.106613",
      "comments": [
        {
          "author": "hzjane",
          "body": "I follow [this repo](https://github.com/WisdomShell/vllm_for_codeshell/commits/v0.5.3/) to test running `CodeShell-7B-Chat` but encounter this error `RuntimeError: Unsupported block size: 64`. I now try to solve it to run.",
          "created_at": "2024-09-18T01:57:44Z"
        },
        {
          "author": "hzjane",
          "body": "Refer to this [repo](https://github.com/WisdomShell/vllm_for_codeshell/tree/v0.5.3).\r\nOn image intelanalytics/ipex-llm-serving-xpu-vllm-0.5.4-experimental:2.2.0b1, you can hard code run it.\r\n\r\nAdd these two files  https://github.com/WisdomShell/vllm_for_codeshell/blob/v0.5.3/vllm/model_executor/mode",
          "created_at": "2024-09-19T01:24:24Z"
        }
      ]
    },
    {
      "issue_number": 12086,
      "title": "Llama.cpp + Intel Arc A770, RuntimeError: PyTorch is not linked with support for opencl devices",
      "body": "Hi, \r\nI'm trying to get llama.cpp up and running on Ubuntu 24.04 (kernel 6.8.0-44 generic) with Ipex-llm, and it seems I can't select opencl as the api to run the model on. I'm not sure what option to choose, considering I thought the A770 was an openCL device.  \r\n\r\n\r\nHere's the readout:\r\n\r\n```\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 15.93it/s]\r\n2024-09-15 00:56:22,587 - INFO - Converting the current model to sym_int4 format......\r\nTraceback (most recent call last):\r\n  File \"/home/cbytes/demo.py\", line 11, in <module>\r\n    model = model.to('opencl')\r\n            ^^^^^^^^^^^^^^^^^^\r\n  File \"/home/cbytes/miniforge3/envs/llm/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 2905, in to\r\n    return super().to(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/cbytes/miniforge3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1174, in to\r\n    return self._apply(convert)\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/cbytes/miniforge3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\r\n    module._apply(fn)\r\n  File \"/home/cbytes/miniforge3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 780, in _apply\r\n    module._apply(fn)\r\n  File \"/home/cbytes/miniforge3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 805, in _apply\r\n    param_applied = fn(param)\r\n                    ^^^^^^^^^\r\n  File \"/home/cbytes/miniforge3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1160, in convert\r\n    return t.to(\r\n           ^^^^^\r\nRuntimeError: PyTorch is not linked with support for opencl devices\r\n```",
      "state": "open",
      "author": "compellingbytes",
      "author_type": "User",
      "created_at": "2024-09-16T06:46:32Z",
      "updated_at": "2024-09-18T00:41:24Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12086/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12086",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12086",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:19.377135",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "It's a `xpu` device, you can use `model.to('xpu')`.",
          "created_at": "2024-09-18T00:40:24Z"
        }
      ]
    },
    {
      "issue_number": 12067,
      "title": "Garbage output on serving 4 parallel users. ",
      "body": "I started a server with the command ` OLLAMA_NUM_PARALLEL=4 OLLAMA_MAX_LOADED_MODELS=4 ./ollama serve`. We open 4 terminals and executed the command` ./ollama run codellama after which the model loaded. So now on 4 terminals we give the prompt `>>write a long poem.` and execute it simultaneously (four parallel requests). The output is garbage values. \r\n`\r\n![Screenshot_20240911_152331](https://github.com/user-attachments/assets/51aa910e-d24c-4798-b094-0a8fa171c877)\r\n",
      "state": "open",
      "author": "adi-lb-phoenix",
      "author_type": "User",
      "created_at": "2024-09-11T10:00:22Z",
      "updated_at": "2024-09-18T00:41:12Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12067/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12067",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12067",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:19.612324",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @adi-lb-phoenix, could you please provide your env and device config? In our test, ollama was able to run codellama as expected on MTL Linux.",
          "created_at": "2024-09-13T02:36:57Z"
        },
        {
          "author": "adi-lb-phoenix",
          "body": "Hello @sgwhat .\r\nSo I have installed podman and distrobox on kde neon, on which I have created a ubuntu distro using distrobox. Ipex llm is deployed inside the ubuntu distrobox.\r\nInside ubuntu distrobox:\r\n\r\n```\r\nuname -a\r\nLinux ubuntu22_ollama.JOHNAIC 6.5.0-45-generic #45~22.04.1-Ubuntu SMP PREEMPT_",
          "created_at": "2024-09-13T07:49:48Z"
        },
        {
          "author": "sgwhat",
          "body": "We are currently locating the cause of the `codellama` output issue on linux arc770 and will notify you as soon as possible.",
          "created_at": "2024-09-14T02:22:53Z"
        },
        {
          "author": "adi-lb-phoenix",
          "body": "@sgwhat Thank you for picking this up. It has been observed not just for `codellama ` but for other models as well. ",
          "created_at": "2024-09-14T06:23:22Z"
        },
        {
          "author": "adi-lb-phoenix",
          "body": "https://github.com/ggerganov/llama.cpp/issues/9505#issuecomment-2352561991\r\nHere llama.cpp does not output garbage values ",
          "created_at": "2024-09-16T10:43:12Z"
        }
      ]
    },
    {
      "issue_number": 11495,
      "title": "NPU inference error",
      "body": "Hi,\r\nI am interested in the NPU inference for this project.\r\nI tried to run llama on NPU with python\\llm\\example\\NPU\\HF-Transformers-AutoModels\\Model\\llama2\\generate.py.\r\nI used interface `model.save_low_bit` and `AutoModelForCausalLM.load_low_bit` to save and load the converted model, but during the load phase, the error is \r\nAttributeError: 'LlamaAttention' object has no attribute 'llama_attention_forward'\r\nI was not sure if i do this the converted model is not for NPU?\r\n\r\nAny comment or advice is appreciated, thanks !",
      "state": "open",
      "author": "xduzhangjiayu",
      "author_type": "User",
      "created_at": "2024-07-03T01:11:42Z",
      "updated_at": "2024-09-17T08:09:28Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11495/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "leonardozcm"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11495",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11495",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:19.799487",
      "comments": [
        {
          "author": "xduzhangjiayu",
          "body": "During the process there is a warning on console:\r\nsite-packages\\intel_npu_acceleration_library\\backend\\__init__.py:18: UserWarning: NPU is not available in your system. Library will fallback to AUTO device selection mode\r\n\r\nBut I did have Intel's NPU and the latest drivers on my PC, and I ran the L",
          "created_at": "2024-07-03T01:49:33Z"
        },
        {
          "author": "leonardozcm",
          "body": "hi @xduzhangjiayu , for ipex-llm >= 2.1.0b20240704 you may try:\r\n```python\r\nmodel.save_low_bit(model_path)\r\n```\r\nto save low bit model, and\r\n```python\r\n AutoModelForCausalLM.load_low_bit(model_path, trust_remote_code=True)\r\n```\r\nto load low bit model",
          "created_at": "2024-07-04T09:20:35Z"
        },
        {
          "author": "xduzhangjiayu",
          "body": "Cannot install ipex-llm==2.1.0b20240704 for now, i'll try later, thanks !",
          "created_at": "2024-07-04T10:14:37Z"
        },
        {
          "author": "RAKSHITH-JAYANTH",
          "body": "Hello, I too am facing the same error and used ipex-llm==2.1.0b20240704. But it was of no use. I am still getting the below error.\r\n[\\llm\\Lib\\site-packages\\intel_npu_acceleration_library\\backend\\__init__.py:18](file:///C:/Users/raksh/miniconda3/envs/llm/Lib/site-packages/intel_npu_acceleration_libra",
          "created_at": "2024-09-07T23:40:37Z"
        },
        {
          "author": "ch1y0q",
          "body": "> Hello, I too am facing the same error and used ipex-llm==2.1.0b20240704. But it was of no use. I am still getting the below error. \\llm\\Lib\\site-packages\\intel_npu_acceleration_library\\backend__init__.py:18: UserWarning: NPU is not available in your system. Library will fallback to AUTO device sel",
          "created_at": "2024-09-10T07:52:07Z"
        }
      ]
    },
    {
      "issue_number": 10897,
      "title": "Improve First Token Latency for multi-GPU projects (by flash attention or alternative)",
      "body": "For multi-GPU solution, we still have challenges for First Token Latency. The breakdown data is shared in offline.\r\nplease help add more optimization features (like SDP/Flash Attention etc) to improve the First Token Latency.",
      "state": "open",
      "author": "moutainriver",
      "author_type": "User",
      "created_at": "2024-04-26T10:26:39Z",
      "updated_at": "2024-09-14T09:00:02Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/10897/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/10897",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/10897",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:20.010755",
      "comments": [
        {
          "author": "yangqing-yq",
          "body": "mark it",
          "created_at": "2024-09-14T09:00:01Z"
        }
      ]
    },
    {
      "issue_number": 12080,
      "title": "1xArc encounters OOM when running Qwen2-7B, load_in_bit=sym_int4. ",
      "body": "### script\r\n```\r\n  numactl -C 11-15 python ./benchmark_docker_throughput.py \\\r\n  --backend vllm \\\r\n  --dataset /data/ShareGPT_V3_unfiltered_cleaned_split.json \\\r\n  --model Qwen2-7B-Instruct \\\r\n  --trust-remote-code \\\r\n  --num-prompts 100 \\\r\n  --gpu-memory-utilization 0.7 \\ #tried 0.6, 0.7, 0.8, 0.9\r\n  --seed 42 \\\r\n  --device xpu \\\r\n  --dtype float16 \\\r\n  --enforce-eager \\\r\n  --max-model-len 4096 \\\r\n  --max-num-batched-tokens 4096 \\\r\n  --load-in-low-bit sym_int4 \\\r\n  --tensor-parallel-size 1 \r\n```\r\n### Error log\r\n```\r\nfile \"/usr/local/lib/python3.11/dist-packages/ipex_llm/transformers/low_bit_linear.py\", line 797, in forward\r\n    result = xe_linear.forward_new(x_2d, self.weight.data,\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: XPU out of memory. Tried to allocate 130.00 MiB (GPU 0; 15.11 GiB total capacity; 11.09 GiB already allocated; 11.23 GiB reserved in total by PyTorch)\r\n\r\n```",
      "state": "closed",
      "author": "aprilhu01",
      "author_type": "User",
      "created_at": "2024-09-14T02:43:13Z",
      "updated_at": "2024-09-14T07:29:22Z",
      "closed_at": "2024-09-14T07:29:22Z",
      "labels": [
        "user issue",
        "multi-arc"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12080/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Uxito-Ada"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12080",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12080",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:20.239414",
      "comments": [
        {
          "author": "Uxito-Ada",
          "body": "Hi @aprilhu01 ,\r\n\r\nThis is a normal issue caused by too heavy workloads on limited GPU memory resource. I tried to reproduce it with the same parameters and the request sample [here](https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/GPU/vLLM-Serving#3-offline-inferenceservice)",
          "created_at": "2024-09-14T05:53:29Z"
        },
        {
          "author": "aprilhu01",
          "body": "Thanks. Will tune script.",
          "created_at": "2024-09-14T07:29:22Z"
        }
      ]
    },
    {
      "issue_number": 11826,
      "title": "Error: 'Not enough data for satisfy transfer length header.' during vllm serving on Arc",
      "body": "### Scripts and Params\r\n```\r\n# start server\r\npython -m ipex_llm.vllm.xpu.entrypoints.openai.api_server --served-model-name qwen-32b --port 8000 --model /opt/Qwen1.5-32B-Chat --trust-remote-code --gpu-memory-utilization 0.8 --device xpu --dtype float16 --enforce-eager --load-in-low-bit sym_int4 --max-model-len 9408 --max-num-batched-tokens 9408 --tensor-parallel-size 4\r\n\r\n# start client\r\npython benchmark_serving.py --model qwen-32b --tokenizer /opt/Qwen1.5-32B-Chat --random-input-len 7408 --random-output-len 900 --num-prompts 1 --dataset-name random\r\n\r\n```\r\n### Server API Error\r\n```\r\nERROR 08-16 18:23:54 async_llm_engine.py:470] Engine iteration timed out. This should never happen!\r\nERROR 08-16 18:23:54 async_llm_engine.py:41] Engine background task failed\r\nERROR 08-16 18:23:54 async_llm_engine.py:41] Traceback (most recent call last):\r\nERROR 08-16 18:23:54 async_llm_engine.py:41]   File \"/usr/lib/python3.11/asyncio/tasks.py\", line 500, in wait_for\r\nERROR 08-16 18:23:54 async_llm_engine.py:41]     return fut.result()\r\nERROR 08-16 18:23:54 async_llm_engine.py:41]            ^^^^^^^^^^^^\r\nERROR 08-16 18:23:54 async_llm_engine.py:41]   File \"/llm/vllm/vllm/engine/async_llm_engine.py\", line 441, in engine_step\r\nERROR 08-16 18:23:54 async_llm_engine.py:41]     request_outputs = await self.engine.step_async()\r\nERROR 08-16 18:23:54 async_llm_engine.py:41]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 08-16 18:23:54 async_llm_engine.py:41]   File \"/llm/vllm/vllm/engine/async_llm_engine.py\", line 211, in step_async\r\nERROR 08-16 18:23:54 async_llm_engine.py:41]     output = await self.model_executor.execute_model_async(\r\nERROR 08-16 18:23:54 async_llm_engine.py:41]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 08-16 18:23:54 async_llm_engine.py:41]   File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/ipex_llm_gpu_executor.py\", line 443, in execute_model_async\r\nERROR 08-16 18:23:54 async_llm_engine.py:41]     all_outputs = await self._run_workers_async(\r\nERROR 08-16 18:23:54 async_llm_engine.py:41]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 08-16 18:23:54 async_llm_engine.py:41]   File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/ipex_llm_gpu_executor.py\", line 433, in _run_workers_async\r\nERROR 08-16 18:23:54 async_llm_engine.py:41]     all_outputs = await asyncio.gather(*coros)\r\nERROR 08-16 18:23:54 async_llm_engine.py:41]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 08-16 18:23:54 async_llm_engine.py:41] asyncio.exceptions.CancelledError\r\nERROR 08-16 18:23:54 async_llm_engine.py:41]\r\nERROR 08-16 18:23:54 async_llm_engine.py:41] The above exception was the direct cause of the following exception:\r\nERROR 08-16 18:23:54 async_llm_engine.py:41]\r\nERROR 08-16 18:23:54 async_llm_engine.py:41] Traceback (most recent call last):\r\nERROR 08-16 18:23:54 async_llm_engine.py:41]   File \"/llm/vllm/vllm/engine/async_llm_engine.py\", line 36, in _raise_exception_on_finish\r\nERROR 08-16 18:23:54 async_llm_engine.py:41]     task.result()\r\nERROR 08-16 18:23:54 async_llm_engine.py:41]   File \"/llm/vllm/vllm/engine/async_llm_engine.py\", line 467, in run_engine_loop\r\nERROR 08-16 18:23:54 async_llm_engine.py:41]     has_requests_in_progress = await asyncio.wait_for(\r\nERROR 08-16 18:23:54 async_llm_engine.py:41]                                ^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 08-16 18:23:54 async_llm_engine.py:41]   File \"/usr/lib/python3.11/asyncio/tasks.py\", line 502, in wait_for\r\nERROR 08-16 18:23:54 async_llm_engine.py:41]     raise exceptions.TimeoutError() from exc\r\nERROR 08-16 18:23:54 async_llm_engine.py:41] TimeoutError\r\n2024-08-16 18:23:54,093 - ERROR - Exception in callback functools.partial(<function _raise_exception_on_finish at 0x7f42b3835b20>, error_callback=<bound method AsyncLLMEngine._error_callback of <ipex_llm.vllm.xpu.engine.engine.IPEXLLMAsyncLLMEngine object at 0x7f42aa1a07d0>>)\r\nhandle: <Handle functools.partial(<function _raise_exception_on_finish at 0x7f42b3835b20>, error_callback=<bound method AsyncLLMEngine._error_callback of <ipex_llm.vllm.xpu.engine.engine.IPEXLLMAsyncLLMEngine object at 0x7f42aa1a07d0>>)>\r\n```\r\n### Client side error log\r\n```\r\nValueError: Initial test run failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.11/dist-packages/aiohttp/client_proto.py\", line 94, in connection_lost\r\n    uncompleted = self._parser.feed_eof()\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"aiohttp/_http_parser.pyx\", line 507, in aiohttp._http_parser.HttpParser.feed_eof\r\naiohttp.http_exceptions.TransferEncodingError: 400, message:\r\n  Not enough data for satisfy transfer length header.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/llm/backend_request_func.py\", line 256, in async_request_openai_completions\r\n    async for chunk_bytes in response.content:\r\n  File \"/usr/local/lib/python3.11/dist-packages/aiohttp/streams.py\", line 50, in __anext__\r\n    rv = await self.read_func()\r\n         ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/aiohttp/streams.py\", line 317, in readline\r\n    return await self.readuntil()\r\n           ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/aiohttp/streams.py\", line 351, in readuntil\r\n    await self._wait(\"readuntil\")\r\n  File \"/usr/local/lib/python3.11/dist-packages/aiohttp/streams.py\", line 312, in _wait\r\n    await waiter\r\naiohttp.client_exceptions.ClientPayloadError: Response payload is not completed: <TransferEncodingError: 400, message='Not enough data for satisfy transfer length header.'>\r\n\r\n\r\n```",
      "state": "closed",
      "author": "aprilhu01",
      "author_type": "User",
      "created_at": "2024-08-16T02:33:39Z",
      "updated_at": "2024-09-14T03:09:37Z",
      "closed_at": "2024-09-14T03:09:13Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11826/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "gc-fu"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11826",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11826",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:20.446931",
      "comments": [
        {
          "author": "gc-fu",
          "body": "This problem is due to oom, try reduce gpu-utilization-rate, or max-num-batched-tokens",
          "created_at": "2024-08-16T03:12:21Z"
        },
        {
          "author": "aprilhu01",
          "body": "Thanks. It is indeed due to OOM. Will adopt \"enable split_qkv\" patch and try again.",
          "created_at": "2024-08-19T06:40:17Z"
        },
        {
          "author": "aprilhu01",
          "body": "Improved with new release.",
          "created_at": "2024-09-14T03:09:36Z"
        }
      ]
    },
    {
      "issue_number": 12015,
      "title": "Inference speed and memory usage of Qwen1.5-14b ",
      "body": "I have tested the inference speed and memory usage of Qwen1.5-14b on my machine using the example in ipex-llm. The peek cpu usage to load Qwen1.5-14b in 4-bit is about 24GB. The peek GPU usage is about 10GB. The Inference speed is about 4~5 token/s. I export the environment variables ```set SYCL_CACHE_PERSISTENT=1```  and ```set BIGDL_LLM_XMX_DISABLED=1```.  Does the inference speed and CPU/GPU memory usage meet the expectation? I think the CPU peak usage is too high and the speed is a little slow.\r\n\r\n**device**\r\nIntel(R) Core(TM) Ultra 7 155H   3.80 GHz\r\n32.0 GB (31.6 GB 可用)\r\n![image](https://github.com/user-attachments/assets/6b3e2549-4697-42a4-82c5-a4755ea1f21e)\r\n\r\n**env**\r\nintel-extension-for-pytorch   2.1.10+xpu\r\ntorch                         2.1.0a0+cxx11.abi\r\ntransformers                  4.44.2\r\n",
      "state": "open",
      "author": "WeiguangHan",
      "author_type": "User",
      "created_at": "2024-09-04T11:33:59Z",
      "updated_at": "2024-09-11T02:47:41Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12015/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "JinheTang"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12015",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12015",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:20.635101",
      "comments": [
        {
          "author": "JinheTang",
          "body": "Hi @WeiguangHan , we will take a look at this issue and try to reproduce it first. We'll let you know if there's any progress.",
          "created_at": "2024-09-06T01:42:39Z"
        },
        {
          "author": "JinheTang",
          "body": "Hi @WeiguangHan , we can not reproduce the issue on an Ultra 5 125H CPU.\r\n\r\nThe CPU usage when running qwen1.5 example script turned out pretty normal:\r\n![image](https://github.com/user-attachments/assets/9c2664dc-ed57-41d0-83fc-65251665bd0f)\r\ngiven that the initial usage is about 9GB, the peak CPU ",
          "created_at": "2024-09-10T07:53:04Z"
        },
        {
          "author": "WeiguangHan",
          "body": "> Hi @WeiguangHan , we can not reproduce the issue on an Ultra 5 125H CPU.\r\n> \r\n> The CPU usage when running qwen1.5 example script turned out pretty normal: ![image](https://private-user-images.githubusercontent.com/97284834/365940270-9c2664dc-ed57-41d0-83fc-65251665bd0f.png?jwt=eyJhbGciOiJIUzI1NiI",
          "created_at": "2024-09-11T02:47:40Z"
        }
      ]
    },
    {
      "issue_number": 12037,
      "title": "Ollama Error on MTL-iGPU ",
      "body": "Hi IPEX-LLM Team.\r\n\r\nWe testing the OLLAMA follow this guide:\r\nhttps://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.md\r\n\r\nThe platform that we using is MTL iGPU and ubuntu 22.04\r\n\r\nAnd tested on oneAPI version 2024.1 & 2024.2 detected same issue.\r\n\r\nSteps:\r\n1. On terminal input command ./ollama serve\r\n2. On 2nd terminal input command ./ollma run phi3\r\n\r\nError:\r\n![image](https://github.com/user-attachments/assets/479255c5-d813-433c-9f9b-1952c681e30b)\r\n",
      "state": "closed",
      "author": "weiseng-yeap",
      "author_type": "User",
      "created_at": "2024-09-09T01:25:10Z",
      "updated_at": "2024-09-11T02:47:24Z",
      "closed_at": "2024-09-11T02:47:24Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12037/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12037",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12037",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:20.852615",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @weiseng-yeap, you may try to install oneAPI 2024.0 and run phi3 again, it works fine based on my testing.",
          "created_at": "2024-09-09T15:42:38Z"
        }
      ]
    },
    {
      "issue_number": 12004,
      "title": "Models failed while running harness test (winogrande dataset) with latest IPEX-LLM nightly build",
      "body": "A lot of models failed while running harness test (winogrande dataset) with IPEX-LLM nightly build after 2.1.0b20240820\r\n\r\nFor example:\r\nModel: codegemma-7b-it\r\nIPEX-LLM version: 2.2.0b20240902\r\ntransformers version: 4.38.2\r\n\r\nEnvironment:\r\n```\r\nHardware: ThinkBook14G6+IMH-Ultra7-155H-32GB\r\nOS: Windows 11\r\n```\r\n\r\nSteps to reproduce:\r\n\r\n1. Covert the model to 'sym_int4' by `<ipex-llm-repo-root>/python/llm/dev/benchmark/all-in-one/save.py`\r\n2. Install harness packages by following the instruction of [link](https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/dev/benchmark/harness#install-harness)\r\n3. Go to directory `<ipex-llm-repo-root>/python/llm/dev/benchmark/harness`\r\n4. Run command `python run_llb.py --model ipex-llm --pretrained <model_hub_path>\\codegemma-7b-it-int4 --precision sym_int4 --device xpu --tasks winogrande --no_cache --batch 1`\r\n\r\nError log:\r\n```\r\nRunning IPEX-LLM harness test ...\r\nCould not import signal.SIGPIPE (this is expected on Windows machines)\r\nC:\\Users\\superbuilder\\miniforge3\\envs\\ipex-llm-acc\\Lib\\site-packages\\transformers\\deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\r\n  warnings.warn(\r\nC:\\Users\\superbuilder\\miniforge3\\envs\\ipex-llm-acc\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\superbuilder\\miniforge3\\envs\\ipex-llm-acc\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n  warn(\r\n2024-09-04 15:21:12,329 - INFO - intel_extension_for_pytorch auto imported\r\nSelected Tasks: ['winogrande']\r\n2024-09-04 15:21:13,484 - INFO - Converting the current model to sym_int4 format......\r\nLoading checkpoint shards:   0%|                                                                                                     | 0/2 [00:00<?, ?it/s]C:\\Users\\superbuilder\\miniforge3\\envs\\ipex-llm-acc\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.31s/it]\r\nTask: winogrande; number of docs: 1267\r\nTask: winogrande; document 0; context prompt (starting on next line):\r\nNatalie took basic French lessons from Betty after school because Betty is strong at that language.\r\n\r\nMy friends tried to drive the car through the alleyway but the car was too wide.\r\n\r\nSarah didn't practice ballet much but Mary practiced all the time. Sarah wasn't chosen to be a lead dancer.\r\n\r\nThe trainer tried to put the exercise equipment in the van but it wouldn't fit; the van was too small.\r\n\r\nNatalie never checks the air in the tires while Tanya does and you just knew Natalie would have flat tires.\r\n\r\nPeople think Rebecca\r\n(end of prompt on previous line)\r\nRequests: [Req_loglikelihood(\"Natalie took basic French lessons from Betty after school because Betty is strong at that language.\\n\\nMy friends tried to drive the car through the alleyway but the car was too wide.\\n\\nSarah didn't practice ballet much but Mary practiced all the time. Sarah wasn't chosen to be a lead dancer.\\n\\nThe trainer tried to put the exercise equipment in the van but it wouldn't fit; the van was too small.\\n\\nNatalie never checks the air in the tires while Tanya does and you just knew Natalie would have flat tires.\\n\\nPeople think Samantha\", ' is embarassed, because Samantha made snide comments about the shirt Rebecca was wearing.')[0]\r\n, Req_loglikelihood(\"Natalie took basic French lessons from Betty after school because Betty is strong at that language.\\n\\nMy friends tried to drive the car through the alleyway but the car was too wide.\\n\\nSarah didn't practice ballet much but Mary practiced all the time. Sarah wasn't chosen to be a lead dancer.\\n\\nThe trainer tried to put the exercise equipment in the van but it wouldn't fit; the van was too small.\\n\\nNatalie never checks the air in the tires while Tanya does and you just knew Natalie would have flat tires.\\n\\nPeople think Rebecca\", ' is embarassed, because Samantha made snide comments about the shirt Rebecca was wearing.')[0]\r\n]\r\nRunning loglikelihood requests\r\n  0%|                                                                                                                             | 0/2534 [00:02<?, ?it/s]\r\nJob config of task=winogrande, precision=sym_int4 failed. Error Message: The size of tensor a (0) must match the size of tensor b (10) at non-singleton dimension 1\r\nHere are results of all successful tasks:\r\nTraceback (most recent call last):\r\n  File \"C:\\WorkSpace\\superbuilder\\llm.model.benchmark\\ipex_llm_harness\\run_llb.py\", line 159, in <module>\r\n    main()\r\n  File \"C:\\WorkSpace\\superbuilder\\llm.model.benchmark\\ipex_llm_harness\\run_llb.py\", line 155, in main\r\n    raise RuntimeError('\\n'.join(fail))\r\nRuntimeError: Job config of task=winogrande, precision=sym_int4 failed. Error Message: The size of tensor a (0) must match the size of tensor b (10) at non-singleton dimension 1\r\n```\r\n\r\n\r\n",
      "state": "closed",
      "author": "yzha107",
      "author_type": "User",
      "created_at": "2024-09-04T07:37:18Z",
      "updated_at": "2024-09-11T02:06:37Z",
      "closed_at": "2024-09-11T02:06:37Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12004/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "lalalapotter",
        "cranechu0131"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12004",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12004",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:21.076327",
      "comments": [
        {
          "author": "cranechu0131",
          "body": "We encountered some problems when reproducing the issue and have made a contact with the user. Still in progress. ",
          "created_at": "2024-09-06T02:13:40Z"
        },
        {
          "author": "cranechu0131",
          "body": "Hi yzha107，\r\n We have looked into the issue and reproduced the problem. The current conclusion is that ipex-llm after 2.1.0b20240820 introduced this change [set IPEX_LLM_LAST_LM_HEAD=1 as default](https://github.com/intel-analytics/ipex-llm/commit/0236de3ac2042b79941744d18ed4107ab70cbf09)   which de",
          "created_at": "2024-09-06T06:24:07Z"
        },
        {
          "author": "lalalapotter",
          "body": "Issue is already solved for user, close issue.",
          "created_at": "2024-09-11T02:06:37Z"
        }
      ]
    },
    {
      "issue_number": 12000,
      "title": "Qwen2 in benchmark all-in-one fails (NotImplementedError: Cannot copy out of meta tensor; no data!)",
      "body": "Hello, \r\nI tried to run Qwen2 with the following config.\r\n \r\n(changed configs;)\r\nrepo_id : Qwen/Qwen2-7B\r\nin_out_pairs: - '1024-128'\r\ntest_api: - \"transformer_int4_fp16_gpu_win\"\r\n\r\nHowever, it failed as below: \r\n\r\n python .\\run.py\r\nC:\\Users\\MTL\\miniforge3\\envs\\taylor_ipex\\Lib\\site-packages\\transformers\\deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\r\n  warnings.warn(\r\nC:\\Users\\MTL\\miniforge3\\envs\\taylor_ipex\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\MTL\\miniforge3\\envs\\taylor_ipex\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n  warn(\r\n2024-09-04 07:26:23,245 - INFO - intel_extension_for_pytorch auto imported\r\nLoading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:20<00:00,  3.43s/it]\r\nTraceback (most recent call last):\r\n  File \"C:\\dev\\taylor\\ipex-llm\\python\\llm\\dev\\benchmark\\all-in-one\\run.py\", line 2066, in <module>\r\n    run_model(model, api, in_out_pairs, conf['local_model_hub'], conf['warm_up'], conf['num_trials'], conf['num_beams'],\r\n  File \"C:\\dev\\taylor\\ipex-llm\\python\\llm\\dev\\benchmark\\all-in-one\\run.py\", line 170, in run_model\r\n    result = run_transformer_int4_fp16_gpu_win(repo_id, local_model_hub, in_out_pairs, warm_up, num_trials, num_beams, low_bit, cpu_embedding, batch_size, streaming)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\dev\\taylor\\ipex-llm\\python\\llm\\dev\\benchmark\\all-in-one\\run.py\", line 1149, in run_transformer_int4_fp16_gpu_win\r\n    model = AutoModelForCausalLM.from_pretrained(model_path, optimize_model=True, load_in_low_bit=low_bit,\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\MTL\\miniforge3\\envs\\taylor_ipex\\Lib\\unittest\\mock.py\", line 1378, in patched\r\n    return func(*newargs, **newkeywargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\MTL\\miniforge3\\envs\\taylor_ipex\\Lib\\site-packages\\ipex_llm\\transformers\\model.py\", line 366, in from_pretrained\r\n    model = cls.load_convert(q_k, optimize_model, *args, **kwargs)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\MTL\\miniforge3\\envs\\taylor_ipex\\Lib\\site-packages\\ipex_llm\\transformers\\model.py\", line 520, in load_convert\r\n    model = model.to(\"cpu\")\r\n            ^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\MTL\\miniforge3\\envs\\taylor_ipex\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 2597, in to\r\n    return super().to(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\MTL\\miniforge3\\envs\\taylor_ipex\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1160, in to\r\n    return self._apply(convert)\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\MTL\\miniforge3\\envs\\taylor_ipex\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 810, in _apply\r\n    module._apply(fn)\r\n  File \"C:\\Users\\MTL\\miniforge3\\envs\\taylor_ipex\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 833, in _apply\r\n    param_applied = fn(param)\r\n                    ^^^^^^^^^\r\n  File \"C:\\Users\\MTL\\miniforge3\\envs\\taylor_ipex\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1158, in convert\r\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n**NotImplementedError: Cannot copy out of meta tensor; no data!**\r\n\r\n\r\n\r\nCould I get help with resolving this issue? \r\nThanks!",
      "state": "closed",
      "author": "yeonbok",
      "author_type": "User",
      "created_at": "2024-09-03T22:25:00Z",
      "updated_at": "2024-09-05T17:16:02Z",
      "closed_at": "2024-09-05T17:16:02Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/12000/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Oscilloscope98"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/12000",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/12000",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:21.295420",
      "comments": [
        {
          "author": "Oscilloscope98",
          "body": "Hi @yeonbok,\r\n\r\nWe did not reproduced your issue. Would you mind providing us with more information regarding your test environment through pasting the log of [our env-check script](https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/scripts#env-check)?",
          "created_at": "2024-09-04T08:54:55Z"
        },
        {
          "author": "yeonbok",
          "body": "@Oscilloscope98 Hi, i could run with same setup for llama3 and chatglm3. Only Qwen was failing with that error. Below is the env check log.\r\n(taylor_ipex) PS C:\\dev\\taylor\\ipex-llm\\python\\llm\\scripts> type log\r\nPython 3.11.9\r\nName: ipex-llm\r\nVersion: 2.2.0b20240903\r\nSummary: Large Language Model Dev",
          "created_at": "2024-09-04T20:25:52Z"
        },
        {
          "author": "Oscilloscope98",
          "body": "Hi @yeonbok,\r\n\r\nIt seems that the `Qwen\\Qwen2-7B` model you are using is not the same as the latest one on Hugging Face (you have 6 checkpoints to load, and there are only 4 checkpoints on Hugging Face now): https://huggingface.co/Qwen/Qwen2-7B/tree/main\r\n\r\nWould you mind having a try on the latest ",
          "created_at": "2024-09-05T02:27:11Z"
        },
        {
          "author": "yeonbok",
          "body": "Thank you very much @Oscilloscope98! It is working with the new model.",
          "created_at": "2024-09-05T17:15:49Z"
        }
      ]
    },
    {
      "issue_number": 11996,
      "title": "llama3-8b gets OOM on MTL iGPU WSL docker vllm",
      "body": "llama3-8b gets OOM on MTL iGPU WSL docker vllm.\r\nRAM:17GB   vRAM：9GB\r\n\r\ncommad:\r\n```\r\n#!/bin/bash\r\nmodel=\"./models/\"\r\nserved_model_name=\"llama3-8b\"\r\n \r\nsource /opt/intel/1ccl-wks/setvars.sh\r\n \r\npython -m ipex_llm.vllm.xpu.entrypoints.openai.api_server \\\r\n  --served-model-name $served_model_name \\\r\n  --port 8000 \\\r\n  --model $model \\\r\n  --trust-remote-code \\\r\n  --gpu-memory-utilization 0.9 \\\r\n  --device xpu \\\r\n  --dtype float16 \\\r\n  --enforce-eager \\\r\n  --load-in-low-bit sym_int4 \\\r\n  --max-model-len 512 \\\r\n  --max-num-batched-tokens 1000 \\\r\n  --max-num-seqs 12 \\\r\n  --tensor-parallel-size 1\r\n```\r\n\r\nlog:\r\n\r\n```\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nINFO 09-02 16:43:31 attention.py:71] flash_attn is not found. Using xformers backend.\r\n2024-09-02 16:44:34,970 - INFO - Converting the current model to sym_int4 format......\r\n2024-09-02 16:44:34,998 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\r\n[2024-09-02 16:44:36,387] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to xpu (auto detect)\r\n2024-09-02 16:45:17,723 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\r\nINFO 09-02 16:46:19 model_convert.py:257] Loading model weights took 4.7184 GB\r\nINFO 09-02 16:49:14 gpu_executor.py:100] # GPU blocks: 2440, # CPU blocks: 2048\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/entrypoints/openai/api_server.py\", line 267, in <module>\r\n    engine = IPEXLLMAsyncLLMEngine.from_engine_args(engine_args,\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 57, in from_engine_args\r\n    engine = cls(parallel_config.worker_use_ray,\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 30, in __init__\r\n    super().__init__(*args, **kwargs)\r\n  File \"/llm/vllm/vllm/engine/async_llm_engine.py\", line 309, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/llm/vllm/vllm/engine/async_llm_engine.py\", line 409, in _init_engine\r\n    return engine_class(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/llm/vllm/vllm/engine/llm_engine.py\", line 106, in __init__\r\n    self.model_executor = executor_class(model_config, cache_config,\r\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/llm/vllm/vllm/executor/gpu_executor.py\", line 46, in __init__\r\n    self._init_cache()\r\n  File \"/llm/vllm/vllm/executor/gpu_executor.py\", line 110, in _init_cache\r\n    self.driver_worker.init_cache_engine(cache_config=self.cache_config)\r\n  File \"/llm/vllm/vllm/worker/worker.py\", line 162, in init_cache_engine\r\n    self.cache_engine = CacheEngine(self.cache_config, self.model_config,\r\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/llm/vllm/vllm/worker/cache_engine.py\", line 55, in __init__\r\n    self.gpu_cache = self.allocate_gpu_cache()\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/llm/vllm/vllm/worker/cache_engine.py\", line 84, in allocate_gpu_cache\r\n    key_blocks = torch.empty(\r\n                 ^^^^^^^^^^^^\r\nRuntimeError: XPU out of memory. Tried to allocate 78.00 MiB (GPU 0; 12.65 GiB total capacity; 6.21 GiB already allocated; 6.33 GiB reserved in total by PyTorch)\r\n```\r\n\r\nversion:\r\nipexl-llm 0828",
      "state": "closed",
      "author": "violet17",
      "author_type": "User",
      "created_at": "2024-09-03T05:41:12Z",
      "updated_at": "2024-09-05T06:21:18Z",
      "closed_at": "2024-09-05T06:21:18Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11996/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hzjane"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11996",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11996",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:21.477962",
      "comments": [
        {
          "author": "hzjane",
          "body": "I think this issue will only encounter on windows mtl. Because vllm will first calculate how many `gpu_blocks` can be allocated for the current `gpu_rate` before applying. This error occurs when vllm applies memory for `gpu_blocks`. I suspect that some of the  memory requested by vllm is occupied by",
          "created_at": "2024-09-03T05:51:20Z"
        },
        {
          "author": "violet17",
          "body": "Solved. Thanks.",
          "created_at": "2024-09-05T06:21:18Z"
        }
      ]
    },
    {
      "issue_number": 11993,
      "title": "intelanalytics/ipex-llm-inference-cpp-xpu:2.2.0 docker image causes memory issue with intel arc a380",
      "body": "Hey. Not a computer scientist here, but thought you guys'd like to know that the latest pushed container image is causing issues with gpu inference for me.\r\n\r\nSystem specs\r\nCPU: AMD Ryzen 3600\r\nGPU: Intel arc a380\r\nRAM: DDR4 ECC RAM unregistered 3200mhz single channel 16gb\r\nOS: Debian 12\r\nKernel: 6.7.12+bpo-amd64\r\nDocker version 27.2.0, build 3ab4256\r\n\r\nlogs attached.\r\n[Logs_Latest.txt](https://github.com/user-attachments/files/16840430/Logs_Latest.txt)\r\n[Logs_2.1.0.txt](https://github.com/user-attachments/files/16840434/Logs_2.1.0.txt)\r\n\r\n",
      "state": "open",
      "author": "bobsdacool",
      "author_type": "User",
      "created_at": "2024-09-02T18:19:51Z",
      "updated_at": "2024-09-05T02:31:24Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11993/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hzjane",
        "JinheTang"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11993",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11993",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:21.707373",
      "comments": [
        {
          "author": "hzjane",
          "body": "`Native API failed. Native API returns: -6 (PI_ERROR_OUT_OF_HOST_MEMORY) -6 (PI_ERROR_OUT_OF_HOST_MEMORY)` .This looks like an OOM error. You can try a smaller model such as `dolphin-phi:latest`.",
          "created_at": "2024-09-03T01:49:27Z"
        },
        {
          "author": "bobsdacool",
          "body": "Hi, yes, a smaller model does work for me on the latest container ~0.3Gb. I think there is an issue though as using version 2.1.0 allows me to use models that match the systems vram ~6gb. Even when I have all other docker containers shut down, when using the new container with ~14Gb of free system m",
          "created_at": "2024-09-03T07:37:18Z"
        },
        {
          "author": "hzjane",
          "body": "I don't really know what problem you meet again? Do you mean that this problem exists in the latest 2.2.0 version, and the 2.1.0 is normal? But the docker image is basically not updated between 2.1.0 and 2.2.0. I have tested 2.2.0-snapshot on Arc A770 and no meet any OOM problem. Maybe it's caused b",
          "created_at": "2024-09-04T01:38:23Z"
        },
        {
          "author": "bobsdacool",
          "body": "Hi, yes, whilst I can run llms at like 5Gb in size in 2.1.0 I cant run them in 2.2.0 with the exact same docker setup. I can run much smaller llms in 2.2.0 so the ollama functionality is not totally bust, there does seem to be a memory issue.\r\n\r\nI'm not sure where the issue lies though. Please let m",
          "created_at": "2024-09-04T07:23:23Z"
        },
        {
          "author": "hzjane",
          "body": "Thanks for your question. There was indeed a [llama.cpp/Ollama upgrade](https://github.com/intel-analytics/ipex-llm/pull/11930) between image 2.2.0 and 2.1.0, which may be the root cause. We will confirm the issue again. And You can run it with 2.1.0 first. ",
          "created_at": "2024-09-05T01:54:27Z"
        }
      ]
    },
    {
      "issue_number": 11984,
      "title": "NPU Acceleration Library model loaing",
      "body": "Can we support NPU acceleration library, NPU inference model save/load in low bits?\r\nIt takes about 48s to load the 7B model directly. ",
      "state": "open",
      "author": "juan-OY",
      "author_type": "User",
      "created_at": "2024-09-02T01:40:08Z",
      "updated_at": "2024-09-04T02:01:56Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11984/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "rnwang04"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11984",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11984",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:23.693202",
      "comments": [
        {
          "author": "rnwang04",
          "body": "Hi @juan-OY , we will look at this issue, once it is done, will update here to let you know.",
          "created_at": "2024-09-03T01:24:57Z"
        },
        {
          "author": "rnwang04",
          "body": "Hi @juan-OY , this is supported by https://github.com/intel-analytics/ipex-llm/pull/11999.\r\nYou may refer the usage in pr description now. We will update our example later.\r\n",
          "created_at": "2024-09-04T02:01:54Z"
        }
      ]
    },
    {
      "issue_number": 11983,
      "title": "IPEX-LLM GPU - GraphRAG: No module named \"past\" && ApiKeyMissingError && No text files found in input",
      "body": "Hi, I am following IPEX-LLM GraphRAG_quickstart.md, I met two issues.\r\n\r\n1) No module named \"past\"\r\n\r\nPrepare Input Corpus \r\nSome [sample documents](https://github.com/TheAiSingularity/graphrag-local-ollama/tree/main/input) are used here as input corpus for indexing GraphRAG, based on which LLM will create a knowledge graph.\r\n\r\nPerpare the input corpus, and then initialize the workspace:\r\n\r\nFor Linux users:\r\n\r\n# define inputs corpus\r\nmkdir -p ./ragtest/input\r\ncp input/* ./ragtest/input\r\n\r\nexport no_proxy=localhost,127.0.0.1\r\n\r\n# initialize ragtest folder\r\npython -m graphrag.index --init --root ./ragtest\r\n\r\nRaised ModuleNotFoundError: No module named 'past'\r\n\r\nI have checked internet and suggested to use \r\npip install future\r\nto resolve the issue. Just an awareness, maybe you would like to update the documentation?\r\n\r\n2) Raise ApiKeyMissingError graphrag.config.error.ApiKeyMissingError: API Key is required for Completion API. Please set either the OPENAI_API_KEY, GRAPHRAG_API_KEY or GRAPHRAG_LLM_API_KEY environment variable.\r\n\r\nConduct GraphRAG indexing\r\nFinally, conduct GraphRAG indexing, which may take a while:\r\n\r\npython -m graphrag.index --root ragtest\r\n\r\nPlease let me know if API is required\r\n\r\nMethod to resolve this issue:\r\nexport GRAPHRAG_API_KEY=1234, following https://github.com/TheAiSingularity/graphrag-local-ollama/issues/7\r\n\r\n3) ValueError: No text files found in input\r\nAfter resolving the issue above, encounter another error No text files found in input \r\n",
      "state": "open",
      "author": "Kpeacef",
      "author_type": "User",
      "created_at": "2024-09-01T11:54:33Z",
      "updated_at": "2024-09-03T07:59:01Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11983/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Oscilloscope98"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11983",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11983",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:23.882534",
      "comments": [
        {
          "author": "Oscilloscope98",
          "body": "Hi @Kpeacef,\r\n\r\nThank you for the suggestions on our documentation. We are reproducing the issues, and will let you know for any updates :)",
          "created_at": "2024-09-02T02:16:13Z"
        },
        {
          "author": "Oscilloscope98",
          "body": "Hi @Kpeacef ,\r\n\r\nWe updated our [GraphRAG QuickStart](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/graphrag_quickstart.md). You could follow it and have a try again :)\r\n\r\nHere are a few points you might want to pay attention to:\r\n\r\n1. IPEX-LLM Ollama used a separate P",
          "created_at": "2024-09-03T07:58:37Z"
        }
      ]
    },
    {
      "issue_number": 11977,
      "title": "Extensions to ipex-llm",
      "body": "hi\r\nJuly 23, intel launched the application ai playground, downloaded and installed in it found the ipex-llm framework, and the corresponding environment has been installed under the \"resources\" folder, ai playground has many advantages, such as in the cpu:ultra7-155h, 1t, Running phi3-medium-14b unquantized version on 32g notebook can have a good effect, I would like to ask if the ideal situation of installing ollama and opening ollama server can get the same effect as ai playground?\r\n![image](https://github.com/user-attachments/assets/911949f2-7610-44e0-91da-6eab70d090c7)\r\nUnder the \"resources\" folder\r\n![image](https://github.com/user-attachments/assets/bb31c01d-0617-4acb-875d-49de6ffcedf8)\r\n\r\n",
      "state": "open",
      "author": "brownplayer",
      "author_type": "User",
      "created_at": "2024-08-30T08:58:23Z",
      "updated_at": "2024-09-02T03:10:38Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11977/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11977",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11977",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:24.069331",
      "comments": []
    },
    {
      "issue_number": 11974,
      "title": "vllm support GLM-4V-9B model",
      "body": "https://hf-mirror.com/THUDM/glm-4v-9b\r\nplease enable this model for customer's requirement.",
      "state": "open",
      "author": "Fred-cell",
      "author_type": "User",
      "created_at": "2024-08-30T07:57:01Z",
      "updated_at": "2024-09-02T02:28:50Z",
      "closed_at": null,
      "labels": [
        "user issue",
        "multi-arc"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11974/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hzjane"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11974",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11974",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:24.069353",
      "comments": [
        {
          "author": "hzjane",
          "body": "https://github.com/vllm-project/vllm/pull/5358 GLM-4V-9B model is not yet supported by vllm now.",
          "created_at": "2024-09-02T02:28:49Z"
        }
      ]
    },
    {
      "issue_number": 11951,
      "title": "[All-in-one benchmark] [GPT2-large] The size of tensor a (1024) must match the size of tensor b (1025) at non-singleton dimension 3",
      "body": "Hi I am trying to benchmark GPT2-large and experienced RuntimeError: The size of tensor a (1024) must match the size of tensor b (1025) at non-singleton dimension 3.\r\n\r\nThe inputs should able to accept up to 1024 consecutive tokens. I have tried different in/out tokens and the max in/out pairs i tried is 512/512.\r\n\r\nInputs for 640 to 2048 will face this RuntimeError: The size of tensor a (1024) must match the size of tensor b (1025) at non-singleton dimension 3.\r\n\r\nAPI used: transformer_int4_fp16_gpu & optimize_model_gpu\r\n\r\nModel used: openai-community/gpt2-large\r\n\r\nVersions:\r\nbigdl-core-xe-21 2.6.0b20240827\r\n\r\nThank you.",
      "state": "open",
      "author": "Kpeacef",
      "author_type": "User",
      "created_at": "2024-08-28T08:10:14Z",
      "updated_at": "2024-09-02T01:34:46Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11951/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "cranechu0131"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11951",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11951",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:24.285567",
      "comments": [
        {
          "author": "cranechu0131",
          "body": "Hi Kpeacef，\r\nWe have looked into this issue. We have tried running the GPT2-large model using only native transformers. The error is also reported under the input size you mentioned. So we suppose this issue is not introduced by ipex-llm and probably due to GPT2's incompatibility with the current ve",
          "created_at": "2024-09-02T01:34:45Z"
        }
      ]
    },
    {
      "issue_number": 11902,
      "title": "The open-webui fails to be started",
      "body": "Displays after I type start start_windows.bat\r\n![屏幕截图 2024-08-22 232759](https://github.com/user-attachments/assets/e72d78f5-dfc0-465d-8fae-47f1df45ec23)\r\n![屏幕截图 2024-08-22 232822](https://github.com/user-attachments/assets/b630218a-8591-41d4-b026-32031aeadff0)\r\n![image](https://github.com/user-attachments/assets/c597c335-352e-424c-971a-20da797f049c)\r\n![image](https://github.com/user-attachments/assets/f67f68b2-77a1-4216-805a-d70ff0b1c83c)\r\n",
      "state": "closed",
      "author": "brownplayer",
      "author_type": "User",
      "created_at": "2024-08-22T15:29:40Z",
      "updated_at": "2024-08-31T03:36:46Z",
      "closed_at": "2024-08-31T03:36:46Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11902/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11902",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11902",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:24.511267",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @brownplayer , could you show the versions of the dependencies you have installed to run open-webui (the result displayed by pip list)? Also, you may refer to [ipex-llm open-webui quickstart](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/open_webui_with_ollama_quick",
          "created_at": "2024-08-25T12:52:37Z"
        },
        {
          "author": "brownplayer",
          "body": "> 您好，您能否展示您为运行 open-webui 而安装的依赖项版本（pip list 显示的结果）？此外，你可以参考 [ipex-llm open-webui quickstart](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/open_webui_with_ollama_quickstart.md) 来准备你的环境。\r\n\r\nYes, I followed the official guidelines",
          "created_at": "2024-08-25T14:42:45Z"
        },
        {
          "author": "brownplayer",
          "body": "> 您好，您能否展示您为运行 open-webui 而安装的依赖项版本（pip list 显示的结果）？此外，你可以参考 [ipex-llm open-webui quickstart](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/open_webui_with_ollama_quickstart.md) 来准备你的环境。\r\n\r\nof course\r\nPackage                                  Version\r\n------------------",
          "created_at": "2024-08-25T14:43:48Z"
        },
        {
          "author": "sgwhat",
          "body": "Same as https://github.com/intel-analytics/ipex-llm/issues/11907\r\n\r\nCould you please try to downgrade transformers version through `pip install transformers==4.37.0 accelerate`",
          "created_at": "2024-08-27T01:59:24Z"
        }
      ]
    },
    {
      "issue_number": 11917,
      "title": "An error occurred while running mistral-nemo in ollama",
      "body": "There is an error when running mistral nemo in ollama, but there is no problem when running qwen0.5b/It seems that ollama version is too low, how can I upgrade ollama version\r\n![屏幕截图 2024-08-25 142723](https://github.com/user-attachments/assets/437eebab-6e04-4a7a-bc7b-cf74a1fa1431)\r\n",
      "state": "closed",
      "author": "brownplayer",
      "author_type": "User",
      "created_at": "2024-08-25T06:30:10Z",
      "updated_at": "2024-08-30T04:35:10Z",
      "closed_at": "2024-08-30T04:35:10Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11917/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11917",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11917",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:24.729625",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @brownplayer , the current version of `ipex-llm ollama` can be found in the [Ollama QuickStart](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.md). We do not support users upgrading it themselves, but we will be updating the ollama version in recent",
          "created_at": "2024-08-26T02:21:30Z"
        },
        {
          "author": "brownplayer",
          "body": "That's great, thanks for your contributions, I can't wait to try the latest ollama\n\n---- Replied Message ----\n| From | SONG ***@***.***> |\n| Date | 08/26/2024 10:21 |\n| To | intel-analytics/ipex-llm ***@***.***> |\n| Cc | brownplayer ***@***.***>,\nMention ***@***.***> |\n| Subject | Re: [intel-analyti",
          "created_at": "2024-08-26T02:28:10Z"
        },
        {
          "author": "rnwang04",
          "body": "Hi @brownplayer ,\r\nipex-llm‘s ollama is upgrade to 0.3.6 with `ipex-llm[cpp]>=2.2.0b20240827`, you may have a try with latest llama.cpp / ollama 😊",
          "created_at": "2024-08-27T13:30:35Z"
        },
        {
          "author": "brownplayer",
          "body": "wow，it's so great",
          "created_at": "2024-08-27T14:22:04Z"
        }
      ]
    },
    {
      "issue_number": 11893,
      "title": "re-run init-llama-cpp will cause file exist",
      "body": "in the same folder and if old links broken, I want to re init llama cpp but got file exist,\r\nseems the soft link cannot be overrride, please help. /-f\r\n\r\n(llm-cpp) ubuntu@A $ ls\r\nbaby-llama     convert-hf-to-gguf.py  gguf-py              llama.log                lookup          perplexity      save-load-state  ubuntu\r\nbatched        convert.py             imatrix              llama.yetanotherlog.log  ls-sycl-device  quantize        server\r\nbatched-bench  embedding              llama_autonamed.log  llava-cli                main            quantize-stats  speculative\r\nbenchmark      gguf                   llama-bench          lookahead                main.log        run.bat         tokenize\r\n(llm-cpp) ubuntu@A $ init-llama-cpp\r\nln: failed to create symbolic link './batched': File exists\r\nln: failed to create symbolic link './quantize': File exists\r\nln: failed to create symbolic link './llava-cli': File exists\r\nln: failed to create symbolic link './gguf': File exists\r\nln: failed to create symbolic link './perplexity': File exists\r\nln: failed to create symbolic link './main': File exists\r\nln: failed to create symbolic link './speculative': File exists\r\nln: failed to create symbolic link './tokenize': File exists\r\nln: failed to create symbolic link './llama-bench': File exists\r\nln: failed to create symbolic link './quantize-stats': File exists\r\nln: failed to create symbolic link './lookahead': File exists\r\nln: failed to create symbolic link './benchmark': File exists\r\nln: failed to create symbolic link './baby-llama': File exists\r\nln: failed to create symbolic link './imatrix': File exists\r\nln: failed to create symbolic link './embedding': File exists\r\nln: failed to create symbolic link './server': File exists\r\nln: failed to create symbolic link './batched-bench': File exists\r\nln: failed to create symbolic link './ls-sycl-device': File exists\r\nln: failed to create symbolic link './lookup': File exists\r\nln: failed to create symbolic link './save-load-state': File exists\r\n",
      "state": "closed",
      "author": "kevin-t-tang",
      "author_type": "User",
      "created_at": "2024-08-22T03:15:29Z",
      "updated_at": "2024-08-29T03:25:08Z",
      "closed_at": "2024-08-29T03:25:08Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11893/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "JinheTang"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11893",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11893",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:25.244808",
      "comments": [
        {
          "author": "JinheTang",
          "body": "Hi @kevin-t-tang, we will take a look at this request. Once it's done, we will update here to let you know.",
          "created_at": "2024-08-22T08:51:10Z"
        },
        {
          "author": "JinheTang",
          "body": "Hi @kevin-t-tang , we have already covered this issue in our latest version. With `ipex-llm[cpp]>=2.2.0b20240827`, running `init-llama-cpp` and `init-ollama` will forcefully recreate all the symbolic links.",
          "created_at": "2024-08-28T00:40:15Z"
        }
      ]
    },
    {
      "issue_number": 11907,
      "title": "Compatibility issues between transformers and accelerate versions.",
      "body": "  A critical issue occurs during the open-webui installation, and \"raise RuntimeError()\" is displayed.\r\n    raise RuntimeError(\r\nRuntimeError: Failed to import transformers.trainer because of the following error (look up to see its traceback):\r\ncannot import name 'is_mlu_available' from 'accelerate.utils' (D:\\ipex-llm-demo\\miniforge\\envs\\llm-cpp\\Lib\\site-packages\\accelerate\\utils\\__init__.py) \r\n  What should I do if the accelerate version is incompatible?\r\n\r\nThe following is version information for both modules:\r\n(llm-cpp) D:\\ipex-llm-demo\\demo\\ipex-llm\\open-webui-main\\backend>pip show transformers accelerate\r\nName: transformers\r\nVersion: 4.44.2\r\nSummary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\r\nHome-page: https://github.com/huggingface/transformers\r\nAuthor: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\r\nAuthor-email: transformers@huggingface.co\r\nLicense: Apache 2.0 License\r\nLocation: D:\\ipex-llm-demo\\miniforge\\envs\\llm-cpp\\Lib\\site-packages\r\nRequires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\r\nRequired-by: bigdl-core-cpp, sentence-transformers\r\n---\r\nName: accelerate\r\nVersion: 0.21.0\r\nSummary: Accelerate\r\nHome-page: https://github.com/huggingface/accelerate\r\nAuthor: The HuggingFace team\r\nAuthor-email: sylvain@huggingface.co\r\nLicense: Apache\r\nLocation: D:\\ipex-llm-demo\\miniforge\\envs\\llm-cpp\\Lib\\site-packages\r\nRequires: numpy, packaging, psutil, pyyaml, torch\r\nRequired-by: bigdl-core-cpp",
      "state": "closed",
      "author": "brownplayer",
      "author_type": "User",
      "created_at": "2024-08-23T07:49:21Z",
      "updated_at": "2024-08-28T13:41:01Z",
      "closed_at": "2024-08-28T13:41:01Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 11,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11907/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "JinheTang",
        "rnwang04"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11907",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11907",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:25.459139",
      "comments": [
        {
          "author": "JinheTang",
          "body": "Hi @brownplayer, we will take a look to see if we can reproduce the issue first. If there is any progress, we will update here to let you know.",
          "created_at": "2024-08-26T08:30:44Z"
        },
        {
          "author": "brownplayer",
          "body": "OK，thank you\n\n---- Replied Message ----\n| From | ***@***.***> |\n| Date | 08/26/2024 16:31 |\n| To | intel-analytics/ipex-llm ***@***.***> |\n| Cc | brownplayer ***@***.***>,\nMention ***@***.***> |\n| Subject | Re: [intel-analytics/ipex-llm] Compatibility issues between transformers and accelerate versi",
          "created_at": "2024-08-26T09:25:01Z"
        },
        {
          "author": "JinheTang",
          "body": "Hi @brownplayer , it seems like your `accelerate` version is inconsistent with your `transformers` version. Running `pip install --pre --upgrade accelerate`  shall work.",
          "created_at": "2024-08-28T06:22:04Z"
        },
        {
          "author": "brownplayer",
          "body": "> Hi @brownplayer , it seems like your version is inconsistent with your version. Running shall work.`accelerate``transformers``pip install --pre --upgrade accelerate`\r\n\r\nI have upgraded the accelerate and transformers, but there are new mistakes\r\n![image](https://github.com/user-attachments/assets/",
          "created_at": "2024-08-28T07:09:40Z"
        },
        {
          "author": "JinheTang",
          "body": "> > Hi @brownplayer , it seems like your version is inconsistent with your version. Running shall work.`` accelerate``transformers``pip install --pre --upgrade accelerate ``\r\n> \r\n> I have upgraded the accelerate and transformers, but there are new mistakes ![image](https://private-user-images.github",
          "created_at": "2024-08-28T07:44:30Z"
        }
      ]
    },
    {
      "issue_number": 11944,
      "title": "vllm package is missing in intelanalytics/ipex-llm-serving-xpu:2.1.0",
      "body": "When you pull this docker file:intelanalytics/ipex-llm-serving-xpu:2.1.0 and start vllm serving like the following cmd:\r\npython -m ipex_llm.vllm.xpu.entrypoints.openai.api_server \\\r\n  --served-model-name $served_model_name \\\r\n  --port 8000 \\\r\n  --model $model \\\r\n  --trust-remote-code \\\r\n  --gpu-memory-utilization 0.7 \\\r\n  --device xpu \\\r\n  --dtype float16 \\\r\n  --enforce-eager \\\r\n  --load-in-low-bit fp8 \\\r\n  --max-model-len 6656 \\\r\n  --max-num-batched-tokens 6656 \\\r\n  --tensor-parallel-size 4\r\n\r\nIt will report ModuleNotFoundError: No module named 'vllm'",
      "state": "closed",
      "author": "HoppeDeng",
      "author_type": "User",
      "created_at": "2024-08-27T12:33:56Z",
      "updated_at": "2024-08-28T07:49:15Z",
      "closed_at": "2024-08-28T00:53:16Z",
      "labels": [
        "user issue",
        "multi-arc"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11944/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "liu-shaojun"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11944",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11944",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:25.714988",
      "comments": [
        {
          "author": "liu-shaojun",
          "body": "Hi @HoppeDeng Could you please share the image ID you're using?\r\n\r\nI’ve pulled the latest Docker image and conducted VLLM serving, but I wasn't able to reproduce the issue on my end.\r\n![image](https://github.com/user-attachments/assets/b045373f-a2ef-45fd-997e-c5d46906ae0a)\r\n\r\nIf you're not using the",
          "created_at": "2024-08-27T14:34:23Z"
        },
        {
          "author": "HoppeDeng",
          "body": "@liu-shaojun It is my mistake. Mounting local dir to /llm is not correct. Use /llm/models dir will work\r\n",
          "created_at": "2024-08-28T00:53:16Z"
        }
      ]
    },
    {
      "issue_number": 11183,
      "title": "[Feature Request] Provide IPEX-LLM as an executable to install in Windows",
      "body": "Hello! Thank you for this awesome project. :)\r\nI am trying to build one application that uses `IPEX-LLM` to run llm on Intel GPUs. \r\nI want to do something like create an executable so that the non-tech users don't have to go through the hassle of installation. \r\nFor instance: `Ollama` provides executable for windows installation. I am expecting something similar.\r\n\r\nSo, is there some executable already available for ipex-llm somewhere? Or, can somebody give me any hints on creating one?\r\nI want to serve llm using ollama on Intel GPUs in windows OS. So, I need an executable of ipex-llm with llama.cpp backend `ipex-llm[cpp]`\r\n\r\nThank you in advance for your help!",
      "state": "closed",
      "author": "bibekyess",
      "author_type": "User",
      "created_at": "2024-05-31T09:41:08Z",
      "updated_at": "2024-08-28T05:13:10Z",
      "closed_at": "2024-06-12T05:42:02Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11183/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11183",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11183",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:25.904546",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @bibekyess , you may see https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/llama_cpp_quickstart.html to get the ipex-llm's support for llama.cpp. ",
          "created_at": "2024-06-03T02:21:34Z"
        },
        {
          "author": "bibekyess",
          "body": "Hi @sgwhat! Thank you so much for your message.\r\nI have gone through the documentation you provided before, but it didn't fit my requirements so I questioned. FYI, I want to create a service for users who should not go through these low level installations and should just install with one click.\r\n\r\n",
          "created_at": "2024-06-03T03:32:59Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @bibekyess ,\r\n\r\n1. For your question1, you may extract the `main/server` executable files from the installed `ipex-llm[cpp]` and provide them directly to users for use.\r\n2. For your question2, yes, these executable files can be copied to another windows pc and will run correctly.",
          "created_at": "2024-06-03T07:44:56Z"
        },
        {
          "author": "bibekyess",
          "body": "Ok great! Thank you so much for your response! :)",
          "created_at": "2024-06-03T23:25:44Z"
        },
        {
          "author": "bibekyess",
          "body": "@sgwhat Hi! I can copy to another windows PC and run it, but I need to copy the entire miniforge/conda environment, activate it and then run the `ollama.exe serve`. If I don't activate the environment, it will throw `svml_dispmd.dll was not found`. Is there any better way to share the executables? S",
          "created_at": "2024-08-28T05:13:09Z"
        }
      ]
    },
    {
      "issue_number": 11401,
      "title": "Looking for a workaround to install IPEX-LLM on Windows with an Intel GPU but running with a CPU and not running with a GPU",
      "body": "All I need is to run ollama3 on an Intel GPU (Arc™ A750) and I follow the steps as described in the IPEX-LLM documentation, but it runs on the CPU. Search engines can't find a solution to the problem. Is there a big guy to see where the problem is, thank you.\r\n\r\nHere are the steps I followed in the quickstart of the official IPEX-LLM Document\r\n\r\n## 1 . Install IPEX-LLM on Windows with Intel GPU\r\n\r\n###  1.1Setup Python Environment\r\n\r\n1.  conda create -n llm python=3.11 libuv\r\n2.  conda activate llm\r\n3.  pip install --pre --upgrade ipex-llm[xpu] --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/\r\n\r\n## 2. Run llama.cpp with IPEX-LLM on Intel GPU\r\n\r\nInstead of executing conda create -n llm-cpp python=3.11 conda activate llm-cpp as stated in the document, I directly use the llm virtual environment in step 1.\r\n\r\n1. pip install --pre --upgrade ipex-llm[cpp]\r\n\r\n2. mkdir llama-cpp\r\n\r\n3. cd llama-cpp\r\n\r\n4.  init-llama-cpp.bat\r\n\r\n## 3. Run Llama 3 on Intel GPU using llama.cpp and ollama with IPEX-LLM\r\n\r\n#### 3.1 Run Llama3 using Ollama\r\n\r\n##### 3.1.1 Run Ollama Serve\r\n\r\n set OLLAMA_NUM_GPU=999\r\n set no_proxy=localhost,127.0.0.1\r\n set ZES_ENABLE_SYSMAN=1\r\n set SYCL_CACHE_PERSISTENT=1\r\n\r\n ollama serve\r\n\r\n\r\n\r\n\r\n<img width=\"855\" alt=\"1\" src=\"https://github.com/intel-analytics/ipex-llm/assets/104905916/9f89b73d-45bf-4a9b-8314-82a9a009b2dc\">\r\n\r\n\r\n<img width=\"1236\" alt=\"2\" src=\"https://github.com/intel-analytics/ipex-llm/assets/104905916/95a973e6-7c86-4c14-8cde-aadeb7a12ee8\">\r\n",
      "state": "closed",
      "author": "ChordGL",
      "author_type": "User",
      "created_at": "2024-06-22T23:39:19Z",
      "updated_at": "2024-08-28T03:00:24Z",
      "closed_at": "2024-08-28T03:00:24Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 8,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11401/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11401",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11401",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:26.112971",
      "comments": [
        {
          "author": "TriDefender",
          "body": "try adding this after loading the model;\r\n`model=model.to('xpu')`\r\nor else it will remain on the cpu and will not move to gpu\r\n",
          "created_at": "2024-06-23T03:18:03Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @Fucalors ,\r\n\r\n1. Could you please provide the complete runtime log of the Ollama server side during model inference?\r\n2. Could you please run `ls-sycl-device.exe` and reply us the output?",
          "created_at": "2024-06-24T03:38:14Z"
        },
        {
          "author": "ChordGL",
          "body": "> 嗨，\r\n> \r\n> 1. 您能否在模型推理期间提供 Ollama 服务器端的完整运行时日志？\r\n> 2. 您能否运行并回复我们输出？`ls-sycl-device.exe`\r\n\r\nAre you talking about these two logs?\r\n\r\n<img width=\"1916\" alt=\"屏幕截图 2024-06-24 151005\" src=\"https://github.com/intel-analytics/ipex-llm/assets/104905916/3c37a7a4-01b1-473e-a351-d02b5fcc1d67\">\r\n\r\n<img width=\"",
          "created_at": "2024-06-24T07:35:34Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @Fucalors, I don't think you are running ipex-llm ollama. Please double-check your environment and installation method. You may refer to our documentation at https://ipex-llm-latest.readthedocs.io/en/latest/doc/LLM/Quickstart/ollama_quickstart.html for installing Ollama.",
          "created_at": "2024-06-24T09:42:01Z"
        },
        {
          "author": "ayttop",
          "body": "\r\n\r\n\r\n\r\n\r\n\r\nOLLAMA_INTEL_GPU:false?!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n\r\n\r\n\r\n(1) C:\\Users\\ArabTech\\Desktop\\1>ollama serve\r\n2024/08/27 17:15:31 routes.go:1125: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH",
          "created_at": "2024-08-28T00:17:51Z"
        }
      ]
    },
    {
      "issue_number": 11924,
      "title": "A \"SYCL error\" happened when load Phi3 model to Intel GPU",
      "body": "We follow the guideline to setup IPEX-LLM[CPP] and also Intel OneAPI. but when we load Phi3 model to Intel GPU, it always have \"SYCL error\" happened. and we also use main.exe to load model manually, same error.\r\nI attached ollama and llama.cpp log files:\r\n[llama.cpp_log.txt](https://github.com/user-attachments/files/16745705/llama.cpp_log.txt)\r\n[ollama_log.txt](https://github.com/user-attachments/files/16745706/ollama_log.txt)\r\n\r\nI'm not sure is there is any environment issue for my setup.",
      "state": "open",
      "author": "soulyet",
      "author_type": "User",
      "created_at": "2024-08-26T08:02:31Z",
      "updated_at": "2024-08-28T02:55:47Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11924/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "rnwang04"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11924",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11924",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:26.339775",
      "comments": [
        {
          "author": "rnwang04",
          "body": "Hi @soulyet ,\r\nQ4_K has some error at your current machine, we may try to fix it later.\r\nBut for now, I think you can try Q4_0 first, which should work.",
          "created_at": "2024-08-27T03:26:12Z"
        },
        {
          "author": "soulyet",
          "body": "> Hi @soulyet , Q4_K has some error at your current machine, we may try to fix it later. But for now, I think you can try Q4_0 first, which should work.\r\n\r\nThanks for reply. \r\nNow I use Q4_0 which download from https://huggingface.co/SanctumAI/Phi-3-mini-4k-instruct-GGUF/tree/main for try, but met a",
          "created_at": "2024-08-28T02:55:46Z"
        }
      ]
    },
    {
      "issue_number": 11898,
      "title": "Too many python process when running python/llm/example/GPU/HuggingFace/LLM/codeshell/server.py",
      "body": "Hi,\r\nWhen running python/llm/example/GPU/HuggingFace/LLM/codeshell/server.py \r\n\r\npython server.py --checkpoint-path /home/user/Qwen2-7B-Instruct --device xpu --multi-turn --max-context 1024\r\n\r\n40+ python processes would be running,  ps aux | grep \"python\"\r\n![image](https://github.com/user-attachments/assets/e3e2c69b-e93b-4ab6-b165-7e04a6c2f231)\r\n\r\nIn the code, the uvicorn workers is set to 1, not sure why so many process were running.\r\nuvicorn.run(app, host=args.server_name, port=args.server_port, workers=1)",
      "state": "open",
      "author": "wluo1007",
      "author_type": "User",
      "created_at": "2024-08-22T09:25:28Z",
      "updated_at": "2024-08-28T02:42:31Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11898/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11898",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11898",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:26.528347",
      "comments": [
        {
          "author": "hkvision",
          "body": "We have produced this issue and are looking into it. Will keep you updated as soon as possible.",
          "created_at": "2024-08-26T02:27:52Z"
        },
        {
          "author": "cranechu0131",
          "body": "Hi wluo1007，\r\n  We have looked into the issue. The current conclusion is that ipex-llm will import `intel_extension_for_pytorch` and just only importing `intel_extension_for_pytorch` will cause this issue. The following is a quick example to verify our discovery\r\n```\r\nimport uvicorn\r\nimport intel_ex",
          "created_at": "2024-08-28T02:34:00Z"
        }
      ]
    },
    {
      "issue_number": 11884,
      "title": "Please add ollama version/tag number in ipex-llm[cpp]",
      "body": "when I run with cmd 'ollama --version'\r\nIt always returns with 0.0.0.0\r\n\r\nPlease add version or tag number to ollama.exe so we can do better management in our project.",
      "state": "open",
      "author": "jianjungu",
      "author_type": "User",
      "created_at": "2024-08-21T05:24:12Z",
      "updated_at": "2024-08-27T13:32:00Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11884/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "rnwang04"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11884",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11884",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:26.754693",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @jianjungu, you may also see [ipex-llm ollama quickstart](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.md) for the current ollama version. \r\n\r\n![image](https://github.com/user-attachments/assets/a946a99f-0254-4fc8-95a1-1eb95d5c4a6d)\r\n",
          "created_at": "2024-08-22T01:31:44Z"
        },
        {
          "author": "rnwang04",
          "body": "Hi @jianjungu , with `ipex-llm[cpp]>=2.2.0b20240827`,  the output of `ollama --version` will look like `ollama version is 0.3.6-ipexllm-20240827 `, hope it meet your requirement 😊",
          "created_at": "2024-08-27T13:28:42Z"
        }
      ]
    },
    {
      "issue_number": 11887,
      "title": "Minicpm-V-2.6 Llama.cpp and Ollama Support",
      "body": "Hi, \r\n\r\nMinicpm-V is a very popular model these days, although it has not been supported by the current version of llama.cpp and ollama, please refer to the following submission:\r\nhttps://github.com/ollama/ollama/issues/6417\r\nhttps://github.com/ggerganov/llama.cpp/releases/tag/b3598\r\nBR",
      "state": "open",
      "author": "RobinJing",
      "author_type": "User",
      "created_at": "2024-08-21T07:06:40Z",
      "updated_at": "2024-08-27T13:30:03Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11887/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 1,
        "eyes": 0
      },
      "assignees": [
        "rnwang04"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11887",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11887",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:26.940254",
      "comments": [
        {
          "author": "rnwang04",
          "body": "Hi @RobinJing ,\r\nipex-llm‘s ollama is upgrade to 0.3.6 with `ipex-llm[cpp]>=2.2.0b20240827`, you may have a try with latest llama.cpp / ollama 😊",
          "created_at": "2024-08-27T13:30:02Z"
        }
      ]
    },
    {
      "issue_number": 11886,
      "title": "Minicpm-V-2.5 Llama.cpp and Ollama Support",
      "body": "Hi, \r\n\r\nMinicpm-V is a very popular model these days, although it has not been supported by the current version of llama.cpp and ollama, please refer to the following submission:\r\nhttps://github.com/ollama/ollama/issues/6307\r\nhttps://github.com/ggerganov/llama.cpp/pull/7599\r\n\r\nBR",
      "state": "open",
      "author": "RobinJing",
      "author_type": "User",
      "created_at": "2024-08-21T07:05:29Z",
      "updated_at": "2024-08-27T13:29:50Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11886/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "rnwang04"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11886",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11886",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:27.160433",
      "comments": [
        {
          "author": "rnwang04",
          "body": "Hi @RobinJing ,\r\nipex-llm‘s ollama is upgrade to 0.3.6 with `ipex-llm[cpp]>=2.2.0b20240827`, you may have a try with latest llama.cpp / ollama 😊",
          "created_at": "2024-08-27T13:29:48Z"
        }
      ]
    },
    {
      "issue_number": 11883,
      "title": "Can ipex-llm[cpp] support the bge-m3 model?",
      "body": "The official ollama supports this model in v0.3.4\r\n[https://github.com/ollama/ollama/releases/tag/v0.3.4](https://github.com/ollama/ollama/releases/tag/v0.3.4)\r\n\r\nTried with ollama in 2.1.0b20240820, but failed with 0xc0000005 \r\n\r\n`\r\ntime=2024-08-21T13:10:17.961+08:00 level=INFO source=memory.go:133 msg=\"offload to gpu\" layers.requested=-1 layers.real=25 memory.available=\"40.5 GiB\" memory.required.full=\"1.1 GiB\" memory.required.partial=\"1.1 GiB\" memory.required.kv=\"12.0 MiB\" memory.weights.total=\"1.0 GiB\" memory.weights.repeating=\"577.2 MiB\" memory.weights.nonrepeating=\"488.3 MiB\" memory.graph.full=\"32.0 MiB\" memory.graph.partial=\"32.0 MiB\"\r\ntime=2024-08-21T13:10:17.963+08:00 level=INFO source=server.go:342 msg=\"starting llama server\" cmd=\"C:\\\\Users\\\\intel\\\\ipex-llm-ollama_\\\\dist\\\\windows-amd64\\\\ollama_runners\\\\cpu_avx2\\\\ollama_llama_server.exe --model C:\\\\Users\\\\intel\\\\.ollama\\\\models\\\\blobs\\\\sha256-daec91ffb5dd0c27411bd71f29932917c49cf529a641d0168496c3a501e3062c --ctx-size 2048 --batch-size 512 --embedding --log-disable --n-gpu-layers 999 --parallel 1 --port 55829\"\r\ntime=2024-08-21T13:10:17.966+08:00 level=INFO source=sched.go:338 msg=\"loaded runners\" count=1\r\ntime=2024-08-21T13:10:17.966+08:00 level=INFO source=server.go:529 msg=\"waiting for llama runner to start responding\"\r\ntime=2024-08-21T13:10:17.966+08:00 level=INFO source=server.go:566 msg=\"waiting for server to become available\" status=\"llm server error\"\r\nINFO [wmain] build info | build=1 commit=\"f6b084d\" tid=\"39304\" timestamp=1724217017\r\nINFO [wmain] system info | n_threads=12 n_threads_batch=-1 system_info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \" tid=\"39304\" timestamp=1724217017 total_threads=24\r\nINFO [wmain] HTTP server listening | hostname=\"127.0.0.1\" n_threads_http=\"23\" port=\"55829\" tid=\"39304\" timestamp=1724217017\r\nllama_model_loader: loaded meta data with 33 key-value pairs and 389 tensors from C:\\Users\\intel\\.ollama\\models\\blobs\\sha256-daec91ffb5dd0c27411bd71f29932917c49cf529a641d0168496c3a501e3062c (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = bert\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                         general.size_label str              = 567M\r\nllama_model_loader: - kv   3:                            general.license str              = mit\r\nllama_model_loader: - kv   4:                               general.tags arr[str,4]       = [\"sentence-transformers\", \"feature-ex...\r\nllama_model_loader: - kv   5:                           bert.block_count u32              = 24\r\nllama_model_loader: - kv   6:                        bert.context_length u32              = 8192\r\nllama_model_loader: - kv   7:                      bert.embedding_length u32              = 1024\r\nllama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 4096\r\nllama_model_loader: - kv   9:                  bert.attention.head_count u32              = 16\r\nllama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  11:                          general.file_type u32              = 1\r\nllama_model_loader: - kv  12:                      bert.attention.causal bool             = false\r\nllama_model_loader: - kv  13:                          bert.pooling_type u32              = 2\r\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = t5\r\nllama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = default\r\nllama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,250002]  = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \",\"...\r\nllama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,250002]  = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,250002]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  19:            tokenizer.ggml.add_space_prefix bool             = true\r\nllama_model_loader: - kv  20:            tokenizer.ggml.token_type_count u32              = 1\r\nllama_model_loader: - kv  21:    tokenizer.ggml.remove_extra_whitespaces bool             = true\r\nllama_model_loader: - kv  22:        tokenizer.ggml.precompiled_charsmap arr[u8,237539]   = [0, 180, 2, 0, 0, 132, 0, 0, 0, 0, 0,...\r\nllama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 0\r\nllama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  25:            tokenizer.ggml.unknown_token_id u32              = 3\r\nllama_model_loader: - kv  26:          tokenizer.ggml.seperator_token_id u32              = 2\r\nllama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 1\r\nllama_model_loader: - kv  28:                tokenizer.ggml.cls_token_id u32              = 0\r\nllama_model_loader: - kv  29:               tokenizer.ggml.mask_token_id u32              = 250001\r\nllama_model_loader: - kv  30:               tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader: - kv  31:               tokenizer.ggml.add_eos_token bool             = true\r\nllama_model_loader: - kv  32:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:  244 tensors\r\nllama_model_loader: - type  f16:  145 tensors\r\nllm_load_vocab: unknown tokenizer: 't5'llm_load_vocab: using default tokenizer: 'llama'llm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = bert\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 250002\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 8192\r\nllm_load_print_meta: n_embd           = 1024\r\nllm_load_print_meta: n_head           = 16\r\nllm_load_print_meta: n_head_kv        = 16\r\nllm_load_print_meta: n_layer          = 24\r\nllm_load_print_meta: n_rot            = 64\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 64\r\nllm_load_print_meta: n_embd_head_v    = 64\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 1.0e-05\r\nllm_load_print_meta: f_norm_rms_eps   = 0.0e+00\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 4096\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 0\r\nllm_load_print_meta: pooling type     = 2\r\nllm_load_print_meta: rope type        = 2\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 8192\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 335M\r\nllm_load_print_meta: model ftype      = F16\r\nllm_load_print_meta: model params     = 566.70 M\r\nllm_load_print_meta: model size       = 1.07 GiB (16.25 BPW)\r\nllm_load_print_meta: general.name     = n/a\r\ntime=2024-08-21T13:10:18.226+08:00 level=ERROR source=sched.go:344 msg=\"error loading llama server\" error=\"llama runner process has terminated: exit status 0xc0000005 \"\r\n[GIN] 2024/08/21 - 13:10:18 | 500 |    1.5625998s |       127.0.0.1 | POST     \"/api/embeddings\"\r\n`",
      "state": "open",
      "author": "jianjungu",
      "author_type": "User",
      "created_at": "2024-08-21T05:16:42Z",
      "updated_at": "2024-08-27T13:26:49Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11883/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "rnwang04"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11883",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11883",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:27.391742",
      "comments": [
        {
          "author": "rnwang04",
          "body": "Hi @jianjungu,\r\nipex-llm‘s ollama is upgrade to 0.3.6 with `ipex-llm[cpp]>=2.2.0b20240827`, you may have a try with it 😊",
          "created_at": "2024-08-27T13:26:48Z"
        }
      ]
    },
    {
      "issue_number": 11709,
      "title": "update ollama 0.3.x support",
      "body": "''ipex-llm[cpp]==2.5.0b20240527 is consistent with [v0.1.34] of ollama.\r\n\r\nOur current version is consistent with [v0.1.39] of ollama.''\r\n\r\nIs it possible to update supported ollama version to 0.3.x?",
      "state": "open",
      "author": "przybjul",
      "author_type": "User",
      "created_at": "2024-08-02T14:15:17Z",
      "updated_at": "2024-08-27T13:26:17Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 9,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11709/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11709",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11709",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:27.614570",
      "comments": [
        {
          "author": "sgwhat",
          "body": "hi @przybjul, we can upgrade the version of ipex-llm ollama, and we will notify you immediately after completing the upgrade.",
          "created_at": "2024-08-05T02:17:07Z"
        },
        {
          "author": "AlbertXu233",
          "body": "> hi @przybjul, we can upgrade the version of ipex-llm ollama, and we will notify you immediately after completing the upgrade.\r\n\r\nI wanna know whether the upgrade is completed or not.",
          "created_at": "2024-08-13T09:24:46Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @AlbertXu233 , our current version is still consistent with [v0.1.39](https://github.com/ollama/ollama/releases/tag/v0.1.39) of ollama. Do you need the newly supported models or other latest features in ollama version 0.3.x?",
          "created_at": "2024-08-14T02:10:01Z"
        },
        {
          "author": "rebootcheng",
          "body": "@sgwhat Some new models are only supported by higher versions of Olama",
          "created_at": "2024-08-15T01:03:49Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @rebootcheng, we have now supported models available in higher versions of Ollama, including Llama 3.1, Qwen 2, Gemma 2, Phi 3, and others.",
          "created_at": "2024-08-15T02:16:50Z"
        }
      ]
    },
    {
      "issue_number": 11916,
      "title": "Error 403 occurs when the browser plug-in calls the ollama api",
      "body": "Error 403 occurs when the browser plug-in calls the ollama api\r\n![image](https://github.com/user-attachments/assets/0a346489-0a68-4b7f-8598-e6f0c3c56dd5)\r\nThe cors unblock plug-in is enabled on the browser\r\n",
      "state": "closed",
      "author": "brownplayer",
      "author_type": "User",
      "created_at": "2024-08-24T10:14:51Z",
      "updated_at": "2024-08-27T01:59:02Z",
      "closed_at": "2024-08-27T01:59:02Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11916/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11916",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11916",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:27.835379",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @brownplayer , I am not able to reproduce your issue, could you please provide the detailed cmd that you ran ollama api? By the way, you may set your env before running `ollama serve` as below:\r\n```\r\nset OLLAMA_NUM_GPU=999\r\nset no_proxy=localhost,127.0.0.1\r\nset ZES_ENABLE_SYSMAN=1\r\nset SYCL_CACHE",
          "created_at": "2024-08-27T01:55:36Z"
        },
        {
          "author": "brownplayer",
          "body": "Thanks for your reply, I have solved the problem by adding set OLLAMA_ORIGINS=chrome-extension://* to the environment variable",
          "created_at": "2024-08-27T01:59:00Z"
        }
      ]
    },
    {
      "issue_number": 11914,
      "title": "ipex Llama.cpp server fails with Phi3 models",
      "body": "Hi, \r\n\r\nI've trying to serve different Phi3 models using the Llama.cpp server that is created by the init-llama-cpp ipex.\r\n\r\nWhen I server with this version I have two problems:\r\n\r\n1) The server doesn't stop on <end> token, it just keeps generating stuff (I need to set the -n parameter to make it stop somewhere).\r\n2) I also get inconsistent output, mostly gibberish. \r\n\r\nDoing the same inference on mainstream Llama.cpp server with SYSCL backend works.\r\n\r\nThe same happens for Phi3-medium (Q6, Q4), Phi3-mini and Phi3.5 mini.\r\n\r\nI have the latests versions of everything (driver, ipex), and I use an ARC 770 GPU.\r\n\r\nThanks!",
      "state": "open",
      "author": "hvico",
      "author_type": "User",
      "created_at": "2024-08-23T22:11:26Z",
      "updated_at": "2024-08-26T02:55:20Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11914/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11914",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11914",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:28.067665",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @hvico, we are trying to reproduce your issue. Could you please show more details that displayed by ollama served side?",
          "created_at": "2024-08-26T01:30:18Z"
        },
        {
          "author": "rnwang04",
          "body": "Hi @hvico , could you please also provide us with your detail cmd so that we can try to reproduce it ?",
          "created_at": "2024-08-26T02:55:19Z"
        }
      ]
    },
    {
      "issue_number": 11906,
      "title": "Improt error: from ipex_llm.llamaindex.llms import IpexLLM.",
      "body": "from ipex_llm.llamaindex.llms import IpexLLM\r\nerror:\r\n```\r\n----> 3 from ipex_llm.llamaindex.llms import IpexLLM\r\n\r\nFile /usr/local/lib/python3.10/site-packages/ipex_llm/llamaindex/llms/__init__.py:25\r\n     22 \"\"\"Wrappers on top of large language models APIs.\"\"\"\r\n     23 from typing import Dict, Type\r\n---> 25 from .bigdlllm import *\r\n     26 from llama_index.core.base.llms.base import BaseLLM\r\n     28 __all__ = [\r\n     29     \"IpexLLm\",\r\n     30 ]\r\n\r\nFile /usr/local/lib/python3.10/site-packages/ipex_llm/llamaindex/llms/bigdlllm.py:90\r\n     85 DEFAULT_HUGGINGFACE_MODEL = \"meta-llama/Llama-2-7b-chat-hf\"\r\n     87 logger = logging.getLogger(__name__)\r\n---> 90 class IpexLLM(CustomLLM):\r\n     91     \"\"\"Wrapper around the IPEX-LLM\r\n     92 \r\n     93     Example:\r\n   (...)\r\n     97             llm = IpexLLM(model_path=\"/path/to/llama/model\")\r\n     98     \"\"\"\r\n    100     model_name: str = Field(\r\n    101         default=DEFAULT_HUGGINGFACE_MODEL,\r\n    102         description=(\r\n   (...)\r\n    105         ),\r\n    106     )\r\n\r\nFile /usr/local/lib/python3.10/site-packages/ipex_llm/llamaindex/llms/bigdlllm.py:170, in IpexLLM()\r\n    159 model_kwargs: dict = Field(\r\n    160     default_factory=dict,\r\n    161     description=\"The kwargs to pass to the model during initialization.\",\r\n    162 )\r\n    163 generate_kwargs: dict = Field(\r\n    164     default_factory=dict,\r\n    165     description=\"The kwargs to pass to the model during generation.\",\r\n    166 )\r\n    167 is_chat_model: bool = Field(\r\n    168     default=False,\r\n    169     description=(\r\n--> 170         LLMMetadata.__fields__[\"is_chat_model\"].field_info.description\r\n    171         + \" Be sure to verify that you either pass an appropriate tokenizer \"\r\n    172         \"that can convert prompts to properly formatted chat messages or a \"\r\n    173         \"`messages_to_prompt` that does so.\"\r\n    174     ),\r\n    175 )\r\n    176 load_low_bit: bool = Field(\r\n    177     default=False,\r\n    178     description=\"The model is low_bit model or not\"\r\n    179 )\r\n    181 _model: Any = PrivateAttr()\r\n\r\nAttributeError: 'FieldInfo' object has no attribute 'field_info'\r\n```\r\nHowever, it works fine by (from llama_index.llms.ipex_llm import IpexLLM).\r\nIs there any version different between llama_index.ipexllm and ipexllm.llama_index",
      "state": "open",
      "author": "ningwebbeginner",
      "author_type": "User",
      "created_at": "2024-08-23T07:07:48Z",
      "updated_at": "2024-08-26T02:33:22Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11906/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Oscilloscope98"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11906",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11906",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:28.332015",
      "comments": [
        {
          "author": "Oscilloscope98",
          "body": "Hi @ningwebbeginner,\r\n\r\nWe recommend using `llama-index-llms-ipex-llm`. As `ipex-llm` has been upstreamed to Llama-Index, we may no longer provide active maintenance for `ipex_llm.llamaindex.llms`.\r\n\r\nYou could refer to [IPEX-LLM on Intel GPU](https://docs.llamaindex.ai/en/stable/examples/llm/ipex_l",
          "created_at": "2024-08-26T02:33:21Z"
        }
      ]
    },
    {
      "issue_number": 11911,
      "title": "Cannot run llama3.1 models with ollama",
      "body": "I've installed ipex-llm with llama.cpp and ollama support following the instructions:\r\n```bash\r\npip install --pre --upgrade ipex-llm[cpp]\r\ninit-llama-cpp\r\ninit-ollama\r\n```\r\n\r\nI'm trying to use model `llama3.1:8b-instruct-q4_K_M` with ollama and langchain. When downloading the model with ollama I get an error:\r\n\r\n```txt\r\n$ ollama pull llama3.1:8b-instruct-q4_K_M\r\n\r\nError: pull model manifest: 412: \r\n\r\nThe model you are attempting to pull requires a newer version of Ollama.\r\n\r\nPlease download the latest version at:\r\n\r\n        https://ollama.com/download\r\n```\r\n\r\nipex-llm version is 2.2.0b20240822\r\n\r\nMinimal reproducer:\r\n\r\n```python\r\nfrom langchain_community.llms import Ollama\r\n\r\nllm = Ollama(model=\"llama3.1:8b-instruct-q4_K_M\")\r\n\r\nq = \"What is the capital of Spain?\"\r\nprint(f\"Query: {q}\")\r\nresponse = llm.invoke(q)\r\nprint('Response:')\r\nprint(response)\r\n```\r\n\r\nExecution:\r\n```bash\r\npip install langchain langchain_community\r\n./ollama serve &\r\npython test.py\r\n```\r\n",
      "state": "closed",
      "author": "tkarna",
      "author_type": "User",
      "created_at": "2024-08-23T08:54:54Z",
      "updated_at": "2024-08-23T09:44:24Z",
      "closed_at": "2024-08-23T09:42:20Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11911/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "rnwang04"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11911",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11911",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:28.570555",
      "comments": [
        {
          "author": "rnwang04",
          "body": "Hi @tkarna ,\r\n> ```\r\n> $ ollama pull llama3.1:8b-instruct-q4_K_M\r\n> \r\n> Error: pull model manifest: 412: \r\n> \r\n> The model you are attempting to pull requires a newer version of Ollama.\r\n> \r\n> Please download the latest version at:\r\n> \r\n>         https://ollama.com/download\r\n \r\nMaybe try it again wi",
          "created_at": "2024-08-23T09:04:51Z"
        },
        {
          "author": "tkarna",
          "body": "Thanks, an older version of ollama was indeed interfering. Fixed that now. I can pull the model correctly. But when I run the example, I get an error:\r\n\r\n```txt\r\nllama_model_load: error loading model: done_getting_tensors: wrong number of tensors; expected 292, got 291\r\n```\r\n\r\nCan you reproduce this",
          "created_at": "2024-08-23T09:30:04Z"
        },
        {
          "author": "rnwang04",
          "body": "Here is a related user issue: https://github.com/intel-analytics/ipex-llm/issues/11836\r\nBut I think we have fixed it. Could you please check it again ?",
          "created_at": "2024-08-23T09:33:23Z"
        },
        {
          "author": "tkarna",
          "body": "Thanks, I was testing on another system that had a slightly older version of ipex. The test works now.",
          "created_at": "2024-08-23T09:42:20Z"
        },
        {
          "author": "rnwang04",
          "body": "Glad to hear to that : )",
          "created_at": "2024-08-23T09:44:23Z"
        }
      ]
    },
    {
      "issue_number": 11843,
      "title": "ipex-llm with vllm failed to run on Core ultra 7 165H iGPU",
      "body": "Platform: Core ultra 7 165H iGPU\r\n\r\nModel: Qwen/Qwen2-7B-Instruct\r\n\r\nFollowing the steps on https://testbigdldocshane.readthedocs.io/en/perf-docs/doc/LLM/Quickstart/vLLM_quickstart.html#\r\n\r\nwhen running python offline_inference.py, error would ocurr:\r\n\r\n(vllm_ipex_env) user@user-Meteor-Lake-Client-Platform:~/vllm$ python offline_inference.py\r\n/home/user/vllm_ipex_env/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\r\n  warnings.warn(\r\n/home/user/vllm_ipex_env/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n  warn(\r\n2024-08-19 13:40:32,185 - INFO - intel_extension_for_pytorch auto imported\r\nWARNING 08-19 13:40:32 config.py:710] Casting torch.bfloat16 to torch.float16.\r\nINFO 08-19 13:40:32 llm_engine.py:68] Initializing an LLM engine (v0.3.3) with config: model='/home/user/Qwen2-7B-Instruct', tokenizer='/home/user/Qwen2-7B-Instruct', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=xpu, seed=0, max_num_batched_tokens=32768, max_num_seqs=256, max_model_len=32768)\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nINFO 08-19 13:40:32 attention.py:71] flash_attn is not found. Using xformers backend.\r\n2024-08-19 13:40:34,255 - INFO - Converting the current model to sym_int4 format......\r\n2024-08-19 13:40:34,255 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\r\n2024-08-19 13:40:38,071 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\r\nINFO 08-19 13:40:40 model_convert.py:249] Loading model weights took 4.5222 GB\r\nLLVM ERROR: Diag: aborted\r\n\r\nLIBXSMM_VERSION: main_stable-1.17-3651 (25693763)\r\nLIBXSMM_TARGET: adl [Intel(R) Core(TM) Ultra 7 165H]\r\nRegistry and code: 13 MB\r\nCommand: python offline_inference.py\r\nUptime: 19.974422 s\r\nAborted (core dumped)\r\n\r\nI've also tried the whole process on Data center dGPU flex, which works fine, wondering if this issue only occurs on iGPU. ",
      "state": "open",
      "author": "wluo1007",
      "author_type": "User",
      "created_at": "2024-08-19T06:32:41Z",
      "updated_at": "2024-08-23T07:51:45Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11843/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hzjane"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11843",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11843",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:30.623922",
      "comments": [
        {
          "author": "hzjane",
          "body": "We haven't tested it on mtl iGPU before. I tried to reproduce it but encountered a different error. Maybe You can try it on docker according to [this docker guide](https://github.com/intel-analytics/ipex-llm/tree/main/docker/llm/serving/xpu/docker).",
          "created_at": "2024-08-20T01:37:43Z"
        },
        {
          "author": "wluo1007",
          "body": "Tried docker, still got error, do you have plans for iGPU support?\r\n\r\nroot@user-Meteor-Lake-Client-Platform:/llm# python vllm_offline_inference.py\r\n/usr/local/lib/python3.11/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in ",
          "created_at": "2024-08-20T05:01:47Z"
        },
        {
          "author": "gc-fu",
          "body": "Hi, I have verified that vLLM works on iGPU with model `chatglm3-6b` on Linux and does not encounter the problem you mentioned in the thread.\r\n\r\nThe vLLM we provided does have a problem that are related to `Qwen2-7B-Instruct` but should not report the error in the first thread.\r\n\r\nCan you provide th",
          "created_at": "2024-08-20T15:33:11Z"
        },
        {
          "author": "wluo1007",
          "body": "Hi, Thanks for the reply, The previous environment is currently not available now, so I've installed the recent one to try on both chatglm3-6b and qwen2-7b-instruct, got the same error msg like below.   \r\n\r\n(vllm_ipex_env) user@user-Meteor-Lake-Client-Platform:~/vllm$ python offline_inference.py\r\n/h",
          "created_at": "2024-08-22T08:44:10Z"
        },
        {
          "author": "gc-fu",
          "body": "Hi, please try `ipex-llm[xpu]==2.1.0`.\r\n\r\nThere is a new feature that breaks the vLLM for the version `2.1.0b20240821`\r\n\r\nAlso, the 7b model might be too big.  `Qwen2-1.5b-Instruct` might be better.",
          "created_at": "2024-08-22T09:47:10Z"
        }
      ]
    },
    {
      "issue_number": 11853,
      "title": "tg speed of gemma2 is slower than upstream llama.cpp",
      "body": "Hi. I found the token generation speed of gemma2 in llama.cpp in `ipex-llm[cpp]` is slower than upstream llama.cpp. Can it be optimized?\r\n\r\n`ipex-llm[cpp]`:\r\n```\r\n| model                          |       size |     params | backend    | ngl |          test |              t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | ------------: | ---------------: |\r\n[SYCL] call ggml_init_sycl\r\nggml_init_sycl: GGML_SYCL_DEBUG: 0\r\nggml_init_sycl: GGML_SYCL_F16: no\r\nfound 1 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|                Intel Arc A750 Graphics|    1.3|    448|    1024|   32|  8096M|            1.3.27642|\r\nggml_backend_sycl_set_mul_device_mode: true\r\ndetect 1 SYCL GPUs: [0] with top Max compute units:448\r\n| gemma2 9B Q4_0                 |   5.76 GiB |    10.16 B | SYCL       |  99 |         pp512 |    791.23 ± 7.36 |\r\n| gemma2 9B Q4_0                 |   5.76 GiB |    10.16 B | SYCL       |  99 |         tg128 |     16.29 ± 0.45 |\r\n\r\nbuild: f6b084d (1)\r\n```\r\n\r\nupstream:\r\n```\r\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nggml_sycl_init: SYCL_USE_XMX: yes\r\nggml_sycl_init: found 1 SYCL devices:\r\n| model                          |       size |     params | backend    | ngl |          test |              t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | ------------: | ---------------: |\r\n[SYCL] call ggml_check_sycl\r\nggml_check_sycl: GGML_SYCL_DEBUG: 0\r\nggml_check_sycl: GGML_SYCL_F16: no\r\nfound 1 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|                Intel Arc A750 Graphics|    1.3|    448|    1024|   32|  8096M|            1.3.27642|\r\n| gemma2 9B Q4_0                 |   5.76 GiB |    10.16 B | SYCL       |  99 |         pp512 |    642.29 ± 6.25 |\r\n| gemma2 9B Q4_0                 |   5.76 GiB |    10.16 B | SYCL       |  99 |         tg128 |     20.90 ± 0.17 |\r\n\r\nbuild: cfac111e (3605)\r\n```",
      "state": "closed",
      "author": "ruihe774",
      "author_type": "User",
      "created_at": "2024-08-19T13:55:20Z",
      "updated_at": "2024-08-22T09:01:22Z",
      "closed_at": "2024-08-22T01:45:19Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11853/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "rnwang04"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11853",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11853",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:30.846040",
      "comments": [
        {
          "author": "rnwang04",
          "body": "Hi @ruihe774 ， we will take a look to see if we can reproduce this issue.\r\nCould you please also provide us with your detailed env information with our [env check script](https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/scripts)? Besides, what's the script you used to obtain above re",
          "created_at": "2024-08-20T02:11:31Z"
        },
        {
          "author": "ruihe774",
          "body": "benchmark command:\r\n```console\r\n$ ./llama-bench -m gemma2-9b-q4_0.gguf\r\n```\r\nThe GGUF model file is pulled from https://ollama.com/library/gemma2\r\n\r\nenv check (I only use `ipex-llm[cpp]`, so `ipex-llm[xpu]` is not installed):\r\n```\r\n-----------------------------------------------------------------\r\nP",
          "created_at": "2024-08-20T03:02:32Z"
        },
        {
          "author": "rnwang04",
          "body": "Hi @ruihe774 , we can't reproduce this issue on our Linux A750 machine.\r\n\r\n### upstream\r\n```bash\r\n$ \r\n./build/bin/llama-bench -m ~/gemma2-9b-it-q4_0.gguf\r\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nggml_sycl_init: SYCL_USE_XMX: yes\r\nggml_sycl_init: found 1 SYCL devices:\r\n| model                     ",
          "created_at": "2024-08-20T09:11:28Z"
        },
        {
          "author": "ruihe774",
          "body": "I did not set `SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1`. I'm using a very new kernel (6.10) and I found setting it degraded the performance.\r\n\r\nWith `SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1` set, the performance of upstream llama.cpp:\r\n```\r\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\n",
          "created_at": "2024-08-20T09:18:49Z"
        },
        {
          "author": "rnwang04",
          "body": "> I did not set `SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1`. I'm using a very new kernel (6.10) and I found setting it degraded the performance.\r\n\r\nThanks for pointing this ! \r\nWithout this env setting, \r\n\r\n## upstream\r\n```\r\nggml_sycl_init: found 1 SYCL devices:\r\n| model                       ",
          "created_at": "2024-08-21T01:43:54Z"
        }
      ]
    },
    {
      "issue_number": 11882,
      "title": "Qwen1.5-32B failed to start vllm serving on XeonW+4Arc workstation",
      "body": "HW platform: XeonW + 4Arc workstation\r\ndocker image： intelanalytics/ipex-llm-serving-xpu:2.1.0b\r\nServing start commands:\r\n# cat start_Qwen1.5-32B-Chat_serving.sh\r\n#!/bin/bash\r\nmodel=\"/llm/models/Qwen1.5-32B-Chat\"\r\nserved_model_name=\"Qwen1.5-32B-Chat\"\r\n\r\nexport USE_XETLA=OFF\r\nexport SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1\r\nexport SYCL_CACHE_PERSISTENT=1\r\nexport TORCH_LLM_ALLREDUCE=0\r\nexport CCL_DG2_ALLREDUCE=1\r\n\r\n# Tensor parallel related arguments:\r\nexport CCL_WORKER_COUNT=4\r\nexport FI_PROVIDER=shm\r\nexport CCL_ATL_TRANSPORT=ofi\r\nexport CCL_ZE_IPC_EXCHANGE=sockets\r\nexport CCL_ATL_SHM=1\r\n\r\nsource /opt/intel/oneapi/setvars.sh\r\nsource /opt/intel/1ccl-wks/setvars.sh\r\n\r\npython -m ipex_llm.vllm.xpu.entrypoints.openai.api_server \\\r\n  --served-model-name $served_model_name \\\r\n  --port 8000 \\\r\n  --model $model \\\r\n  --trust-remote-code \\\r\n  --gpu-memory-utilization 0.7 \\\r\n  --device xpu \\\r\n  --dtype float16 \\\r\n  --enforce-eager \\\r\n  --load-in-low-bit fp8 \\\r\n  --max-model-len 6656 \\\r\n  --max-num-batched-tokens 6656 \\\r\n  --tensor-parallel-size 4\r\n\r\n ERROR Message:\r\n2024-08-21 11:27:07,943 - INFO - intel_extension_for_pytorch auto imported\r\nINFO 08-21 11:27:09 api_server.py:258] vLLM API server version 0.3.3\r\nINFO 08-21 11:27:09 api_server.py:259] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name='Qwen1.5-32B-Chat', lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], load_in_low_bit='fp8', model='/llm/models/Qwen1.5-32B-Chat', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, download_dir=None, load_format='auto', dtype='float16', kv_cache_dtype='auto', max_model_len=6656, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, seed=0, swap_space=4, gpu_memory_utilization=0.7, max_num_batched_tokens=6656, max_num_seqs=256, max_paddings=256, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=True, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='xpu', engine_use_ray=False, disable_log_requests=False, max_log_len=None)\r\nWARNING 08-21 11:27:09 config.py:710] Casting torch.bfloat16 to torch.float16.\r\nINFO 08-21 11:27:09 config.py:523] Custom all-reduce kernels are temporarily disabled due to stability issues. We will re-enable them once the issues are resolved.\r\n2024-08-21 11:27:10,255 WARNING services.py:2017 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67067904 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\r\n2024-08-21 11:27:11,321 INFO worker.py:1781 -- Started a local Ray instance.\r\nINFO 08-21 11:27:12 llm_engine.py:68] Initializing an LLM engine (v0.3.3) with config: model='/llm/models/Qwen1.5-32B-Chat', tokenizer='/llm/models/Qwen1.5-32B-Chat', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=6656, download_dir=None, load_format=auto, tensor_parallel_size=4, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=xpu, seed=0, max_num_batched_tokens=6656, max_num_seqs=256, max_model_len=6656)\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n(RayWorkerVllm pid=15265) /usr/local/lib/python3.11/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\r\n(RayWorkerVllm pid=15265)   warnings.warn(\r\n(RayWorkerVllm pid=15265) /usr/local/lib/python3.11/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n(RayWorkerVllm pid=15265)   warn(\r\n(RayWorkerVllm pid=15265) 2024-08-21 11:27:19,927 - INFO - intel_extension_for_pytorch auto imported\r\nINFO 08-21 11:27:20 attention.py:71] flash_attn is not found. Using xformers backend.\r\n(RayWorkerVllm pid=15265) INFO 08-21 11:27:20 attention.py:71] flash_attn is not found. Using xformers backend.\r\n2024-08-21 11:27:21,607 - INFO - Converting the current model to fp8_e5m2 format......\r\n2024-08-21 11:27:21,607 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\r\n[2024-08-21 11:27:21,919] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to xpu (auto detect)\r\n2024-08-21 11:27:25,727 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\r\nINFO 08-21 11:27:28 model_convert.py:249] Loading model weights took 8.3120 GB\r\n(RayWorkerVllm pid=15265) 2024-08-21 11:27:29,611 - INFO - Converting the current model to fp8_e5m2 format......\r\n(RayWorkerVllm pid=15265) 2024-08-21 11:27:29,611 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\r\n(RayWorkerVllm pid=15407) /usr/local/lib/python3.11/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\r\n(RayWorkerVllm pid=15407)   warnings.warn( [repeated 2x across cluster]\r\n(RayWorkerVllm pid=15407) /usr/local/lib/python3.11/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source? [repeated 2x across cluster]\r\n(RayWorkerVllm pid=15407)   warn( [repeated 2x across cluster]\r\n(RayWorkerVllm pid=15407) 2024-08-21 11:27:19,964 - INFO - intel_extension_for_pytorch auto imported [repeated 2x across cluster]\r\n(RayWorkerVllm pid=15265) [2024-08-21 11:27:29,915] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to xpu (auto detect)\r\n(RayWorkerVllm pid=15407) INFO 08-21 11:27:20 attention.py:71] flash_attn is not found. Using xformers backend. [repeated 2x across cluster]\r\n(RayWorkerVllm pid=15337) 2024-08-21 11:27:29,885 - INFO - Converting the current model to fp8_e5m2 format...... [repeated 2x across cluster]\r\n(RayWorkerVllm pid=15265) 2024-08-21 11:27:49,136 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations [repeated 3x across cluster]\r\n(RayWorkerVllm pid=15265) INFO 08-21 11:27:54 model_convert.py:249] Loading model weights took 8.3120 GB\r\n(RayWorkerVllm pid=15337) [2024-08-21 11:27:30,198] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to xpu (auto detect) [repeated 2x across cluster]\r\n2024:08:21-11:27:55:(11976) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\r\n2024:08:21-11:27:55:(11976) |CCL_WARN| fallback to 'sockets' mode of ze exchange mechanism, to use CCL_ZE_IPC_EXHANGE=drmfd, set CCL_LOCAL_RANK/SIZE explicitly  or use process launcher\r\n(RayWorkerVllm pid=15265) 2024:08:21-11:27:55:(15265) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\r\n(RayWorkerVllm pid=15265) 2024:08:21-11:27:55:(15265) |CCL_WARN| fallback to 'sockets' mode of ze exchange mechanism, to use CCL_ZE_IPC_EXHANGE=drmfd, set CCL_LOCAL_RANK/SIZE explicitly  or use process launcher\r\n2024:08:21-11:27:55:(11976) |CCL_ERROR| atl_ofi_helper.cpp:867 atl_ofi_get_prov_list: fi_getinfo error: ret -61, providers 0\r\n2024:08:21-11:27:55:(11976) |CCL_ERROR| atl_ofi_helper.cpp:907 atl_ofi_get_prov_list: can't create providers for name shm\r\n2024:08:21-11:27:55:(11976) |CCL_ERROR| atl_ofi_helper.cpp:1243 atl_ofi_open_nw_provs: atl_ofi_get_prov_list(ctx, prov_name, base_hints, &prov_list)\r\n fails with status: 1\r\n2024:08:21-11:27:55:(11976) |CCL_ERROR| atl_ofi_helper.cpp:1384 atl_ofi_open_nw_provs: can not open network providers\r\n2024:08:21-11:27:55:(11976) |CCL_ERROR| atl_ofi.cpp:1036 open_providers: atl_ofi_open_nw_provs failed with status: 1\r\n2024:08:21-11:27:55:(11976) |CCL_ERROR| atl_ofi.cpp:175 init: open_providers(prov_env, coord, attr, base_hints, open_nw_provs, fi_version, pmi, true )\r\n fails with status: 1\r\n2024:08:21-11:27:55:(11976) |CCL_ERROR| atl_ofi.cpp:243 init: can't find suitable provider\r\n2024:08:21-11:27:55:(11976) |CCL_ERROR| atl_ofi_comm.cpp:278 init_transport: condition transport->init(nullptr, nullptr, &attr, nullptr, pmi) == ATL_STATUS_SUCCESS failed\r\nfailed to initialize ATL\r\n(RayWorkerVllm pid=15265) 2024:08:21-11:27:55:(15265) |CCL_ERROR| atl_ofi_helper.cpp:867 atl_ofi_get_prov_list: fi_getinfo error: ret -61, providers 0\r\n(RayWorkerVllm pid=15265) 2024:08:21-11:27:55:(15265) |CCL_ERROR| atl_ofi_helper.cpp:907 atl_ofi_get_prov_list: can't create providers for name shmTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/entrypoints/openai/api_server.py\", line 267, in <module>\r\n    engine = IPEXLLMAsyncLLMEngine.from_engine_args(engine_args,\r\n             ^^^\r\n(RayWorkerVllm pid=15265) 2024:08:21-11:27:55:(15265) |CCL_ERROR| atl_ofi_helper.cpp:1243 atl_ofi_open_nw_provs: atl_ofi_get_prov_list(ctx, prov_name, base_hints, &prov_list)\r\n(RayWorkerVllm pid=15265)  fails with status: 1^^^^^\r\n^(RayWorkerVllm pid=15265) 2024:08:21-11:27:55:(15265) |CCL_ERROR| atl_ofi_helper.cpp:1384 atl_ofi_open_nw_provs: can not open network providers\r\n(RayWorkerVllm pid=15265) 2024:08:21-11:27:55:(15265) |CCL_ERROR| atl_ofi.cpp:1036 open_providers: atl_ofi_open_nw_provs failed with status: 1\r\n(RayWorkerVllm pid=15265) 2024:08:21-11:27:55:(15265) |CCL_ERROR| atl_ofi.cpp:175 init: open_providers(prov_env, coord, attr, base_hints, open_nw_provs, fi_version, pmi, true )\r\n(RayWorkerVllm pid=15265)  fails with status: 1\r\n(RayWorkerVllm pid=15265) 2024:08:21-11:27:55:(15265) |CCL_ERROR| atl_ofi.cpp:243 init: can't find suitable provider\r\n^(RayWorkerVllm pid=15265) 2024:08:21-11:27:55:(15265) |CCL_ERROR| atl_ofi_comm.cpp:278 init_transport: condition transport->init(nullptr, nullptr, &attr, nullptr, pmi) == ATL_STATUS_SUCCESS failed\r\n(RayWorkerVllm pid=15265) failed to initialize ATL\r\n(RayWorkerVllm pid=15407) 2024-08-21 11:27:49,673 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations [repeated 2x across cluster]\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 57, in from_engine_args\r\n    engine = cls(parallel_config.worker_use_ray,\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 30, in __init__\r\n    super().__init__(*args, **kwargs)\r\n  File \"/llm/vllm/vllm/engine/async_llm_engine.py\", line 309, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/llm/vllm/vllm/engine/async_llm_engine.py\", line 409, in _init_engine\r\n    return engine_class(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/llm/vllm/vllm/engine/llm_engine.py\", line 106, in __init__\r\n    self.model_executor = executor_class(model_config, cache_config,\r\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/ipex_llm_gpu_executor.py\", line 77, in __init__\r\n    self._init_cache()\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/ipex_llm_gpu_executor.py\", line 249, in _init_cache\r\n    num_blocks = self._run_workers(\r\n                 ^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/ipex_llm_gpu_executor.py\", line 347, in _run_workers\r\n    driver_worker_output = getattr(self.driver_worker,\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/llm/vllm/vllm/worker/worker.py\", line 136, in profile_num_available_blocks\r\n    self.model_runner.profile_run()\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/llm/vllm/vllm/worker/model_runner.py\", line 645, in profile_run\r\n    self.execute_model(seqs, kv_caches)\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/llm/vllm/vllm/worker/model_runner.py\", line 569, in execute_model\r\n    lora_mapping) = self.prepare_input_tensors(seq_group_metadata_list)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/llm/vllm/vllm/worker/model_runner.py\", line 538, in prepare_input_tensors\r\n    broadcast_tensor_dict(metadata_dict, src=0)\r\n  File \"/llm/vllm/vllm/model_executor/parallel_utils/communication_op.py\", line 175, in broadcast_tensor_dict\r\n    torch.distributed.broadcast_object_list([metadata_list],\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 47, in wrapper\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 2603, in broadcast_object_list\r\n    broadcast(object_sizes_tensor, src=src, group=group)\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 47, in wrapper\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1906, in broadcast\r\n    work = default_pg.broadcast([tensor], opts)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: oneCCL: atl_ofi_comm.cpp:278 init_transport: EXCEPTION: failed to initialize ATL\r\n(RayWorkerVllm pid=15407) INFO 08-21 11:27:55 model_convert.py:249] Loading model weights took 8.3120 GB [repeated 2x across cluster]\r\n(RayWorkerVllm pid=15407) 2024:08:21-11:27:55:(15407) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL [repeated 2x across cluster]\r\n(RayWorkerVllm pid=15407) 2024:08:21-11:27:55:(15407) |CCL_WARN| fallback to 'sockets' mode of ze exchange mechanism, to use CCL_ZE_IPC_EXHANGE=drmfd, set CCL_LOCAL_RANK/SIZE explicitly  or use process launcher [repeated 2x across cluster]\r\n(RayWorkerVllm pid=15407) 2024:08:21-11:27:55:(15407) |CCL_ERROR| atl_ofi_helper.cpp:867 atl_ofi_get_prov_list: fi_getinfo error: ret -61, providers 0 [repeated 2x across cluster]\r\n(RayWorkerVllm pid=15407) 2024:08:21-11:27:55:(15407) |CCL_ERROR| atl_ofi_helper.cpp:907 atl_ofi_get_prov_list: can't create providers for name shm [repeated 2x across cluster]\r\n(RayWorkerVllm pid=15407) 2024:08:21-11:27:55:(15407) |CCL_ERROR| atl_ofi_helper.cpp:1243 atl_ofi_open_nw_provs: atl_ofi_get_prov_list(ctx, prov_name, base_hints, &prov_list) [repeated 2x across cluster]\r\n(RayWorkerVllm pid=15407)  fails with status: 1 [repeated 4x across cluster]\r\n(RayWorkerVllm pid=15407) 2024:08:21-11:27:55:(15407) |CCL_ERROR| atl_ofi_helper.cpp:1384 atl_ofi_open_nw_provs: can not open network providers [repeated 2x across cluster]\r\n(RayWorkerVllm pid=15407) 2024:08:21-11:27:55:(15407) |CCL_ERROR| atl_ofi.cpp:1036 open_providers: atl_ofi_open_nw_provs failed with status: 1 [repeated 2x across cluster]\r\n(RayWorkerVllm pid=15407) 2024:08:21-11:27:55:(15407) |CCL_ERROR| atl_ofi.cpp:175 init: open_providers(prov_env, coord, attr, base_hints, open_nw_provs, fi_version, pmi, true ) [repeated 2x across cluster]\r\n(RayWorkerVllm pid=15407) 2024:08:21-11:27:55:(15407) |CCL_ERROR| atl_ofi.cpp:243 init: can't find suitable provider [repeated 2x across cluster]\r\n(RayWorkerVllm pid=15407) 2024:08:21-11:27:55:(15407) |CCL_ERROR| atl_ofi_comm.cpp:278 init_transport: condition transport->init(nullptr, nullptr, &attr, nullptr, pmi) == ATL_STATUS_SUCCESS failed [repeated 2x across cluster]\r\n(RayWorkerVllm pid=15407) failed to initialize ATL [repeated 2x across cluster]\r\n",
      "state": "closed",
      "author": "jessie-zhao",
      "author_type": "User",
      "created_at": "2024-08-21T03:30:07Z",
      "updated_at": "2024-08-22T02:34:53Z",
      "closed_at": "2024-08-22T02:34:53Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11882/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11882",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11882",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:31.102505",
      "comments": [
        {
          "author": "gc-fu",
          "body": "Hi, this is probably caused by omitting the `--shm-size=\"16g\"` when starting the container.\r\n\r\nCheck here for more details: https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/DockerGuides/vllm_docker_quickstart.md#start-docker-container",
          "created_at": "2024-08-21T05:57:23Z"
        }
      ]
    },
    {
      "issue_number": 11870,
      "title": "May I ask how to pull the gptq format model that has been downloaded for the first time the second time?",
      "body": "I downloaded the model in GPTQ format for the first time (ChenMnZ/ Mistral-large-instruction-2407-efficientqat-W2G64-GPTQ) by following the \"save and load\" section of the homepage. How can I use the first pulled model after I shut down the computer and restart it?",
      "state": "closed",
      "author": "brownplayer",
      "author_type": "User",
      "created_at": "2024-08-20T10:15:12Z",
      "updated_at": "2024-08-22T02:06:46Z",
      "closed_at": "2024-08-22T02:06:46Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11870/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Oscilloscope98"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11870",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11870",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:31.350537",
      "comments": [
        {
          "author": "Oscilloscope98",
          "body": "Hi @brownplayer,\r\n\r\nBy default, Hugging Face model loaded the 1st time by repo id will be downloaded into `HF_HOME` folder, which is default to be `~/.cache/huggingface` on Linux and `C:\\Users\\your_user_name\\.cache\\huggingface` on Windows.\r\n\r\nThe next time when you load the model by the same repo id",
          "created_at": "2024-08-21T03:13:17Z"
        },
        {
          "author": "brownplayer",
          "body": "OK，thank you for your reply\n\n---- Replied Message ----\n| From | Yuwen ***@***.***> |\n| Date | 08/21/2024 11:13 |\n| To | intel-analytics/ipex-llm ***@***.***> |\n| Cc | brownplayer ***@***.***>,\nMention ***@***.***> |\n| Subject | Re: [intel-analytics/ipex-llm] May I ask how to pull the gptq format mod",
          "created_at": "2024-08-21T03:17:45Z"
        },
        {
          "author": "hkvision",
          "body": "Closing this issue. Feel free to tell us if you have further questions!",
          "created_at": "2024-08-22T02:06:46Z"
        }
      ]
    },
    {
      "issue_number": 11877,
      "title": "Qwen1.5-14 failed to start vllm serving ",
      "body": "HW platform: XeonW + 4Arc workstation\r\ndocker image： intelanalytics/ipex-llm-serving-xpu:2.1.0b\r\nServing start commands: \r\n\r\n# cat start_Qwen1.5-14B-Chat_serving.sh\r\n#!/bin/bash\r\nmodel=\"/llm/models/Qwen1.5-14B-Chat\"\r\nserved_model_name=\"Qwen1.5-14B-Chat\"\r\n\r\nexport USE_XETLA=OFF\r\nexport SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1\r\nexport SYCL_CACHE_PERSISTENT=1\r\nexport TORCH_LLM_ALLREDUCE=0\r\nexport CCL_DG2_ALLREDUCE=1\r\n\r\n# Tensor parallel related arguments:\r\nexport CCL_WORKER_COUNT=2\r\nexport FI_PROVIDER=shm\r\nexport CCL_ATL_TRANSPORT=ofi\r\nexport CCL_ZE_IPC_EXCHANGE=sockets\r\nexport CCL_ATL_SHM=1\r\n\r\nsource /opt/intel/oneapi/setvars.sh\r\nsource /opt/intel/1ccl-wks/setvars.sh\r\n\r\npython -m ipex_llm.vllm.xpu.entrypoints.openai.api_server \\\r\n  --served-model-name $served_model_name \\\r\n  --port 8000 \\\r\n  --model $model \\\r\n  --trust-remote-code \\\r\n  --gpu-memory-utilization 0.95 \\\r\n  --device xpu \\\r\n  --dtype float16 \\\r\n  --enforce-eager \\\r\n  --load-in-low-bit fp8 \\\r\n  --max-model-len 2048 \\\r\n  --max-num-batched-tokens 4000 \\\r\n  --tensor-parallel-size 2\r\n\r\nERROR message:\r\n2024-08-21 10:23:49,136 - INFO - intel_extension_for_pytorch auto imported\r\nINFO 08-21 10:23:50 api_server.py:258] vLLM API server version 0.3.3\r\nINFO 08-21 10:23:50 api_server.py:259] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name='Qwen1.5-14B-Chat', lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], load_in_low_bit='fp8', model='/llm/models/Qwen1.5-14B-Chat', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, download_dir=None, load_format='auto', dtype='float16', kv_cache_dtype='auto', max_model_len=2048, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=2, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, seed=0, swap_space=4, gpu_memory_utilization=0.95, max_num_batched_tokens=4000, max_num_seqs=256, max_paddings=256, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=True, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='xpu', engine_use_ray=False, disable_log_requests=False, max_log_len=None)\r\nWARNING 08-21 10:23:50 config.py:710] Casting torch.bfloat16 to torch.float16.\r\nINFO 08-21 10:23:50 config.py:523] Custom all-reduce kernels are temporarily disabled due to stability issues. We will re-enable them once the issues are resolved.\r\n2024-08-21 10:23:51,166 WARNING services.py:2017 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67067904 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\r\n2024-08-21 10:23:52,227 INFO worker.py:1781 -- Started a local Ray instance.\r\nINFO 08-21 10:23:53 llm_engine.py:68] Initializing an LLM engine (v0.3.3) with config: model='/llm/models/Qwen1.5-14B-Chat', tokenizer='/llm/models/Qwen1.5-14B-Chat', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=xpu, seed=0, max_num_batched_tokens=4000, max_num_seqs=256, max_model_len=2048)\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n(RayWorkerVllm pid=7746) /usr/local/lib/python3.11/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\r\n(RayWorkerVllm pid=7746)   warnings.warn(\r\n(RayWorkerVllm pid=7746) /usr/local/lib/python3.11/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n(RayWorkerVllm pid=7746)   warn(\r\n(RayWorkerVllm pid=7746) 2024-08-21 10:23:58,055 - INFO - intel_extension_for_pytorch auto imported\r\nINFO 08-21 10:23:58 attention.py:71] flash_attn is not found. Using xformers backend.\r\n(RayWorkerVllm pid=7746) INFO 08-21 10:23:58 attention.py:71] flash_attn is not found. Using xformers backend.\r\n2024-08-21 10:23:59,240 - INFO - Converting the current model to fp8_e5m2 format......\r\n2024-08-21 10:23:59,240 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\r\n[2024-08-21 10:23:59,547] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to xpu (auto detect)\r\n2024-08-21 10:24:02,683 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\r\nINFO 08-21 10:24:04 model_convert.py:249] Loading model weights took 7.4476 GB\r\n(RayWorkerVllm pid=7746) 2024-08-21 10:24:05,822 - INFO - Converting the current model to fp8_e5m2 format......\r\n(RayWorkerVllm pid=7746) 2024-08-21 10:24:05,822 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\r\n(RayWorkerVllm pid=7746) [2024-08-21 10:24:06,124] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to xpu (auto detect)\r\n(RayWorkerVllm pid=7746) 2024-08-21 10:24:21,767 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\r\n2024:08:21-10:24:25:( 4456) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\r\n2024:08:21-10:24:25:( 4456) |CCL_WARN| fallback to 'sockets' mode of ze exchange mechanism, to use CCL_ZE_IPC_EXHANGE=drmfd, set CCL_LOCAL_RANK/SIZE explicitly  or use process launcher\r\n(RayWorkerVllm pid=7746) INFO 08-21 10:24:25 model_convert.py:249] Loading model weights took 7.4476 GB\r\n(RayWorkerVllm pid=7746) 2024:08:21-10:24:26:( 7746) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\r\n(RayWorkerVllm pid=7746) 2024:08:21-10:24:26:( 7746) |CCL_WARN| fallback to 'sockets' mode of ze exchange mechanism, to use CCL_ZE_IPC_EXHANGE=drmfd, set CCL_LOCAL_RANK/SIZE explicitly  or use process launcher\r\n2024:08:21-10:24:26:( 4456) |CCL_ERROR| atl_ofi_helper.cpp:867 atl_ofi_get_prov_list: fi_getinfo error: ret -61, providers 0\r\n2024:08:21-10:24:26:( 4456) |CCL_ERROR| atl_ofi_helper.cpp:907 atl_ofi_get_prov_list: can't create providers for name shm\r\n2024:08:21-10:24:26:( 4456) |CCL_ERROR| atl_ofi_helper.cpp:1243 atl_ofi_open_nw_provs: atl_ofi_get_prov_list(ctx, prov_name, base_hints, &prov_list)\r\n fails with status: 1\r\n2024:08:21-10:24:26:( 4456) |CCL_ERROR| atl_ofi_helper.cpp:1384 atl_ofi_open_nw_provs: can not open network providers\r\n2024:08:21-10:24:26:( 4456) |CCL_ERROR| atl_ofi.cpp:1036 open_providers: atl_ofi_open_nw_provs failed with status: 1\r\n2024:08:21-10:24:26:( 4456) |CCL_ERROR| atl_ofi.cpp:175 init: open_providers(prov_env, coord, attr, base_hints, open_nw_provs, fi_version, pmi, true )\r\n fails with status: 1\r\n2024:08:21-10:24:26:( 4456) |CCL_ERROR| atl_ofi.cpp:243 init: can't find suitable provider\r\n2024:08:21-10:24:26:( 4456) |CCL_ERROR| atl_ofi_comm.cpp:278 init_transport: condition transport->init(nullptr, nullptr, &attr, nullptr, pmi) == ATL_STATUS_SUCCESS failed\r\nfailed to initialize ATL\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/entrypoints/openai/api_server.py\", line 267, in <module>\r\n    engine = IPEXLLMAsyncLLMEngine.from_engine_args(engine_args,\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 57, in from_engine_args\r\n    engine = cls(parallel_config.worker_use_ray,\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 30, in __init__\r\n    super().__init__(*args, **kwargs)\r\n  File \"/llm/vllm/vllm/engine/async_llm_engine.py\", line 309, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/llm/vllm/vllm/engine/async_llm_engine.py\", line 409, in _init_engine\r\n    return engine_class(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/llm/vllm/vllm/engine/llm_engine.py\", line 106, in __init__\r\n    self.model_executor = executor_class(model_config, cache_config,\r\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/ipex_llm_gpu_executor.py\", line 77, in __init__\r\n    self._init_cache()\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/ipex_llm_gpu_executor.py\", line 249, in _init_cache\r\n    num_blocks = self._run_workers(\r\n                 ^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/ipex_llm_gpu_executor.py\", line 347, in _run_workers\r\n    driver_worker_output = getattr(self.driver_worker,\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/llm/vllm/vllm/worker/worker.py\", line 136, in profile_num_available_blocks\r\n    self.model_runner.profile_run()\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/llm/vllm/vllm/worker/model_runner.py\", line 645, in profile_run\r\n    self.execute_model(seqs, kv_caches)\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/llm/vllm/vllm/worker/model_runner.py\", line 569, in execute_model\r\n    lora_mapping) = self.prepare_input_tensors(seq_group_metadata_list)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/llm/vllm/vllm/worker/model_runner.py\", line 538, in prepare_input_tensors\r\n    broadcast_tensor_dict(metadata_dict, src=0)\r\n  File \"/llm/vllm/vllm/model_executor/parallel_utils/communication_op.py\", line 175, in broadcast_tensor_dict\r\n    torch.distributed.broadcast_object_list([metadata_list],\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 47, in wrapper\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 2603, in broadcast_object_list\r\n    broadcast(object_sizes_tensor, src=src, group=group)\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 47, in wrapper\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1906, in broadcast\r\n    work = default_pg.broadcast([tensor], opts)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: oneCCL: atl_ofi_comm.cpp:278 init_transport: EXCEPTION: failed to initialize ATL\r\n(RayWorkerVllm pid=7746) 2024:08:21-10:24:26:( 7746) |CCL_ERROR| atl_ofi_helper.cpp:867 atl_ofi_get_prov_list: fi_getinfo error: ret -61, providers 0\r\n(RayWorkerVllm pid=7746) 2024:08:21-10:24:26:( 7746) |CCL_ERROR| atl_ofi_helper.cpp:907 atl_ofi_get_prov_list: can't create providers for name shm\r\n(RayWorkerVllm pid=7746) 2024:08:21-10:24:26:( 7746) |CCL_ERROR| atl_ofi_helper.cpp:1243 atl_ofi_open_nw_provs: atl_ofi_get_prov_list(ctx, prov_name, base_hints, &prov_list)\r\n(RayWorkerVllm pid=7746)  fails with status: 1\r\n(RayWorkerVllm pid=7746) 2024:08:21-10:24:26:( 7746) |CCL_ERROR| atl_ofi_helper.cpp:1384 atl_ofi_open_nw_provs: can not open network providers\r\n(RayWorkerVllm pid=7746) 2024:08:21-10:24:26:( 7746) |CCL_ERROR| atl_ofi.cpp:1036 open_providers: atl_ofi_open_nw_provs failed with status: 1\r\n(RayWorkerVllm pid=7746) 2024:08:21-10:24:26:( 7746) |CCL_ERROR| atl_ofi.cpp:175 init: open_providers(prov_env, coord, attr, base_hints, open_nw_provs, fi_version, pmi, true )\r\n(RayWorkerVllm pid=7746)  fails with status: 1\r\n(RayWorkerVllm pid=7746) 2024:08:21-10:24:26:( 7746) |CCL_ERROR| atl_ofi.cpp:243 init: can't find suitable provider\r\n(RayWorkerVllm pid=7746) 2024:08:21-10:24:26:( 7746) |CCL_ERROR| atl_ofi_comm.cpp:278 init_transport: condition transport->init(nullptr, nullptr, &attr, nullptr, pmi) == ATL_STATUS_SUCCESS failed\r\n(RayWorkerVllm pid=7746) failed to initialize ATL\r\n",
      "state": "closed",
      "author": "jessie-zhao",
      "author_type": "User",
      "created_at": "2024-08-21T02:27:22Z",
      "updated_at": "2024-08-21T09:37:23Z",
      "closed_at": "2024-08-21T09:37:23Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11877/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "gc-fu"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11877",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11877",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:31.583670",
      "comments": [
        {
          "author": "gc-fu",
          "body": "This is due to not specifying `--shm-size`, use this when starting docker should fix the problem.",
          "created_at": "2024-08-21T09:37:23Z"
        }
      ]
    },
    {
      "issue_number": 11857,
      "title": "An exception occurred while installing llama.cpp",
      "body": "Create llama. Unable to initialize llama after cpp folder. The cpp. bat\r\n\r\n",
      "state": "closed",
      "author": "brownplayer",
      "author_type": "User",
      "created_at": "2024-08-20T03:24:12Z",
      "updated_at": "2024-08-20T10:10:20Z",
      "closed_at": "2024-08-20T10:10:20Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11857/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "ch1y0q"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11857",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11857",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:31.800325",
      "comments": [
        {
          "author": "ch1y0q",
          "body": "Hi @brownplayer , could you please kindly provide some more information including your OS, commands for reproducing this issue, logs/screenshots of the error message, and anything that you believe might be helpful?\r\n\r\nThere are some common causes for the issue. For example, you have to be in an acti",
          "created_at": "2024-08-20T04:45:35Z"
        },
        {
          "author": "brownplayer",
          "body": "thank for you help\n\n---- Replied Message ----\n| From | ***@***.***> |\n| Date | 08/20/2024 12:45 |\n| To | intel-analytics/ipex-llm ***@***.***> |\n| Cc | brownplayer ***@***.***>,\nMention ***@***.***> |\n| Subject | Re: [intel-analytics/ipex-llm] An exception occurred while installing llama.cpp (Issue ",
          "created_at": "2024-08-20T06:49:56Z"
        },
        {
          "author": "rnwang04",
          "body": "@ch1y0q  Let's also add this issue into llama.cpp / ollama's trouble shooting section.",
          "created_at": "2024-08-20T07:44:56Z"
        },
        {
          "author": "brownplayer",
          "body": "OK，Thank you for your service😄\n\n---- Replied Message ----\n| From | Ruonan ***@***.***> |\n| Date | 08/20/2024 15:45 |\n| To | intel-analytics/ipex-llm ***@***.***> |\n| Cc | brownplayer ***@***.***>,\nMention ***@***.***> |\n| Subject | Re: [intel-analytics/ipex-llm] An exception occurred while installin",
          "created_at": "2024-08-20T07:48:28Z"
        }
      ]
    },
    {
      "issue_number": 11800,
      "title": "Inference produced repetitive and erroneous output by a fintuned qwen2 model ",
      "body": "device: Intel Arc A770 & MTL iGfx\r\nbigdl-core: 2.5.0b20240811\r\ntransformers: 4.37.0\r\nmodel: finetuned Qwen2-1.5B\r\n\r\nipex-llm generate the same error with CPU/GPU inference. The model runs ok with NV gfx\r\n \r\nHere is the code：\r\n`import torch\r\nimport time\r\nimport argparse\r\n\r\nfrom transformers import AutoTokenizer\r\nfrom ipex_llm import optimize_model\r\nimport numpy as np\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser(description='Qwen2-7B-Instruct')\r\n    parser.add_argument('--repo-id-or-model-path', type=str, default=\"Qwen/Qwen2-7B-Instruct\",\r\n                        help='The huggingface repo id for the Qwen2 model to be downloaded'\r\n                             ', or the path to the huggingface checkpoint folder')\r\n    parser.add_argument('--prompt', type=str, default=\"AI是什么？\",\r\n                        help='Prompt to infer') \r\n    parser.add_argument('--n-predict', type=int, default=2048,\r\n                        help='Max tokens to predict')\r\n\r\n    args = parser.parse_args()\r\n    # model_path = args.repo_id_or_model_path\r\n    model_path = 'D:/temp/Qwen1.5B/Qwen2-1.5B-bf16'\r\n    print(f'model_path: {model_path} ')\r\n\r\n    \r\n    from ipex_llm.transformers import AutoModelForCausalLM\r\n    # Load model in 4 bit,\r\n    # which convert the relevant layers in the model into INT4 format\r\n    model = AutoModelForCausalLM.from_pretrained(model_path,\r\n                                                 load_in_4bit=True,\r\n                                                 optimize_model=True,\r\n                                                 trust_remote_code=True,\r\n                                                 use_cache=True)\r\n    model = model.half()\r\n    model = model.to(\"xpu\")\r\n\r\n    # Load tokenizer\r\n    tokenizer = AutoTokenizer.from_pretrained(model_path,\r\n                                              trust_remote_code=True)\r\n    \r\n    # prompt = args.prompt\r\n    prompt = \"请帮我解释以下成语：大相径庭 南辕北辙 独树一帜 历久弥新 层出不穷 未雨绸缪 司空见惯 一蹴而就 日新月异 一成不变 缘木求鱼 相辅相成 有的放矢 根深蒂固 水到渠成 背道而驰?\"\r\n\r\n    # Generate predicted tokens\r\n    with torch.inference_mode():\r\n        # The following code for generation is adapted from https://huggingface.co/Qwen/Qwen2-7B-Instruct#quickstart\r\n        messages = [\r\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\r\n            {\"role\": \"user\", \"content\": prompt}\r\n            ]\r\n        text = tokenizer.apply_chat_template(\r\n            messages,\r\n            tokenize=False,\r\n            add_generation_prompt=True\r\n            )\r\n        model_inputs = tokenizer([text], return_tensors=\"pt\").to(\"xpu\")\r\n        # warmup\r\n        generated_ids = model.generate(\r\n            model_inputs.input_ids,\r\n            max_new_tokens=args.n_predict\r\n            )\r\n        \r\n        st = time.time()\r\n        generated_ids = model.generate(\r\n            model_inputs.input_ids,\r\n            max_new_tokens=args.n_predict\r\n            )\r\n        torch.xpu.synchronize()\r\n        end = time.time()\r\n        generated_ids = generated_ids.cpu()\r\n        generated_ids = [\r\n            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\r\n            ]\r\n\r\n        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\r\n        print(f'Inference time: {end-st} s')\r\n        print('-'*20, 'Prompt', '-'*20)\r\n        print(prompt)\r\n        print('-'*20, 'Output', '-'*20)\r\n        print(response)\r\n`\r\n\r\nand the output looks like:\r\n`\r\n(ipex-llm-qwen2) D:\\temp\\Qwen2-1.5B-bf16>python generate_cpu.py\r\nC:\\Users\\intel\\miniforge3\\envs\\ipex-llm-qwen2\\lib\\site-packages\\transformers\\deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\r\n  warnings.warn(\r\nC:\\Users\\intel\\miniforge3\\envs\\ipex-llm-qwen2\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n  warn(\r\n2024-08-12 18:16:14,645 - INFO - intel_extension_for_pytorch auto imported\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 19.20it/s]\r\n2024-08-12 18:16:15,773 - INFO - Converting the current model to sym_int4 format......\r\nC:\\Users\\intel\\miniforge3\\envs\\ipex-llm-qwen2\\lib\\site-packages\\torch\\nn\\init.py:412: UserWarning: Initializing zero-element tensors is a no-op\r\n  warnings.warn(\"Initializing zero-element tensors is a no-op\")\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\r\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\r\nInference time: 98.57605004310608 s\r\n-------------------- Prompt --------------------\r\n请帮我解释以下成语：大相径庭 南 辕北辙 独树一帜 历久弥新 层出不穷 未雨绸缪 司空见惯 一蹴而就 日新月异 一成不变 缘木求鱼 相辅相成 有的放矢 根深蒂固 水到 渠成 背道而驰?\r\n-------------------- Output --------------------\r\n- \"大相径庭\": 形容彼此观点、看法或事实存在巨大差异。\r\n\r\n- \"南辕北辙\": 比喻行动与目的相反，或比喻行动与目的完全相反。\r\n\r\n- \"独树一帜\": 形容在众多事物中独树一帜，与众不同。\r\n\r\n- \"历久弥新\": 指随着时间的推移，事物变得更加有价值和有意义。\r\n\r\n- \"层出不穷\": 比喻接连不断，没有穷尽。\r\n\r\n- \"未雨绸缪\": 比喻事情发生前采取措施，以避免问题。\r\n\r\n- \"司空见惯\": 比喻某事常见，习以为常，不再感到惊奇。\r\n\r\n- \"一蹴而就\": 比喻事情容易成功，迅速完成。\r\n\r\n- \"日新月异\": 比喻变化迅速，不断更新。\r\n\r\n- \"一成不变\": 比喻事物或情况没有变化，保持不变。\r\n\r\n- \"缘木求鱼\": 比喻做事情不切合实际，反而徒劳无功。\r\n\r\n- \"相辅相成\": 指两个事物相互配合，相互促进，缺一不可。\r\n\r\n- \"有的放矢\": 比喻做事有针对性，不偏离目标。\r\n\r\n- \"根深蒂固\": 比喻基础牢固，不容易动摇。\r\n\r\n- \"水到渠成\": 比喻事情完成后，自然形成，无需再做努力。\r\n\r\n- \"背道而驰\": 比喻行动的方向相反，无法达成一致。\r\n\r\n- \"相得益彰\": 指两件事物相互配合，相互促进，更加出色。\r\n\r\n- \"有的放矢\": 比喻做事有针对性，不偏离目标。\r\n\r\n- \"根深蒂固\": 比喻基础牢固，不容易动摇。\r\n\r\n- \"水到渠成\": 比喻事情完成后，自然形成，无需再做努力。\r\n\r\n- \"背道而驰\": 比喻行动的方向相反，无法达成一致。\r\n\r\n- \"相得益彰\": 比喻两件事物相互配合，相互促进，更加出色。\r\n\r\n- \"有的放矢\": 比喻做事有针对性，不偏离目标。\r\n\r\n- \"根深蒂固\": 比喻基础牢固，不容易动摇。\r\n\r\n- \"水到渠成\": 比喻事情完成后，自然形成，无需再做努力。\r\n\r\n- \"背道而驰\": 比喻行动的方向相反，无法达成一致。\r\n\r\n- \"相得益彰\": 比喻两件事物相互配合，相互促进，更加出色。\r\n\r\n- \"有的放矢\": 比喻做事有针对性，不偏离目标。\r\n\r\n- \"根深蒂固\": 比喻基础牢固，不容易动摇。\r\n\r\n- \"水到渠成\": 比喻事情完成后，自然形成，无需再做努力。\r\n\r\n- \"背道而驰\": 比喻行动的方向相反，无法达成一致。\r\n\r\n- \"相得益彰\": 比喻两件事物相互配合，相互促进，更加出色。\r\n\r\n- \"有的放矢\": 比喻做事有针对性，不偏离目标。\r\n\r\n- \"根深蒂固\": 比喻基础牢固，不容易动摇。\r\n\r\n- \"水到渠成\": 比喻事情完成后，自然形成，无需再做努力。\r\n\r\n- \"背道而驰\": 比喻行动的方向相反，无法达成一致。\r\n\r\n- \"相得益彰\": 比喻两件事物相互配合，相互促进，更加出色。\r\n\r\n- \"有的放矢\": 比喻做事有针对性，不偏离目标。\r\n\r\n- \"根深蒂固\": 比喻基础牢固，不容易动摇。\r\n\r\n- \"水到渠成\": 比喻事情完成后，自然形成，无需再做努力。\r\n\r\n- \"背道而驰\": 比喻行动的方向相反，无法达成一致。\r\n\r\n- \"相得益彰\": 比喻两件事物相互配合，相互促进，更加出色。\r\n\r\n- \"有的放矢\": 比喻做事有针对性，不偏离目标。\r\n\r\n- \"根深蒂固\": 比喻基础牢固，不容易动摇。\r\n\r\n- \"水到渠成\": 比喻事情完成后，自然形成，无需再做努力。\r\n\r\n- \"背道而驰\": 比喻行动的方向相反，无法达成一致。\r\n\r\n- \"相得益彰\": 比喻两件事物相互配合，相互促进，更加出色。\r\n\r\n- \"有的放矢\": 比喻做事有针对性，不偏离目标。\r\n\r\n- \"根深蒂固\": 比喻基础牢固，不容易动摇。\r\n\r\n- \"水到渠成\": 比喻事情完成后，自然形成，无需再做努力。\r\n\r\n- \"背道而驰\": 比喻行动的方向相反，无法达成一致。\r\n\r\n- \"相得益彰\": 比喻两件事物相互配合，相互促进，更加出色。\r\n\r\n- \"有的放矢\": 比喻做事有针对性，不偏离目标。\r\n\r\n- \"根深蒂固\": 比喻基础牢固，不容易动摇。\r\n\r\n- \"水到渠成\": 比喻事情完成后，自然形成，无需再做努力。\r\n\r\n- \"背道而驰\": 比喻行动的方向相反，无法达成一致。\r\n\r\n- \"相得益彰\": 比喻两件事物相互配合，相互促进，更加出色。\r\n\r\n- \"有的放矢\": 比喻做事有针对性，不偏离目标。\r\n\r\n- \"根深蒂固\": 比喻基础牢固，不容易动摇。\r\n\r\n- \"水到渠成\": 比喻事情完成后，自然形成，无需再做努力。\r\n\r\n- \"背道而驰\": 比喻行动的方向相反，无法达成一致。\r\n\r\n- \"相得益彰\": 比喻两件事物相互配合，相互促进，更加出色。\r\n\r\n- \"有的放矢\": 比喻做事有针对性，不偏离目标。\r\n\r\n- \"根深蒂固\": 比喻基础牢固，不容易动摇。\r\n\r\n- \"水到渠成\": 比喻事情完成后，自然形成，无需再做努力。\r\n\r\n- \"背道而驰\": 比喻行动的方向相反，无法达成一致。\r\n\r\n- \"相得益彰\": 比喻两件事物相互配合，相互促进，更加出色。\r\n\r\n- \"有的放矢\": 比喻做事有针对性，不偏离目标。\r\n\r\n- \"根深蒂固\": 比喻基础牢固，不容易动摇。\r\n\r\n- \"水到渠成\": 比喻事情完成后，自然形成，无需再做努力。\r\n\r\n- \"背道而驰\": 比喻行动的方向相反，无法达成一致。\r\n\r\n- \"相得益彰\": 比喻两件事物相互配合，相互促进，更加出色。\r\n\r\n- \"有的放矢\": 比喻做事有针对性，不偏离目标。\r\n\r\n- \"根深蒂固\": 比喻基础牢固，不容易动摇。\r\n\r\n- \"水到渠成\": 比喻事情完成后，自然形成，无需再做努力。\r\n\r\n- \"背道而驰\": 比喻行动的方向相反，无法达成一致。\r\n\r\n- \"相得益彰\": 比喻两件事物相互配合，相互促进，更加出色。\r\n\r\n- \"有的放矢\": 比喻做事有针对性，不偏离目标。\r\n\r\n- \"根深蒂固\": 比喻基础牢固，不容易动摇。\r\n\r\n- \"水到渠成\": 比喻事情完成后，自然形成，无需再做努力。\r\n\r\n- \"背道而驰\": 比喻行动的方向相反，无法达成一致。\r\n\r\n- \"相得益彰\": 比喻两件事物相互配合，相互促进，更加出色。\r\n\r\n- \"有的放矢\": 比喻做事有针对性，不偏离目标。\r\n\r\n- \"根深蒂固\": 比喻基础牢固，不容易动摇。\r\n\r\n- \"水到渠成\": 比喻事情完成后，自然形成，无需再做努力。\r\n\r\n- \"背道而驰\": 比喻行动的方向相反，无法达成一致。\r\n\r\n- \"相得益彰\": 比喻两件事物相互配合，相互促进，更加出色。\r\n\r\n- \"有的放矢\": 比喻做事有针对性，不偏离目标。\r\n\r\n- \"根深蒂固\": 比喻基础牢固，不容易动摇。\r\n\r\n- \"水到渠成\": 比喻事情完成后，自然形成，无需再做努力。\r\n\r\n- \"背道而驰\": 比喻行动的方向相反，无法达成一致。\r\n\r\n- \"相得益彰\": 比喻两件事物相互配合，相互促进，更加出色。\r\n\r\n- \"有的放矢\": 比喻做事有针对性，不偏离目标。\r\n\r\n- \"根深蒂固\": 比喻基础牢固，不容易动摇。\r\n\r\n- \"水到渠成\": 比喻事情完成后，自然形成，无需再做努力。\r\n\r\n- \"背道而驰\": 比喻行动的方向相反，无法达成一致。\r\n\r\n- \"相得益彰\": 比喻两件事物相互配合，相互促进，更加出色。\r\n\r\n- \"有的放矢\": 比喻做事有针对性，不偏离目标。\r\n\r\n- \"根深蒂固\": 比喻基础牢固，不容易动摇。\r\n\r\n- \"水到渠成\": 比喻事情完成后，自然形成，无需再做努力。\r\n\r\n- \"背道而驰\": 比喻行动的方向相反，无法达成一致。\r\n\r\n- \"相得益彰\": 比喻两件事物相互配合，相互促进，更加出色。\r\n\r\n- \"有的放矢\": 比喻做事有针对性，不偏离目标。\r\n\r\n- \"根深蒂固\": 比喻基础牢固，不容易动摇。\r\n\r\n- \"水到渠成\": 比喻事情完成后，自然形成，无需再做努力\r\n`",
      "state": "closed",
      "author": "jianjungu",
      "author_type": "User",
      "created_at": "2024-08-14T09:15:41Z",
      "updated_at": "2024-08-20T05:48:58Z",
      "closed_at": "2024-08-20T05:48:58Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11800/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "qiuxin2012"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11800",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11800",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:32.004155",
      "comments": [
        {
          "author": "hkvision",
          "body": "Synced offline, original model doesn't have this issue but finetuned model does.",
          "created_at": "2024-08-15T02:17:49Z"
        },
        {
          "author": "qiuxin2012",
          "body": "I got user's model, and this issue could be reproduced. But I found NV RTX4090's generations are repetitive too. Maybe it's samething wrong with the finetuned model.\r\nBelow it's the output of RTX4090:\r\n```\r\n-------------------- Prompt --------------------\r\n请帮我解释以下成语：大相径庭 南辕北辙 独树一帜 历久弥新 层出不穷 未雨绸缪 司空见",
          "created_at": "2024-08-16T02:46:00Z"
        },
        {
          "author": "qiuxin2012",
          "body": "It's a missmatch of the `eos_token_id` in model and tokenizer. In some files, `eos_token_id` is the same with `bos_token_id`, while in some files not. I make them the same. The output will be right.\r\nAnd I find sym_int4 and asym_int4 are not good for this model, even mixed_precision is ON. Sym int5 ",
          "created_at": "2024-08-20T05:45:39Z"
        },
        {
          "author": "jianjungu",
          "body": "Got it. After changed the eos_token_id manually, the issue cannot be reproduced. ",
          "created_at": "2024-08-20T05:48:58Z"
        }
      ]
    },
    {
      "issue_number": 11844,
      "title": "Does ipex llm support the LLaMA 3.1 model?",
      "body": "Does ipex llm support the LLaMA 3.1 model?\r\ni try to run LLaMA 3.1 failed.",
      "state": "closed",
      "author": "JerryXu2023",
      "author_type": "User",
      "created_at": "2024-08-19T07:03:08Z",
      "updated_at": "2024-08-20T03:14:05Z",
      "closed_at": "2024-08-20T03:14:05Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11844/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "rnwang04"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11844",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11844",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:32.186039",
      "comments": [
        {
          "author": "rnwang04",
          "body": "Hi @JerryXu2023 , do you want to run LLaMA 3.1 with ipex-llm[cpp] or ipex-llm[xpu] ?",
          "created_at": "2024-08-19T07:13:05Z"
        },
        {
          "author": "rnwang04",
          "body": "If you mean ipex-llm[cpp], Llama 3.1 is supported with `ipex-llm[cpp] >= 2.1.0b20240819`, you may try it again tomorrow : )",
          "created_at": "2024-08-19T11:14:33Z"
        },
        {
          "author": "JerryXu2023",
          "body": "Hi @rnwang04 \r\nI want to run llama3.1 with cpp.\r\ni will update to >= 2.1.0b20240819 and try again.\r\nThanks for you reply soon",
          "created_at": "2024-08-20T01:15:32Z"
        },
        {
          "author": "JerryXu2023",
          "body": "Hi @rnwang04 \r\nIt's work fine now.Thanks",
          "created_at": "2024-08-20T01:23:51Z"
        }
      ]
    },
    {
      "issue_number": 11833,
      "title": "Qwen1.5-14B-Chat 支持问题，返回空",
      "body": "启动 Qwen1.5-14B-Chat 模型后，请求 completions 进行测试时，出现返回空的情况。\r\n![image](https://github.com/user-attachments/assets/4ffc8eb7-754f-4aba-aaeb-6b852856bb77)\r\n![image](https://github.com/user-attachments/assets/a94573e9-0184-489d-999a-17c9c756b48f)\r\n",
      "state": "closed",
      "author": "shabooboo86",
      "author_type": "User",
      "created_at": "2024-08-16T08:13:48Z",
      "updated_at": "2024-08-20T02:11:37Z",
      "closed_at": "2024-08-20T02:11:36Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11833/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hzjane"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11833",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11833",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:32.418969",
      "comments": [
        {
          "author": "hzjane",
          "body": "![image](https://github.com/user-attachments/assets/a57f4ee0-a54d-427b-b5af-e26cfd489528)\r\nI can't reproduce this issue. I think it was caused by the streaming response. Maybe you can set the config `stream` = false to test like this:\r\n```bash\r\n  curl http://localhost:8000/v1/completions \\\r\n  -H \"Co",
          "created_at": "2024-08-16T08:52:15Z"
        }
      ]
    },
    {
      "issue_number": 11797,
      "title": "MiniCPM-V-2_6 load_low_bit mode.chat fails on MTL iGPU",
      "body": "MiniCPM-V-2_6 load_low_bit mode.chat fails on MTL iGPU windows\r\nlog：\r\n```\r\npython test_cpm.py\r\nC:\\Users\\test\\miniforge3\\lib\\site-packages\\transformers\\deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\r\n  warnings.warn(\r\nC:\\Users\\test\\miniforge3\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\test\\miniforge3\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n  warn(\r\n2024-08-14 16:14:47,978 - INFO - intel_extension_for_pytorch auto imported\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n2024-08-14 16:14:48,316 - INFO - vision_config is None, using default vision config\r\n2024-08-14 16:14:49,763 - INFO - Converting the current model to sym_int4 format......\r\nC:\\Users\\test\\miniforge3\\lib\\site-packages\\torch\\nn\\init.py:412: UserWarning: Initializing zero-element tensors is a no-op\r\n  warnings.warn(\"Initializing zero-element tensors is a no-op\")\r\nmodel.config.model_type == minicpmv\r\nmodel.vpm.config.model_type  siglip_vision_model\r\nLoading checkpoint shards:   0%|                                                                 | 0/2 [00:00<?, ?it/s]C:\\Users\\test\\miniforge3\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:10<00:00,  5.15s/it]\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\test\\Documents\\LLM\\test_cpm.py\", line 51, in <module>\r\n    res = model.chat(\r\n  File \"C:\\Users\\test\\.cache\\huggingface\\modules\\transformers_modules\\MiniCPM-V-2_6-int4\\modeling_minicpmv.py\", line 378, in chat\r\n    res = self.generate(\r\n  File \"C:\\Users\\test\\miniforge3\\lib\\site-packages\\ipex_llm\\transformers\\models\\minicpmv.py\", line 76, in generate\r\n    return origin_generate(\r\n  File \"C:\\Users\\test\\.cache\\huggingface\\modules\\transformers_modules\\MiniCPM-V-2_6-int4\\modeling_minicpmv.py\", line 257, in generate\r\n    ) = self.get_vllm_embedding(model_inputs)\r\n  File \"C:\\Users\\test\\.cache\\huggingface\\modules\\transformers_modules\\MiniCPM-V-2_6-int4\\modeling_minicpmv.py\", line 107, in get_vllm_embedding\r\n    vision_embedding = self.vpm(all_pixel_values.type(dtype), patch_attention_mask=patch_attn_mask, tgt_sizes=tgt_sizes).last_hidden_state\r\n  File \"C:\\Users\\test\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"C:\\Users\\test\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"C:\\Users\\test\\.cache\\huggingface\\modules\\transformers_modules\\MiniCPM-V-2_6-int4\\modeling_navit_siglip.py\", line 918, in forward\r\n    encoder_outputs = self.encoder(\r\n  File \"C:\\Users\\test\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"C:\\Users\\test\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"C:\\Users\\test\\.cache\\huggingface\\modules\\transformers_modules\\MiniCPM-V-2_6-int4\\modeling_navit_siglip.py\", line 826, in forward\r\n    layer_outputs = encoder_layer(\r\n  File \"C:\\Users\\test\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"C:\\Users\\test\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"C:\\Users\\test\\.cache\\huggingface\\modules\\transformers_modules\\MiniCPM-V-2_6-int4\\modeling_navit_siglip.py\", line 670, in forward\r\n    hidden_states, attn_weights = self.self_attn(\r\n  File \"C:\\Users\\test\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"C:\\Users\\test\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"C:\\Users\\test\\.cache\\huggingface\\modules\\transformers_modules\\MiniCPM-V-2_6-int4\\modeling_navit_siglip.py\", line 390, in forward\r\n    query_states = self.q_proj(hidden_states)\r\n  File \"C:\\Users\\test\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1695, in __getattr__\r\n    raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\r\nAttributeError: 'SiglipAttention' object has no attribute 'q_proj'. Did you mean: 'qkv_proj'?\r\n```\r\ncode：\r\n```\r\nimport time\r\nimport torch\r\nfrom PIL import Image\r\nfrom transformers import AutoModel, AutoTokenizer, AutoProcessor\r\nfrom ipex_llm.transformers import AutoModel\r\n\r\nmodel_path = r\"models/MiniCPM-V-2_6\"\r\nmodel_path = r\"models/MiniCPM-V-2_6-int4\"\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\r\n\r\nif 0:\r\n\r\n    model = AutoModel.from_pretrained(model_path, trust_remote_code=True,optimize_model=True,\r\n                                    load_in_low_bit=\"sym_int4\").eval()\r\n    \r\n\r\n    processor = AutoProcessor.from_pretrained(model_path,\r\n                                                load_in_4bit=True,\r\n                                                optimize_model=True,\r\n                                                trust_remote_code=True)\r\n\r\n    model_path_int4 = model_path+\"-int4\"\r\n\r\n    # save model, processor and tokenizer before you run load_low_bit\r\n    print(\"save model----------\")\r\n    model.save_low_bit(model_path_int4 )\r\n    processor.save_pretrained(model_path_int4 )\r\n    tokenizer.save_pretrained(model_path_int4 )\r\nelse:\r\n    model = AutoModel.load_low_bit(model_path,trust_remote_code=True, optimize_model=True).eval()\r\n\r\n\r\nmodel = model.half()\r\n\r\n\r\nmodel = model.to('xpu')\r\n\r\n# print(model)\r\n\r\nimage = Image.open('test_image/guo.png').convert('RGB')\r\nquestion = 'What is in the image?'\r\nquestion = '描述这幅图'\r\nmsgs = [{'role': 'user', 'content': [image, question]}]\r\n\r\n\r\nwith torch.inference_mode():\r\n    for i in range(1):\r\n        st = time.time()\r\n        res = model.chat(\r\n            image=None,\r\n            msgs=msgs,\r\n            tokenizer=tokenizer,\r\n            max_new_tokens=127,\r\n        )\r\n        et = time.time()\r\n        print(res)\r\n        print(et - st)\r\n```\r\n\r\nversion:\r\n```\r\ntransformers                4.40.0\r\nipex-llm                    2.1.0b20240813\r\nbigdl-core-xe-21            2.5.0b20240813\r\nbigdl-core-xe-addons-21     2.5.0b20240813\r\nbigdl-core-xe-batch-21      2.5.0b20240813\r\n```\r\n",
      "state": "closed",
      "author": "violet17",
      "author_type": "User",
      "created_at": "2024-08-14T08:20:17Z",
      "updated_at": "2024-08-19T08:54:28Z",
      "closed_at": "2024-08-19T08:54:27Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11797/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "leonardozcm"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11797",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11797",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:32.617163",
      "comments": [
        {
          "author": "leonardozcm",
          "body": "fixed in https://github.com/intel-analytics/ipex-llm/issues/11762",
          "created_at": "2024-08-19T08:54:27Z"
        }
      ]
    },
    {
      "issue_number": 11803,
      "title": "Running benchmark/all-in-one with GLM-4-9B-Chat model report \"AutoTP not support for models\"",
      "body": "Please help to confirm if the GLM-4-9B-Chat is supported , thanks so much. \r\n\r\nDocker images：intelanalytics/ipex-llm-serving-vllm-xpu-experiment   \r\nTag：2.1.0b2   \r\nImage ID：0e20af44ad46 \r\n\r\nstep:\r\n  cd /benchmark/all-in-one\r\n  edit config.yaml \r\n  bash run-deepspeed-arc.sh \r\n\r\nAttached the error trace details:\r\n![CHATGLM4-9B-Trace](https://github.com/user-attachments/assets/4a10f3a2-aabd-4386-b3a3-10f67825016b)\r\n\r\n\r\n\r\n",
      "state": "open",
      "author": "dukelee111",
      "author_type": "User",
      "created_at": "2024-08-15T00:58:49Z",
      "updated_at": "2024-08-19T08:04:06Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11803/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Uxito-Ada"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11803",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11803",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:32.800399",
      "comments": [
        {
          "author": "Uxito-Ada",
          "body": "Hi @dukelee111 ,\r\n\r\nI reproduced and got the same error. `\"Not able to determine model policy automatically` means that GLM-4-9B-Chat is not supported by AutoTP as shown [here](https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/module_inject/auto_tp.py#L281). It is not found in deepspeed's",
          "created_at": "2024-08-19T08:03:39Z"
        }
      ]
    },
    {
      "issue_number": 11762,
      "title": "MiniCPM-V-2.6 load_low_bit fails",
      "body": "error log:\r\n```\r\n2024-08-12 12:01:43,432 - INFO - vision_config is None, using default vision config\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\test\\Documents\\LLM\\cpm.py\", line 339, in <module>\r\n    res = model.chat(\r\n  File \"C:\\Users\\test\\.cache\\huggingface\\modules\\transformers_modules\\MiniCPM-V-2_6-int4\\modeling_minicpmv.py\", line 302, in chat\r\n    assert self.config.query_num == processor.image_processor.image_feature_size, \"These two values should be the same. Check `config.json` and `preprocessor_config.json`.\"\r\nAttributeError: 'MiniCPMVTokenizerFast' object has no attribute 'image_processor'\r\n```\r\n\r\nCan you help to fix this issue? Thanks.\r\n\r\nVersion:\r\nipex-llm 2.1.0b20240811",
      "state": "closed",
      "author": "violet17",
      "author_type": "User",
      "created_at": "2024-08-12T05:36:50Z",
      "updated_at": "2024-08-19T08:01:48Z",
      "closed_at": "2024-08-15T02:44:16Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11762/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "leonardozcm"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11762",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11762",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:32.994443",
      "comments": [
        {
          "author": "leonardozcm",
          "body": "> AttributeError: 'MiniCPMVTokenizerFast' object has no attribute 'image_processor'\r\n\r\nI believe this is because of you pass a tokenizer instance to where the arg expects a  `MiniCPMVImageProcessor `instance in `chat`.\r\nYou also need to call `save_pretrained `to both dump `MiniCPMVImageProcessor ` i",
          "created_at": "2024-08-13T06:56:12Z"
        },
        {
          "author": "violet17",
          "body": "Thanks.\r\nAnd how to load all modules in INT4 instead of using `modules_to_not_convert=[\"vpm\", \"resampler\"],`?",
          "created_at": "2024-08-14T03:32:52Z"
        },
        {
          "author": "leonardozcm",
          "body": "> Thanks. And how to load all modules in INT4 instead of using `modules_to_not_convert=[\"vpm\", \"resampler\"],`?\r\n\r\nsynced offline, now load_low_bit is working on all modules of minicpm-v",
          "created_at": "2024-08-15T02:43:23Z"
        },
        {
          "author": "leonardozcm",
          "body": "Close as completed.",
          "created_at": "2024-08-15T02:44:16Z"
        }
      ]
    },
    {
      "issue_number": 11840,
      "title": "codegeex4 使用lightweight serving搭建stream模式失败",
      "body": "codegeex4 使用lightweight serving搭建stream mode在MTL上不成功。",
      "state": "open",
      "author": "juan-OY",
      "author_type": "User",
      "created_at": "2024-08-19T02:46:36Z",
      "updated_at": "2024-08-19T03:02:44Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11840/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hzjane"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11840",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11840",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:33.212033",
      "comments": [
        {
          "author": "hzjane",
          "body": "This issue cannot be reproduced on Arc, only on MTL. This may be caused by different scheduling methods of different systems. And it was fixed by [this pr](https://github.com/intel-analytics/ipex-llm/pull/11822).",
          "created_at": "2024-08-19T02:50:07Z"
        }
      ]
    },
    {
      "issue_number": 11757,
      "title": "Lightweight-serving support for codegeex is broken",
      "body": "It reports below error:\r\n\r\nFile \"C:\\Users\\MTL\\AppData\\Local\\miniforge3\\envs\\codegeex\\Lib\\site-packages\\ipex_llm\\serving\\fastapi\\api_server.py\", line 337, in create_chat_completion\r\n    prompt, image_list = get_prompt(request.messages)\r\n    ^^^^^^^^^^^^^^^^^^\r\nValueError: too many values to unpack (expected 2)\r\nINFO:     127.0.0.1:59708 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\r\nERROR:    Exception in ASGI application\r\n",
      "state": "closed",
      "author": "juan-OY",
      "author_type": "User",
      "created_at": "2024-08-11T03:19:47Z",
      "updated_at": "2024-08-19T01:19:57Z",
      "closed_at": "2024-08-19T01:17:19Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11757/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "hzjane"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11757",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11757",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:33.420209",
      "comments": [
        {
          "author": "hzjane",
          "body": "Thanks for your issue. This issue will be fixed by this [PR](https://github.com/intel-analytics/ipex-llm/pull/11759/files).",
          "created_at": "2024-08-12T02:01:00Z"
        },
        {
          "author": "juan-OY",
          "body": "Yes, the issue is resolved",
          "created_at": "2024-08-19T01:19:56Z"
        }
      ]
    },
    {
      "issue_number": 11786,
      "title": "failure load the Qwen2-72B-Instruct with FP6 on 4 ARC GPU",
      "body": "\r\n\r\n(ipex-llm-0812) llm@GPU-Xeon4410Y-ARC770:~/ipex-llm-0812/python/llm/dev/benchmark/all-in-one$ bash run-deepspeed-arc.sh\r\n\r\n:: initializing oneAPI environment ...\r\n   run-deepspeed-arc.sh: BASH_VERSION = 5.1.16(1)-release\r\n   args: Using \"$@\" for oneapi-vars.sh arguments: --force\r\n:: advisor -- processing etc/advisor/vars.sh\r\n:: ccl -- processing etc/ccl/vars.sh\r\n:: compiler -- processing etc/compiler/vars.sh\r\n:: dal -- processing etc/dal/vars.sh\r\n:: debugger -- processing etc/debugger/vars.sh\r\n:: dpct -- processing etc/dpct/vars.sh\r\n:: dpl -- processing etc/dpl/vars.sh\r\n:: ipp -- processing etc/ipp/vars.sh\r\n:: ippcp -- processing etc/ippcp/vars.sh\r\n:: mkl -- processing etc/mkl/vars.sh\r\n:: mpi -- processing etc/mpi/vars.sh\r\n:: tbb -- processing etc/tbb/vars.sh\r\n:: vtune -- processing etc/vtune/vars.sh\r\n:: oneAPI environment initialized ::\r\n\r\n[0] /home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\r\n[0]   warnings.warn(\r\n[2] /home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\r\n[2]   warnings.warn(\r\n[1] /home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\r\n[1]   warnings.warn(\r\n[3] /home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\r\n[3]   warnings.warn(\r\n[0] /home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n[0]   warn(\r\n[2] /home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n[2]   warn(\r\n[1] /home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n[1]   warn(\r\n[3] /home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n[3]   warn(\r\n[0] [2024-08-14 08:46:09,295] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to xpu (auto detect)\r\n[2] [2024-08-14 08:46:09,296] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to xpu (auto detect)\r\n[1] [2024-08-14 08:46:09,346] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to xpu (auto detect)\r\n[0] model_path: /home/llm/local_models/Qwen/Qwen2-72B-Instruct\r\n[0] [2024-08-14 08:46:09,451] [INFO] [real_accelerator.py:211:set_accelerator] Setting ds_accelerator to cpu (model specified)\r\n[2] model_path: /home/llm/local_models/Qwen/Qwen2-72B-Instruct\r\n[2] [2024-08-14 08:46:09,452] [INFO] [real_accelerator.py:211:set_accelerator] Setting ds_accelerator to cpu (model specified)\r\n[1] model_path: /home/llm/local_models/Qwen/Qwen2-72B-Instruct\r\n[1] [2024-08-14 08:46:09,509] [INFO] [real_accelerator.py:211:set_accelerator] Setting ds_accelerator to cpu (model specified)\r\n[3] [2024-08-14 08:46:09,575] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to xpu (auto detect)\r\n[3] model_path: /home/llm/local_models/Qwen/Qwen2-72B-Instruct\r\n[3] [2024-08-14 08:46:09,735] [INFO] [real_accelerator.py:211:set_accelerator] Setting ds_accelerator to cpu (model specified)\r\nLoading checkpoint shards: 100%|██████████| 37/37 [05:21<00:00,  8.69s/it][0]\r\nLoading checkpoint shards: 100%|██████████| 37/37 [05:21<00:00,  8.69s/it]\r\nLoading checkpoint shards: 100%|██████████| 37/37 [05:21<00:00,  8.69s/it][3]\r\nLoading checkpoint shards: 100%|██████████| 37/37 [05:21<00:00,  8.70s/it]\r\n[3] Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n[3] [2024-08-14 08:51:33,893] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.1+ed8aed57, git-hash=ed8aed57, git-branch=HEAD\r\n[3] [2024-08-14 08:51:33,894] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\r\n[3] [2024-08-14 08:51:33,894] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\r\n[3] [2024-08-14 08:51:33,894] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\r\n[0] [2024-08-14 08:51:33,899] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.1+ed8aed57, git-hash=ed8aed57, git-branch=HEAD\r\n[0] Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n[0] [2024-08-14 08:51:33,900] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\r\n[0] [2024-08-14 08:51:33,900] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\r\n[0] [2024-08-14 08:51:33,900] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\r\n[1] Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n[1] [2024-08-14 08:51:33,901] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.1+ed8aed57, git-hash=ed8aed57, git-branch=HEAD\r\n[1] [2024-08-14 08:51:33,901] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\r\n[1] [2024-08-14 08:51:33,902] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\r\n[1] [2024-08-14 08:51:33,902] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\r\n[2] Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n[2] [2024-08-14 08:51:33,916] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.1+ed8aed57, git-hash=ed8aed57, git-branch=HEAD\r\n[2] [2024-08-14 08:51:33,917] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\r\n[2] [2024-08-14 08:51:33,917] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\r\n[2] [2024-08-14 08:51:33,917] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\r\n[3] Using /home/llm/.cache/torch_extensions/py311_cpu as PyTorch extensions root...\r\n[3] Emitting ninja build file /home/llm/.cache/torch_extensions/py311_cpu/deepspeed_ccl_comm/build.ninja...\r\n[3] Building extension module deepspeed_ccl_comm...\r\n[3] Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\r\n[3] ninja: no work to do.\r\n[3] Loading extension module deepspeed_ccl_comm...\r\n[2] Using /home/llm/.cache/torch_extensions/py311_cpu as PyTorch extensions root...\r\n[2] Emitting ninja build file /home/llm/.cache/torch_extensions/py311_cpu/deepspeed_ccl_comm/build.ninja...\r\n[2] Building extension module deepspeed_ccl_comm...\r\n[2] Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\r\n[0] Using /home/llm/.cache/torch_extensions/py311_cpu as PyTorch extensions root...\r\n[2] ninja: no work to do.\r\n[2] Loading extension module deepspeed_ccl_comm...\r\n[1] Using /home/llm/.cache/torch_extensions/py311_cpu as PyTorch extensions root...\r\n[1] Emitting ninja build file /home/llm/.cache/torch_extensions/py311_cpu/deepspeed_ccl_comm/build.ninja...\r\n[1] Building extension module deepspeed_ccl_comm...\r\n[1] Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\r\n[1] ninja: no work to do.\r\n[1] Loading extension module deepspeed_ccl_comm...\r\n[0] Loading extension module deepspeed_ccl_comm...\r\n[0] Time to load deepspeed_ccl_comm op: 0.20324254035949707 seconds\r\n[0] DeepSpeed deepspeed.ops.comm.deepspeed_ccl_comm_op built successfully\r\n[0] [2024-08-14 08:53:14,600] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend\r\n[1] Time to load deepspeed_ccl_comm op: 0.08019113540649414 seconds\r\n[1] DeepSpeed deepspeed.ops.comm.deepspeed_ccl_comm_op built successfully\r\n[1] [2024-08-14 08:53:14,600] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend\r\n[2] Time to load deepspeed_ccl_comm op: 0.07995343208312988 seconds\r\n[2] DeepSpeed deepspeed.ops.comm.deepspeed_ccl_comm_op built successfully\r\n[2] [2024-08-14 08:53:14,600] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend\r\n[3] Time to load deepspeed_ccl_comm op: 0.0811920166015625 seconds\r\n[3] DeepSpeed deepspeed.ops.comm.deepspeed_ccl_comm_op built successfully\r\n[3] [2024-08-14 08:53:14,600] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend\r\n[0] [2024-08-14 08:53:14,600] [INFO] [comm.py:637:init_distributed] cdb=<deepspeed.comm.ccl.CCLBackend object at 0x7691cbeba210>\r\n[1] [2024-08-14 08:53:14,600] [INFO] [comm.py:637:init_distributed] cdb=<deepspeed.comm.ccl.CCLBackend object at 0x768e6fb41ed0>\r\n[0] [2024-08-14 08:53:14,601] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\r\n[1] [2024-08-14 08:53:14,601] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\r\n[2] [2024-08-14 08:53:14,600] [INFO] [comm.py:637:init_distributed] cdb=<deepspeed.comm.ccl.CCLBackend object at 0x79d917879690>\r\n[2] [2024-08-14 08:53:14,601] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\r\n[3] [2024-08-14 08:53:14,600] [INFO] [comm.py:637:init_distributed] cdb=<deepspeed.comm.ccl.CCLBackend object at 0x76966c33c790>\r\n[3] [2024-08-14 08:53:14,601] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\r\n[0] My guessed rank = 0\r\n[1] My guessed rank = 1\r\n[3] My guessed rank = 3\r\n[2] My guessed rank = 2\r\n[0] [2024-08-14 08:53:14,995] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=4, master_addr=10.240.108.91, master_port=29500\r\n[0] [2024-08-14 08:53:14,995] [INFO] [comm.py:662:init_distributed] Distributed backend already initialized\r\n[2] [2024-08-14 08:53:14,995] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=2, local_rank=2, world_size=4, master_addr=10.240.108.91, master_port=29500\r\n[3] [2024-08-14 08:53:14,995] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=3, local_rank=3, world_size=4, master_addr=10.240.108.91, master_port=29500\r\n[1] [2024-08-14 08:53:14,996] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=1, local_rank=1, world_size=4, master_addr=10.240.108.91, master_port=29500\r\n[0] 2024-08-14 08:53:37,101 - INFO - Converting the current model to fp6 format......\r\n[3] 2024-08-14 08:53:37,101 - INFO - Converting the current model to fp6 format......\r\n[1] 2024-08-14 08:53:37,102 - INFO - Converting the current model to fp6 format......\r\n[3] /home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/torch/nn/init.py:412: UserWarning: Initializing zero-element tensors is a no-op\r\n[3]   warnings.warn(\"Initializing zero-element tensors is a no-op\")\r\n[0] /home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/torch/nn/init.py:412: UserWarning: Initializing zero-element tensors is a no-op\r\n[0]   warnings.warn(\"Initializing zero-element tensors is a no-op\")\r\n[1] /home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/torch/nn/init.py:412: UserWarning: Initializing zero-element tensors is a no-op\r\n[1]   warnings.warn(\"Initializing zero-element tensors is a no-op\")\r\n[2] 2024-08-14 08:53:37,139 - INFO - Converting the current model to fp6 format......\r\n[2] /home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/torch/nn/init.py:412: UserWarning: Initializing zero-element tensors is a no-op\r\n[2]   warnings.warn(\"Initializing zero-element tensors is a no-op\")\r\n[3] AutoTP:  [(<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>, ['mlp.down_proj', 'self_attn.o_proj'])]\r\n[3] >> loading of model costs 447.36217987899727s\r\n[3] [2024-08-14 08:55:08,948] [INFO] [real_accelerator.py:211:set_accelerator] Setting ds_accelerator to xpu (model specified)\r\n[1] AutoTP:  [(<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>, ['self_attn.o_proj', 'mlp.down_proj'])]\r\n[1] >> loading of model costs 447.5882513519973s\r\n[1] [2024-08-14 08:55:08,980] [INFO] [real_accelerator.py:211:set_accelerator] Setting ds_accelerator to xpu (model specified)\r\n[2] AutoTP:  [(<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>, ['mlp.down_proj', 'self_attn.o_proj'])]\r\n[2] >> loading of model costs 447.6830987180001s\r\n[2] [2024-08-14 08:55:09,438] [INFO] [real_accelerator.py:211:set_accelerator] Setting ds_accelerator to xpu (model specified)\r\n[0] AutoTP:  [(<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>, ['mlp.down_proj', 'self_attn.o_proj'])]\r\n[0] >> loading of model costs 447.6456385010024s\r\n[0] [2024-08-14 08:55:10,077] [INFO] [real_accelerator.py:211:set_accelerator] Setting ds_accelerator to xpu (model specified)\r\n[3] Traceback (most recent call last):\r\n[3]   File \"/home/llm/ipex-llm-0812/python/llm/dev/benchmark/all-in-one/run.py\", line 2002, in <module>\r\n[3]     run_model(model, api, in_out_pairs, conf['local_model_hub'], conf['warm_up'], conf['num_trials'], conf['num_beams'],\r\n[3]   File \"/home/llm/ipex-llm-0812/python/llm/dev/benchmark/all-in-one/run.py\", line 170, in run_model\r\n[3]     result = run_deepspeed_optimize_model_gpu(repo_id, local_model_hub, in_out_pairs, warm_up, num_trials, num_beams, low_bit, batch_size, cpu_embedding)\r\n[3]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[3]   File \"/home/llm/ipex-llm-0812/python/llm/dev/benchmark/all-in-one/run.py\", line 1689, in run_deepspeed_optimize_model_gpu\r\n[3]     model = model.to(f'xpu:{local_rank}')\r\n[3]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[3]   File \"/home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 2597, in to\r\n[3]     return super().to(*args, **kwargs)\r\n[3]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[3]   File \"/home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1160, in to\r\n[3]     return self._apply(convert)\r\n[3]            ^^^^^^^^^^^^^^^^^^^^\r\n[3]   File \"/home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 810, in _apply\r\n[3]     module._apply(fn)\r\n[3]   File \"/home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 833, in _apply\r\n[3]     param_applied = fn(param)\r\n[3]                     ^^^^^^^^^\r\n[3]   File \"/home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1158, in convert\r\n[3]     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\r\n[3]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[3]   File \"/home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/ipex_llm/transformers/low_bit_linear.py\", line 492, in to\r\n[3]     new_param = FP4Params(super().to(device=device,\r\n[3]                           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[3] RuntimeError: Native API failed. Native API returns: -5 (PI_ERROR_OUT_OF_RESOURCES) -5 (PI_ERROR_OUT_OF_RESOURCES)\r\n[2] Traceback (most recent call last):\r\n[2]   File \"/home/llm/ipex-llm-0812/python/llm/dev/benchmark/all-in-one/run.py\", line 2002, in <module>\r\n[2]     run_model(model, api, in_out_pairs, conf['local_model_hub'], conf['warm_up'], conf['num_trials'], conf['num_beams'],\r\n[2]   File \"/home/llm/ipex-llm-0812/python/llm/dev/benchmark/all-in-one/run.py\", line 170, in run_model\r\n[2]     result = run_deepspeed_optimize_model_gpu(repo_id, local_model_hub, in_out_pairs, warm_up, num_trials, num_beams, low_bit, batch_size, cpu_embedding)\r\n[2]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2]   File \"/home/llm/ipex-llm-0812/python/llm/dev/benchmark/all-in-one/run.py\", line 1689, in run_deepspeed_optimize_model_gpu\r\n[2]     model = model.to(f'xpu:{local_rank}')\r\n[2]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2]   File \"/home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 2597, in to\r\n[2]     return super().to(*args, **kwargs)\r\n[2]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2]   File \"/home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1160, in to\r\n[2]     return self._apply(convert)\r\n[2]            ^^^^^^^^^^^^^^^^^^^^\r\n[2]   File \"/home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 810, in _apply\r\n[2]     module._apply(fn)\r\n[2]   File \"/home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 833, in _apply\r\n[2]     param_applied = fn(param)\r\n[2]                     ^^^^^^^^^\r\n[2]   File \"/home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1158, in convert\r\n[2]     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\r\n[2]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2]   File \"/home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/ipex_llm/transformers/low_bit_linear.py\", line 492, in to\r\n[2]     new_param = FP4Params(super().to(device=device,\r\n[2]                           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2] RuntimeError: Native API failed. Native API returns: -5 (PI_ERROR_OUT_OF_RESOURCES) -5 (PI_ERROR_OUT_OF_RESOURCES)\r\n[1] Traceback (most recent call last):\r\n[1]   File \"/home/llm/ipex-llm-0812/python/llm/dev/benchmark/all-in-one/run.py\", line 2002, in <module>\r\n[1]     run_model(model, api, in_out_pairs, conf['local_model_hub'], conf['warm_up'], conf['num_trials'], conf['num_beams'],\r\n[1]   File \"/home/llm/ipex-llm-0812/python/llm/dev/benchmark/all-in-one/run.py\", line 170, in run_model\r\n[1]     result = run_deepspeed_optimize_model_gpu(repo_id, local_model_hub, in_out_pairs, warm_up, num_trials, num_beams, low_bit, batch_size, cpu_embedding)\r\n[1]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[1]   File \"/home/llm/ipex-llm-0812/python/llm/dev/benchmark/all-in-one/run.py\", line 1689, in run_deepspeed_optimize_model_gpu\r\n[1]     model = model.to(f'xpu:{local_rank}')\r\n[1]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[1]   File \"/home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 2597, in to\r\n[1]     return super().to(*args, **kwargs)\r\n[1]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[1]   File \"/home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1160, in to\r\n[1]     return self._apply(convert)\r\n[1]            ^^^^^^^^^^^^^^^^^^^^\r\n[1]   File \"/home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 810, in _apply\r\n[1]     module._apply(fn)\r\n[1]   File \"/home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 833, in _apply\r\n[1]     param_applied = fn(param)\r\n[1]                     ^^^^^^^^^\r\n[1]   File \"/home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1158, in convert\r\n[1]     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\r\n[1]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[1]   File \"/home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/ipex_llm/transformers/low_bit_linear.py\", line 492, in to\r\n[1]     new_param = FP4Params(super().to(device=device,\r\n[1]                           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[1] RuntimeError: Native API failed. Native API returns: -5 (PI_ERROR_OUT_OF_RESOURCES) -5 (PI_ERROR_OUT_OF_RESOURCES)\r\n[0] Traceback (most recent call last):\r\n[0]   File \"/home/llm/ipex-llm-0812/python/llm/dev/benchmark/all-in-one/run.py\", line 2002, in <module>\r\n[0]     run_model(model, api, in_out_pairs, conf['local_model_hub'], conf['warm_up'], conf['num_trials'], conf['num_beams'],\r\n[0]   File \"/home/llm/ipex-llm-0812/python/llm/dev/benchmark/all-in-one/run.py\", line 170, in run_model\r\n[0]     result = run_deepspeed_optimize_model_gpu(repo_id, local_model_hub, in_out_pairs, warm_up, num_trials, num_beams, low_bit, batch_size, cpu_embedding)\r\n[0]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[0]   File \"/home/llm/ipex-llm-0812/python/llm/dev/benchmark/all-in-one/run.py\", line 1689, in run_deepspeed_optimize_model_gpu\r\n[0]     model = model.to(f'xpu:{local_rank}')\r\n[0]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[0]   File \"/home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 2597, in to\r\n[0]     return super().to(*args, **kwargs)\r\n[0]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[0]   File \"/home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1160, in to\r\n[0]     return self._apply(convert)\r\n[0]            ^^^^^^^^^^^^^^^^^^^^\r\n[0]   File \"/home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 810, in _apply\r\n[0]     module._apply(fn)\r\n[0]   File \"/home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 833, in _apply\r\n[0]     param_applied = fn(param)\r\n[0]                     ^^^^^^^^^\r\n[0]   File \"/home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1158, in convert\r\n[0]     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\r\n[0]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[0]   File \"/home/llm/venv/ipex-llm-0812/lib/python3.11/site-packages/ipex_llm/transformers/low_bit_linear.py\", line 492, in to\r\n[0]     new_param = FP4Params(super().to(device=device,\r\n[0]                           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[0] RuntimeError: Native API failed. Native API returns: -5 (PI_ERROR_OUT_OF_RESOURCES) -5 (PI_ERROR_OUT_OF_RESOURCES)\r\n\r\n\r\n\r\n\r\n(ipex-llm-0812) llm@GPU-Xeon4410Y-ARC770:~/ipex-llm-0812/python/llm/scripts$ bash env-check.sh\r\n-----------------------------------------------------------------\r\nPYTHON_VERSION=3.11.9\r\n-----------------------------------------------------------------\r\nTransformers is not installed.\r\n-----------------------------------------------------------------\r\nPyTorch is not installed.\r\n-----------------------------------------------------------------\r\nipex-llm Version: 2.1.0b20240811\r\n-----------------------------------------------------------------\r\nIPEX is not installed.\r\n-----------------------------------------------------------------\r\nCPU Information:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        52 bits physical, 57 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               48\r\nOn-line CPU(s) list:                  0-47\r\nVendor ID:                            GenuineIntel\r\nModel name:                           Intel(R) Xeon(R) Silver 4410Y\r\nCPU family:                           6\r\nModel:                                143\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   12\r\nSocket(s):                            2\r\nStepping:                             8\r\nCPU max MHz:                          3900.0000\r\nCPU min MHz:                          800.0000\r\nBogoMIPS:                             4000.00\r\n-----------------------------------------------------------------\r\nTotal CPU Memory: 755.547 GB\r\n-----------------------------------------------------------------\r\nOperating System:\r\nUbuntu 22.04.4 LTS \\n \\l\r\n\r\n-----------------------------------------------------------------\r\nLinux GPU-Xeon4410Y-ARC770 6.8.0-39-generic #39~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Jul 10 15:35:09 UTC 2 x86_64 x86_64 x86_64 GNU/Linux\r\n-----------------------------------------------------------------\r\nCLI:\r\n    Version: 1.2.27.20240626\r\n    Build ID: 7f002d24\r\n\r\nService:\r\n    Version: 1.2.27.20240626\r\n    Build ID: 7f002d24\r\n    Level Zero Version: 1.16.0\r\n-----------------------------------------------------------------\r\n  Driver UUID                                     32342e31-332e-3239-3133-382e37000000\r\n  Driver Version                                  24.13.29138.7\r\n  Driver UUID                                     32342e31-332e-3239-3133-382e37000000\r\n  Driver Version                                  24.13.29138.7\r\n  Driver UUID                                     32342e31-332e-3239-3133-382e37000000\r\n  Driver Version                                  24.13.29138.7\r\n  Driver UUID                                     32342e31-332e-3239-3133-382e37000000\r\n  Driver Version                                  24.13.29138.7\r\n  Driver UUID                                     32342e31-332e-3239-3133-382e37000000\r\n  Driver Version                                  24.13.29138.7\r\n  Driver UUID                                     32342e31-332e-3239-3133-382e37000000\r\n  Driver Version                                  24.13.29138.7\r\n  Driver UUID                                     32342e31-332e-3239-3133-382e37000000\r\n  Driver Version                                  24.13.29138.7\r\n  Driver UUID                                     32342e31-332e-3239-3133-382e37000000\r\n  Driver Version                                  24.13.29138.7\r\n-----------------------------------------------------------------\r\nDriver related package version:\r\nii  intel-fw-gpu                                   2024.17.5-329~22.04                     all          Firmware package for Intel integrated and discrete GPUs\r\nii  intel-i915-dkms                                1.24.3.23.240419.26+i30-1               all          Out of tree i915 driver.\r\nii  intel-level-zero-gpu                           1.3.29138.7                             amd64        Intel(R) Graphics Compute Runtime for oneAPI Level Zero.\r\nii  level-zero-dev                                 1.16.15-881~22.04                       amd64        Intel(R) Graphics Compute Runtime for oneAPI Level Zero.\r\n-----------------------------------------------------------------\r\nenv-check.sh: line 167: sycl-ls: command not found\r\nigpu not detected\r\n-----------------------------------------------------------------\r\nxpu-smi is properly installed.\r\n-----------------------------------------------------------------\r\n+-----------+--------------------------------------------------------------------------------------+\r\n| Device ID | Device Information                                                                   |\r\n+-----------+--------------------------------------------------------------------------------------+\r\n| 0         | Device Name: Intel(R) Arc(TM) A770 Graphics                                          |\r\n|           | Vendor Name: Intel(R) Corporation                                                    |\r\n|           | SOC UUID: 00000000-0000-0019-0000-000856a08086                                       |\r\n|           | PCI BDF Address: 0000:19:00.0                                                        |\r\n|           | DRM Device: /dev/dri/card1                                                           |\r\n|           | Function Type: physical                                                              |\r\n+-----------+--------------------------------------------------------------------------------------+\r\n| 1         | Device Name: Intel(R) Arc(TM) A770 Graphics                                          |\r\n|           | Vendor Name: Intel(R) Corporation                                                    |\r\n|           | SOC UUID: 00000000-0000-002c-0000-000856a08086                                       |\r\n|           | PCI BDF Address: 0000:2c:00.0                                                        |\r\n|           | DRM Device: /dev/dri/card2                                                           |\r\n|           | Function Type: physical                                                              |\r\n+-----------+--------------------------------------------------------------------------------------+\r\n| 2         | Device Name: Intel(R) Arc(TM) A770 Graphics                                          |\r\n|           | Vendor Name: Intel(R) Corporation                                                    |\r\n|           | SOC UUID: 00000000-0000-0052-0000-000856a08086                                       |\r\n|           | PCI BDF Address: 0000:52:00.0                                                        |\r\n|           | DRM Device: /dev/dri/card3                                                           |\r\n|           | Function Type: physical                                                              |\r\n+-----------+--------------------------------------------------------------------------------------+\r\n| 3         | Device Name: Intel(R) Arc(TM) A770 Graphics                                          |\r\n|           | Vendor Name: Intel(R) Corporation                                                    |\r\n|           | SOC UUID: 00000000-0000-0065-0000-000856a08086                                       |\r\n|           | PCI BDF Address: 0000:65:00.0                                                        |\r\n|           | DRM Device: /dev/dri/card4                                                           |\r\n|           | Function Type: physical                                                              |\r\n+-----------+--------------------------------------------------------------------------------------+\r\n| 4         | Device Name: Intel(R) Arc(TM) A770 Graphics                                          |\r\n|           | Vendor Name: Intel(R) Corporation                                                    |\r\n|           | SOC UUID: 00000000-0000-009b-0000-000856a08086                                       |\r\n|           | PCI BDF Address: 0000:9b:00.0                                                        |\r\n|           | DRM Device: /dev/dri/card5                                                           |\r\n|           | Function Type: physical                                                              |\r\n+-----------+--------------------------------------------------------------------------------------+\r\n| 5         | Device Name: Intel(R) Arc(TM) A770 Graphics                                          |\r\n|           | Vendor Name: Intel(R) Corporation                                                    |\r\n|           | SOC UUID: 00000000-0000-00ad-0000-000856a08086                                       |\r\n|           | PCI BDF Address: 0000:ad:00.0                                                        |\r\n|           | DRM Device: /dev/dri/card6                                                           |\r\n|           | Function Type: physical                                                              |\r\n+-----------+--------------------------------------------------------------------------------------+\r\n| 6         | Device Name: Intel(R) Arc(TM) A770 Graphics                                          |\r\n|           | Vendor Name: Intel(R) Corporation                                                    |\r\n|           | SOC UUID: 00000000-0000-00d1-0000-000856a08086                                       |\r\n|           | PCI BDF Address: 0000:d1:00.0                                                        |\r\n|           | DRM Device: /dev/dri/card7                                                           |\r\n|           | Function Type: physical                                                              |\r\n+-----------+--------------------------------------------------------------------------------------+\r\n| 7         | Device Name: Intel(R) Arc(TM) A770 Graphics                                          |\r\n|           | Vendor Name: Intel(R) Corporation                                                    |\r\n|           | SOC UUID: 00000000-0000-00e3-0000-000856a08086                                       |\r\n|           | PCI BDF Address: 0000:e3:00.0                                                        |\r\n|           | DRM Device: /dev/dri/card8                                                           |\r\n|           | Function Type: physical                                                              |\r\n+-----------+--------------------------------------------------------------------------------------+\r\nGPU0 Memory size=16M\r\nGPU1 Memory size=16G\r\nGPU2 Memory size=16G\r\nGPU3 Memory size=16G\r\nGPU4 Memory size=16G\r\nGPU5 Memory size=16G\r\nGPU6 Memory size=16G\r\nGPU7 Memory size=16G\r\nGPU8 Memory size=16G\r\n-----------------------------------------------------------------\r\n03:00.0 VGA compatible controller: ASPEED Technology, Inc. ASPEED Graphics Family (rev 52) (prog-if 00 [VGA controller])\r\n        DeviceName: Onboard VGA\r\n        Subsystem: ASPEED Technology, Inc. ASPEED Graphics Family\r\n        Flags: medium devsel, IRQ 16, NUMA node 0\r\n        Memory at 94000000 (32-bit, non-prefetchable) [size=16M]\r\n        Memory at 95000000 (32-bit, non-prefetchable) [size=256K]\r\n        I/O ports at 2000 [size=128]\r\n        Capabilities: <access denied>\r\n        Kernel driver in use: ast\r\n        Kernel modules: ast\r\n--\r\n19:00.0 VGA compatible controller: Intel Corporation DG2 [Arc A770] (rev 08) (prog-if 00 [VGA controller])\r\n        Subsystem: Shenzhen Gunnir Technology Development Co., Ltd Device 1334\r\n        Flags: bus master, fast devsel, latency 0, IRQ 130, NUMA node 0\r\n        Memory at 9e000000 (64-bit, non-prefetchable) [size=16M]\r\n        Memory at 5f800000000 (64-bit, prefetchable) [size=16G]\r\n        Expansion ROM at 9f000000 [disabled] [size=2M]\r\n        Capabilities: <access denied>\r\n        Kernel driver in use: i915\r\n        Kernel modules: xe, i915\r\n--\r\n2c:00.0 VGA compatible controller: Intel Corporation DG2 [Arc A770] (rev 08) (prog-if 00 [VGA controller])\r\n        Subsystem: Shenzhen Gunnir Technology Development Co., Ltd Device 1334\r\n        Flags: bus master, fast devsel, latency 0, IRQ 133, NUMA node 0\r\n        Memory at a8000000 (64-bit, non-prefetchable) [size=16M]\r\n        Memory at 6f800000000 (64-bit, prefetchable) [size=16G]\r\n        Expansion ROM at a9000000 [disabled] [size=2M]\r\n        Capabilities: <access denied>\r\n        Kernel driver in use: i915\r\n        Kernel modules: xe, i915\r\n--\r\n52:00.0 VGA compatible controller: Intel Corporation DG2 [Arc A770] (rev 08) (prog-if 00 [VGA controller])\r\n        Subsystem: Shenzhen Gunnir Technology Development Co., Ltd Device 1334\r\n        Flags: bus master, fast devsel, latency 0, IRQ 136, NUMA node 0\r\n        Memory at bc000000 (64-bit, non-prefetchable) [size=16M]\r\n        Memory at 8f800000000 (64-bit, prefetchable) [size=16G]\r\n        Expansion ROM at bd000000 [disabled] [size=2M]\r\n        Capabilities: <access denied>\r\n        Kernel driver in use: i915\r\n        Kernel modules: xe, i915\r\n--\r\n65:00.0 VGA compatible controller: Intel Corporation DG2 [Arc A770] (rev 08) (prog-if 00 [VGA controller])\r\n        Subsystem: Shenzhen Gunnir Technology Development Co., Ltd Device 1334\r\n        Flags: bus master, fast devsel, latency 0, IRQ 139, NUMA node 0\r\n        Memory at c6000000 (64-bit, non-prefetchable) [size=16M]\r\n        Memory at 9f800000000 (64-bit, prefetchable) [size=16G]\r\n        Expansion ROM at c7000000 [disabled] [size=2M]\r\n        Capabilities: <access denied>\r\n        Kernel driver in use: i915\r\n        Kernel modules: xe, i915\r\n--\r\n9b:00.0 VGA compatible controller: Intel Corporation DG2 [Arc A770] (rev 08) (prog-if 00 [VGA controller])\r\n        Subsystem: Shenzhen Gunnir Technology Development Co., Ltd Device 1334\r\n        Flags: bus master, fast devsel, latency 0, IRQ 142, NUMA node 1\r\n        Memory at d8000000 (64-bit, non-prefetchable) [size=16M]\r\n        Memory at cf800000000 (64-bit, prefetchable) [size=16G]\r\n        Expansion ROM at d9000000 [disabled] [size=2M]\r\n        Capabilities: <access denied>\r\n        Kernel driver in use: i915\r\n        Kernel modules: xe, i915\r\n--\r\nad:00.0 VGA compatible controller: Intel Corporation DG2 [Arc A770] (rev 08) (prog-if 00 [VGA controller])\r\n        Subsystem: Shenzhen Gunnir Technology Development Co., Ltd Device 1334\r\n        Flags: bus master, fast devsel, latency 0, IRQ 145, NUMA node 1\r\n        Memory at e0000000 (64-bit, non-prefetchable) [size=16M]\r\n        Memory at df800000000 (64-bit, prefetchable) [size=16G]\r\n        Expansion ROM at e1000000 [disabled] [size=2M]\r\n        Capabilities: <access denied>\r\n        Kernel driver in use: i915\r\n        Kernel modules: xe, i915\r\n--\r\nd1:00.0 VGA compatible controller: Intel Corporation DG2 [Arc A770] (rev 08) (prog-if 00 [VGA controller])\r\n        Subsystem: Shenzhen Gunnir Technology Development Co., Ltd Device 1334\r\n        Flags: bus master, fast devsel, latency 0, IRQ 148, NUMA node 1\r\n        Memory at f1000000 (64-bit, non-prefetchable) [size=16M]\r\n        Memory at ff800000000 (64-bit, prefetchable) [size=16G]\r\n        Expansion ROM at f2000000 [disabled] [size=2M]\r\n        Capabilities: <access denied>\r\n        Kernel driver in use: i915\r\n        Kernel modules: xe, i915\r\n--\r\ne3:00.0 VGA compatible controller: Intel Corporation DG2 [Arc A770] (rev 08) (prog-if 00 [VGA controller])\r\n        Subsystem: Intel Corporation Device 1020\r\n        Flags: bus master, fast devsel, latency 0, IRQ 151, NUMA node 1\r\n        Memory at f9000000 (64-bit, non-prefetchable) [size=16M]\r\n        Memory at 10f800000000 (64-bit, prefetchable) [size=16G]\r\n        Expansion ROM at fa000000 [disabled] [size=2M]\r\n        Capabilities: <access denied>\r\n        Kernel driver in use: i915\r\n        Kernel modules: xe, i915\r\n-----------------------------------------------------------------\r\n",
      "state": "open",
      "author": "oldmikeyang",
      "author_type": "User",
      "created_at": "2024-08-14T01:04:21Z",
      "updated_at": "2024-08-17T09:44:26Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11786/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "qiyuangong",
        "plusbang"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11786",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11786",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:33.608962",
      "comments": [
        {
          "author": "plusbang",
          "body": "Hi, @oldmikeyang, the error is caused by out of GPU memory. We haven't experimented 72B & fp6 through deepspeed autotp on 4 ARC, and please try vllm tp and pipeline parallel instead.\r\n\r\nTake pipeline parallel for example, you could set `cpu_embedding=True` and `export IPEX_LLM_LOW_MEM=1` to run 72B ",
          "created_at": "2024-08-14T02:01:28Z"
        }
      ]
    },
    {
      "issue_number": 11741,
      "title": "vllm cpu docker. Error: TypeError: invalidInputError() missing 1 required positional argument: 'errMsg' ",
      "body": "参考文章：vLLM Serving with IPEX-LLM on Intel CPU via Docker\r\n参考链接：https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/DockerGuides/vllm_cpu_docker_quickstart.md#vllm-serving-with-ipex-llm-on-intel-cpu-via-docker\r\n\r\n服务启动代码：\r\n```\r\nbash start-vllm-service.sh\r\n```\r\n\r\nstart-vllm-service.sh 文件为\r\n```\r\n#!/bin/bash\r\nmodel=\"/home/usr/A000Files/A003Model/qwen/Qwen2-0___5B-Instruct\"\r\nserved_model_name=\"qwen\"\r\n\r\n\r\npython -m ipex_llm.vllm.cpu.entrypoints.openai.api_server \\\r\n  --served-model-name $served_model_name \\\r\n  --port 8899 \\\r\n  --model $model \\\r\n  --trust-remote-code \\\r\n  --device cpu \\\r\n  --dtype bfloat16 \\\r\n  --enforce-eager \\\r\n  --load-in-low-bit bf16 \\\r\n  --max-model-len 4096 \\\r\n  --max-num-batched-tokens 10240 \\\r\n  --max-num-seqs 12 \\\r\n  --tensor-parallel-size 1\r\n```\r\n\r\n请求代码：\r\n```\r\n## openai 调用已经部署的模型 \r\nfrom openai import OpenAI\r\nclient = OpenAI(base_url=\"http://0.0.0.0:8899/v1\", api_key=\"EMPTY\")\r\n\r\n\r\ncompletion = client.chat.completions.create(\r\n    model=\"qwen\",\r\n    messages=[\r\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\r\n        {\"role\": \"user\", \"content\": \"hi, how are you\"},\r\n    ], \r\n    temperature=0.45,\r\n    max_tokens=2048,\r\n    top_p=0.7,\r\n    n=1,\r\n    frequency_penalty=0.5,\r\n    presence_penalty=0.6,\r\n    stop=None\r\n)\r\nresponse = completion.choices[0].message.content\r\nresponse\r\n```\r\n\r\n\r\nvllm服务端报错结果展示：\r\n\r\n```\r\nINFO 08-08 05:47:56 async_llm_engine.py:120] Finished request cmpl-d1bf029528104215b052b07957271a95.\r\nINFO:     127.0.0.1:56592 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\r\nERROR:    Exception in ASGI application\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 399, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/middleware/proxy_headers.py\", line 70, in __call__\r\n    return await self.app(scope, receive, send)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/fastapi/applications.py\", line 1054, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/applications.py\", line 123, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\r\n    raise exc\r\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/cors.py\", line 85, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/exceptions.py\", line 65, in __call__\r\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 756, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 776, in app\r\n    await route.handle(scope, receive, send)\r\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 297, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 77, in app\r\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 72, in app\r\n    response = await func(request)\r\n               ^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/fastapi/routing.py\", line 278, in app\r\n    raw_response = await run_endpoint_function(\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/fastapi/routing.py\", line 191, in run_endpoint_function\r\n    return await dependant.call(**values)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/cpu/entrypoints/openai/api_server.py\", line 117, in create_chat_completion\r\n    invalidInputError(isinstance(generator, ChatCompletionResponse))\r\nTypeError: invalidInputError() missing 1 required positional argument: 'errMsg'\r\n```\r\n\r\nopenai 请求端报错展示：\r\n```\r\nFile /usr/local/lib/python3.11/dist-packages/openai/_base_client.py:1079, in SyncAPIClient._retry_request(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\r\n   1075 # In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\r\n   1076 # different thread if necessary.\r\n   1077 time.sleep(timeout)\r\n-> 1079 return self._request(\r\n   1080     options=options,\r\n   1081     cast_to=cast_to,\r\n   1082     remaining_retries=remaining,\r\n   1083     stream=stream,\r\n   1084     stream_cls=stream_cls,\r\n   1085 )\r\n\r\nFile /usr/local/lib/python3.11/dist-packages/openai/_base_client.py:1046, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)\r\n   1043         err.response.read()\r\n   1045     log.debug(\"Re-raising status error\")\r\n-> 1046     raise self._make_status_error_from_response(err.response) from None\r\n   1048 return self._process_response(\r\n   1049     cast_to=cast_to,\r\n   1050     options=options,\r\n   (...)\r\n   1053     stream_cls=stream_cls,\r\n   1054 )\r\n\r\nInternalServerError: Internal Server Error\r\n```\r\n",
      "state": "closed",
      "author": "Starrylun",
      "author_type": "User",
      "created_at": "2024-08-08T05:55:15Z",
      "updated_at": "2024-08-16T10:11:55Z",
      "closed_at": "2024-08-16T10:11:55Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11741/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "xiangyuT"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11741",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11741",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:33.837220",
      "comments": [
        {
          "author": "Starrylun",
          "body": "此外还尝试了 request.post 请求，同样也报错了 \r\n\r\nrequest.post 请求代码：\r\n```\r\n%%time\r\nimport requests\r\nimport json\r\n\r\nurl = \"http://0.0.0.0:8899/v1/chat/completions\"  # 注意路径可能需要根据实际API有所不同\r\n\r\nheaders = {\r\n    \"Content-Type\": \"application/json\",\r\n    \"Authorization\": \"Bearer sk-no-key-required\"  # 你的API密钥\r\n}\r\n\r\ndata = ",
          "created_at": "2024-08-08T05:58:19Z"
        },
        {
          "author": "Starrylun",
          "body": "另外还直接使用了 文档中提供的 curl 请求，同样也报错了 \r\n\r\n请求代码：\r\n```\r\ncurl http://localhost:8000/v1/completions \\\r\n-H \"Content-Type: application/json\" \\\r\n-d '{\r\n  \"model\": \"qwen\",\r\n  \"prompt\": \"San Francisco is a\",\r\n  \"max_tokens\": 128,\r\n  \"temperature\": 0\r\n}' | jq '.choices[0].text'\r\n\r\n```\r\n\r\n服务端请求报错类型：\r\n```\r\nroot@sophon",
          "created_at": "2024-08-08T06:00:44Z"
        },
        {
          "author": "Starrylun",
          "body": "设备类型：\r\n```\r\nroot@sophon-test-001:/llm# lscpu \r\nArchitecture:            x86_64\r\n  CPU op-mode(s):        32-bit, 64-bit\r\n  Address sizes:         46 bits physical, 57 bits virtual\r\n  Byte Order:            Little Endian\r\nCPU(s):                  48\r\n  On-line CPU(s) list:   0-47\r\nVendor ID:         ",
          "created_at": "2024-08-08T06:06:09Z"
        },
        {
          "author": "xiangyuT",
          "body": "Hi @Starrylun,\r\nI have reproduced the issue, and it appears to be a bug in the `/v1/chat/completions` endpoint within `ipex_llm.vllm.cpu.entrypoints.openai.apiserver`. This should be fixed by [#11748](https://github.com/intel-analytics/ipex-llm/pull/11748). You can try the next nightly-build Docker ",
          "created_at": "2024-08-09T02:55:47Z"
        },
        {
          "author": "xiangyuT",
          "body": "Hi @Starrylun,\r\nThe new Docker image has been released. Please update to the latest version and try again. It works fine in my environment.",
          "created_at": "2024-08-12T05:47:16Z"
        }
      ]
    },
    {
      "issue_number": 11670,
      "title": "Can we support InternVL2-4B?",
      "body": "New model support request on MTL:\r\n\r\nhttps://huggingface.co/OpenGVLab/InternVL2-4B",
      "state": "closed",
      "author": "juan-OY",
      "author_type": "User",
      "created_at": "2024-07-28T15:58:20Z",
      "updated_at": "2024-08-16T09:55:58Z",
      "closed_at": "2024-08-16T09:55:58Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11670/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "MeouSker77"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11670",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11670",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:34.121064",
      "comments": [
        {
          "author": "notsyncing",
          "body": "Hello, InternVL2 already works on ipex-llm (2.1.0b20240719) according to my tests with InternVL2-8B on A770 16GB, around 15~17 token/s with `load_in_4bit`.\r\n\r\nbtw, will we get any performance improvement on this model? like ~40t/s on codeqwen1.5-7b on my A770",
          "created_at": "2024-07-29T03:11:20Z"
        },
        {
          "author": "MeouSker77",
          "body": "Supported in latest ipex-llm",
          "created_at": "2024-08-16T09:55:58Z"
        }
      ]
    },
    {
      "issue_number": 11745,
      "title": "[MTL][Internvl2-4B] GPU OOM for 3k input tokens",
      "body": "Running Internvl2-4B, \r\n\r\ninput token: 1024; Mem: 4.69GB\r\ninput token:2048;Mem: 9.3 GB\r\ninput token:3096 Mem: OOM\r\n\r\neverything looks to be ok when input token is less or equal to 1k, but from 1k to 2k, GPU memory almost doubles, and it leads to OOM at 3K input tokens.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "state": "closed",
      "author": "juan-OY",
      "author_type": "User",
      "created_at": "2024-08-08T16:13:44Z",
      "updated_at": "2024-08-16T09:52:59Z",
      "closed_at": "2024-08-16T09:52:59Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11745/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "MeouSker77"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11745",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11745",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:34.698580",
      "comments": [
        {
          "author": "MeouSker77",
          "body": "fixed in latest ipex-llm",
          "created_at": "2024-08-16T09:52:59Z"
        }
      ]
    },
    {
      "issue_number": 11795,
      "title": "Result is wrong when running Qwen2-1.5B-Instruct on Intel NPU",
      "body": "Just follow the example https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/NPU/HF-Transformers-AutoModels/LLM/generate.py\r\n\r\nwhen I use load_low_bit=sym_int4, the result is wrong.\r\n-------------------- Output --------------------\r\n<|im_start|>system\r\nYou are a helpful assistant.<|im_end|>\r\n<|im_start|>user\r\n什么是电子竞技<|im_end|>\r\n<|im_start|>assistant\r\n League League League trail trail trail trail trail trail trail trail trail trail trail trail trail trail trail trail trail trail trail trail trail trail trail trail trail\r\n--------------------------------------------------------------------------------\r\n\r\n\r\nwhile using load_low_bit=sym_int8, result is correct.\r\n-------------------- Output --------------------\r\n<|im_start|>system\r\nYou are a helpful assistant.<|im_end|>\r\n<|im_start|>user\r\n什么是电子竞技<|im_end|>\r\n<|im_start|>assistant\r\n电子竞技（Electronic Sports）是一种以电子游戏为比赛项目，通过网络进行的体育运动。它包括了多种类型的游戏，如《英雄联盟\r\n--------------------------------------------------------------------------------",
      "state": "open",
      "author": "grandxin",
      "author_type": "User",
      "created_at": "2024-08-14T07:48:24Z",
      "updated_at": "2024-08-16T03:29:06Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11795/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "plusbang"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11795",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11795",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:34.928469",
      "comments": [
        {
          "author": "plusbang",
          "body": "`Qwen2-7B-Instruct` is verified and we will try to reproduce your error first. We will inform you immediately once there is progress.",
          "created_at": "2024-08-15T02:27:55Z"
        },
        {
          "author": "plusbang",
          "body": "Hi, @grandxin , I could not reproduce such error on MTL with `32.0.100.2540` driver.\r\n\r\nBy using `ipex-llm==2.1.0b20240814`, the output of `Qwen2-1.5B-Instruct` with `load_low_bit=sym_int4` is\r\n```bash\r\n-------------------- Output --------------------\r\n<|im_start|>system\r\nYou are a helpful assistant",
          "created_at": "2024-08-15T03:09:54Z"
        },
        {
          "author": "grandxin",
          "body": "> Hi, @grandxin , I could not reproduce such error on MTL with `32.0.100.2540` driver.\r\n> \r\n> By using `ipex-llm==2.1.0b20240814`, the output of `Qwen2-1.5B-Instruct` with `load_low_bit=sym_int4` is\r\n> \r\n> ```shell\r\n> -------------------- Output --------------------\r\n> <|im_start|>system\r\n> You are ",
          "created_at": "2024-08-16T03:29:05Z"
        }
      ]
    },
    {
      "issue_number": 11664,
      "title": "Question about benchmark result",
      "body": "I have used all-in-one benchmark to test on Intel Ultra 9 185H's NPU,  The model used is Owen/Qwen2-7B.\r\nI'm confused about the result.  In this [repo's image](https://llm-assets.readthedocs.io/en/latest/_images/MTL_perf.jpg) shows tokens/s for 32 tokens/input is 19.6 under Intel Ultra 7 165H.\r\nMy result in csv file is\r\n```\r\n1st token average latency(ms): 617.64\r\n2+ avg latency(ms/token): 340.45\r\nencoder time(ms): 0\r\ninput/ouput tokens: 32-32\r\nbatch_size: 1\r\nactual input/output tokens: 32-32\r\nnum_beams: 1\r\nlow_bit: sym_int\r\ncpu_embendding: False\r\nmodel loading time(s): 88.19\r\npeak mem(GB): N/A\r\nstreaming: False\r\nuse_fp16_torch_dtype: N/A\r\n```\r\nMy questions are\r\n- Is \"tokens/s\" calculated from \"2+ avg latency(ms/token)\"? If so, that will be 1000 / 340.45 = 2.94\r\n- Is the benchmark result provided by this repo using cpu, igpu, or npu?  If npu is used during the benchmark, then my result is far from 19.6.\r\n\r\nmy config is:\r\n```yaml\r\nrepo_id:\r\n  - 'Qwen/Qwen2-7B'\r\nlocal_model_hub: 'path/to/local/model'\r\nwarm_up: 1\r\nnum_trials: 3\r\nnum_beams: 1\r\nlow_bit: 'sym_int4'\r\nbatch_size: 1\r\nin_out_pairs:\r\n  - '32-32'\r\n  - '1024-128'\r\ntest_api:\r\n  - \"transformers_int4_npu_win\" \r\ncpu_embedding: False # whether put embedding to CPU\r\nstreaming: False\r\ntask: 'continuation'\r\n```\r\nWhen running the benchmark, I can see the NPU's resource is being used in task manager. ",
      "state": "open",
      "author": "xeasonx",
      "author_type": "User",
      "created_at": "2024-07-26T06:26:41Z",
      "updated_at": "2024-08-16T03:15:55Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11664/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11664",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11664",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:35.231408",
      "comments": [
        {
          "author": "jason-dai",
          "body": "It uses iGPU; as mentioned in readme, please refer to [[2]](https://www.intel.com/content/www/us/en/developer/articles/technical/accelerate-meta-llama3-with-intel-ai-solutions.html)[[3]](https://www.intel.com/content/www/us/en/developer/articles/technical/accelerate-microsoft-phi-3-models-intel-ai-s",
          "created_at": "2024-07-26T06:40:16Z"
        },
        {
          "author": "grandxin",
          "body": "have you solved this problem?\r\nI also run the qwen2-7b(int4) example using NPU.  Inference speed is too slow, only 2-3 tokens/s.",
          "created_at": "2024-08-16T03:15:54Z"
        }
      ]
    },
    {
      "issue_number": 11789,
      "title": "Failure to load the LLM model in vLLM on 8 ARC",
      "body": "With the ipex-llm docker container, \r\nintelanalytics/ipex-llm-serving-vllm-xpu-experiment:2.1.0b2\r\n\r\nit successfully load model in 4 ARC. But when load model in 8 ARC, it will have the following error.\r\n\r\nroot@GPU-Xeon4410Y-ARC770:/llm# bash start-vllm-service.sh\r\n/usr/local/lib/python3.11/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n  warn(\r\n2024-08-14 11:07:55,600 - INFO - intel_extension_for_pytorch auto imported\r\nINFO 08-14 11:07:56 api_server.py:258] vLLM API server version 0.3.3\r\nINFO 08-14 11:07:56 api_server.py:259] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name='Qwen1.5-7B-Chat', lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], load_in_low_bit='fp6', model='/llm/models/Qwen/Qwen1.5-7B-Chat', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, download_dir=None, load_format='auto', dtype='float16', kv_cache_dtype='auto', max_model_len=4096, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=8, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, seed=0, swap_space=4, gpu_memory_utilization=0.75, max_num_batched_tokens=10240, max_num_seqs=12, max_paddings=256, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=True, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='xpu', engine_use_ray=False, disable_log_requests=False, max_log_len=None)\r\nWARNING 08-14 11:07:56 config.py:710] Casting torch.bfloat16 to torch.float16.\r\nINFO 08-14 11:07:56 config.py:523] Custom all-reduce kernels are temporarily disabled due to stability issues. We will re-enable them once the issues are resolved.\r\n2024-08-14 11:07:58,897 INFO worker.py:1788 -- Started a local Ray instance.\r\nINFO 08-14 11:07:59 llm_engine.py:68] Initializing an LLM engine (v0.3.3) with config: model='/llm/models/Qwen/Qwen1.5-7B-Chat', tokenizer='/llm/models/Qwen/Qwen1.5-7B-Chat', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=xpu, seed=0, max_num_batched_tokens=10240, max_num_seqs=12, max_model_len=4096)\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n(RayWorkerVllm pid=32282) /usr/local/lib/python3.11/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n(RayWorkerVllm pid=32282)   warn(\r\n(RayWorkerVllm pid=32483) 2024-08-14 11:08:17,825 - INFO - intel_extension_for_pytorch auto imported\r\nINFO 08-14 11:08:18 attention.py:71] flash_attn is not found. Using xformers backend.\r\n(RayWorkerVllm pid=32094) INFO 08-14 11:08:18 attention.py:71] flash_attn is not found. Using xformers backend.\r\n2024-08-14 11:08:19,069 - INFO - Converting the current model to fp6 format......\r\n2024-08-14 11:08:19,069 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\r\n[2024-08-14 11:08:20,124] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to xpu (auto detect)\r\n(RayWorkerVllm pid=32483) 2024-08-14 11:08:20,271 - INFO - Converting the current model to fp6 format......\r\n(RayWorkerVllm pid=32483) 2024-08-14 11:08:20,272 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\r\n(RayWorkerVllm pid=32094) /usr/local/lib/python3.11/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source? [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\r\n(RayWorkerVllm pid=32094)   warn( [repeated 6x across cluster]\r\n2024-08-14 11:08:21,272 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\r\n(RayWorkerVllm pid=32483) [2024-08-14 11:08:21,256] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to xpu (auto detect)\r\nINFO 08-14 11:08:21 model_convert.py:249] Loading model weights took 1.0264 GB\r\n(RayWorkerVllm pid=32349) 2024-08-14 11:08:18,290 - INFO - intel_extension_for_pytorch auto imported [repeated 6x across cluster]\r\n(RayWorkerVllm pid=32551) 2024-08-14 11:08:20,708 - INFO - Converting the current model to fp6 format...... [repeated 6x across cluster]\r\n(RayWorkerVllm pid=32483) 2024-08-14 11:08:25,761 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations [repeated 7x across cluster]\r\n(RayWorkerVllm pid=32483) INFO 08-14 11:08:26 model_convert.py:249] Loading model weights took 1.0264 GB\r\n(RayWorkerVllm pid=32551) INFO 08-14 11:08:18 attention.py:71] flash_attn is not found. Using xformers backend. [repeated 6x across cluster]\r\n(RayWorkerVllm pid=32551) [2024-08-14 11:08:21,778] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to xpu (auto detect) [repeated 6x across cluster]\r\n2024:08:14-11:08:27:(28904) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\r\n2024:08:14-11:08:27:(28904) |CCL_WARN| fallback to 'sockets' mode of ze exchange mechanism, to use CCL_ZE_IPC_EXHANGE=drmfd, set CCL_LOCAL_RANK/SIZE explicitly  or use process launcher\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:28:(32094) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:28:(32094) |CCL_WARN| fallback to 'sockets' mode of ze exchange mechanism, to use CCL_ZE_IPC_EXHANGE=drmfd, set CCL_LOCAL_RANK/SIZE explicitly  or use process launcher\r\n2024:08:14-11:08:29:(33884) |CCL_WARN| no membind support for NUMA node 1, skip thread membind\r\n2024:08:14-11:08:29:(33896) |CCL_WARN| no membind support for NUMA node 1, skip thread membind\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:29:(33886) |CCL_WARN| no membind support for NUMA node 1, skip thread membind\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:29:(33892) |CCL_WARN| no membind support for NUMA node 1, skip thread membind\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:30:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32094) 2024:08:14-11:08:30:(32094) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n2024:08:14-11:08:32:(28904) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices\r\n(RayWorkerVllm pid=32162) INFO 08-14 11:08:27 model_convert.py:249] Loading model weights took 1.0264 GB [repeated 6x across cluster]\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/entrypoints/openai/api_server.py\", line 267, in <module>\r\n    engine = IPEXLLMAsyncLLMEngine.from_engine_args(engine_args,\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 57, in from_engine_args\r\n    engine = cls(parallel_config.worker_use_ray,\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/engine/engine.py\", line 30, in __init__\r\n    super().__init__(*args, **kwargs)\r\n  File \"/llm/vllm/vllm/engine/async_llm_engine.py\", line 309, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/llm/vllm/vllm/engine/async_llm_engine.py\", line 409, in _init_engine\r\n    return engine_class(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/llm/vllm/vllm/engine/llm_engine.py\", line 106, in __init__\r\n    self.model_executor = executor_class(model_config, cache_config,\r\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/ipex_llm_gpu_executor.py\", line 77, in __init__\r\n    self._init_cache()\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/ipex_llm_gpu_executor.py\", line 249, in _init_cache\r\n    num_blocks = self._run_workers(\r\n                 ^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/ipex_llm/vllm/xpu/ipex_llm_gpu_executor.py\", line 347, in _run_workers\r\n    driver_worker_output = getattr(self.driver_worker,\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/llm/vllm/vllm/worker/worker.py\", line 136, in profile_num_available_blocks\r\n    self.model_runner.profile_run()\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/llm/vllm/vllm/worker/model_runner.py\", line 645, in profile_run\r\n    self.execute_model(seqs, kv_caches)\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/llm/vllm/vllm/worker/model_runner.py\", line 581, in execute_model\r\n    hidden_states = model_executable(\r\n                    ^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/llm/vllm/vllm/model_executor/models/qwen2.py\", line 316, in forward\r\n    hidden_states = self.model(input_ids, positions, kv_caches,\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/llm/vllm/vllm/model_executor/models/qwen2.py\", line 257, in forward\r\n    hidden_states, residual = layer(\r\n                              ^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/llm/vllm/vllm/model_executor/models/qwen2.py\", line 208, in forward\r\n    hidden_states, residual = self.input_layernorm(\r\n                              ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/llm/vllm/vllm/model_executor/layers/layernorm.py\", line 52, in forward\r\n    ops.fused_add_rms_norm(\r\nTypeError: fused_add_rms_norm(): incompatible function arguments. The following argument types are supported:\r\n    1. (arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: float) -> None\r\n\r\nInvoked with: (tensor([[[-0.0239,  0.0522,  0.0044,  ..., -0.0462,  0.1113,  0.0284],\r\n         [-0.0239,  0.0522,  0.0044,  ..., -0.0462,  0.1113,  0.0284],\r\n         [-0.0239,  0.0522,  0.0044,  ..., -0.0462,  0.1113,  0.0284],\r\n         ...,\r\n         [-0.0240,  0.0522,  0.0044,  ..., -0.0462,  0.1114,  0.0284],\r\n         [-0.0240,  0.0522,  0.0044,  ..., -0.0462,  0.1114,  0.0284],\r\n         [-0.0240,  0.0522,  0.0044,  ..., -0.0462,  0.1114,  0.0284]],\r\n\r\n        [[-0.0240,  0.0522,  0.0044,  ..., -0.0462,  0.1114,  0.0284],\r\n         [-0.0240,  0.0522,  0.0044,  ..., -0.0462,  0.1114,  0.0284],\r\n         [-0.0240,  0.0522,  0.0044,  ..., -0.0462,  0.1114,  0.0284],\r\n         ...,\r\n         [-0.0239,  0.0521,  0.0044,  ..., -0.0462,  0.1113,  0.0284],\r\n         [-0.0239,  0.0521,  0.0044,  ..., -0.0462,  0.1113,  0.0284],\r\n         [-0.0239,  0.0521,  0.0044,  ..., -0.0462,  0.1113,  0.0284]],\r\n\r\n        [[-0.0239,  0.0521,  0.0044,  ..., -0.0462,  0.1113,  0.0284],\r\n         [-0.0239,  0.0521,  0.0044,  ..., -0.0462,  0.1113,  0.0284],\r\n         [-0.0239,  0.0521,  0.0044,  ..., -0.0462,  0.1113,  0.0284],\r\n         ...,\r\n         [-0.0239,  0.0522,  0.0044,  ..., -0.0462,  0.1113,  0.0284],\r\n         [-0.0239,  0.0522,  0.0044,  ..., -0.0462,  0.1113,  0.0284],\r\n         [-0.0239,  0.0522,  0.0044,  ..., -0.0462,  0.1113,  0.0284]],\r\n\r\n        ...,\r\n\r\n        [[-0.0239,  0.0521,  0.0044,  ..., -0.0462,  0.1113,  0.0284],\r\n         [-0.0239,  0.0521,  0.0044,  ..., -0.0462,  0.1113,  0.0284],\r\n         [-0.0239,  0.0521,  0.0044,  ..., -0.0462,  0.1113,  0.0284],\r\n         ...,\r\n         [-0.0239,  0.0521,  0.0044,  ..., -0.0462,  0.1113,  0.0284],\r\n         [-0.0239,  0.0521,  0.0044,  ..., -0.0462,  0.1113,  0.0284],\r\n         [-0.0239,  0.0521,  0.0044,  ..., -0.0462,  0.1113,  0.0284]],\r\n\r\n        [[-0.0239,  0.0521,  0.0044,  ..., -0.0462,  0.1113,  0.0284],\r\n         [-0.0239,  0.0521,  0.0044,  ..., -0.0462,  0.1113,  0.0284],\r\n         [-0.0239,  0.0521,  0.0044,  ..., -0.0462,  0.1113,  0.0284],\r\n         ...,\r\n         [-0.0239,  0.0522,  0.0045,  ..., -0.0461,  0.1113,  0.0284],\r\n         [-0.0239,  0.0522,  0.0045,  ..., -0.0461,  0.1113,  0.0284],\r\n         [-0.0239,  0.0522,  0.0045,  ..., -0.0461,  0.1113,  0.0284]],\r\n\r\n        [[-0.0239,  0.0522,  0.0045,  ..., -0.0461,  0.1113,  0.0284],\r\n         [-0.0239,  0.0522,  0.0045,  ..., -0.0461,  0.1113,  0.0284],\r\n         [-0.0239,  0.0522,  0.0045,  ..., -0.0461,  0.1113,  0.0284],\r\n         ...,\r\n         [-0.0239,  0.0522,  0.0044,  ..., -0.0462,  0.1113,  0.0284],\r\n         [-0.0239,  0.0522,  0.0044,  ..., -0.0462,  0.1113,  0.0284],\r\n         [-0.0239,  0.0522,  0.0044,  ..., -0.0462,  0.1113,  0.0284]]],\r\n       device='xpu:0', dtype=torch.float16), None), tensor([[[-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0165],\r\n         [-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0165],\r\n         [-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0165],\r\n         ...,\r\n         [-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0166],\r\n         [-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0166],\r\n         [-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0166]],\r\n\r\n        [[-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0166],\r\n         [-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0166],\r\n         [-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0166],\r\n         ...,\r\n         [-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0166],\r\n         [-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0166],\r\n         [-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0166]],\r\n\r\n        [[-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0166],\r\n         [-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0166],\r\n         [-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0166],\r\n         ...,\r\n         [-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0165],\r\n         [-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0165],\r\n         [-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0165]],\r\n\r\n        ...,\r\n\r\n        [[-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0166],\r\n         [-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0166],\r\n         [-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0166],\r\n         ...,\r\n         [-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0166],\r\n         [-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0166],\r\n         [-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0166]],\r\n\r\n        [[-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0166],\r\n         [-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0166],\r\n         [-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0166],\r\n         ...,\r\n         [-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0165],\r\n         [-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0165],\r\n         [-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0165]],\r\n\r\n        [[-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0165],\r\n         [-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0165],\r\n         [-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0165],\r\n         ...,\r\n         [-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0165],\r\n         [-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0165],\r\n         [-0.0142, -0.0132,  0.0210,  ...,  0.0883,  0.0250,  0.0165]]],\r\n       device='xpu:0', dtype=torch.float16), tensor([0.1367, 0.0952, 0.1030,  ..., 0.1338, 0.0845, 0.0928], device='xpu:0',\r\n       dtype=torch.float16), 1e-06\r\n(RayWorkerVllm pid=32551) 2024:08:14-11:08:28:(32551) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL [repeated 6x across cluster]\r\n(RayWorkerVllm pid=32551) 2024:08:14-11:08:28:(32551) |CCL_WARN| fallback to 'sockets' mode of ze exchange mechanism, to use CCL_ZE_IPC_EXHANGE=drmfd, set CCL_LOCAL_RANK/SIZE explicitly  or use process launcher [repeated 6x across cluster]\r\n(RayWorkerVllm pid=32551) 2024:08:14-11:08:29:(33894) |CCL_WARN| no membind support for NUMA node 0, skip thread membind [repeated 12x across cluster]\r\n(RayWorkerVllm pid=32551) 2024:08:14-11:08:32:(32551) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices [repeated 728x across cluster]\r\n(RayWorkerVllm pid=32162) 2024-08-14 11:08:27,278 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations [repeated 6x across cluster]\r\n",
      "state": "open",
      "author": "oldmikeyang",
      "author_type": "User",
      "created_at": "2024-08-14T03:12:35Z",
      "updated_at": "2024-08-16T03:11:41Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11789/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "gc-fu"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11789",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11789",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:35.465920",
      "comments": [
        {
          "author": "gc-fu",
          "body": "Hi, I am currently investigating this issue.  Will update to this issue once I fix it",
          "created_at": "2024-08-15T01:15:11Z"
        },
        {
          "author": "gc-fu",
          "body": "Hi, this should have been fixed by PR: https://github.com/intel-analytics/ipex-llm/pull/11817\r\n\r\nYou can upgrade ipex-llm tomorrow and see if this works.",
          "created_at": "2024-08-15T11:04:42Z"
        },
        {
          "author": "oldmikeyang",
          "body": "with latest IPEX-LLM, the following error during inference\r\n\r\nINFO 08-16 10:12:59 metrics.py:217] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\r\nINFO 08-16 10:12:59 a",
          "created_at": "2024-08-16T02:56:56Z"
        },
        {
          "author": "gc-fu",
          "body": "Hi, this problem is due to out of memory.  You can reduce gpu-utilization-rate, or reduce max-num-batched-tokens.\r\n\r\n\r\nUse this command will fix the problem:\r\n```bash\r\n#!/bin/bash\r\nmodel=\"/home/llm/local_models/Qwen/Qwen2-72B-Instruct\"\r\nserved_model_name=\"Qwen2-72B-Instruct\"\r\nexport USE_XETLA=OFF\r\ne",
          "created_at": "2024-08-16T03:10:59Z"
        }
      ]
    },
    {
      "issue_number": 11692,
      "title": "about npu support",
      "body": "If this version ipex support npu with GPT2 model and MiniCPM-V model.\r\nWhat can I do to test this function if it support.",
      "state": "closed",
      "author": "K-Alex13",
      "author_type": "User",
      "created_at": "2024-07-31T03:44:24Z",
      "updated_at": "2024-08-16T02:43:42Z",
      "closed_at": "2024-08-16T02:43:42Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11692/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11692",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11692",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:35.644623",
      "comments": [
        {
          "author": "hkvision",
          "body": "We haven't supported these models on NPU yet at this moment. Will keep you updated when the support is ready. Thanks!",
          "created_at": "2024-08-01T13:57:57Z"
        }
      ]
    },
    {
      "issue_number": 11608,
      "title": "gpu第一次以及放久了为什么每次询问都要初始化，影响返回值速度",
      "body": "gpu第一或者放5分钟以上，必现，用ollama问个问题，要等30多秒或20多秒才会答复，一旦又答复了以后，再进行问答，答复就很快，但是放个5分钟以上，再重新问，就要等20多秒或30多秒\r\n![Uploading 微信图片_20240718090726.jpg…]()\r\n",
      "state": "closed",
      "author": "dayskk",
      "author_type": "User",
      "created_at": "2024-07-18T01:15:01Z",
      "updated_at": "2024-08-16T01:42:29Z",
      "closed_at": "2024-08-16T01:42:29Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11608/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11608",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11608",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:35.815148",
      "comments": [
        {
          "author": "dayskk",
          "body": "![微信图片_20240718090633](https://github.com/user-attachments/assets/c95bd407-0d2d-4311-959f-2d0d3427f9e7)\r\n![微信图片_20240718090708](https://github.com/user-attachments/assets/e71d42de-65dd-47d2-b5da-8df223f7ef9d)\r\n![微信图片_20240718090715](https://github.com/user-attachments/assets/9b95aa73-af4b-4562-b4e9-",
          "created_at": "2024-07-18T01:15:59Z"
        },
        {
          "author": "biyuehuang",
          "body": "可以把CPU和iGPU推理情况做一组对照试验，看看CPU是否存在同样的问题。因为用CPU推理时是原生的ollama。\r\n把这个变量设成0，就是ollama在CPU上推理。\r\nset OLLAMA_NUM_GPU=0",
          "created_at": "2024-07-18T01:30:49Z"
        },
        {
          "author": "dayskk",
          "body": "cpu速度目前还比gpu快，当gpu第一次或间隔5分钟后再问答\r\ncpu很均衡，都是5，6秒就答复上来了",
          "created_at": "2024-07-18T01:33:44Z"
        },
        {
          "author": "dayskk",
          "body": "gpu第一次或间隔5分钟后，再问答，回答回来速度是20多秒或30多秒",
          "created_at": "2024-07-18T01:34:22Z"
        },
        {
          "author": "biyuehuang",
          "body": "ollama 模型放iGPU的时候，5分钟后，任务管理器可以看到模型已经不在iGPU内存里了。需求是修改成模型一直留在iGPU，不被释放，以减少ollama静止5分钟后对话的等待时间",
          "created_at": "2024-07-18T01:49:36Z"
        }
      ]
    },
    {
      "issue_number": 11798,
      "title": "Issue running throughput test with vllm on 4 Arc A770: \"Current platform can NOT allocate memory block with size larger than 4GB! \"",
      "body": "### Test condition\r\n\r\n```\r\nModel: Qwen1.5-32B-Chat\r\nPrecision: int4\r\nGPU: 4x A770\r\n```\r\n\r\n### SW compatibility\r\n```\r\ndocker: intelanalytics/ipex-llm-serving-vllm-xpu-experiment:2.1.0b2\r\nintel-extension-for-pytorch: 2.1.10+xpu\r\nipex-llm: 2.1.0b20240716\r\nvllm: 0.3.3+xpu0.0.1\r\n```\r\n### Benchmark test params\r\n        --backend vllm \\\r\n        --dataset /opt/ShareGPT_V3_unfiltered_cleaned_split.json \\\r\n        --model $MODEL \\\r\n        --num-prompts 100 \\\r\n        --seed 42 \\\r\n        --trust-remote-code \\\r\n        --enforce-eager \\\r\n        --dtype float16 \\\r\n        --device xpu \\\r\n        --load-in-low-bit sym_int4 \\\r\n        --gpu-memory-utilization 0.66 \\\r\n        --max-num-seqs $n \\\r\n        --tensor-parallel-size 4  ## 2 means 2*A770\r\n### Issue description\r\nRunning benchmark_throughput.py with above params, ended with error. Used dataset but filtered prompt with length between 1024-4096.\r\n\r\n```\r\n  File \"/llm/vllm/vllm/model_executor/layers/attention/backends/torch_sdpa.py\", line 94, in forward\r\n    out = torch.nn.functional.scaled_dot_product_attention(\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: Current platform can NOT allocate memory block with size larger than 4GB! Tried to allocate 40.00 GiB (GPU  0; 15.11 GiB total capacity; 10.03 GiB already allocated; 11.94 GiB reserved in total by PyTorch)\r\n\u001b[36m(RayWorkerVllm pid=53082)\u001b[0m 2024-08-15 00:00:28,678 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\u001b[32m [repeated 2x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerVllm pid=53082)\u001b[0m INFO 08-15 00:00:31 model_convert.py:249] Loading model weights took 4.3549 GB\u001b[32m [repeated 2x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerVllm pid=53279)\u001b[0m 2024:08:15-00:00:32:(53279) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL\u001b[32m [repeated 2x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerVllm pid=53279)\u001b[0m 2024:08:15-00:00:32:(53279) |CCL_WARN| sockets exchange mode is set. It may cause potential problem of 'Too many open file descriptors'\u001b[32m [repeated 2x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerVllm pid=53279)\u001b[0m 2024:08:15-00:00:33:(53940) |CCL_WARN| no membind support for NUMA node 0, skip thread membind\u001b[32m [repeated 8x across cluster]\u001b[0m\r\n```\r\n\r\n",
      "state": "closed",
      "author": "aprilhu01",
      "author_type": "User",
      "created_at": "2024-08-14T08:33:01Z",
      "updated_at": "2024-08-15T04:51:11Z",
      "closed_at": "2024-08-15T04:51:10Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11798/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "gc-fu"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11798",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11798",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:37.939858",
      "comments": [
        {
          "author": "gc-fu",
          "body": "I guess you will need to restrict --max-num-batched-tokens.  If not specified, it will use the max-model-len as the default config.\r\n\r\n\r\nTry adding --max-num-batched-tokens 4096, --max-model-len 4096",
          "created_at": "2024-08-15T02:16:30Z"
        },
        {
          "author": "aprilhu01",
          "body": "Thanks. That worked",
          "created_at": "2024-08-15T04:51:10Z"
        }
      ]
    },
    {
      "issue_number": 11635,
      "title": "ipex-llm inference with deepspeed of Qwen1.5-32B consumes too many memory",
      "body": "optimize memory utilization for DP-AP to less than 256GB",
      "state": "open",
      "author": "Fred-cell",
      "author_type": "User",
      "created_at": "2024-07-22T09:01:14Z",
      "updated_at": "2024-08-15T00:58:04Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11635/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11635",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11635",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:38.154948",
      "comments": [
        {
          "author": "qiyuangong",
          "body": "32B model requires 64GB memory in FP16 format. \r\n\r\nThis peak memory usage is caused by multiple processors converting the model in CPU memory. If we use 4 GPUs, we will have 4 processors reading and converting models in CPU memory (i.e., 64 GB* 4).\r\n\r\nWe can add a few sleep before loading LLM model ",
          "created_at": "2024-07-30T07:57:57Z"
        },
        {
          "author": "oldmikeyang",
          "body": "This doesn't fix the issue.\r\n\r\nIn the benchmark code, deepspeed.init_inference is the sync call.\r\nIf you add sleep before this call, all the process will be synced after this call.\r\nBut the process release the malloced memory only after move the model to XPU. \r\nMove the model to XPU is called after ",
          "created_at": "2024-08-01T09:46:13Z"
        },
        {
          "author": "qiyuangong",
          "body": "> This doesn't fix the issue.\r\n> \r\n> In the benchmark code, deepspeed.init_inference is the sync call. If you add sleep before this call, all the process will be synced after this call. But the process release the malloced memory only after move the model to XPU. Move the model to XPU is called afte",
          "created_at": "2024-08-02T04:15:40Z"
        },
        {
          "author": "qiyuangong",
          "body": "Hi @oldmikeyang \r\n\r\nAs an alternative, please add a larger swap in case OOM. SWAP to SSD is quite fast, especially for large memory blocks. These memory will be free up after moving to xpu.",
          "created_at": "2024-08-07T03:05:22Z"
        },
        {
          "author": "oldmikeyang",
          "body": "The Linux kernel have some issue on the swap. I am using the Ubuntu 22.04 with both 6.5 and 6.8 kernel. \r\nIf we use 512G swap disk file, the Linux kernel will crash, due the kswapd crash.\r\nI had file a bug to Ubuntu community.\r\nhttps://bugs.launchpad.net/ubuntu/+source/compiz-plugins-main/+bug/20766",
          "created_at": "2024-08-14T01:16:50Z"
        }
      ]
    },
    {
      "issue_number": 11723,
      "title": "ollama loading model gemma2 error：llama runner process has terminated: exit status 0xc0000409",
      "body": "After running the ollama serve, there was an error when loading the gemma2 model. However, it's strange that there was no error in loading other models, such as loading qwen2 and llama3, which didn't have any issues.\r\nI have updated the ipex-llm[cpp] to version 2.1.0b20240805\r\n\r\nError message:\r\nGGML_ASSERT: C:/Users/Administrator/actions-runner/cpp-release/_work/llm.cpp/llm.cpp/ollama-internal/llm/llama.cpp/llama.cpp:10739: false\r\ntime=2024-08-06T16:07:03.878+08:00 level=INFO source=server.go:566 msg=\"waiting for server to become available\" status=\"llm server not responding\"\r\ntime=2024-08-06T16:07:04.591+08:00 level=INFO source=server.go:566 msg=\"waiting for server to become available\" status=\"llm server error\"\r\ntime=2024-08-06T16:07:04.851+08:00 level=ERROR source=sched.go:344 msg=\"error loading llama server\" error=\"llama runner process has terminated: exit status 0xc0000409 \"",
      "state": "closed",
      "author": "JerryXu2023",
      "author_type": "User",
      "created_at": "2024-08-06T08:12:53Z",
      "updated_at": "2024-08-14T02:42:56Z",
      "closed_at": "2024-08-14T02:42:56Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 12,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11723/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat",
        "rnwang04"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11723",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11723",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:38.375715",
      "comments": [
        {
          "author": "rnwang04",
          "body": "Hi @JerryXu2023 , could you please run https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/scripts to check system environment and reply us the output? At the same time, could you please provide us with more detailed error log at server side and client side ?",
          "created_at": "2024-08-07T02:30:32Z"
        },
        {
          "author": "JerryXu2023",
          "body": "Hi @rnwang04 \r\nThanks for you reply.\r\n\r\nI ran the check program.\r\n\r\nbelow is checking information:\r\nPython 3.11.9\r\n-----------------------------------------------------------------\r\ntransformers=4.43.1\r\n-----------------------------------------------------------------\r\ntorch=2.2.0+cpu\r\n-------------",
          "created_at": "2024-08-07T04:34:37Z"
        },
        {
          "author": "rnwang04",
          "body": "Hi @JerryXu2023 , got it, thanks for quick reply. Could you please also provide us your details cmd / ollama server log / ollama client log ? Then we will try to see whether we can reproduce this issue on our Iris iGPU : )",
          "created_at": "2024-08-07T05:36:11Z"
        },
        {
          "author": "JerryXu2023",
          "body": "Hi @rnwang04 \r\nBelow is run olllama serve log:\r\n2024/08/07 13:55:22 routes.go:1028: INFO server config env=\"map[OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST: OLLAMA_KEEP_ALIVE: OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:1 OLLAMA_MAX_QUEUE:512 OLLAMA_MAX_VRAM:0 OLLAMA_MODELS: OLLAMA_",
          "created_at": "2024-08-07T06:07:53Z"
        },
        {
          "author": "rnwang04",
          "body": "Hi @JerryXu2023 ,\r\nI have reproduced your error.\r\nActually we only added support for gemma2-9b before, and gemma2-2b is not supported now.\r\nI will try to add support for it, and once it's done, will update here to let you know.",
          "created_at": "2024-08-07T06:40:36Z"
        }
      ]
    },
    {
      "issue_number": 11711,
      "title": "Llama.cpp with IPEX-LLM is crashing",
      "body": "I followed the instruction here to install Core Ultra 9 MTL 185H : https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/llama_cpp_quickstart.md\r\n\r\nIt worked 3-4 months ago but not right now and showing like this error message. I tried 155H and 165H but error messages are all same. ARC driver 32.0.101.5768 and OneAPI is 2024.1\r\n\r\n----\r\n\r\n(llm-cpp) PS C:\\Users\\kimtaeyo\\PRJ\\llama-cpp> .\\main -m .\\models\\mistral-7b-instruct-v0.1.Q4_K_M.gguf -n 32 --prompt \"Once upon a time, there existed a little girl who liked to have adventures. She wanted to go to places and meet new people, and have fun\" -t 8 -e -ngl 33 --color -sm none\r\nLog start\r\nmain: build = 1 (b791c1a)\r\nmain: built with IntelLLVM 2024.0.2 for\r\nmain: seed  = 1722811827\r\nllama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from .\\models\\mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\r\nllama_model_loader: - kv  11:                          general.file_type u32              = 15\r\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  19:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q4_K:  193 tensors\r\nllama_model_loader: - type q6_K:   33 tensors\r\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\r\nllm_load_print_meta: format           = GGUF V2\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 32768\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 32768\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q4_K - Medium\r\nllm_load_print_meta: model params     = 7.24 B\r\nllm_load_print_meta: model size       = 4.07 GiB (4.83 BPW)\r\nllm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\n[SYCL] call ggml_init_sycl\r\nggml_init_sycl: GGML_SYCL_DEBUG: 0\r\nggml_init_sycl: GGML_SYCL_F16: no\r\nNative API failed. Native API returns: -30 (PI_ERROR_INVALID_VALUE) -30 (PI_ERROR_INVALID_VALUE)\r\nException caught at file:C:/Users/Administrator/actions-runner/cpp-release/_work/llm.cpp/llm.cpp/llama-cpp-bigdl/ggml-sycl.cpp, line:13140, func:operator()\r\nggml_backend_sycl_set_single_device: use single device: [0]\r\nGGML_ASSERT: C:/Users/Administrator/actions-runner/cpp-release/_work/llm.cpp/llm.cpp/llama-cpp-bigdl/ggml-sycl.cpp:18283: main_gpu_id<g_all_sycl_device_count\r\n\r\n",
      "state": "closed",
      "author": "kimtaeyo",
      "author_type": "User",
      "created_at": "2024-08-04T22:59:58Z",
      "updated_at": "2024-08-14T02:41:55Z",
      "closed_at": "2024-08-14T02:41:55Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11711/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "rnwang04"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11711",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11711",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:38.592459",
      "comments": [
        {
          "author": "rnwang04",
          "body": "Hi @kimtaeyo ,\r\nThis error seems be caused by llama.cpp can't find sycl device .\r\nCould you please provide us more details with [this script](https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/scripts/env-check.sh) ?",
          "created_at": "2024-08-05T02:31:15Z"
        },
        {
          "author": "kimtaeyo",
          "body": "Python 3.10.14\r\npython: can't open file 'C:\\\\Users\\\\kimtaeyo\\\\Downloads\\\\check.py': [Errno 2] No such file or directory\r\n-----------------------------------------------------------------\r\nSystem Information\r\n\r\nHost Name:                 MSI\r\nOS Name:                   Microsoft Windows 11 Pro\r\nOS Ve",
          "created_at": "2024-08-05T02:36:55Z"
        },
        {
          "author": "rnwang04",
          "body": "After offline sync with @kimtaeyo , it looks more like GPU driver issue.\r\nWe can't reproduce this error on our MTL machines. He will try same script on our MTL machine.\r\nbtw, could you please run [env-check.bat](https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/scripts/env-check.bat) ",
          "created_at": "2024-08-05T03:51:51Z"
        },
        {
          "author": "kimtaeyo",
          "body": "(llm-cpp) PS C:\\Users\\kimtaeyo\\Downloads> .\\env-check.bat\r\nPython 3.11.9\r\n-----------------------------------------------------------------\r\nC:\\Users\\kimtaeyo\\miniforge3\\envs\\llm-cpp\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprec",
          "created_at": "2024-08-05T04:12:09Z"
        },
        {
          "author": "kimtaeyo",
          "body": "The issue was resolved by eliminating a peculiar compatibility pack from Microsoft. Essentially, the installation of the ARC driver led to a forced installation of the OpenCL, OpenGL, and Vulkan Compatibility Pack by Microsoft. This action inadvertently blocked the system from locating Sycl devices.",
          "created_at": "2024-08-12T18:06:51Z"
        }
      ]
    },
    {
      "issue_number": 11708,
      "title": "Error: llama runner process has terminated: exit status 0xc0000409",
      "body": "I have setup ipex-llm by following [install ipex-llm for llamacpp]( https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/llama_cpp_quickstart.md#1-install-ipex-llm-for-llamacpp) until step 2 as my main goal was to run ollama on my integrated intel corporation UHD graphics, and 3rd step was example.\r\n\r\nThen, from [initialise ollama quickstart](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.md#2-initialize-ollama) I initialised ollama and set the enviorment variables:\r\n```\r\nset OLLAMA_NUM_GPU=999\r\nset no_proxy=localhost,127.0.0.1\r\nset ZES_ENABLE_SYSMAN=1\r\nset SYCL_CACHE_PERSISTENT=1\r\nset SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1\r\n\r\n```\r\nand then served ollama:\r\nollama serve\r\n\r\nNow, after serving ollama, i saw this output in terminal:\r\n```\r\n2024/08/02 19:07:04 routes.go:1028: INFO server config env=\"map[OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST: OLLAMA_KEEP_ALIVE: OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:1 OLLAMA_MAX_QUEUE:512 OLLAMA_MAX_VRAM:0 OLLAMA_MODELS: OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:*] OLLAMA_RUNNERS_DIR:C:\\\\Users\\\\ayush\\\\llama-cpp\\\\dist\\\\windows-amd64\\\\ollama_runners OLLAMA_TMPDIR:]\"\r\ntime=2024-08-02T19:07:04.511+05:30 level=INFO source=images.go:729 msg=\"total blobs: 10\"\r\ntime=2024-08-02T19:07:04.512+05:30 level=INFO source=images.go:736 msg=\"total unused blobs removed: 0\"\r\n[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.\r\n\r\n[GIN-debug] [WARNING] Running in \"debug\" mode. Switch to \"release\" mode in production.\r\n - using env:   export GIN_MODE=release\r\n - using code:  gin.SetMode(gin.ReleaseMode)\r\n\r\n[GIN-debug] POST   /api/pull                 --> github.com/ollama/ollama/server.(*Server).PullModelHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/generate             --> github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/chat                 --> github.com/ollama/ollama/server.(*Server).ChatHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/embeddings           --> github.com/ollama/ollama/server.(*Server).EmbeddingsHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/create               --> github.com/ollama/ollama/server.(*Server).CreateModelHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/push                 --> github.com/ollama/ollama/server.(*Server).PushModelHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/copy                 --> github.com/ollama/ollama/server.(*Server).CopyModelHandler-fm (5 handlers)\r\n[GIN-debug] DELETE /api/delete               --> github.com/ollama/ollama/server.(*Server).DeleteModelHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/show                 --> github.com/ollama/ollama/server.(*Server).ShowModelHandler-fm (5 handlers)\r\n[GIN-debug] POST   /api/blobs/:digest        --> github.com/ollama/ollama/server.(*Server).CreateBlobHandler-fm (5 handlers)\r\n[GIN-debug] HEAD   /api/blobs/:digest        --> github.com/ollama/ollama/server.(*Server).HeadBlobHandler-fm (5 handlers)\r\n[GIN-debug] GET    /api/ps                   --> github.com/ollama/ollama/server.(*Server).ProcessHandler-fm (5 handlers)\r\n[GIN-debug] POST   /v1/chat/completions      --> github.com/ollama/ollama/server.(*Server).ChatHandler-fm (6 handlers)\r\n[GIN-debug] GET    /                         --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\r\n[GIN-debug] GET    /api/tags                 --> github.com/ollama/ollama/server.(*Server).ListModelsHandler-fm (5 handlers)\r\n[GIN-debug] GET    /api/version              --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\r\n[GIN-debug] HEAD   /                         --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\r\n[GIN-debug] HEAD   /api/tags                 --> github.com/ollama/ollama/server.(*Server).ListModelsHandler-fm (5 handlers)\r\n[GIN-debug] HEAD   /api/version              --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\r\ntime=2024-08-02T19:07:04.522+05:30 level=INFO source=routes.go:1074 msg=\"Listening on 127.0.0.1:11434 (version 0.0.0)\"\r\ntime=2024-08-02T19:07:04.524+05:30 level=INFO source=payload.go:44 msg=\"Dynamic LLM libraries [cpu_avx cpu_avx2 cpu]\"\r\n```\r\n\r\nNow, when i try to run any model, for example tinyllama, i see this output in miniforge prompt (where ollama is running):\r\n```\r\n[GIN] 2024/08/02 - 19:07:07 | 200 |            0s |       127.0.0.1 | HEAD     \"/\"\r\n[GIN] 2024/08/02 - 19:07:07 | 200 |      2.0067ms |       127.0.0.1 | POST     \"/api/show\"\r\ntime=2024-08-02T19:07:07.969+05:30 level=INFO source=memory.go:133 msg=\"offload to gpu\" layers.requested=-1 layers.real=23 memory.available=\"3.0 GiB\" memory.required.full=\"787.0 MiB\" memory.required.partial=\"787.0 MiB\" memory.required.kv=\"44.0 MiB\" memory.weights.total=\"571.4 MiB\" memory.weights.repeating=\"520.1 MiB\" memory.weights.nonrepeating=\"51.3 MiB\" memory.graph.full=\"148.0 MiB\" memory.graph.partial=\"144.3 MiB\"\r\ntime=2024-08-02T19:07:07.970+05:30 level=INFO source=server.go:342 msg=\"starting llama server\" cmd=\"C:\\\\Users\\\\ayush\\\\llama-cpp\\\\dist\\\\windows-amd64\\\\ollama_runners\\\\cpu_avx2\\\\ollama_llama_server.exe --model C:\\\\Users\\\\ayush\\\\.ollama\\\\models\\\\blobs\\\\sha256-2af3b81862c6be03c769683af18efdadb2c33f60ff32ab6f83e42c043d6c7816 --ctx-size 2048 --batch-size 512 --embedding --log-disable --n-gpu-layers 999 --parallel 1 --port 51653\"\r\ntime=2024-08-02T19:07:07.975+05:30 level=INFO source=sched.go:338 msg=\"loaded runners\" count=1\r\ntime=2024-08-02T19:07:07.980+05:30 level=INFO source=server.go:529 msg=\"waiting for llama runner to start responding\"\r\ntime=2024-08-02T19:07:07.980+05:30 level=INFO source=server.go:566 msg=\"waiting for server to become available\" status=\"llm server error\"\r\nINFO [wmain] build info | build=1 commit=\"b791c1a\" tid=\"3128\" timestamp=1722605828\r\nINFO [wmain] system info | n_threads=4 n_threads_batch=-1 system_info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \" tid=\"3128\" timestamp=1722605828 total_threads=4\r\nINFO [wmain] HTTP server listening | hostname=\"127.0.0.1\" n_threads_http=\"3\" port=\"51653\" tid=\"3128\" timestamp=1722605828\r\nllama_model_loader: loaded meta data with 23 key-value pairs and 201 tensors from C:\\Users\\ayush\\.ollama\\models\\blobs\\sha256-2af3b81862c6be03c769683af18efdadb2c33f60ff32ab6f83e42c043d6c7816 (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = TinyLlama\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 2048\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 2048\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 22\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5632\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 64\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 4\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\r\nllama_model_loader: - kv  11:                          general.file_type u32              = 2\r\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\r\nllama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\r\nllama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\r\nllama_model_loader: - kv  22:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   45 tensors\r\nllama_model_loader: - type q4_0:  155 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 2048\r\nllm_load_print_meta: n_embd           = 2048\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 4\r\nllm_load_print_meta: n_layer          = 22\r\nllm_load_print_meta: n_rot            = 64\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 64\r\nllm_load_print_meta: n_embd_head_v    = 64\r\nllm_load_print_meta: n_gqa            = 8\r\nllm_load_print_meta: n_embd_k_gqa     = 256\r\nllm_load_print_meta: n_embd_v_gqa     = 256\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 5632\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 2048\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 1B\r\nllm_load_print_meta: model ftype      = Q4_0\r\nllm_load_print_meta: model params     = 1.10 B\r\nllm_load_print_meta: model size       = 606.53 MiB (4.63 BPW)\r\nllm_load_print_meta: general.name     = TinyLlama\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: PAD token        = 2 '</s>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\n[SYCL] call ggml_init_sycl\r\nggml_init_sycl: GGML_SYCL_DEBUG: 0\r\nggml_init_sycl: GGML_SYCL_F16: no\r\ntime=2024-08-02T19:07:08.239+05:30 level=INFO source=server.go:566 msg=\"waiting for server to become available\" status=\"llm server loading model\"\r\nfound 1 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0|     [opencl:gpu:0]|                     Intel UHD Graphics|    3.0|     32|     256|   32|  3344M|       27.20.100.8935|\r\nggml_backend_sycl_set_mul_device_mode: true\r\nllama_model_load: error loading model: DeviceList is empty. -30 (PI_ERROR_INVALID_VALUE)\r\nllama_load_model_from_file: exception loading model\r\ntime=2024-08-02T19:07:08.600+05:30 level=INFO source=server.go:566 msg=\"waiting for server to become available\" status=\"llm server error\"\r\ntime=2024-08-02T19:07:08.852+05:30 level=ERROR source=sched.go:344 msg=\"error loading llama server\" error=\"llama runner process has terminated: exit status 0xc0000409 \"\r\n[GIN] 2024/08/02 - 19:07:08 | 500 |    1.5379956s |       127.0.0.1 | POST     \"/api/chat\"\r\n```\r\nAbove is the continuation of the previous output\r\n\r\nAnd in the terminal where i tried to run the model, by running:\r\n`ollama run tinyllama`\r\n\r\nI see this:\r\n`Error: llama runner process has terminated: exit status 0xc0000409`\r\n\r\nPlease help me fix this issue!\r\n",
      "state": "open",
      "author": "EMPERORAYUSH",
      "author_type": "User",
      "created_at": "2024-08-02T13:48:09Z",
      "updated_at": "2024-08-14T01:59:48Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 10,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11708/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11708",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11708",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:39.013174",
      "comments": [
        {
          "author": "sgwhat",
          "body": "hi @EMPERORAYUSH, could you please run https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/scripts to check system environment and reply us the output?",
          "created_at": "2024-08-05T02:22:12Z"
        },
        {
          "author": "EMPERORAYUSH",
          "body": "```\r\n(llm-cpp) C:\\AYUSH PANDEY\\xpu-smi-1.2.38-20240718.060120.0db09695_win>.\\env-check\r\nPython 3.11.9\r\n-----------------------------------------------------------------\r\ntransformers=4.43.3\r\n-----------------------------------------------------------------\r\ntorch=2.2.0+cpu\r\n-------------------------",
          "created_at": "2024-08-05T17:52:29Z"
        },
        {
          "author": "sgwhat",
          "body": "In your output, it shows `Error: Level Zero Initialization Error`, which is the reason why your Ollama cannot run the model. You may refer to [this guide](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/install_windows_gpu.md#optional-update-gpu-driver) to install prereq",
          "created_at": "2024-08-06T03:25:46Z"
        },
        {
          "author": "EMPERORAYUSH",
          "body": "@sgwhat So after installation (and completing the complete guide and running the qwen 1.8b model, how can I run it on ollama? Should I clear my miniforge and go again scrach? ",
          "created_at": "2024-08-06T15:00:47Z"
        },
        {
          "author": "EMPERORAYUSH",
          "body": "@sgwhat \r\ni created a new enviorment, downloaded all the pip packages and then ran the python code. But after importing ipexllm.transformers, i saw this error immidietly (although the import was successful) :\r\n`C:\\Users\\ayush\\miniforge-pypy3\\envs\\llm\\Lib\\site-packages\\intel_extension_for_pytorch\\xpu",
          "created_at": "2024-08-06T15:52:49Z"
        }
      ]
    },
    {
      "issue_number": 11764,
      "title": "GPU memory continue increase when in Deepspeed TP benchmark ",
      "body": "GPU memory usage continue increase when run Deepspeed TP benchmark on 4 ARC GPU.\r\nQwen1.5-14B-Chat with sys_int4\r\nWill cause out of memory in the latest test.\r\n\r\n![err1](https://github.com/user-attachments/assets/ebfdc342-e7f1-44a8-b1bd-aa080a2c159e)\r\n![err2](https://github.com/user-attachments/assets/f0f6266a-9d8c-453c-ae5b-deba27d5d012)\r\n\r\n\r\n\r\n",
      "state": "closed",
      "author": "oldmikeyang",
      "author_type": "User",
      "created_at": "2024-08-12T07:16:30Z",
      "updated_at": "2024-08-14T01:07:17Z",
      "closed_at": "2024-08-14T01:06:23Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11764/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Uxito-Ada"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11764",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11764",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:39.321643",
      "comments": [
        {
          "author": "Uxito-Ada",
          "body": "Hi @oldmikeyang , could you share your `config.yaml` file under all-in-one directory, as configurations e.g. in_out_pairs and batch_size also influence the memory consumption. We will then try to reproduce your issue.",
          "created_at": "2024-08-13T01:36:00Z"
        },
        {
          "author": "oldmikeyang",
          "body": "Resolve this issue after set the following ENV.\r\nPlease close this issue.\r\n\r\nexport IPEX_LLM_QUANTIZE_KV_CACHE=1\r\nexport IPEX_LLM_LOW_MEM=1",
          "created_at": "2024-08-13T10:18:36Z"
        },
        {
          "author": "oldmikeyang",
          "body": "Resolve this issue after set the following ENV.\r\n\r\nexport IPEX_LLM_QUANTIZE_KV_CACHE=1\r\nexport IPEX_LLM_LOW_MEM=1",
          "created_at": "2024-08-14T01:07:16Z"
        }
      ]
    },
    {
      "issue_number": 11771,
      "title": "ollama runs gemma:2b, asks a question, does not answer, and reports no error.",
      "body": "设备：intel n97\r\n内存：8gb\r\nipex-llm版本:0811\r\n现象：能够正常加载并运行gemma:2b,但是提问没有回答，也没有报错信息。\r\n如果是使用qwen2:1.5b是能够正常回答的。\r\n![99dd79b9f5f5e8d2df0d5543cf29965](https://github.com/user-attachments/assets/47504fb7-2a73-470c-8b81-858660e6dae7)\r\n",
      "state": "closed",
      "author": "Jieszs",
      "author_type": "User",
      "created_at": "2024-08-13T01:47:34Z",
      "updated_at": "2024-08-13T08:02:08Z",
      "closed_at": "2024-08-13T08:02:08Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11771/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "rnwang04"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11771",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11771",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:39.554229",
      "comments": [
        {
          "author": "rnwang04",
          "body": "Hi @Jieszs ,\r\nI have reproduced this issue, and this issue seems be caused by current version of ipex-llm ollama has something wrong with gemma2:2b's default template.\r\nBelow is a workaround for gemma2:2b:\r\n```\r\n# Modelfile\r\nFROM gemma2:2b\r\ntemplate \"\"\"<start_of_turn>user\r\n{{ if .System }}{{ .System",
          "created_at": "2024-08-13T06:58:09Z"
        },
        {
          "author": "Jieszs",
          "body": "Thanks,it works",
          "created_at": "2024-08-13T07:17:32Z"
        }
      ]
    },
    {
      "issue_number": 11750,
      "title": "Please provide a method to benchmark Multimodal InternVL-4B on MTL‘s iGPU",
      "body": "Model link：[OpenGVLab/InternVL2-4B · Hugging Face](https://huggingface.co/OpenGVLab/InternVL2-4B)\r\n\r\n",
      "state": "closed",
      "author": "zhouzhaojing",
      "author_type": "User",
      "created_at": "2024-08-09T06:51:42Z",
      "updated_at": "2024-08-13T06:33:22Z",
      "closed_at": "2024-08-13T05:21:50Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11750/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "MeouSker77"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11750",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11750",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:39.826775",
      "comments": [
        {
          "author": "MeouSker77",
          "body": "You can run it following the example in this PR <https://github.com/intel-analytics/ipex-llm/pull/11718>\r\n\r\nFow now, our benchmark script doesn't support multimodal benchmark.\r\n\r\nYou can set `max_new_tokens=1` to get time `t1`, `t1` is the 1st token latency, \r\n\r\nthen set `max_new_tokens=128` to get ",
          "created_at": "2024-08-09T07:22:42Z"
        },
        {
          "author": "zhouzhaojing",
          "body": "Thanks clear response for this request.\r\nNow benchmark codes can work on my MTL Ultra 7 155H very well.\r\nWe mainly test iGPU and load model to \"xpu\", obtained:\r\n(loaded prompt and picture)\r\nFor int4-sym optimized, we got first token latency: 1.09s , token generation speed: 82.1 tokens/s\r\nFor int8-sy",
          "created_at": "2024-08-13T05:21:50Z"
        },
        {
          "author": "MeouSker77",
          "body": "glad to hear that you can run benchmark sucessfully, but it's impossible to get 82 token/s when running a 4B model on MTL, it's too fast\r\n\r\nmaybe you should check whether it really generated 128 tokens when you set `max_new_tokens` to 128, generation will stop earlier if it generated a `eos` token, ",
          "created_at": "2024-08-13T05:58:53Z"
        },
        {
          "author": "zhouzhaojing",
          "body": "Yup, you are right..\r\nOutput tokens are not achieving max tokens limitation, so real output tokens are less than the max token limitation I set.\r\nEventually, wrong token generation time is shorter so token generation speed is faster than the actual speed.\r\nI re-tested these two benchmark, obtained:\r",
          "created_at": "2024-08-13T06:16:16Z"
        },
        {
          "author": "zhouzhaojing",
          "body": "BTW, these is a waring still existing ..\r\nFlash attention is not available, I didn't found a way to easily install flash attention on Windows 11.\r\nPlease let me know whether Flash attention make an impact on our benchmarks result ?",
          "created_at": "2024-08-13T06:31:46Z"
        }
      ]
    },
    {
      "issue_number": 11731,
      "title": "Could you help to optimize MiniCPM-V-2_6?",
      "body": "Hi, I run MiniCPM-V-2_6 on MTL iGPU. But it is a little slow and needs to optimize.\r\nCould you help to optimize MiniCPM-V-2_6?\r\nThanks.\r\n\r\nWhen using FP16 to caculate, get the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\test\\Documents\\LLM\\cpm.py\", line 332, in <module>\r\n    res = model.chat(\r\n  File \"C:\\Users\\test\\.cache\\huggingface\\modules\\transformers_modules\\MiniCPM-V-2_6\\modeling_minicpmv.py\", line 378, in chat\r\n    res = self.generate(\r\n  File \"C:\\Users\\test\\.cache\\huggingface\\modules\\transformers_modules\\MiniCPM-V-2_6\\modeling_minicpmv.py\", line 262, in generate\r\n    result = self._decode(model_inputs[\"inputs_embeds\"], tokenizer, attention_mask, decode_text=decode_text, **kwargs)\r\n  File \"C:\\Users\\test\\.cache\\huggingface\\modules\\transformers_modules\\MiniCPM-V-2_6\\modeling_minicpmv.py\", line 186, in _decode\r\n    output = self.llm.generate(\r\n  File \"C:\\Users\\test\\miniforge3\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\test\\miniforge3\\lib\\site-packages\\ipex_llm\\transformers\\lookup.py\", line 88, in generate\r\n    return original_generate(self,\r\n  File \"C:\\Users\\test\\miniforge3\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\test\\miniforge3\\lib\\site-packages\\ipex_llm\\transformers\\speculative.py\", line 109, in generate\r\n    return original_generate(self,\r\n  File \"C:\\Users\\test\\miniforge3\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\test\\miniforge3\\lib\\site-packages\\ipex_llm\\transformers\\pipeline_parallel.py\", line 280, in generate\r\n    return original_generate(self,\r\n  File \"C:\\Users\\test\\miniforge3\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\test\\miniforge3\\lib\\site-packages\\transformers\\generation\\utils.py\", line 1592, in generate\r\n    return self.sample(\r\n  File \"C:\\Users\\test\\miniforge3\\lib\\site-packages\\transformers\\generation\\utils.py\", line 2734, in sample\r\n    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\r\nRuntimeError: probability tensor contains either `inf`, `nan` or element < 0\r\n```\r\nWe can use FP16 to caculate for this model, right?\r\n",
      "state": "closed",
      "author": "violet17",
      "author_type": "User",
      "created_at": "2024-08-07T05:33:28Z",
      "updated_at": "2024-08-13T03:16:55Z",
      "closed_at": "2024-08-13T03:08:00Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11731/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "MeouSker77"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11731",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11731",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:40.068219",
      "comments": [
        {
          "author": "MeouSker77",
          "body": "solved in latest nightly released ipex_llm ",
          "created_at": "2024-08-13T03:08:00Z"
        },
        {
          "author": "violet17",
          "body": "Thank you very much!",
          "created_at": "2024-08-13T03:16:54Z"
        }
      ]
    },
    {
      "issue_number": 11756,
      "title": "minicpm-v-2-6 can't run on A770 Ubuntu",
      "body": "A770 Ubuntu22.04\r\n\r\ncpm.py\r\n```\r\nimport torch\r\nfrom PIL import Image\r\nfrom ipex_llm.transformers import AutoModel\r\n#from transformers import AutoModel\r\nfrom transformers import AutoTokenizer\r\nimport time\r\n\r\nmodel_path = \"./models/MiniCPM-V-2_6\"\r\nmodel = AutoModel.from_pretrained(model_path, trust_remote_code=True, load_in_low_bit=\"asym_int4\") #\"fp8\",\r\n                               #   optimize_model=True, modules_to_not_convert=[\"vpm\", \"resampler\"])\r\n\r\n\r\nmodel = model.eval()\r\n#model = model.float()\r\n\r\nmodel = model.half() # /transformers/generation/utils.py\", line 2415, in _sample\r\n                    #   next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\r\n                    #  RuntimeError: probability tensor contains either `inf`, `nan` or element < 0\r\n#model = model.bfloat16() #RuntimeError: unsupported dtype, only fp32 and fp16 are supported\r\nmodel = model.to('xpu')\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\r\n\r\ndef run_minicpm(image_path,question):\r\n    image = Image.open(image_path).convert('RGB')    \r\n    msgs = [{'role': 'user', 'content': question}]\r\n    torch.xpu.synchronize()\r\n    timeStart = time.time()\r\n    \r\n    res = model.chat(\r\n        image=image,\r\n        msgs=msgs,\r\n        tokenizer=tokenizer,\r\n        sampling=True,\r\n        stream=True,\r\n        temperature=0.7\r\n    )\r\n\r\n    timeFirstRecord = False\r\n    \r\n    generated_text = \"\"\r\n    for new_text in res:\r\n        if timeFirstRecord == False:\r\n            torch.xpu.synchronize()\r\n            timeFirst = time.time() - timeStart\r\n            timeFirstRecord = True\r\n        generated_text += new_text\r\n        print(new_text, flush=True, end='')\r\n\r\n    torch.xpu.synchronize()\r\n    timeCost = time.time() - timeStart\r\n    token_count_input = len(tokenizer.tokenize(question))\r\n    token_count_output = len(tokenizer.tokenize(generated_text))\r\n\r\n    ms_first_token = timeFirst# * 1000\r\n    ms_rest_token = (timeCost - timeFirst) / (token_count_output - 1+1e-8) * 1000\r\n    print(\"\\ninput: \", question)\r\n    print(\"output: \", generated_text)\r\n    print(\"token count input: \", token_count_input)\r\n    print(\"token count output: \", token_count_output)\r\n    print(\"time cost(s): \", timeCost)\r\n    print(\"First token latency(s): \", ms_first_token)\r\n    print(\"After token latency(ms/token)\", ms_rest_token)\r\n    print(\"output token/s: \", token_count_output/timeCost)\r\n    print(\"output char/s\",len(generated_text)/timeCost)\r\n    print(\"******** image path = \",image_path)\r\n    print(\"_______________\")\r\n   # print(res)\r\n\r\n#run_minicpm('./test_image/guo.png','What are in the image?')\r\nrun_minicpm('./mini_0730/cat.JPG','这是什么品种的猫')\r\n```\r\n\r\n\r\n```\r\n(ipex3.10) test@root1-Z690-AORUS-ELITE-DDR4:~/cpm$ pip list\r\nPackage                       Version\r\n----------------------------- ------------------\r\naccelerate                    0.31.0\r\naiofiles                      23.2.1\r\naiohappyeyeballs              2.3.5\r\naiohttp                       3.10.1\r\naiolimiter                    1.1.0\r\naiosignal                     1.3.1\r\nalabaster                     0.7.13\r\nalbucore                      0.0.9\r\nalbumentations                1.4.8\r\nannotated-types               0.7.0\r\nantlr4-python3-runtime        4.9.3\r\nanyio                         4.4.0\r\nanytree                       2.12.1\r\narchspec                      0.2.3\r\nasttokens                     2.4.1\r\nasync-timeout                 4.0.3\r\nattrs                         23.2.0\r\nautograd                      1.6.2\r\nazure-common                  1.1.28\r\nazure-core                    1.30.2\r\nazure-identity                1.17.1\r\nazure-search-documents        11.5.0\r\nazure-storage-blob            12.21.0\r\nBabel                         2.12.1\r\nbeartype                      0.18.5\r\nbigdl-core-xe-21              2.5.0b20240807\r\nbigdl-core-xe-addons-21       2.5.0b20240807\r\nbigdl-core-xe-batch-21        2.5.0b20240807\r\nboltons                       23.1.1\r\nBrotli                        1.1.0\r\ncachetools                    5.4.0\r\ncertifi                       2024.2.2\r\ncffi                          1.16.0\r\ncharset-normalizer            3.3.2\r\nclick                         8.1.7\r\ncloudpickle                   3.0.0\r\ncolorama                      0.4.6\r\ncoloredlogs                   15.0.1\r\ncontourpy                     1.2.1\r\ncramjam                       2.8.3\r\ncryptography                  43.0.0\r\ncycler                        0.12.1\r\nCython                        3.0.10\r\ndacite                        1.8.1\r\ndask                          2024.7.1\r\ndask-expr                     1.1.9\r\ndatasets                      2.20.0\r\ndatashaper                    0.0.49\r\ndecorator                     5.1.1\r\ndeprecation                   2.1.0\r\ndevtools                      0.12.2\r\ndiffusers                     0.29.0\r\ndill                          0.3.8\r\ndiskcache                     5.6.3\r\ndistlib                       0.3.7\r\ndistro                        1.9.0\r\ndocstring_parser              0.16\r\ndocutils                      0.18.1\r\neasydict                      1.13\r\neinops                        0.8.0\r\nenvirons                      11.0.0\r\nexceptiongroup                1.2.2\r\nexecuting                     2.0.1\r\nfairscale                     0.4.13\r\nfastapi                       0.112.0\r\nfastparquet                   2024.5.0\r\nffmpy                         0.4.0\r\nfilelock                      3.14.0\r\nfire                          0.6.0\r\nflatbuffers                   24.3.25\r\nfonttools                     4.53.0\r\nfrozenlist                    1.4.1\r\nfsspec                        2024.5.0\r\nftfy                          6.2.0\r\nfuture                        1.0.0\r\ngensim                        4.3.3\r\ngradio                        4.40.0\r\ngradio_client                 1.2.0\r\ngraspologic                   3.4.1\r\ngraspologic-native            1.2.1\r\nh11                           0.14.0\r\nh5py                          3.11.0\r\nhttpcore                      1.0.5\r\nhttpx                         0.27.0\r\nhuggingface-hub               0.23.3\r\nhumanfriendly                 10.0\r\nhyppo                         0.4.0\r\nidna                          3.6\r\nimageio                       2.34.1\r\nimagesize                     1.4.1\r\nimportlib_metadata            7.1.0\r\nimportlib_resources           6.4.0\r\ninsightface                   0.7.3\r\nintel-cmplr-lib-ur            2024.2.1\r\nintel-extension-for-pytorch   2.1.10+xpu\r\nintel-openmp                  2024.2.1\r\nipex-llm                      2.1.0b20240807\r\nisodate                       0.6.1\r\nJinja2                        3.1.4\r\njoblib                        1.4.2\r\njsonpatch                     1.33\r\njsonpointer                   2.4\r\njsonschema                    4.23.0\r\njsonschema-specifications     2023.12.1\r\nkiwisolver                    1.4.5\r\nlancedb                       0.9.0\r\nlazy_loader                   0.4\r\nlinkify-it-py                 2.0.3\r\nllvmlite                      0.43.0\r\nlocket                        1.0.0\r\nmarkdown-it-py                3.0.0\r\nMarkupSafe                    2.1.5\r\nmarshmallow                   3.21.3\r\nmatplotlib                    3.9.0\r\nmdit-py-plugins               0.4.1\r\nmdurl                         0.1.2\r\nmeson                         1.2.0\r\nmpmath                        1.3.0\r\nmsal                          1.30.0\r\nmsal-extensions               1.2.0\r\nmultidict                     6.0.5\r\nmultiprocess                  0.70.16\r\nnest-asyncio                  1.6.0\r\nnetworkx                      3.3\r\nnltk                          3.8.1\r\nnumba                         0.60.0\r\nnumpy                         1.26.4\r\nollama                        0.3.0\r\nomegaconf                     2.3.0\r\nonnx                          1.16.1\r\nonnxruntime                   1.18.0\r\nopen_clip_torch               2.26.1\r\nopenai                        1.37.1\r\nopencv-python                 4.10.0.82\r\nopencv-python-headless        4.10.0.82\r\norjson                        3.10.6\r\noverrides                     7.7.0\r\npackaging                     24.0\r\npandas                        2.2.2\r\npartd                         1.4.2\r\npatsy                         0.5.6\r\npillow                        10.3.0\r\npip                           24.0\r\nplatformdirs                  4.2.0\r\nplotly                        5.23.0\r\npluggy                        1.4.0\r\nportalocker                   2.10.1\r\nPOT                           0.9.4\r\nprettytable                   3.10.0\r\nprotobuf                      5.27.1\r\npsutil                        5.9.8\r\npy                            1.11.0\r\npy-cpuinfo                    9.0.0\r\npyaml-env                     1.2.1\r\npyarrow                       15.0.0\r\npyarrow-hotfix                0.6\r\npycosat                       0.6.6\r\npycparser                     2.21\r\npydantic                      2.7.3\r\npydantic_core                 2.18.4\r\npydub                         0.25.1\r\nPygments                      2.18.0\r\nPyJWT                         2.8.0\r\npylance                       0.13.0\r\npynndescent                   0.5.13\r\npyparsing                     3.1.2\r\npyreadline3                   3.4.1\r\nPySocks                       1.7.1\r\npython-dateutil               2.9.0.post0\r\npython-dotenv                 1.0.1\r\npython-multipart              0.0.9\r\npytz                          2024.1\r\nPyYAML                        6.0.1\r\nratelimiter                   1.2.0.post0\r\nreferencing                   0.35.1\r\nregex                         2024.5.15\r\nrequests                      2.32.3\r\nretry                         0.9.2\r\nrich                          13.7.1\r\nrpds-py                       0.19.1\r\nruamel.yaml                   0.18.6\r\nruamel.yaml.clib              0.2.8\r\nruff                          0.5.6\r\nsafetensors                   0.4.3\r\nscikit-image                  0.23.2\r\nscikit-learn                  1.5.0\r\nscipy                         1.12.0\r\nseaborn                       0.13.2\r\nsemantic-version              2.10.0\r\nsentencepiece                 0.2.0\r\nsetuptools                    69.2.0\r\nshapely                       2.0.5\r\nshellingham                   1.5.4\r\nshtab                         1.7.1\r\nsix                           1.16.0\r\nsmart-open                    7.0.4\r\nsniffio                       1.3.1\r\nsnowballstemmer               2.2.0\r\nSphinx                        6.2.1\r\nsphinx-rtd-theme              1.2.2\r\nsphinxcontrib-applehelp       1.0.4\r\nsphinxcontrib-devhelp         1.0.2\r\nsphinxcontrib-htmlhelp        2.0.1\r\nsphinxcontrib-jquery          4.1\r\nsphinxcontrib-jsmath          1.0.1\r\nsphinxcontrib-qthelp          1.0.3\r\nsphinxcontrib-serializinghtml 1.1.5\r\nstarlette                     0.37.2\r\nstatsmodels                   0.14.2\r\nswifter                       1.4.0\r\nsympy                         1.12.1\r\ntabulate                      0.9.0\r\ntenacity                      8.5.0\r\ntermcolor                     2.4.0\r\ntextual                       0.70.0\r\nthreadpoolctl                 3.5.0\r\ntifffile                      2024.5.22\r\ntiktoken                      0.7.0\r\ntimm                          1.0.7\r\ntokenizers                    0.19.1\r\ntomli                         2.0.1\r\ntomlkit                       0.12.0\r\ntoolz                         0.12.1\r\ntorch                         2.1.0a0+cxx11.abi\r\ntorchvision                   0.16.0a0+cxx11.abi\r\ntqdm                          4.66.5\r\ntransformers                  4.40.0\r\ntrl                           0.9.6\r\ntruststore                    0.8.0\r\ntyper                         0.12.3\r\ntyping_extensions             4.12.2\r\ntyro                          0.8.5\r\ntzdata                        2024.1\r\nuc-micro-py                   1.0.3\r\numap-learn                    0.5.6\r\nurllib3                       2.2.1\r\nuvicorn                       0.30.5\r\nvirtualenv                    20.24.2\r\nwcwidth                       0.2.13\r\nwebsockets                    12.0\r\nwheel                         0.43.0\r\nwin-inet-pton                 1.1.0\r\nwrapt                         1.16.0\r\nxxhash                        3.4.1\r\nyarl                          1.9.4\r\nzipp                          3.19.2\r\nzstandard                     0.22.0\r\n\r\n[notice] A new release of pip is available: 24.0 -> 24.2\r\n[notice] To update, run: pip install --upgrade pip\r\n```\r\n\r\n```\r\n/home/test/miniforge3/envs/ipex3.10/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\r\n  warnings.warn(\r\n/home/test/miniforge3/envs/ipex3.10/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n  warn(\r\n2024-08-10 22:37:59,403 - INFO - intel_extension_for_pytorch auto imported\r\n2024-08-10 22:37:59,479 - INFO - vision_config is None, using default vision config\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████| 4/4 [00:00<00:00, 13.40it/s]\r\n2024-08-10 22:38:00,451 - INFO - Converting the current model to asym_int4 format......\r\n/home/test/miniforge3/envs/ipex3.10/lib/python3.10/site-packages/torch/nn/init.py:412: UserWarning: Initializing zero-element tensors is a no-op\r\n  warnings.warn(\"Initializing zero-element tensors is a no-op\")\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n哈哈哈哈The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\r\nException in thread Thread-4 (generate):\r\nTraceback (most recent call last):\r\n  File \"/home/test/miniforge3/envs/ipex3.10/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\r\n    self.run()\r\n  File \"/home/test/miniforge3/envs/ipex3.10/lib/python3.10/threading.py\", line 953, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/test/miniforge3/envs/ipex3.10/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/test/miniforge3/envs/ipex3.10/lib/python3.10/site-packages/ipex_llm/transformers/lookup.py\", line 88, in generate\r\n    return original_generate(self,\r\n  File \"/home/test/miniforge3/envs/ipex3.10/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/test/miniforge3/envs/ipex3.10/lib/python3.10/site-packages/ipex_llm/transformers/speculative.py\", line 109, in generate\r\n    return original_generate(self,\r\n  File \"/home/test/miniforge3/envs/ipex3.10/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/test/miniforge3/envs/ipex3.10/lib/python3.10/site-packages/ipex_llm/transformers/pipeline_parallel.py\", line 281, in generate\r\n    return original_generate(self,\r\n  File \"/home/test/miniforge3/envs/ipex3.10/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/test/miniforge3/envs/ipex3.10/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1622, in generate\r\n    result = self._sample(\r\n  File \"/home/test/miniforge3/envs/ipex3.10/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2829, in _sample\r\n    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\r\nRuntimeError: probability tensor contains either `inf`, `nan` or element < 0\r\n\r\n```\r\n",
      "state": "closed",
      "author": "biyuehuang",
      "author_type": "User",
      "created_at": "2024-08-10T13:56:11Z",
      "updated_at": "2024-08-13T03:09:56Z",
      "closed_at": "2024-08-13T03:08:50Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11756/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "MeouSker77"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11756",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11756",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:40.348286",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "similar issue: https://github.com/intel-analytics/ipex-llm/issues/11731",
          "created_at": "2024-08-12T02:10:27Z"
        },
        {
          "author": "jason-dai",
          "body": "> similar issue: #11731\r\n\r\nI think it is already supported; we need to add an example",
          "created_at": "2024-08-12T02:24:09Z"
        },
        {
          "author": "MeouSker77",
          "body": "solved in offline discussion",
          "created_at": "2024-08-13T03:08:50Z"
        },
        {
          "author": "biyuehuang",
          "body": "verified OK on other A770 Ubuntu device\r\n\r\n```\r\nsource /opt/intel/oneapi/2024.0/oneapi-vars.sh\r\nconda create -n ipex-llm python=3.11\r\nconda activate ipex-llm\r\npip install --pre --upgrade ipex-llm[xpu] --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/cn/\r\n\r\npip install tim",
          "created_at": "2024-08-13T03:09:56Z"
        }
      ]
    },
    {
      "issue_number": 11744,
      "title": "Run InternLM2 , reports error:TypeError: internlm2_attention_forward() got an unexpected keyword argument 'cache_position'",
      "body": "When ran InternLM2 inference , it reported errors as below:\r\n\r\noneAPI :2024.0.1.46\r\nipex-llm: 2.1.0b2\r\ntransformers: 4.37.2 ,4.38.2 \r\n\r\n---------------------------------------------------------------------------\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[1], line 33\r\n     31 input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to('xpu')\r\n     32 # ipex_llm model needs a warmup, then inference time can be accurate\r\n---> 33 output = model.generate(input_ids,max_new_tokens=32)\r\n     34 # if your selected model is capable of utilizing previous key[/value](http://127.0.0.1:8898/value) attentions\r\n     35 # to enhance decoding speed, but has `\"use_cache\": false` in its model config,\r\n     36 # it is important to set `use_cache=True` explicitly in the `generate` function\r\n     37 # to obtain optimal performance with IPEX-LLM INT4 optimizations\r\n     38 output = model.generate(input_ids,max_new_tokens=32)\r\n\r\nFile [~/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/torch/utils/_contextlib.py:115](http://127.0.0.1:8898/home/arc/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/torch/utils/_contextlib.py#line=114), in context_decorator.<locals>.decorate_context(*args, **kwargs)\r\n    112 @functools.wraps(func)\r\n    113 def decorate_context(*args, **kwargs):\r\n    114     with ctx_factory():\r\n--> 115         return func(*args, **kwargs)\r\n\r\nFile [~/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/ipex_llm/transformers/lookup.py:88](http://127.0.0.1:8898/home/arc/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/ipex_llm/transformers/lookup.py#line=87), in generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\r\n     77             logger.warning(\"Since you call the generate with lookahead parameter, \"\r\n     78                            f\"Speculative decoding parameters {spec_params} are \"\r\n     79                            \"removed in the generation.\")\r\n     80         return self.lookup_generate(inputs=inputs,\r\n     81                                     num_output_tokens=lookahead,\r\n     82                                     generation_config=generation_config,\r\n   (...)\r\n     85                                     prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\r\n     86                                     **kwargs)\r\n---> 88 return original_generate(self,\r\n     89                          inputs=inputs,\r\n     90                          generation_config=generation_config,\r\n     91                          logits_processor=logits_processor,\r\n     92                          stopping_criteria=stopping_criteria,\r\n     93                          prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\r\n     94                          synced_gpus=synced_gpus,\r\n     95                          assistant_model=assistant_model,\r\n     96                          streamer=streamer,\r\n     97                          **kwargs)\r\n\r\nFile [~/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/torch/utils/_contextlib.py:115](http://127.0.0.1:8898/home/arc/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/torch/utils/_contextlib.py#line=114), in context_decorator.<locals>.decorate_context(*args, **kwargs)\r\n    112 @functools.wraps(func)\r\n    113 def decorate_context(*args, **kwargs):\r\n    114     with ctx_factory():\r\n--> 115         return func(*args, **kwargs)\r\n\r\nFile [~/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/ipex_llm/transformers/speculative.py:109](http://127.0.0.1:8898/home/arc/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/ipex_llm/transformers/speculative.py#line=108), in generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\r\n    105 for var in ['max_step_draft', 'th_stop_draft', 'hf_adjust',\r\n    106             'auto_th_stop_draft', 'auto_parameters', 'min_step_draft',\r\n    107             'th_batch_num']:\r\n    108     kwargs.pop(var, None)\r\n--> 109 return original_generate(self,\r\n    110                          inputs=inputs,\r\n    111                          generation_config=generation_config,\r\n    112                          logits_processor=logits_processor,\r\n    113                          stopping_criteria=stopping_criteria,\r\n    114                          prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\r\n    115                          synced_gpus=synced_gpus,\r\n    116                          assistant_model=assistant_model,\r\n    117                          streamer=streamer,\r\n    118                          **kwargs)\r\n\r\nFile [~/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/torch/utils/_contextlib.py:115](http://127.0.0.1:8898/home/arc/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/torch/utils/_contextlib.py#line=114), in context_decorator.<locals>.decorate_context(*args, **kwargs)\r\n    112 @functools.wraps(func)\r\n    113 def decorate_context(*args, **kwargs):\r\n    114     with ctx_factory():\r\n--> 115         return func(*args, **kwargs)\r\n\r\nFile [~/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/ipex_llm/transformers/pipeline_parallel.py:280](http://127.0.0.1:8898/home/arc/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/ipex_llm/transformers/pipeline_parallel.py#line=279), in generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\r\n    273         max_new_tokens = kwargs.pop(\"max_new_tokens\", None)\r\n    275     return self.pipeline_parallel_generate(inputs=inputs,\r\n    276                                            max_new_tokens=max_new_tokens,\r\n    277                                            generation_config=generation_config,\r\n    278                                            **kwargs)\r\n--> 280 return original_generate(self,\r\n    281                          inputs=inputs,\r\n    282                          generation_config=generation_config,\r\n    283                          logits_processor=logits_processor,\r\n    284                          stopping_criteria=stopping_criteria,\r\n    285                          prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\r\n    286                          synced_gpus=synced_gpus,\r\n    287                          assistant_model=assistant_model,\r\n    288                          streamer=streamer,\r\n    289                          **kwargs)\r\n\r\nFile [~/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/torch/utils/_contextlib.py:115](http://127.0.0.1:8898/home/arc/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/torch/utils/_contextlib.py#line=114), in context_decorator.<locals>.decorate_context(*args, **kwargs)\r\n    112 @functools.wraps(func)\r\n    113 def decorate_context(*args, **kwargs):\r\n    114     with ctx_factory():\r\n--> 115         return func(*args, **kwargs)\r\n\r\nFile [~/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/transformers/generation/utils.py:1544](http://127.0.0.1:8898/home/arc/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/transformers/generation/utils.py#line=1543), in GenerationMixin.generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\r\n   1526     return self.assisted_decoding(\r\n   1527         input_ids,\r\n   1528         candidate_generator=candidate_generator,\r\n   (...)\r\n   1540         **model_kwargs,\r\n   1541     )\r\n   1542 if generation_mode == GenerationMode.GREEDY_SEARCH:\r\n   1543     # 11. run greedy search\r\n-> 1544     return self.greedy_search(\r\n   1545         input_ids,\r\n   1546         logits_processor=prepared_logits_processor,\r\n   1547         stopping_criteria=prepared_stopping_criteria,\r\n   1548         pad_token_id=generation_config.pad_token_id,\r\n   1549         eos_token_id=generation_config.eos_token_id,\r\n   1550         output_scores=generation_config.output_scores,\r\n   1551         output_logits=generation_config.output_logits,\r\n   1552         return_dict_in_generate=generation_config.return_dict_in_generate,\r\n   1553         synced_gpus=synced_gpus,\r\n   1554         streamer=streamer,\r\n   1555         **model_kwargs,\r\n   1556     )\r\n   1558 elif generation_mode == GenerationMode.CONTRASTIVE_SEARCH:\r\n   1559     if not model_kwargs[\"use_cache\"]:\r\n\r\nFile [~/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/transformers/generation/utils.py:2404](http://127.0.0.1:8898/home/arc/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/transformers/generation/utils.py#line=2403), in GenerationMixin.greedy_search(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\r\n   2401 model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\r\n   2403 # forward pass to get next token\r\n-> 2404 outputs = self(\r\n   2405     **model_inputs,\r\n   2406     return_dict=True,\r\n   2407     output_attentions=output_attentions,\r\n   2408     output_hidden_states=output_hidden_states,\r\n   2409 )\r\n   2411 if synced_gpus and this_peer_finished:\r\n   2412     continue  # don't waste resources running the code we don't need\r\n\r\nFile [~/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/torch/nn/modules/module.py:1518](http://127.0.0.1:8898/home/arc/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/torch/nn/modules/module.py#line=1517), in Module._wrapped_call_impl(self, *args, **kwargs)\r\n   1516     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\r\n   1517 else:\r\n-> 1518     return self._call_impl(*args, **kwargs)\r\n\r\nFile [~/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/torch/nn/modules/module.py:1527](http://127.0.0.1:8898/home/arc/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/torch/nn/modules/module.py#line=1526), in Module._call_impl(self, *args, **kwargs)\r\n   1522 # If we don't have any hooks, we want to skip the rest of the logic in\r\n   1523 # this function, and just call forward.\r\n   1524 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\r\n   1525         or _global_backward_pre_hooks or _global_backward_hooks\r\n   1526         or _global_forward_hooks or _global_forward_pre_hooks):\r\n-> 1527     return forward_call(*args, **kwargs)\r\n   1529 try:\r\n   1530     result = None\r\n\r\nFile [~/.cache/huggingface/modules/transformers_modules/internlm2-chat-7b/modeling_internlm2.py:1204](http://127.0.0.1:8898/home/arc/.cache/huggingface/modules/transformers_modules/internlm2-chat-7b/modeling_internlm2.py#line=1203), in InternLM2ForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\r\n   1201 return_dict = return_dict if return_dict is not None else self.config.use_return_dict\r\n   1203 # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\r\n-> 1204 outputs = self.model(\r\n   1205     input_ids=input_ids,\r\n   1206     attention_mask=attention_mask,\r\n   1207     position_ids=position_ids,\r\n   1208     past_key_values=past_key_values,\r\n   1209     inputs_embeds=inputs_embeds,\r\n   1210     use_cache=use_cache,\r\n   1211     output_attentions=output_attentions,\r\n   1212     output_hidden_states=output_hidden_states,\r\n   1213     return_dict=return_dict,\r\n   1214     cache_position=cache_position,\r\n   1215 )\r\n   1217 hidden_states = outputs[0]\r\n   1218 if self.config.pretraining_tp > 1:\r\n\r\nFile [~/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/torch/nn/modules/module.py:1518](http://127.0.0.1:8898/home/arc/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/torch/nn/modules/module.py#line=1517), in Module._wrapped_call_impl(self, *args, **kwargs)\r\n   1516     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\r\n   1517 else:\r\n-> 1518     return self._call_impl(*args, **kwargs)\r\n\r\nFile [~/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/torch/nn/modules/module.py:1527](http://127.0.0.1:8898/home/arc/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/torch/nn/modules/module.py#line=1526), in Module._call_impl(self, *args, **kwargs)\r\n   1522 # If we don't have any hooks, we want to skip the rest of the logic in\r\n   1523 # this function, and just call forward.\r\n   1524 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\r\n   1525         or _global_backward_pre_hooks or _global_backward_hooks\r\n   1526         or _global_forward_hooks or _global_forward_pre_hooks):\r\n-> 1527     return forward_call(*args, **kwargs)\r\n   1529 try:\r\n   1530     result = None\r\n\r\nFile [~/.cache/huggingface/modules/transformers_modules/internlm2-chat-7b/modeling_internlm2.py:1004](http://127.0.0.1:8898/home/arc/.cache/huggingface/modules/transformers_modules/internlm2-chat-7b/modeling_internlm2.py#line=1003), in InternLM2Model.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\r\n    993     layer_outputs = self._gradient_checkpointing_func(\r\n    994         decoder_layer.__call__,\r\n    995         hidden_states,\r\n   (...)\r\n   1001         cache_position,\r\n   1002     )\r\n   1003 else:\r\n-> 1004     layer_outputs = decoder_layer(\r\n   1005         hidden_states,\r\n   1006         attention_mask=causal_mask,\r\n   1007         position_ids=position_ids,\r\n   1008         past_key_value=past_key_values,\r\n   1009         output_attentions=output_attentions,\r\n   1010         use_cache=use_cache,\r\n   1011         cache_position=cache_position,\r\n   1012     )\r\n   1014 hidden_states = layer_outputs[0]\r\n   1016 if use_cache:\r\n\r\nFile [~/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/torch/nn/modules/module.py:1518](http://127.0.0.1:8898/home/arc/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/torch/nn/modules/module.py#line=1517), in Module._wrapped_call_impl(self, *args, **kwargs)\r\n   1516     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\r\n   1517 else:\r\n-> 1518     return self._call_impl(*args, **kwargs)\r\n\r\nFile [~/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/torch/nn/modules/module.py:1527](http://127.0.0.1:8898/home/arc/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/torch/nn/modules/module.py#line=1526), in Module._call_impl(self, *args, **kwargs)\r\n   1522 # If we don't have any hooks, we want to skip the rest of the logic in\r\n   1523 # this function, and just call forward.\r\n   1524 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\r\n   1525         or _global_backward_pre_hooks or _global_backward_hooks\r\n   1526         or _global_forward_hooks or _global_forward_pre_hooks):\r\n-> 1527     return forward_call(*args, **kwargs)\r\n   1529 try:\r\n   1530     result = None\r\n\r\nFile [~/.cache/huggingface/modules/transformers_modules/internlm2-chat-7b/modeling_internlm2.py:738](http://127.0.0.1:8898/home/arc/.cache/huggingface/modules/transformers_modules/internlm2-chat-7b/modeling_internlm2.py#line=737), in InternLM2DecoderLayer.forward(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\r\n    735 hidden_states = self.attention_norm(hidden_states)\r\n    737 # Self Attention\r\n--> 738 hidden_states, self_attn_weights, present_key_value = self.attention(\r\n    739     hidden_states=hidden_states,\r\n    740     attention_mask=attention_mask,\r\n    741     position_ids=position_ids,\r\n    742     past_key_value=past_key_value,\r\n    743     output_attentions=output_attentions,\r\n    744     use_cache=use_cache,\r\n    745     cache_position=cache_position,\r\n    746 )\r\n    747 hidden_states = residual + hidden_states\r\n    749 # Fully Connected\r\n\r\nFile [~/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/torch/nn/modules/module.py:1518](http://127.0.0.1:8898/home/arc/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/torch/nn/modules/module.py#line=1517), in Module._wrapped_call_impl(self, *args, **kwargs)\r\n   1516     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\r\n   1517 else:\r\n-> 1518     return self._call_impl(*args, **kwargs)\r\n\r\nFile [~/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/torch/nn/modules/module.py:1527](http://127.0.0.1:8898/home/arc/.miniconda_dev_zone/envs/notebook-zone/lib/python3.11/site-packages/torch/nn/modules/module.py#line=1526), in Module._call_impl(self, *args, **kwargs)\r\n   1522 # If we don't have any hooks, we want to skip the rest of the logic in\r\n   1523 # this function, and just call forward.\r\n   1524 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\r\n   1525         or _global_backward_pre_hooks or _global_backward_hooks\r\n   1526         or _global_forward_hooks or _global_forward_pre_hooks):\r\n-> 1527     return forward_call(*args, **kwargs)\r\n   1529 try:\r\n   1530     result = None\r\n\r\nTypeError: internlm2_attention_forward() got an unexpected keyword argument 'cache_position'\r\n\r\n====================================================================================\r\n\r\npip list \r\n\r\nPackage                       Version\r\n----------------------------- ------------------\r\naccelerate                    0.23.0\r\naddict                        2.4.0\r\naiofiles                      23.2.1\r\naiohttp                       3.9.5\r\naiosignal                     1.3.1\r\naliyun-python-sdk-core        2.15.1\r\naliyun-python-sdk-kms         2.16.3\r\naltair                        5.3.0\r\nannotated-types               0.7.0\r\nantlr4-python3-runtime        4.9.3\r\nanyio                         4.4.0\r\nasttokens                     2.4.1\r\nasync-timeout                 4.0.3\r\nattrs                         24.1.0\r\nbigdl-core-xe-21              2.1.0b2\r\nbigdl-core-xe-addons-21       2.1.0b2\r\nbigdl-core-xe-batch-21        2.1.0b2\r\nbitsandbytes                  0.43.1\r\ncertifi                       2024.7.4\r\ncffi                          1.16.0\r\ncharset-normalizer            3.3.2\r\nclick                         8.1.7\r\ncomm                          0.2.2\r\ncontourpy                     1.2.1\r\ncrcmod                        1.7\r\ncryptography                  42.0.8\r\ncycler                        0.12.1\r\ndatasets                      2.20.0\r\ndebugpy                       1.8.1\r\ndecorator                     5.1.1\r\ndill                          0.3.8\r\ndnspython                     2.6.1\r\ndocstring_parser              0.16\r\neinops                        0.8.0\r\nemail_validator               2.1.2\r\neval_type_backport            0.2.0\r\nexceptiongroup                1.2.1\r\nexecuting                     2.0.1\r\nfastapi                       0.111.0\r\nfastapi-cli                   0.0.4\r\nffmpy                         0.3.2\r\nfilelock                      3.15.4\r\nfonttools                     4.53.0\r\nfrozenlist                    1.4.1\r\nfsspec                        2024.6.1\r\ngast                          0.5.4\r\ngguf                          0.6.0\r\ngradio                        4.36.1\r\ngradio_client                 1.0.1\r\nh11                           0.14.0\r\nhttpcore                      1.0.5\r\nhttptools                     0.6.1\r\nhttpx                         0.27.0\r\nhuggingface-hub               0.24.5\r\nidna                          3.7\r\nimportlib_metadata            7.1.0\r\nimportlib_resources           6.4.0\r\nintel-cmplr-lib-ur            2024.2.1\r\nintel-extension-for-pytorch   2.1.10+xpu\r\nintel-openmp                  2024.2.1\r\nipex-llm                      2.1.0b2\r\nipykernel                     6.29.4\r\nipython                       8.18.1\r\nipywidgets                    8.1.3\r\njedi                          0.19.1\r\nJinja2                        3.1.4\r\njmespath                      0.10.0\r\njoblib                        1.4.2\r\njsonschema                    4.22.0\r\njsonschema-specifications     2023.12.1\r\njupyter_client                8.6.2\r\njupyter_core                  5.7.2\r\njupyterlab_widgets            3.0.11\r\nkiwisolver                    1.4.5\r\nkornia                        0.7.3\r\nkornia_rs                     0.1.5\r\nlatex2mathml                  3.77.0\r\nMarkdown                      3.6\r\nmarkdown-it-py                3.0.0\r\nMarkupSafe                    2.1.5\r\nmatplotlib                    3.9.0\r\nmatplotlib-inline             0.1.7\r\nmdtex2html                    1.3.0\r\nmdurl                         0.1.2\r\nmodelscope                    1.11.0\r\nmpmath                        1.3.0\r\nmultidict                     6.0.5\r\nmultiprocess                  0.70.16\r\nnest-asyncio                  1.6.0\r\nnetworkx                      3.2.1\r\nnltk                          3.8.1\r\nnumpy                         1.26.4\r\nomegaconf                     2.3.0\r\norjson                        3.10.5\r\noss2                          2.18.6\r\npackaging                     24.1\r\npandas                        2.2.2\r\nparso                         0.8.4\r\npeft                          0.12.0\r\npexpect                       4.9.0\r\npillow                        10.3.0\r\npip                           24.2\r\nplatformdirs                  4.2.2\r\nprompt_toolkit                3.0.47\r\nprotobuf                      4.25.3\r\npsutil                        6.0.0\r\nptyprocess                    0.7.0\r\npure-eval                     0.2.2\r\npy-cpuinfo                    9.0.0\r\npyarrow                       16.1.0\r\npyarrow-hotfix                0.6\r\npycparser                     2.22\r\npycryptodome                  3.20.0\r\npydantic                      2.7.4\r\npydantic_core                 2.18.4\r\npydub                         0.25.1\r\nPygments                      2.18.0\r\npyparsing                     3.1.2\r\npython-dateutil               2.9.0.post0\r\npython-dotenv                 1.0.1\r\npython-multipart              0.0.9\r\npytz                          2024.1\r\nPyYAML                        6.0.1\r\npyzmq                         26.0.3\r\nreferencing                   0.35.1\r\nregex                         2024.7.24\r\nrequests                      2.32.3\r\nrich                          13.7.1\r\nrpds-py                       0.18.1\r\nruff                          0.4.9\r\nsafetensors                   0.4.4\r\nscikit-learn                  1.5.0\r\nscipy                         1.13.1\r\nsemantic-version              2.10.0\r\nsentence-transformers         2.3.1\r\nsentencepiece                 0.2.0\r\nsetuptools                    69.5.1\r\nshellingham                   1.5.4\r\nshtab                         1.7.1\r\nsimplejson                    3.19.2\r\nsix                           1.16.0\r\nsniffio                       1.3.1\r\nsortedcontainers              2.4.0\r\nspandrel                      0.3.4\r\nsse-starlette                 2.1.3\r\nstack-data                    0.6.3\r\nstarlette                     0.37.2\r\nsympy                         1.12.1\r\ntabulate                      0.9.0\r\nthreadpoolctl                 3.5.0\r\ntiktoken                      0.7.0\r\ntimm                          1.0.7\r\ntokenizers                    0.15.2\r\ntomli                         2.0.1\r\ntomlkit                       0.12.0\r\ntoolz                         0.12.1\r\ntorch                         2.1.0a0+cxx11.abi\r\ntorchsde                      0.2.6\r\ntorchvision                   0.16.0a0+cxx11.abi\r\ntornado                       6.4.1\r\ntqdm                          4.66.5\r\ntraitlets                     5.14.3\r\ntrampoline                    0.1.2\r\ntransformers                  4.38.2\r\ntransformers-stream-generator 0.0.5\r\ntriton                        3.0.0\r\ntrl                           0.9.6\r\ntyper                         0.12.3\r\ntyping_extensions             4.12.2\r\ntyro                          0.8.5\r\ntzdata                        2024.1\r\nujson                         5.10.0\r\nurllib3                       2.2.2\r\nuvicorn                       0.30.1\r\nuvloop                        0.19.0\r\nviola                         0.3.8\r\nwatchfiles                    0.22.0\r\nwcwidth                       0.2.13\r\nwebsockets                    11.0.3\r\nwheel                         0.44.0\r\nwidgetsnbextension            4.0.11\r\nxxhash                        3.4.1\r\nyapf                          0.40.2\r\nyarl                          1.9.4\r\nzipp                          3.19.2\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "state": "open",
      "author": "johnysh",
      "author_type": "User",
      "created_at": "2024-08-08T08:37:40Z",
      "updated_at": "2024-08-12T08:55:04Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11744/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "RyuKosei"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11744",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11744",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:40.563928",
      "comments": [
        {
          "author": "hkvision",
          "body": "We have reproduced this issue and looking for solutions. Will keep you updated when it is resolved.",
          "created_at": "2024-08-09T06:28:52Z"
        },
        {
          "author": "johnysh",
          "body": "    model = AutoModelForCausalLM.from_pretrained(model_path,\r\n                                                 load_in_4bit=True,\r\n                                                 trust_remote_code=True,\r\n                                                 use_cache=True)\r\n\r\n     I set “load_in_4bit=Fa",
          "created_at": "2024-08-09T06:46:34Z"
        },
        {
          "author": "hkvision",
          "body": "Hi, after checking with our team, for internlm2, we only support v1.1.0 of this model: https://huggingface.co/internlm/internlm2-chat-7b/tree/v1.1.0, you can download this model version following the guide here: https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/GPU/HuggingFace",
          "created_at": "2024-08-12T02:47:15Z"
        },
        {
          "author": "johnysh",
          "body": "Follow this bkc ,and add  revision=\"v1.1.0\" ,it can run well.\r\nThx\r\n\r\n\r\nFrom: Kai Huang ***@***.***>\r\nSent: Monday, August 12, 2024 10:48 AM\r\nTo: intel-analytics/ipex-llm ***@***.***>\r\nCc: Shi, Junhan ***@***.***>; Author ***@***.***>\r\nSubject: Re: [intel-analytics/ipex-llm] Run InternLM2 , reports ",
          "created_at": "2024-08-12T08:48:34Z"
        },
        {
          "author": "hkvision",
          "body": "Glad to hear that it works!",
          "created_at": "2024-08-12T08:55:03Z"
        }
      ]
    },
    {
      "issue_number": 11743,
      "title": "vllm can‘t use oneCCL on host",
      "body": "Ubuntu22.04,kernel 5.15.0 with 4 * Arc A770 on Xeon(R) w9-3495X\r\n$ clinfo\r\nDriver Version                                  24.22.29735.27\r\n\r\nscript\r\n```\r\nsource /opt/intel/oneapi/2024.0/oneapi-vars.sh --force\r\nsource /opt/intel/1ccl-wks/setvars.sh --force  # use oneCCL\r\n\r\n\r\nexport MODEL=\"/opt/Meta-Llama-3-8B-Instruct\"\r\n\r\nexport CCL_WORKER_COUNT=2 ## 2 maybe means 2*A770\r\nexport FI_PROVIDER=shm\r\nexport CCL_ATL_TRANSPORT=ofi\r\nexport CCL_ZE_IPC_EXCHANGE=sockets\r\nexport CCL_ATL_SHM=1\r\nexport SYCL_CACHE_PERSISTENT=1\r\nexport SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1\r\n#export LD_PRELOAD=${LD_PRELOAD}:${CONDA_PREFIX}/lib/libtcmalloc.so:${LD_PRELOAD}\r\nexport ZE_AFFINITY_MASK=0,1\r\n\r\nfor n in $(seq 8 2 20); do\r\n    echo \"Model= $MODEL RATE= 0.7 N= $n...\"\r\n    python3 ./benchmark_throughput.py \\\r\n        --backend vllm \\\r\n        --dataset ./ShareGPT_V3_unfiltered_cleaned_split.json \\\r\n        --model $MODEL \\\r\n        --num-prompts 100 \\\r\n        --seed 42 \\\r\n        --trust-remote-code \\\r\n        --enforce-eager \\\r\n        --dtype float16 \\\r\n        --device xpu \\\r\n        --load-in-low-bit sym_int4 \\\r\n        --gpu-memory-utilization 0.7 \\\r\n        --max-num-seqs $n \\\r\n        --tensor-parallel-size 2  ## 2 means 2*A770\r\ndone\r\nsleep 10\r\nexit 0\r\n```\r\n\r\n\r\n```\r\n(ipex-vllm) test@adc-a770:~$ pip list\r\nPackage                       Version               Editable project location\r\n----------------------------- --------------------- -------------------------\r\naccelerate                    0.23.0\r\naiohttp                       3.9.5\r\naiosignal                     1.3.1\r\nannotated-types               0.7.0\r\nantlr4-python3-runtime        4.9.3\r\nanyio                         4.4.0\r\nattrs                         23.2.0\r\nbigdl-core-xe-21              2.5.0b20240805\r\nbigdl-core-xe-addons-21       2.5.0b20240805\r\nbigdl-core-xe-batch-21        2.5.0b20240805\r\ncertifi                       2024.7.4\r\ncharset-normalizer            3.3.2\r\nclick                         8.1.7\r\ncloudpickle                   3.0.0\r\ncmake                         3.30.0\r\ndeepspeed                     0.14.1+ed8aed57\r\ndiskcache                     5.6.3\r\ndnspython                     2.6.1\r\neinops                        0.8.0\r\nemail_validator               2.2.0\r\nfastapi                       0.111.1\r\nfastapi-cli                   0.0.4\r\nfilelock                      3.15.4\r\nfrozenlist                    1.4.1\r\nfsspec                        2024.6.1\r\nh11                           0.14.0\r\nhjson                         3.1.0\r\nhttpcore                      1.0.5\r\nhttptools                     0.6.1\r\nhttpx                         0.27.0\r\nhuggingface-hub               0.24.0\r\nidna                          3.7\r\nintel-cmplr-lib-ur            2024.2.0\r\nintel_extension_for_deepspeed 0.9.4+0eb734b\r\nintel-extension-for-pytorch   2.1.10+xpu\r\nintel-openmp                  2024.2.0\r\ninteregular                   0.3.3\r\nipex-llm                      2.1.0b20240805\r\nJinja2                        3.1.4\r\njoblib                        1.4.2\r\njsonschema                    4.23.0\r\njsonschema-specifications     2023.12.1\r\nlark                          1.1.9\r\nllvmlite                      0.43.0\r\nmarkdown-it-py                3.0.0\r\nMarkupSafe                    2.1.5\r\nmdurl                         0.1.2\r\nmkl                           2024.0.0\r\nmpi4py                        3.1.6\r\nmpmath                        1.3.0\r\nmsgpack                       1.0.8\r\nmultidict                     6.0.5\r\nnest-asyncio                  1.6.0\r\nnetworkx                      3.3\r\nninja                         1.11.1.1\r\nNuitka                        2.4.4\r\nnumba                         0.60.0\r\nnumpy                         1.26.4\r\nomegaconf                     2.3.0\r\noneccl-bind-pt                2.1.300+xpu\r\nordered-set                   4.1.0\r\noutlines                      0.0.34\r\npackaging                     24.1\r\npandas                        2.2.2\r\npillow                        10.4.0\r\npip                           24.0\r\nprometheus_client             0.20.0\r\nprotobuf                      5.27.2\r\npsutil                        6.0.0\r\npy-cpuinfo                    9.0.0\r\npyarrow                       17.0.0\r\npydantic                      2.8.2\r\npydantic_core                 2.20.1\r\nPygments                      2.18.0\r\npynvml                        11.5.0\r\npython-dateutil               2.9.0.post0\r\npython-dotenv                 1.0.1\r\npython-multipart              0.0.9\r\npytz                          2024.1\r\nPyYAML                        6.0.1\r\nray                           2.32.0\r\nreferencing                   0.35.1\r\nregex                         2024.5.15\r\nrequests                      2.32.3\r\nrich                          13.7.1\r\nrpds-py                       0.19.0\r\nsafetensors                   0.4.3\r\nscipy                         1.14.0\r\nsentencepiece                 0.2.0\r\nsetuptools                    69.5.1\r\nshellingham                   1.5.4\r\nsix                           1.16.0\r\nsniffio                       1.3.1\r\nstarlette                     0.37.2\r\nsympy                         1.13.1\r\ntabulate                      0.9.0\r\ntbb                           2021.13.0\r\ntiktoken                      0.7.0\r\ntokenizers                    0.15.2\r\ntorch                         2.1.0a0+cxx11.abi\r\ntorchaudio                    2.1.0.post2+cxx11.abi\r\ntorchvision                   0.16.0a0+cxx11.abi\r\ntqdm                          4.66.4\r\ntransformers                  4.38.2\r\ntransformers-stream-generator 0.0.5\r\ntriton                        2.1.0\r\ntyper                         0.12.3\r\ntyping_extensions             4.12.2\r\ntzdata                        2024.1\r\nurllib3                       2.2.2\r\nuvicorn                       0.30.3\r\nuvloop                        0.19.0\r\nvllm                          0.3.3+xpu0.0.1        /opt/WD/Code/vllm-zoo\r\nwatchfiles                    0.22.0\r\nwebsockets                    12.0\r\nwheel                         0.43.0\r\nxformers                      0.0.27\r\nyarl                          1.9.4\r\nzstandard                     0.23.0\r\n```\r\n\r\n![image](https://github.com/user-attachments/assets/ef62c83d-6d84-4158-8c95-840ba3541f9c)\r\n\r\nsudo xpu-smi dump -m 1,2,18,22,26,31,34\r\n\r\n![image](https://github.com/user-attachments/assets/1e0c73d5-e84b-46d0-b462-ca95cc4e6cf2)\r\n",
      "state": "open",
      "author": "biyuehuang",
      "author_type": "User",
      "created_at": "2024-08-08T08:08:14Z",
      "updated_at": "2024-08-12T03:24:23Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11743/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11743",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11743",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:40.777023",
      "comments": [
        {
          "author": "kevin-t-tang",
          "body": "oneAPI:\r\nl_BaseKit_p_2024.0.1.46_offline.sh\r\n\r\nconda env: ipex-vllm\r\nhttps://github.com/intel-analytics/ipex-llm/blob/66fe2ee46465306e241296b2d3440f6ba31b7305/docs/mddocs/Quickstart/vLLM_quickstart.md",
          "created_at": "2024-08-09T08:27:21Z"
        },
        {
          "author": "glorysdj",
          "body": "It's an known issue. User has successfully run IPEX-LLM vLLM in Docker.",
          "created_at": "2024-08-11T03:06:54Z"
        },
        {
          "author": "moutainriver",
          "body": "I'd like to deep a bit for this issue from CCG. I can take this JIRA offline.",
          "created_at": "2024-08-12T03:24:22Z"
        }
      ]
    },
    {
      "issue_number": 11681,
      "title": "All-in-one Meta-Llama-3.1-8B RuntimeError: Expected all tensors to be on the same device, but found at least two devices, xpu:0 and cpu!",
      "body": "Hi I would like to try out Meta-Llama-3.1-8B with all-in-one benchmark.. seems to be I am facing this issue \"RuntimeError: Expected all tensors to be on the same device, but found at least two devices, xpu:0 and cpu!\"\r\n\r\nThis is my pip list for your reference\r\n\r\nPackage                     Version\r\n--------------------------- ------------------\r\naccelerate                  0.23.0\r\naiohttp                     3.9.5\r\naiosignal                   1.3.1\r\nannotated-types             0.7.0\r\nantlr4-python3-runtime      4.9.3\r\nattrs                       23.2.0\r\nbigdl-core-xe-21            2.5.0b20240726\r\nbigdl-core-xe-addons-21     2.5.0b20240726\r\nbigdl-core-xe-batch-21      2.5.0b20240726\r\ncertifi                     2024.7.4\r\ncharset-normalizer          3.3.2\r\ndatasets                    2.20.0\r\ndill                        0.3.8\r\ndocstring_parser            0.16\r\nfilelock                    3.15.4\r\nfrozenlist                  1.4.1\r\nfsspec                      2024.5.0\r\nhuggingface-hub             0.24.2\r\nidna                        3.7\r\nintel-cmplr-lib-ur          2024.2.0\r\nintel-extension-for-pytorch 2.1.10+xpu\r\nintel-openmp                2024.2.0\r\nipex-llm                    2.1.0b20240726\r\nJinja2                      3.1.4\r\nmarkdown-it-py              3.0.0\r\nMarkupSafe                  2.1.5\r\nmdurl                       0.1.2\r\nmpmath                      1.3.0\r\nmultidict                   6.0.5\r\nmultiprocess                0.70.16\r\nnetworkx                    3.3\r\nnumpy                       1.26.4\r\nomegaconf                   2.3.0\r\npackaging                   24.1\r\npandas                      2.2.2\r\npillow                      10.4.0\r\npip                         24.0\r\nprotobuf                    5.28.0rc1\r\npsutil                      6.0.0\r\npy-cpuinfo                  9.0.0\r\npyarrow                     17.0.0\r\npyarrow-hotfix              0.6\r\npydantic                    2.8.2\r\npydantic_core               2.20.1\r\nPygments                    2.18.0\r\npython-dateutil             2.9.0.post0\r\npytz                        2024.1\r\nPyYAML                      6.0.2rc1\r\nregex                       2024.7.24\r\nrequests                    2.32.3\r\nrich                        13.7.1\r\nsafetensors                 0.4.3\r\nsentencepiece               0.2.0\r\nsetuptools                  69.5.1\r\nshtab                       1.7.1\r\nsix                         1.16.0\r\nsympy                       1.13.1\r\ntabulate                    0.9.0\r\ntokenizers                  0.19.1\r\ntorch                       2.1.0a0+cxx11.abi\r\ntorchvision                 0.16.0a0+cxx11.abi\r\ntqdm                        4.66.4\r\ntransformers                4.43.2\r\ntrl                         0.9.6\r\ntyping_extensions           4.12.2\r\ntyro                        0.8.5\r\ntzdata                      2024.1\r\nurllib3                     2.2.2\r\nwheel                       0.43.0\r\nxxhash                      3.4.1\r\nyarl                        1.9.4\r\n\r\n",
      "state": "open",
      "author": "Kpeacef",
      "author_type": "User",
      "created_at": "2024-07-29T13:59:22Z",
      "updated_at": "2024-08-12T02:44:37Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11681/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11681",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11681",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:41.089418",
      "comments": [
        {
          "author": "lei-sun-intel",
          "body": "I met exact the same problem when run all-in-one benchmark of Llama-3.1 8B.",
          "created_at": "2024-07-30T06:48:38Z"
        },
        {
          "author": "lzivan",
          "body": "Hi, we are trying to reproduce your issue.\r\n\r\n",
          "created_at": "2024-07-31T01:56:00Z"
        },
        {
          "author": "lzivan",
          "body": "Hi, we've already reproduced your error. Will get back to you once we find a solution.",
          "created_at": "2024-07-31T05:38:51Z"
        },
        {
          "author": "lzivan",
          "body": "Hi @Kpeacef @lei-sun-intel ,\r\n\r\nAccording to the device Runtime Error, we modified one line of code:\r\n```python\r\neos_token_mask = torch.isin(vocab_tensor, self.eos_token_id.to('xpu'))\r\n```\r\nat around #line 288 at\r\n```bash\r\n/home/arda/miniforge3/envs/llm/lib/python3.11/site-packages/transformers/gene",
          "created_at": "2024-07-31T07:37:11Z"
        },
        {
          "author": "qiuxin2012",
          "body": "@Kpeacef  @lei-sun-intel \r\nWe have support Llama-3.1 in all-in-one yesterday, you should update your ipex-llm and run.py to latest version.",
          "created_at": "2024-08-06T03:16:49Z"
        }
      ]
    },
    {
      "issue_number": 11755,
      "title": "Ollama already occupying port before running ./ollama serve",
      "body": "On Ubuntu 22 with kernel 6.5 under WSL2.\r\n\r\n1. installed ollama\r\n2. Installe ipex-llm according to this [quickstart](https://github.com/intel-analytics/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_quickstart.md)\r\n3. tried starting with `./ollama serve `. Got error `Error: listen tcp 127.0.0.1:11434: bind: address already in use`\r\n4. killed the ollama process and tried running serve -- successfully but model still runs on CPU\r\n5. uninstalled ollama\r\n6. serving works but got error `{\"error\":\"llama runner process has terminated: exit status 127 \"}⏎`\r\n7. installed ollama (the quickstart guide doesn't state if it should be installed or not)\r\n8. ollama occupy 11434 after installation.\r\n\r\nin comment i will share the ./ollama serve logs\r\n\r\n",
      "state": "open",
      "author": "Shandelier",
      "author_type": "User",
      "created_at": "2024-08-09T13:16:36Z",
      "updated_at": "2024-08-12T02:07:05Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11755/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11755",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11755",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:41.339291",
      "comments": [
        {
          "author": "Shandelier",
          "body": "```\r\n./ollama serve                                                                                                                             (llm-cpp) 0 (02:29.096)\r\n2024/08/09 15:05:11 routes.go:1028: INFO server config env=\"map[OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST: OLLAMA",
          "created_at": "2024-08-09T13:16:48Z"
        },
        {
          "author": "Shandelier",
          "body": "managed to hack it with chaining the commands:\r\n`sudo kill -9 18737; ./ollama serve`\r\n\r\nthe computation now runs on iGPU but doesn't seem to provide any speed benefit compared to only CPU\r\n![image](https://github.com/user-attachments/assets/0fd312ae-6088-42c5-851b-61fae4c4918c)\r\n",
          "created_at": "2024-08-09T13:29:14Z"
        },
        {
          "author": "Shandelier",
          "body": "i've updated the GPU drivers to 32.0.101...\r\nno improvement in performance.",
          "created_at": "2024-08-09T13:50:33Z"
        },
        {
          "author": "jason-dai",
          "body": "Please try native windows or linux (instead of wsl)",
          "created_at": "2024-08-09T14:39:37Z"
        }
      ]
    },
    {
      "issue_number": 11666,
      "title": "MiniCPM-V-2 can't run on A770 Ubuntu. NotImplementedError: Could not run 'aten::_upsample_bicubic2d_aa.out' with arguments from the 'XPU' backend. ",
      "body": "oneapi 2024.0, Ubuntu22.04, A770\r\n\r\nif modify model.to('xpu') to model.to('cpu'), the code can work.\r\nif model.to('xpu'), it will get error\r\n\r\n```\r\n# python minicpm-v2.py\r\n/root/miniconda3/envs/llm/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n  warn(\r\n2024-07-26 16:31:20,769 - INFO - intel_extension_for_pytorch auto imported\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 16.00it/s]\r\n2024-07-26 16:31:21,460 - INFO - Converting the current model to sym_int4 format......\r\nTraceback (most recent call last):\r\n  File \"/home/test/kiwi/minicpm-v2.py\", line 26, in <module>\r\n    res, context, _ = model.chat(\r\n                      ^^^^^^^^^^^\r\n  File \"/root/.cache/huggingface/modules/transformers_modules/MiniCPM-V-2/modeling_minicpmv.py\", line 358, in chat\r\n    res, vision_hidden_states = self.generate(\r\n                                ^^^^^^^^^^^^^^\r\n  File \"/root/.cache/huggingface/modules/transformers_modules/MiniCPM-V-2/modeling_minicpmv.py\", line 286, in generate\r\n    ) = self.get_vllm_embedding(model_inputs)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/.cache/huggingface/modules/transformers_modules/MiniCPM-V-2/modeling_minicpmv.py\", line 88, in get_vllm_embedding\r\n    vision_hidden_states.append(self.get_vision_embedding(pixel_values))\r\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/.cache/huggingface/modules/transformers_modules/MiniCPM-V-2/modeling_minicpmv.py\", line 76, in get_vision_embedding\r\n    vision_embedding = self.vpm.forward_features(pixel_value.unsqueeze(0).type(dtype))\r\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/llm/lib/python3.11/site-packages/timm/models/vision_transformer.py\", line 663, in forward_features\r\n    x = self._pos_embed(x)\r\n        ^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/llm/lib/python3.11/site-packages/timm/models/vision_transformer.py\", line 582, in _pos_embed\r\n    pos_embed = resample_abs_pos_embed(\r\n                ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/llm/lib/python3.11/site-packages/timm/layers/pos_embed.py\", line 46, in resample_abs_pos_embed\r\n    posemb = F.interpolate(posemb, size=new_size, mode=interpolation, antialias=antialias)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/llm/lib/python3.11/site-packages/torch/nn/functional.py\", line 4027, in interpolate\r\n    return torch._C._nn._upsample_bicubic2d_aa(input, output_size, align_corners, scale_factors)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nNotImplementedError: Could not run 'aten::_upsample_bicubic2d_aa.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_upsample_bicubic2d_aa.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\r\n\r\nCPU: registered at /build/pytorch/build/aten/src/ATen/RegisterCPU.cpp:31188 [kernel]\r\nMeta: registered at /build/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26829 [kernel]\r\nBackendSelect: fallthrough registered at /build/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\r\nPython: registered at /build/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\r\nFuncTorchDynamicLayerBackMode: registered at /build/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\r\nFunctionalize: registered at /build/pytorch/build/aten/src/ATen/RegisterFunctionalization_0.cpp:21905 [kernel]\r\nNamed: registered at /build/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\r\nConjugate: registered at /build/pytorch/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\r\nNegative: registered at /build/pytorch/aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\r\nZeroTensor: registered at /build/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\r\nADInplaceOrView: registered at /build/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4733 [kernel]\r\nAutogradOther: registered at /build/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]\r\nAutogradCPU: registered at /build/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]\r\nAutogradCUDA: registered at /build/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]\r\nAutogradHIP: registered at /build/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]\r\nAutogradXLA: registered at /build/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]\r\nAutogradMPS: registered at /build/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]\r\nAutogradIPU: registered at /build/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]\r\nAutogradXPU: registered at /build/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]\r\nAutogradHPU: registered at /build/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]\r\nAutogradVE: registered at /build/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]\r\nAutogradLazy: registered at /build/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]\r\nAutogradMTIA: registered at /build/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]\r\nAutogradPrivateUse1: registered at /build/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]\r\nAutogradPrivateUse2: registered at /build/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]\r\nAutogradPrivateUse3: registered at /build/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]\r\nAutogradMeta: registered at /build/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]\r\nAutogradNestedTensor: registered at /build/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:18610 [autograd kernel]\r\nTracer: registered at /build/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\r\nAutocastCPU: fallthrough registered at /build/pytorch/aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\r\nAutocastXPU: fallthrough registered at /build/intel-pytorch-extension/csrc/gpu/aten/amp/autocast_mode.cpp:45 [backend fallback]\r\nAutocastCUDA: fallthrough registered at /build/pytorch/aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\r\nFuncTorchBatched: registered at /build/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\r\nFuncTorchVmapMode: fallthrough registered at /build/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\r\nBatched: registered at /build/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\r\nVmapMode: fallthrough registered at /build/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\r\nFuncTorchGradWrapper: registered at /build/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\r\nPythonTLSSnapshot: registered at /build/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\r\nFuncTorchDynamicLayerFrontMode: registered at /build/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\r\nPreDispatch: registered at /build/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\r\nPythonDispatcher: registered at /build/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\r\n```\r\n\r\n\r\n\r\n```\r\n#Model Download\r\nfrom modelscope import snapshot_download\r\nmodel_dir = snapshot_download('OpenBMB/MiniCPM-V-2')\r\n\r\nimport torch\r\nfrom PIL import Image\r\nfrom ipex_llm.transformers import AutoModel\r\nfrom transformers import AutoTokenizer\r\n\r\nmodel_path = \"./models/MiniCPM-V-2\"\r\n\r\n#model = AutoModel.from_pretrained(model_path, trust_remote_code=True, torch_dtype=torch.bfloat16)\r\nmodel = AutoModel.from_pretrained(model_path, trust_remote_code=True, load_in_low_bit=\"sym_int4\",modules_to_not_convert=[\"vpm\", \"resampler\"])\r\n# model = AutoModel.from_pretrained(model_path, trust_remote_code=True, load_in_low_bit=\"sym_int4\", modules_to_not_convert=[\"vpm\", \"resampler\"])\r\n#model = AutoModel.load_low_bit(model_path, trust_remote_code=True, modules_to_not_convert=[\"vpm\", \"resampler\"])\r\n\r\nmodel = model.float()\r\n# For Nvidia GPUs support BF16 (like A100, H100, RTX3090)\r\n#model = model.to(device='cuda', dtype=torch.bfloat16)\r\nmodel.to('xpu')\r\nmodel.eval()\r\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\r\n\r\n\r\nimage = Image.open('./test_image/guo.png').convert('RGB')\r\nquestion = 'What is in the image?'\r\nmsgs = [{'role': 'user', 'content': question}]\r\n\r\nres, context, _ = model.chat(\r\n    image=image,\r\n    msgs=msgs,\r\n    context=None,\r\n    tokenizer=tokenizer,\r\n    sampling=True,\r\n    temperature=0.7\r\n)\r\nprint(res)\r\n```\r\n\r\n```\r\nPackage                       Version\r\n----------------------------- ------------------\r\naccelerate                    0.23.0\r\nannotated-types               0.7.0\r\nantlr4-python3-runtime        4.9.3\r\nanyio                         4.4.0\r\nargon2-cffi                   23.1.0\r\nargon2-cffi-bindings          21.2.0\r\narrow                         1.3.0\r\nasttokens                     2.4.1\r\nasync-lru                     2.0.4\r\nattrs                         23.2.0\r\nBabel                         2.15.0\r\nbeautifulsoup4                4.12.3\r\nbigdl-core-xe-21              2.5.0b20240725\r\nbigdl-core-xe-addons-21       2.5.0b20240725\r\nbigdl-core-xe-batch-21        2.5.0b20240725\r\nbigdl-core-xe-esimd-21        2.5.0b20240527\r\nbigdl-llm                     2.5.0b20240527\r\nbleach                        6.1.0\r\ncertifi                       2024.2.2\r\ncffi                          1.16.0\r\ncharset-normalizer            3.3.2\r\ncomm                          0.2.2\r\ndebugpy                       1.8.2\r\ndecorator                     5.1.1\r\ndefusedxml                    0.7.1\r\ndiffusers                     0.29.2\r\neinops                        0.8.0\r\nexecuting                     2.0.1\r\nfastjsonschema                2.20.0\r\nfilelock                      3.14.0\r\nfqdn                          1.5.1\r\nfsspec                        2024.5.0\r\nh11                           0.14.0\r\nhttpcore                      1.0.5\r\nhttpx                         0.27.0\r\nhuggingface-hub               0.23.2\r\nidna                          3.7\r\nimportlib_metadata            8.0.0\r\nintel-extension-for-pytorch   2.1.10+xpu\r\nintel-openmp                  2024.1.2\r\nipex-llm                      2.1.0b20240725\r\nipykernel                     6.29.5\r\nipython                       8.26.0\r\nipywidgets                    8.1.3\r\nisoduration                   20.11.0\r\njedi                          0.19.1\r\nJinja2                        3.1.4\r\njson5                         0.9.25\r\njsonpointer                   3.0.0\r\njsonschema                    4.23.0\r\njsonschema-specifications     2023.12.1\r\njupyter                       1.0.0\r\njupyter_client                8.6.2\r\njupyter-console               6.6.3\r\njupyter_core                  5.7.2\r\njupyter-events                0.10.0\r\njupyter-lsp                   2.2.5\r\njupyter_server                2.14.2\r\njupyter_server_terminals      0.5.3\r\njupyterlab                    4.2.3\r\njupyterlab_pygments           0.3.0\r\njupyterlab_server             2.27.2\r\njupyterlab_widgets            3.0.11\r\nMarkupSafe                    2.1.5\r\nmatplotlib-inline             0.1.7\r\nmistune                       3.0.2\r\nmpmath                        1.3.0\r\nnbclient                      0.10.0\r\nnbconvert                     7.16.4\r\nnbformat                      5.10.4\r\nnest-asyncio                  1.6.0\r\nnetworkx                      3.3\r\nnotebook                      7.2.1\r\nnotebook_shim                 0.2.4\r\nnumpy                         1.26.4\r\nomegaconf                     2.3.0\r\noverrides                     7.7.0\r\npackaging                     24.0\r\npandas                        2.2.2\r\npandocfilters                 1.5.1\r\nparso                         0.8.4\r\npexpect                       4.9.0\r\npillow                        10.3.0\r\npip                           24.0\r\nplatformdirs                  4.2.2\r\nprometheus_client             0.20.0\r\nprompt_toolkit                3.0.47\r\nprotobuf                      5.27.0\r\npsutil                        5.9.8\r\nptyprocess                    0.7.0\r\npure-eval                     0.2.2\r\npy-cpuinfo                    9.0.0\r\npycparser                     2.22\r\npydantic                      2.7.1\r\npydantic_core                 2.18.2\r\nPygments                      2.18.0\r\npython-dateutil               2.9.0.post0\r\npython-json-logger            2.0.7\r\npytz                          2024.1\r\nPyYAML                        6.0.1\r\npyzmq                         26.0.3\r\nqtconsole                     5.5.2\r\nQtPy                          2.4.1\r\nreferencing                   0.35.1\r\nregex                         2024.5.15\r\nrequests                      2.32.2\r\nrfc3339-validator             0.1.4\r\nrfc3986-validator             0.1.1\r\nrpds-py                       0.19.0\r\nsafetensors                   0.4.3\r\nSend2Trash                    1.8.3\r\nsentencepiece                 0.2.0\r\nsetuptools                    69.5.1\r\nsix                           1.16.0\r\nsniffio                       1.3.1\r\nsoupsieve                     2.5\r\nstack-data                    0.6.3\r\nsympy                         1.12.1rc1\r\ntabulate                      0.9.0\r\nterminado                     0.18.1\r\ntiktoken                      0.7.0\r\ntimm                          0.9.10\r\ntinycss2                      1.3.0\r\ntokenizers                    0.15.2\r\ntorch                         2.1.0a0+cxx11.abi\r\ntorchvision                   0.16.0a0+cxx11.abi\r\ntornado                       6.4.1\r\ntqdm                          4.66.4\r\ntraitlets                     5.14.3\r\ntransformers                  4.36.2\r\ntransformers-stream-generator 0.0.5\r\ntypes-python-dateutil         2.9.0.20240316\r\ntyping_extensions             4.12.0\r\ntzdata                        2024.1\r\nuri-template                  1.3.0\r\nurllib3                       2.2.1\r\nwcwidth                       0.2.13\r\nwebcolors                     24.6.0\r\nwebencodings                  0.5.1\r\nwebsocket-client              1.8.0\r\nwheel                         0.43.0\r\nwidgetsnbextension            4.0.11\r\nzipp                          3.19.2\r\n```\r\n",
      "state": "closed",
      "author": "KiwiHana",
      "author_type": "User",
      "created_at": "2024-07-26T08:43:41Z",
      "updated_at": "2024-08-09T12:20:50Z",
      "closed_at": "2024-07-26T09:17:29Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11666/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11666",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11666",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:41.519795",
      "comments": [
        {
          "author": "KiwiHana",
          "body": "Because xpu can't support 'aten::_upsample_bicubic2d_aa.out' . After modify:\r\n~/miniconda3/envs/llm/lib/python3.11/site-packages/timm/layers/pos_embed.py in Line 49\r\nposemb = F.interpolate(posemb.to(\"cpu\"), size=new_size, mode=interpolation, antialias=antialias).to(posemb.device) \r\n\r\nThen  ok\r\n```\r\n",
          "created_at": "2024-07-26T09:13:34Z"
        }
      ]
    },
    {
      "issue_number": 9903,
      "title": "Facing vulnerability issue with pyspark and ray version using bigdl-spark3==2.4.0",
      "body": "Below result is from trivy scan where pyspark==3.1.3 and ray==2.6.3 having vulnerability issue.\r\n![image](https://github.com/intel-analytics/BigDL/assets/78522263/53f204ca-a93b-45e2-b2ce-a3bfe1190266)\r\n\r\nI'm trying to upgrade pyspark to 3.2.2 version but it will incompatible with bigdl-dllib-spark3 2.4.0.\r\n![image](https://github.com/intel-analytics/BigDL/assets/78522263/8625e800-5078-488f-9153-3c432475c3c2)\r\n\r\nwhen trying to upgrade ray to latest version, bigdl-nano 2.4.0 requires protobuf==3.19.5 where only compatible to ray==2.6.3.\r\n![image](https://github.com/intel-analytics/BigDL/assets/78522263/0ed46dfd-ff15-4bb1-8df1-f0df7094a00d)\r\n\r\nIs there a new release that using updated pyspark & ray library?\r\n\r\n",
      "state": "closed",
      "author": "SjeYinTeoIntel",
      "author_type": "User",
      "created_at": "2024-01-15T08:18:12Z",
      "updated_at": "2024-08-09T12:20:35Z",
      "closed_at": "2024-08-01T13:18:29Z",
      "labels": [
        "user issue",
        "DLlib"
      ],
      "label_count": 2,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/9903/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/9903",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/9903",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:41.705162",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @SjeYinTeoIntel, we do not have a release for pyspark 3.2.2 and ray 2.9.0, but you could run BigDL applications on pyspark 3.2.2. Installing BigDL will automatically install the required corresponding versions of dependencies.",
          "created_at": "2024-01-16T02:13:56Z"
        }
      ]
    },
    {
      "issue_number": 11739,
      "title": "win11运行ipex报错：AMX state allocation in the OS failed",
      "body": "### win11专业版下安装wsl2，wsl下安装docker desktop，在镜像中运行pytorch代码报错\r\n\r\n\r\n启动镜像命令\r\n\r\n```\r\ndocker run -itd --privileged --device=/dev/dri -v /c//models:/llm/models -v /usr/lib/wsl:/usr/lib/wsl --name=arc_vllm --shm-size=\"16g\" intelanalytics/ipex-llm-serving-vllm-xpu-experiment:2.1.0b2 \r\n```\r\n\r\n查看容器内设备\r\n\r\n```\r\nroot@d748cc3e41df:/llm/models/resnet# sycl-ls\r\n\r\n[opencl:acc:0] Intel(R) FPGA Emulation Platform for OpenCL(TM), Intel(R) FPGA Emulation Device OpenCL 1.2  [2023.16.12.0.12_195853.xmain-hotfix]\r\n[opencl:cpu:1] Intel(R) OpenCL, Intel(R) Xeon(R) w3-2425 OpenCL 3.0 (Build 0) [2023.16.12.0.12_195853.xmain-hotfix]\r\n[opencl:gpu:2] Intel(R) OpenCL Graphics, Intel(R) Graphics [0x56a0] OpenCL 3.0 NEO  [23.35.27191.42]\r\n[opencl:gpu:3] Intel(R) OpenCL Graphics, Intel(R) Graphics [0x56a0] OpenCL 3.0 NEO  [23.35.27191.42]\r\n[ext_oneapi_level_zero:gpu:0] Intel(R) Level-Zero, Intel(R) Graphics [0x56a0] 1.3 [1.3.26241]\r\n[ext_oneapi_level_zero:gpu:1] Intel(R) Level-Zero, Intel(R) Graphics [0x56a0] 1.3 [1.3.26241]\r\n```\r\n\r\n执行脚本\r\n```\r\nroot@d748cc3e41df:/llm/models/resnet# python test_torch.py\r\n\r\n/usr/local/lib/python3.11/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n  warn(\r\nAbort was called at 62 line in file:\r\n./shared/source/os_interface/os_interface.h\r\n\r\nLIBXSMM_VERSION: main_stable-1.17-3651 (25693763)LIBXSMM WARNING: AMX state allocation in the OS failed!\r\n\r\nLIBXSMM_TARGET: clx [Intel(R) Xeon(R) w3-2425]\r\nRegistry and code: 13 MB\r\nCommand: python test_torch.py\r\nUptime: 2.493819 s\r\nAborted\r\n```\r\n\r\n脚本内容：\r\n```\r\nroot@d748cc3e41df:/llm/models/resnet# cat test_torch.py\r\n\r\nimport torch\r\nimport intel_extension_for_pytorch as ipex\r\n\r\ntensor_1 = torch.randn(1, 1, 40, 128).to('xpu')\r\ntensor_2 = torch.randn(1, 1, 128, 40).to('xpu')\r\nprint(torch.matmul(tensor_1, tensor_2).size())\r\n\r\n# torch.Size([1, 1, 40,40])\r\n```\r\n",
      "state": "open",
      "author": "showyouit",
      "author_type": "User",
      "created_at": "2024-08-08T01:55:12Z",
      "updated_at": "2024-08-09T12:19:56Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 3,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11739/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "liu-shaojun"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11739",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11739",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:42.008781",
      "comments": [
        {
          "author": "liu-shaojun",
          "body": "Will try to reproduce from our side first.",
          "created_at": "2024-08-08T02:16:25Z"
        },
        {
          "author": "glorysdj",
          "body": "![908376fea6c33c461b83f15181e18b5](https://github.com/user-attachments/assets/a45bcde2-0126-4347-93e8-88b3aa068e6a)\r\n",
          "created_at": "2024-08-08T02:27:28Z"
        },
        {
          "author": "liu-shaojun",
          "body": "We've communicated with the user via WeChat, and we couldn't reproduce the issue on our machine, arc17 (System: Windows 11, CPU: i9 13900K, GPU: Arc A770). However, when connecting to the customer's machine (System: Windows 11, CPU: Xeon(R) w3-2425, GPU: Arc A770) via [Sunlogin](https://sunlogin.ora",
          "created_at": "2024-08-09T01:23:23Z"
        }
      ]
    },
    {
      "issue_number": 11611,
      "title": "MiniCPM-V-2 output error",
      "body": "![img_v3_02ct_364c6b00-4aa3-4f66-9d9b-ac908e08ba6g](https://github.com/user-attachments/assets/e91bc4d4-69b5-4c82-bf84-44ea0af912ae)\r\n\r\nipex-llm: 2.1.0b20240714\r\ntransformers: 4.41.2\r\nDriver: 32.0.101.5762\r\nOS: Win11 23H2",
      "state": "open",
      "author": "aitss2017",
      "author_type": "User",
      "created_at": "2024-07-18T02:24:44Z",
      "updated_at": "2024-08-07T06:34:36Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 1,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11611/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11611",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11611",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:42.287625",
      "comments": [
        {
          "author": "jenniew",
          "body": "Please check this example and try again: https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/GPU/HuggingFace/Multimodal/MiniCPM-V-2",
          "created_at": "2024-08-07T06:34:35Z"
        }
      ]
    },
    {
      "issue_number": 11667,
      "title": "MiniCPM-Llama3-V-2_5 can't run on A770 Ubuntu. ",
      "body": "oneapi 2024.0, Ubuntu22.04, A770\r\n\r\n```\r\n#Model Download\r\nfrom modelscope import snapshot_download\r\nmodel_dir = snapshot_download('OpenBMB/MiniCPM-Llama3-V-2_5')\r\n\r\nimport torch\r\nfrom PIL import Image\r\nfrom ipex_llm.transformers import AutoModel\r\n#from transformers import AutoModel\r\nfrom transformers import AutoTokenizer\r\n\r\nmodel_path = \"./models/MiniCPM-Llama3-V-2_5\"\r\n\r\n#model = AutoModel.from_pretrained(model_path, trust_remote_code=True, torch_dtype=torch.bfloat16)\r\nmodel = AutoModel.from_pretrained(model_path, trust_remote_code=True, load_in_low_bit=\"sym_int4\")\r\n# model = AutoModel.from_pretrained(model_path, trust_remote_code=True, load_in_low_bit=\"sym_int4\", modules_to_not_convert=[\"vpm\", \"resampler\"])\r\n\r\n#model = model.float()\r\n\r\nmodel.to('xpu')\r\nmodel.eval()\r\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\r\n\r\n\r\nimage = Image.open('./test_image/guo.png').convert('RGB')\r\nquestion = 'What is in the image?'\r\nmsgs = [{'role': 'user', 'content': question}]\r\n\r\nres, context, _ = model.chat(\r\n    image=image,\r\n    msgs=msgs,\r\n    context=None,\r\n    tokenizer=tokenizer,\r\n    sampling=True,\r\n    temperature=0.7\r\n)\r\nprint(res)\r\n```\r\n\r\n# python llama3-minicpm.py\r\n```\r\n/root/miniconda3/envs/llm/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n  warn(\r\n2024-07-26 17:00:09,796 - INFO - intel_extension_for_pytorch auto imported\r\n2024-07-26 17:00:09,847 - INFO - vision_config is None, using default vision config\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 17.22it/s]\r\n2024-07-26 17:00:11,545 - INFO - Converting the current model to sym_int4 format......\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nTraceback (most recent call last):\r\n  File \"/home/test/kiwi/llama3-minicpm.py\", line 24, in <module>\r\n    res, context, _ = model.chat(\r\n                      ^^^^^^^^^^^\r\n  File \"/root/.cache/huggingface/modules/transformers_modules/MiniCPM-Llama3-V-2_5/modeling_minicpmv.py\", line 416, in chat\r\n    res, vision_hidden_states = self.generate(\r\n                                ^^^^^^^^^^^^^^\r\n  File \"/root/.cache/huggingface/modules/transformers_modules/MiniCPM-Llama3-V-2_5/modeling_minicpmv.py\", line 326, in generate\r\n    ) = self.get_vllm_embedding(model_inputs)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/.cache/huggingface/modules/transformers_modules/MiniCPM-Llama3-V-2_5/modeling_minicpmv.py\", line 92, in get_vllm_embedding\r\n    vision_embedding = self.resampler(vision_embedding, tgt_sizes)\r\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/.cache/huggingface/modules/transformers_modules/MiniCPM-Llama3-V-2_5/resampler.py\", line 150, in forward\r\n    out = self.attn(\r\n          ^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/activation.py\", line 1241, in forward\r\n    attn_output, attn_output_weights = F.multi_head_attention_forward(\r\n                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/llm/lib/python3.11/site-packages/torch/nn/functional.py\", line 5300, in multi_head_attention_forward\r\n    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/llm/lib/python3.11/site-packages/torch/nn/functional.py\", line 4846, in _in_projection_packed\r\n    return linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)\r\n                                ^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: could not create a primitive descriptor for a matmul primitive\r\n```\r\n\r\n# pip list\r\n```\r\nPackage                     Version\r\n--------------------------- ------------------\r\naccelerate                  0.23.0\r\naiohttp                     3.9.5\r\naiosignal                   1.3.1\r\nannotated-types             0.7.0\r\nattrs                       23.2.0\r\nbigdl-core-xe-21            2.5.0b20240728\r\nbigdl-core-xe-addons-21     2.5.0b20240728\r\nbigdl-core-xe-batch-21      2.5.0b20240728\r\ncertifi                     2024.7.4\r\ncharset-normalizer          3.3.2\r\ndatasets                    2.20.0\r\ndill                        0.3.8\r\ndocstring_parser            0.16\r\nfilelock                    3.15.4\r\nfrozenlist                  1.4.1\r\nfsspec                      2024.5.0\r\nhuggingface-hub             0.24.2\r\nidna                        3.7\r\nintel-cmplr-lib-ur          2024.2.0\r\nintel-extension-for-pytorch 2.1.10+xpu\r\nintel-openmp                2024.2.0\r\nipex-llm                    2.1.0b20240728\r\nJinja2                      3.1.4\r\nmarkdown-it-py              3.0.0\r\nMarkupSafe                  2.1.5\r\nmdurl                       0.1.2\r\nmpmath                      1.3.0\r\nmultidict                   6.0.5\r\nmultiprocess                0.70.16\r\nnetworkx                    3.3\r\nnumpy                       1.26.4\r\npackaging                   24.1\r\npandas                      2.2.2\r\npillow                      10.4.0\r\npip                         24.0\r\nprotobuf                    5.28.0rc1\r\npsutil                      6.0.0\r\npy-cpuinfo                  9.0.0\r\npyarrow                     17.0.0\r\npyarrow-hotfix              0.6\r\npydantic                    2.8.2\r\npydantic_core               2.20.1\r\nPygments                    2.18.0\r\npython-dateutil             2.9.0.post0\r\npytz                        2024.1\r\nPyYAML                      6.0.2rc1\r\nregex                       2024.7.24\r\nrequests                    2.32.3\r\nrich                        13.7.1\r\nsafetensors                 0.4.3\r\nsentencepiece               0.2.0\r\nsetuptools                  69.5.1\r\nshtab                       1.7.1\r\nsix                         1.16.0\r\nsympy                       1.13.1\r\ntabulate                    0.9.0\r\ntimm                        1.0.7\r\ntokenizers                  0.19.1\r\ntorch                       2.1.0a0+cxx11.abi\r\ntorchvision                 0.16.0a0+cxx11.abi\r\ntqdm                        4.66.4\r\ntransformers                4.40.0\r\ntrl                         0.9.6\r\ntyping_extensions           4.12.2\r\ntyro                        0.8.5\r\ntzdata                      2024.1\r\nurllib3                     2.2.2\r\nwheel                       0.43.0\r\nxxhash                      3.4.1\r\nyarl                        1.9.4\r\n```",
      "state": "open",
      "author": "KiwiHana",
      "author_type": "User",
      "created_at": "2024-07-26T09:05:38Z",
      "updated_at": "2024-08-06T08:16:58Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11667/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "qiuxin2012"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11667",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11667",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:42.611085",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "We should use transformers==4.41.0. But it's output is not readable.\r\nThe unreadable output may be \r\n```\r\n|\r\n```\r\nor\r\n```\r\n, 4, 3, 1, 2, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 2, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, , 107, 108, 109, 110, 111, 112, 113, ",
          "created_at": "2024-07-30T02:15:39Z"
        },
        {
          "author": "jason-dai",
          "body": "> We should use transformers==4.41.0. But it's output is not readable.\r\n\r\nMaybe add an example",
          "created_at": "2024-07-30T02:17:49Z"
        },
        {
          "author": "qiuxin2012",
          "body": "For the unreadable output, we find it's caused by the `sampling=True`\r\n```python\r\nres = model.chat(\r\n    image=image,\r\n    msgs=msgs,\r\n    context=None,\r\n    tokenizer=tokenizer,\r\n    sampling=True,\r\n    temperature=0.7\r\n)\r\n```\r\nIf `sampling=True`, the llm's generation will use parameters:\r\n```\r\n   ",
          "created_at": "2024-07-31T00:37:26Z"
        },
        {
          "author": "qiuxin2012",
          "body": "The wrong result of repetition_penalty is caused by IPEX, I have raised an issue for them https://github.com/intel/intel-extension-for-pytorch/issues/682",
          "created_at": "2024-07-31T01:25:04Z"
        },
        {
          "author": "qiuxin2012",
          "body": "Example added https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/GPU/HuggingFace/Multimodal/MiniCPM-Llama3-V-2_5",
          "created_at": "2024-08-06T08:05:42Z"
        }
      ]
    },
    {
      "issue_number": 11599,
      "title": "要节约磁盘空间，ollama 使用igpu时能不能把oneapi必要库拷贝到conda环境",
      "body": "要节约磁盘空间，ollama 使用igpu时能不能把oneapi必要库拷贝到conda环境，以下是收到的邮件方法，但是不适用于ollama，没有intel-extension-for-pytorch文件夹，torch的lib现成就有文件了，都不用拷贝\r\n\r\nHi Xutao,\r\n\r\n \r\n\r\n在你们参展的MTL Ultra7 机器可以把oneapi的必要库拷贝到conda，节约机器磁盘空间，也不需要每次启动llm应用前运行 oneapi setvars.bat.\r\n\r\n步骤：\r\n\r\n（1）从C:\\Program Files (x86)\\Intel\\oneAPI把以下这些dll拷贝到conda环境的intel-extension-for-pytorch/bin 路径下\r\n\r\n\r\n\r\n \r\n\r\n（2）从C:\\Program Files (x86)\\Intel\\oneAPI把以下这些dll拷贝到conda环境的torch/lib 路径下\r\n\r\n\r\n",
      "state": "open",
      "author": "dayskk",
      "author_type": "User",
      "created_at": "2024-07-17T06:31:19Z",
      "updated_at": "2024-08-06T08:10:38Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11599/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11599",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11599",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:42.811437",
      "comments": [
        {
          "author": "violet17",
          "body": "理论上可以，实际上需要测试一下",
          "created_at": "2024-07-17T06:40:50Z"
        },
        {
          "author": "dayskk",
          "body": "ollama 使用igpu时，要拷贝哪些oneapi必要库到conda环境的什么地方？",
          "created_at": "2024-07-17T08:54:58Z"
        },
        {
          "author": "biyuehuang",
          "body": "打开一个cmd窗口，用pip安装，这样安装的包约1.4G，比原来节省了20G+。\r\nmkdir intel/dpct\r\nset PYTHONUSERBASE=c:/intel/dpct\r\npip install --user mkl-dpcpp==2024.2 onednn==2024.2 dpcpp-cpp-rt==2024.2\r\n\r\n然后到conda环境的窗口\r\nconda activate your_env\r\nset PATH=c:\\intel\\dpct\\Library\\bin;%PATH%\r\n",
          "created_at": "2024-07-17T09:16:37Z"
        },
        {
          "author": "dayskk",
          "body": "安装oneapi\r\n打开一个cmd窗口，用pip安装，这样安装的包约1.4G，比原来节省了20G+。\r\nmkdir intel/dpct\r\nset PYTHONUSERBASE=c:/intel/dpct\r\npip install --user mkl-dpcpp==2024.2 onednn==2024.2 dpcpp-cpp-rt==2024.2\r\n\r\n然后到conda环境的窗口\r\nconda activate your_env\r\nset PATH=c:\\intel\\dpct\\Library\\bin;%PATH%\r\n\r\n此方法无效",
          "created_at": "2024-08-06T08:10:37Z"
        }
      ]
    },
    {
      "issue_number": 11668,
      "title": "MiniCPM-V-2 got wrong answer on A770 but right on CPU.",
      "body": "oneapi 2024.0, Ubuntu22.04, A770\r\n\r\non CPU, \"sym_int4\", The answer is \r\n```\r\n******** inference time/s: 12.64120602607727\r\nThe image displays a variety of fresh vegetables laid out on a wooden surface.\r\n```\r\non CPU, torch.bfloat16, The answer is \r\n```\r\n******** inference time/s: 15.394450426101685\r\nThe image displays a variety of fresh vegetables laid out on an old wooden table.\r\n```\r\non CPU, torch.float16, The answer is \r\n```\r\n******** inference time/s: 16.71022653579712\r\nThe image displays a variety of fresh vegetables laid out on what appears to be an old wooden table.\r\n```\r\n\r\non A770, \"sym_int4\", The answer is \r\n```\r\n******** inference time/s: 3.851618528366089\r\n抸A variety of fresh vegetables are in the image, including tomatoes and cucumbers.<ref>vegetables\r\n\r\n```\r\n\r\non A770,  torch.float16, The answer is \r\n```\r\n******** inference time/s: 1.9846763610839844\r\n齚The image features a variety of fresh vegetables.\r\n```\r\non A770,  torch.bfloat16, The answer is \r\n```\r\n******** inference time/s: 1.5065085887908936\r\n蝽\r\n```\r\n\r\npython minicpm.py\r\n```\r\n\r\nimport torch\r\nfrom PIL import Image\r\nfrom ipex_llm.transformers import AutoModel\r\n#from transformers import AutoModel\r\nfrom transformers import AutoTokenizer\r\nimport time\r\n\r\nmodel_path = \"./models/MiniCPM-V-2\"\r\n\r\n#model = AutoModel.from_pretrained(model_path, trust_remote_code=True, torch_dtype=torch.bfloat16)\r\nmodel = AutoModel.from_pretrained(model_path, trust_remote_code=True, load_in_low_bit=\"sym_int4\", modules_to_not_convert=[\"vpm\", \"resampler\"])\r\n#model = AutoModel.load_low_bit(model_path, trust_remote_code=True, modules_to_not_convert=[\"vpm\", \"resampler\"])\r\n\r\nmodel = model.float()\r\nmodel.to('xpu')\r\nmodel.eval()\r\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\r\n\r\n\r\nimage = Image.open('./test_image/guo.png').convert('RGB')\r\nquestion = 'What is in the image?'\r\nmsgs = [{'role': 'user', 'content': question}]\r\nst = time.time()\r\nres, context, _ = model.chat(\r\n    image=image,\r\n    msgs=msgs,\r\n    context=None,\r\n    tokenizer=tokenizer,\r\n    sampling=True,\r\n    temperature=0.7\r\n)\r\nprint(\"******** inference time/s:\",time.time()-st)\r\nprint(res)\r\n```\r\n\r\n```\r\nPackage                       Version\r\n----------------------------- ------------------\r\naccelerate                    0.23.0\r\naiohttp                       3.9.5\r\naiosignal                     1.3.1\r\nannotated-types               0.7.0\r\nantlr4-python3-runtime        4.9.3\r\nanyio                         4.4.0\r\nargon2-cffi                   23.1.0\r\nargon2-cffi-bindings          21.2.0\r\narrow                         1.3.0\r\nasttokens                     2.4.1\r\nasync-lru                     2.0.4\r\nattrs                         23.2.0\r\nBabel                         2.15.0\r\nbeautifulsoup4                4.12.3\r\nbigdl-core-xe-21              2.5.0b20240725\r\nbigdl-core-xe-addons-21       2.5.0b20240725\r\nbigdl-core-xe-batch-21        2.5.0b20240725\r\nbigdl-core-xe-esimd-21        2.5.0b20240527\r\nbigdl-llm                     2.5.0b20240527\r\nbleach                        6.1.0\r\ncertifi                       2024.2.2\r\ncffi                          1.16.0\r\ncharset-normalizer            3.3.2\r\ncomm                          0.2.2\r\ndatasets                      2.20.0\r\ndebugpy                       1.8.2\r\ndecorator                     5.1.1\r\ndefusedxml                    0.7.1\r\ndiffusers                     0.29.2\r\ndill                          0.3.8\r\ndocstring_parser              0.16\r\neinops                        0.8.0\r\nexecuting                     2.0.1\r\nfastjsonschema                2.20.0\r\nfilelock                      3.14.0\r\nfqdn                          1.5.1\r\nfrozenlist                    1.4.1\r\nfsspec                        2024.5.0\r\nh11                           0.14.0\r\nhttpcore                      1.0.5\r\nhttpx                         0.27.0\r\nhuggingface-hub               0.23.2\r\nidna                          3.7\r\nimportlib_metadata            8.0.0\r\nintel-extension-for-pytorch   2.1.10+xpu\r\nintel-openmp                  2024.1.2\r\nipex-llm                      2.1.0b20240725\r\nipykernel                     6.29.5\r\nipython                       8.26.0\r\nipywidgets                    8.1.3\r\nisoduration                   20.11.0\r\njedi                          0.19.1\r\nJinja2                        3.1.4\r\njson5                         0.9.25\r\njsonpointer                   3.0.0\r\njsonschema                    4.23.0\r\njsonschema-specifications     2023.12.1\r\njupyter                       1.0.0\r\njupyter_client                8.6.2\r\njupyter-console               6.6.3\r\njupyter_core                  5.7.2\r\njupyter-events                0.10.0\r\njupyter-lsp                   2.2.5\r\njupyter_server                2.14.2\r\njupyter_server_terminals      0.5.3\r\njupyterlab                    4.2.3\r\njupyterlab_pygments           0.3.0\r\njupyterlab_server             2.27.2\r\njupyterlab_widgets            3.0.11\r\nmarkdown-it-py                3.0.0\r\nMarkupSafe                    2.1.5\r\nmatplotlib-inline             0.1.7\r\nmdurl                         0.1.2\r\nmistune                       3.0.2\r\nmpmath                        1.3.0\r\nmultidict                     6.0.5\r\nmultiprocess                  0.70.16\r\nnbclient                      0.10.0\r\nnbconvert                     7.16.4\r\nnbformat                      5.10.4\r\nnest-asyncio                  1.6.0\r\nnetworkx                      3.3\r\nnotebook                      7.2.1\r\nnotebook_shim                 0.2.4\r\nnumpy                         1.26.4\r\nomegaconf                     2.3.0\r\noverrides                     7.7.0\r\npackaging                     24.0\r\npandas                        2.2.2\r\npandocfilters                 1.5.1\r\nparso                         0.8.4\r\npexpect                       4.9.0\r\npillow                        10.3.0\r\npip                           24.0\r\nplatformdirs                  4.2.2\r\nprometheus_client             0.20.0\r\nprompt_toolkit                3.0.47\r\nprotobuf                      5.27.0\r\npsutil                        5.9.8\r\nptyprocess                    0.7.0\r\npure-eval                     0.2.2\r\npy-cpuinfo                    9.0.0\r\npyarrow                       17.0.0\r\npyarrow-hotfix                0.6\r\npycparser                     2.22\r\npydantic                      2.7.1\r\npydantic_core                 2.18.2\r\nPygments                      2.18.0\r\npython-dateutil               2.9.0.post0\r\npython-json-logger            2.0.7\r\npytz                          2024.1\r\nPyYAML                        6.0.1\r\npyzmq                         26.0.3\r\nqtconsole                     5.5.2\r\nQtPy                          2.4.1\r\nreferencing                   0.35.1\r\nregex                         2024.5.15\r\nrequests                      2.32.2\r\nrfc3339-validator             0.1.4\r\nrfc3986-validator             0.1.1\r\nrich                          13.7.1\r\nrpds-py                       0.19.0\r\nsafetensors                   0.4.3\r\nSend2Trash                    1.8.3\r\nsentencepiece                 0.2.0\r\nsetuptools                    69.5.1\r\nshtab                         1.7.1\r\nsix                           1.16.0\r\nsniffio                       1.3.1\r\nsoupsieve                     2.5\r\nstack-data                    0.6.3\r\nsympy                         1.12.1rc1\r\ntabulate                      0.9.0\r\nterminado                     0.18.1\r\ntiktoken                      0.7.0\r\ntimm                          0.9.10\r\ntinycss2                      1.3.0\r\ntokenizers                    0.19.1\r\ntorch                         2.1.0a0+cxx11.abi\r\ntorchvision                   0.16.0a0+cxx11.abi\r\ntornado                       6.4.1\r\ntqdm                          4.66.4\r\ntraitlets                     5.14.3\r\ntransformers                  4.40.0\r\ntransformers-stream-generator 0.0.5\r\ntrl                           0.9.6\r\ntypes-python-dateutil         2.9.0.20240316\r\ntyping_extensions             4.12.0\r\ntyro                          0.8.5\r\ntzdata                        2024.1\r\nuri-template                  1.3.0\r\nurllib3                       2.2.1\r\nwcwidth                       0.2.13\r\nwebcolors                     24.6.0\r\nwebencodings                  0.5.1\r\nwebsocket-client              1.8.0\r\nwheel                         0.43.0\r\nwidgetsnbextension            4.0.11\r\nxxhash                        3.4.1\r\nyarl                          1.9.4\r\nzipp                          3.19.2\r\n\r\n```",
      "state": "closed",
      "author": "KiwiHana",
      "author_type": "User",
      "created_at": "2024-07-26T11:03:24Z",
      "updated_at": "2024-08-06T08:06:39Z",
      "closed_at": "2024-07-30T03:25:59Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11668/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "qiuxin2012"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11668",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11668",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:43.069464",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "We can use asym_int4 for work around.",
          "created_at": "2024-07-30T02:13:19Z"
        },
        {
          "author": "jason-dai",
          "body": "> We can use asym_int4 for work around.\r\n\r\nMaybe add an example",
          "created_at": "2024-07-30T02:18:16Z"
        },
        {
          "author": "KiwiHana",
          "body": "asym INT4 sample code\r\n```\r\nimport torch\r\nfrom PIL import Image\r\nfrom ipex_llm.transformers import AutoModel\r\n#from transformers import AutoModel\r\nfrom transformers import AutoTokenizer\r\nimport time\r\n\r\nmodel_path = \"./models/MiniCPM-V-2\"\r\nmodel = AutoModel.from_pretrained(model_path, trust_remote_co",
          "created_at": "2024-07-30T03:25:59Z"
        },
        {
          "author": "qiuxin2012",
          "body": "Example added: https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/GPU/HuggingFace/Multimodal/MiniCPM-V-2 ",
          "created_at": "2024-08-06T08:06:38Z"
        }
      ]
    },
    {
      "issue_number": 11591,
      "title": "llama runner process has terminated exit status 1",
      "body": "用ollama run qwen2:7b\r\n还是用curl模式，都返回错误llama runner process has terminated exit status 1\r\n![微信图片_20240716184524](https://github.com/user-attachments/assets/ba6ee0cb-6e30-4cfd-ba5c-0c4fd5a7446a)\r\n\r\n![微信图片_20240716184552](https://github.com/user-attachments/assets/b683dd10-6b39-4a8d-8284-e3eeb37b2aa4)\r\n![微信图片_20240716184749](https://github.com/user-attachments/assets/b0c9899b-ab27-4f68-90c1-62bfbf6fa53e)\r\n",
      "state": "open",
      "author": "dayskk",
      "author_type": "User",
      "created_at": "2024-07-16T10:49:37Z",
      "updated_at": "2024-08-06T07:46:33Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 6,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11591/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11591",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11591",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:43.332868",
      "comments": [
        {
          "author": "dayskk",
          "body": "复现步骤如下\r\nconda create -n llm-cpp python=3.11\r\nconda activate llm-cpp\r\npip install --pre --upgrade ipex-llm[cpp]\r\nmkdir llama-cpp\r\ncd llama-cpp\r\ninit-llama-cpp.bat\r\ninit-ollama.bat\r\nset OLLAMA_NUM_GPU=999\r\nset no_proxy=localhost,127.0.0.1\r\nset ZES_ENABLE_SYSMAN=1\r\nset SYCL_CACHE_PERSISTENT=1\r\n\r\nollama",
          "created_at": "2024-07-16T10:59:32Z"
        },
        {
          "author": "dayskk",
          "body": "![微信图片_20240716190344](https://github.com/user-attachments/assets/1ee0cfa3-bfcd-47c0-b75e-ccf8fb3b2b26)\r\n",
          "created_at": "2024-07-16T11:04:33Z"
        },
        {
          "author": "dayskk",
          "body": "重新安装后错误又变了\r\n![Uploading 微信图片_20240717090225.jpg…]()\r\n",
          "created_at": "2024-07-17T01:03:02Z"
        },
        {
          "author": "sgwhat",
          "body": "hi @dayskk , could you please run the ENV-Check script in https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/scripts and reply us with the results?",
          "created_at": "2024-07-17T02:33:35Z"
        },
        {
          "author": "biyuehuang",
          "body": "After upgrade MTL iGPU driver to 5762 and install OneAPI 2024.2, it works.\r\n\r\n```\r\n## open miniforge prompt as administrator\r\nconda activate llm2\r\ncd C:\\Program Files (x86)\\Intel\\oneAPI\r\nsetvars.bat\r\ncd C:\\Users\\Admin\\Documents\\ollama2\r\ninit-ollama.bat\r\n\r\nset OLLAMA_NUM_GPU=999\r\nset no_proxy=localho",
          "created_at": "2024-07-17T05:30:53Z"
        }
      ]
    },
    {
      "issue_number": 10607,
      "title": "starcoder2-3B model for reset token latency",
      "body": "starcoder2-3B code generation model optimization， first token is ok, but the rest token latency is longer than expected. ",
      "state": "closed",
      "author": "juan-OY",
      "author_type": "User",
      "created_at": "2024-04-01T03:15:45Z",
      "updated_at": "2024-08-06T07:32:44Z",
      "closed_at": "2024-08-06T07:32:44Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 0,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/10607/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "MeouSker77"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/10607",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/10607",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:43.603227",
      "comments": []
    },
    {
      "issue_number": 11714,
      "title": "glm4:AttributeError: 'NoneType' object has no attribute 'shape'",
      "body": "python3.11\r\ntransformers4.42.4",
      "state": "open",
      "author": "jjzhu0579",
      "author_type": "User",
      "created_at": "2024-08-05T12:43:38Z",
      "updated_at": "2024-08-06T03:09:32Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 5,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11714/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "Uxito-Ada"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11714",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11714",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:43.603250",
      "comments": [
        {
          "author": "jjzhu0579",
          "body": "Traceback (most recent call last):\r\n  File \"/share/home/aim/aim_zhujj/bc2/glm4_lora_train.py\", line 138, in <module>\r\n    trainer.train()\r\n  File \"/data/aim_nuist/aim_zhujj/.conda/envs/blurb/lib/python3.11/site-packages/transformers/trainer.py\", line 1938, in train\r\n    return inner_training_loop(\r\n",
          "created_at": "2024-08-05T13:00:27Z"
        },
        {
          "author": "Uxito-Ada",
          "body": "Hi @jjzhu0579 ,\r\n\r\nIt seems that there is a ChatGLM-fine-tuneing application wrote by yourself, named `glm4_lora_train.py`, while we have provided LoRA fine-tuning for ChatGLM on IPEX-LLM. Pls follow [here](https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/GPU/LLM-Finetuning/L",
          "created_at": "2024-08-06T02:33:07Z"
        },
        {
          "author": "jjzhu0579",
          "body": "> Hi @jjzhu0579 ,\r\n> \r\n> It seems that there is a ChatGLM-fine-tuneing application wrote by yourself, named `glm4_lora_train.py`, while we have provided LoRA fine-tuning for ChatGLM on IPEX-LLM. Pls follow [here](https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/GPU/LLM-Finetu",
          "created_at": "2024-08-06T03:00:36Z"
        },
        {
          "author": "jjzhu0579",
          "body": "![image](https://github.com/user-attachments/assets/aa409c4e-4f08-41eb-b72d-929512821aba)\r\n",
          "created_at": "2024-08-06T03:03:32Z"
        },
        {
          "author": "Uxito-Ada",
          "body": "Hi @jjzhu0579 ,\r\n\r\nSorry that p-tuning has not been supported. You can find all available fine-tuning algorithms [here](https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/GPU/LLM-Finetuning#running-llm-finetuning-using-ipex-llm-on-intel-gpu). In addition, IPEX-LLM works only on",
          "created_at": "2024-08-06T03:09:31Z"
        }
      ]
    },
    {
      "issue_number": 11650,
      "title": "run GLM4-9b-chat on MTL iGPU get error: ValueError: too many values to unpack (expected 2)",
      "body": "I ran glm4 on MTL iGPU , and it reported this error:\r\n![image](https://github.com/user-attachments/assets/1e0b6537-02da-4962-99aa-f6fd717a14ae)\r\n\r\noneAPI: l_BaseKit_p_2024.0.1.46_offline.sh\r\n\r\nMy env as below:\r\n(notebook-zone) arc@arc:~/ipex-llm/python/llm/example/GPU/HuggingFace/LLM/glm4$ pip list \r\nPackage                       Version\r\n----------------------------- ------------------\r\naccelerate                    0.23.0\r\naddict                        2.4.0\r\naiofiles                      23.2.1\r\naiohttp                       3.9.5\r\naiosignal                     1.3.1\r\naliyun-python-sdk-core        2.15.1\r\naliyun-python-sdk-kms         2.16.3\r\naltair                        5.3.0\r\nannotated-types               0.7.0\r\nantlr4-python3-runtime        4.9.3\r\nanyio                         4.4.0\r\nasttokens                     2.4.1\r\nasync-timeout                 4.0.3\r\nattrs                         23.2.0\r\nbigdl-core-cpp                2.5.0b20240610\r\nbigdl-core-xe-21              2.5.0b20240723\r\nbigdl-core-xe-addons-21       2.5.0b20240723\r\nbigdl-core-xe-batch-21        2.5.0b20240723\r\nbigdl-core-xe-esimd-21        2.5.0b20240528\r\nbitsandbytes                  0.43.1\r\ncertifi                       2024.6.2\r\ncffi                          1.16.0\r\ncharset-normalizer            3.3.2\r\nclick                         8.1.7\r\ncomm                          0.2.2\r\ncontourpy                     1.2.1\r\ncrcmod                        1.7\r\ncryptography                  42.0.8\r\ncycler                        0.12.1\r\ndatasets                      2.20.0\r\ndebugpy                       1.8.1\r\ndecorator                     5.1.1\r\ndill                          0.3.8\r\ndnspython                     2.6.1\r\neinops                        0.8.0\r\nemail_validator               2.1.2\r\nexceptiongroup                1.2.1\r\nexecuting                     2.0.1\r\nfastapi                       0.111.0\r\nfastapi-cli                   0.0.4\r\nffmpy                         0.3.2\r\nfilelock                      3.15.1\r\nfonttools                     4.53.0\r\nfrozenlist                    1.4.1\r\nfsspec                        2024.5.0\r\ngast                          0.6.0\r\ngguf                          0.6.0\r\ngradio                        4.36.1\r\ngradio_client                 1.0.1\r\nh11                           0.14.0\r\nhttpcore                      1.0.5\r\nhttptools                     0.6.1\r\nhttpx                         0.27.0\r\nhuggingface-hub               0.23.4\r\nidna                          3.7\r\nimportlib_metadata            7.1.0\r\nimportlib_resources           6.4.0\r\nintel-extension-for-pytorch   2.1.10+xpu\r\nintel-openmp                  2024.1.2\r\nipex-llm                      2.1.0b20240723\r\nipykernel                     6.29.4\r\nipython                       8.18.1\r\nipywidgets                    8.1.3\r\njedi                          0.19.1\r\nJinja2                        3.1.4\r\njmespath                      0.10.0\r\njsonschema                    4.22.0\r\njsonschema-specifications     2023.12.1\r\njupyter_client                8.6.2\r\njupyter_core                  5.7.2\r\njupyterlab_widgets            3.0.11\r\nkiwisolver                    1.4.5\r\nkornia                        0.7.3\r\nkornia_rs                     0.1.5\r\nlatex2mathml                  3.77.0\r\nMarkdown                      3.6\r\nmarkdown-it-py                3.0.0\r\nMarkupSafe                    2.1.5\r\nmatplotlib                    3.9.0\r\nmatplotlib-inline             0.1.7\r\nmdtex2html                    1.3.0\r\nmdurl                         0.1.2\r\nmodelscope                    1.11.0\r\nmpmath                        1.3.0\r\nmultidict                     6.0.5\r\nmultiprocess                  0.70.16\r\nnest-asyncio                  1.6.0\r\nnetworkx                      3.2.1\r\nnumpy                         1.26.4\r\nomegaconf                     2.3.0\r\norjson                        3.10.5\r\noss2                          2.18.6\r\npackaging                     24.1\r\npandas                        2.2.2\r\nparso                         0.8.4\r\npexpect                       4.9.0\r\npillow                        10.3.0\r\npip                           24.0\r\nplatformdirs                  4.2.2\r\nprompt_toolkit                3.0.47\r\nprotobuf                      4.21.0\r\npsutil                        6.0.0\r\nptyprocess                    0.7.0\r\npure-eval                     0.2.2\r\npy-cpuinfo                    9.0.0\r\npyarrow                       17.0.0\r\npyarrow-hotfix                0.6\r\npycparser                     2.22\r\npycryptodome                  3.20.0\r\npydantic                      2.7.4\r\npydantic_core                 2.18.4\r\npydub                         0.25.1\r\nPygments                      2.18.0\r\npyparsing                     3.1.2\r\npython-dateutil               2.9.0.post0\r\npython-dotenv                 1.0.1\r\npython-multipart              0.0.9\r\npytz                          2024.1\r\nPyYAML                        6.0.1\r\npyzmq                         26.0.3\r\nreferencing                   0.35.1\r\nregex                         2024.5.15\r\nrequests                      2.32.3\r\nrich                          13.7.1\r\nrpds-py                       0.18.1\r\nruff                          0.4.9\r\nsafetensors                   0.4.3\r\nscipy                         1.13.1\r\nsemantic-version              2.10.0\r\nsentencepiece                 0.1.99\r\nsetuptools                    69.5.1\r\nshellingham                   1.5.4\r\nsimplejson                    3.19.2\r\nsix                           1.16.0\r\nsniffio                       1.3.1\r\nsortedcontainers              2.4.0\r\nspandrel                      0.3.4\r\nstack-data                    0.6.3\r\nstarlette                     0.37.2\r\nsympy                         1.12.1\r\ntabulate                      0.9.0\r\ntiktoken                      0.7.0\r\ntimm                          1.0.7\r\ntokenizers                    0.15.2\r\ntomli                         2.0.1\r\ntomlkit                       0.12.0\r\ntoolz                         0.12.1\r\ntorch                         2.1.0a0+cxx11.abi\r\ntorchsde                      0.2.6\r\ntorchvision                   0.16.0a0+cxx11.abi\r\ntornado                       6.4.1\r\ntqdm                          4.66.4\r\ntraitlets                     5.14.3\r\ntrampoline                    0.1.2\r\ntransformers                  4.36.2\r\ntransformers-stream-generator 0.0.5\r\ntyper                         0.12.3\r\ntyping_extensions             4.12.2\r\ntzdata                        2024.1\r\nujson                         5.10.0\r\nurllib3                       2.2.2\r\nuvicorn                       0.30.1\r\nuvloop                        0.19.0\r\nviola                         0.3.8\r\nwatchfiles                    0.22.0\r\nwcwidth                       0.2.13\r\nwebsockets                    11.0.3\r\nwheel                         0.43.0\r\nwidgetsnbextension            4.0.11\r\nxxhash                        3.4.1\r\nyapf                          0.40.2\r\nyarl                          1.9.4\r\nzipp                          3.19.2",
      "state": "open",
      "author": "johnysh",
      "author_type": "User",
      "created_at": "2024-07-24T08:17:57Z",
      "updated_at": "2024-08-05T12:29:16Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 4,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11650/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "qiuxin2012"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11650",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11650",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:43.838986",
      "comments": [
        {
          "author": "qiuxin2012",
          "body": "It's caused by the mismatch of transformers and latest glm4. The glm4 is updated 9 days ago, the line number in error message shows you are using the latest version of `modeling_chatglm.py` file, who required `4.42.4`.\r\nYou can change to the old `modeling_chatglm.py` and `config.json`, who required ",
          "created_at": "2024-07-25T02:06:46Z"
        },
        {
          "author": "qiuxin2012",
          "body": "@JinBridger Please help to test the 4.42.4 and update our example's README.",
          "created_at": "2024-07-25T02:11:59Z"
        },
        {
          "author": "jjzhu0579",
          "body": "> It's caused by the mismatch of transformers and latest glm4. The glm4 is updated 9 days ago, the line number in error message shows you are using the latest version of `modeling_chatglm.py` file, who required `4.42.4`. You can change to the old `modeling_chatglm.py` and `config.json`, who required",
          "created_at": "2024-08-05T11:47:40Z"
        },
        {
          "author": "jjzhu0579",
          "body": "> change to the old `modeling_chatglm.py` and `config.json`\r\n\r\nhow can i change to the old modeling_chatglm.py and config.json,this latest modeling_chatglm.py make too many errors",
          "created_at": "2024-08-05T12:28:59Z"
        }
      ]
    },
    {
      "issue_number": 11269,
      "title": "Ollama Linux seg fault with GPU on Ubuntu 22.04",
      "body": "Ran into seg faults trying to run Ollama on Ubuntu 22.04. This is with an Intel Arc A750 card.\r\n\r\nA few searches showed a similar finigerprint in https://github.com/intel/compute-runtime/issues/710. Reporting this as similar workaround of:\r\n\r\nexport NEOReadDebugKeys=1\r\nexport OverrideGpuAddressSpace=48\r\n\r\ndoes the trick to get things working. Did a:\r\npip install --pre --upgrade ipex-llm[cpp]\r\n\r\njust today, still not getting versions that have this fixed, so maybe coming soon, but wanted to raise failure fingerprint and workaround.\r\n\r\n",
      "state": "closed",
      "author": "doucej",
      "author_type": "User",
      "created_at": "2024-06-09T16:06:02Z",
      "updated_at": "2024-08-04T16:59:15Z",
      "closed_at": "2024-08-04T16:59:15Z",
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 7,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11269/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [
        "sgwhat"
      ],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11269",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11269",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:44.100510",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @doucej , could you please provide more information from the ollama server side (like the ollama server log)? This would be helpful for us in addressing the issue and fixing it.\r\n\r\nAlso, would you mind running this https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/scripts to check ",
          "created_at": "2024-06-11T01:47:32Z"
        },
        {
          "author": "doucej",
          "body": "Sure -- below is failure log, starting via:\r\n\r\nexport OLLAMA_NUM_GPU=999\r\nexport no_proxy=localhost,127.0.0.1\r\nexport ZES_ENABLE_SYSMAN=1\r\nsource /opt/intel/oneapi/setvars.sh\r\nexport SYCL_CACHE_PERSISTENT=1\r\nexport OLLAMA_HOST=0.0.0.0:11434\r\n\r\n\r\n\r\n2024/06/11 10:46:30 routes.go:1028: INFO server conf",
          "created_at": "2024-06-11T14:51:22Z"
        },
        {
          "author": "sgwhat",
          "body": "hi @doucej, this issue is due to oneAPI not being installed correctly. You may run ollama with following steps:\r\n\r\n1. Please run `sycl-ls` to check your sycl devices. The expected output should be as below:\r\n    ```bash\r\n    [opencl:acc:0] Intel(R) FPGA Emulation Platform for OpenCL(TM), Intel(R) FP",
          "created_at": "2024-06-12T02:01:57Z"
        },
        {
          "author": "doucej",
          "body": "Thanks -- yes, I do have my GPU listed:\r\n\r\n```\r\n(base) doucej@kryten:~$ source /opt/intel/oneapi/setvars.sh\r\n \r\n:: initializing oneAPI environment ...\r\n   bash: BASH_VERSION = 5.2.21(1)-release\r\n   args: Using \"$@\" for setvars.sh arguments: \r\n:: advisor -- latest\r\n:: ccl -- latest\r\n:: compiler -- la",
          "created_at": "2024-06-21T12:07:04Z"
        },
        {
          "author": "sgwhat",
          "body": "Hi @doucej , this failure of Ollama to run is because you **haven't correctly installed OneAPI**. If you have installed OneAPI correctly, `[ext_oneapi_level_zero:gpu]` should be present as expected in the output of `sycl-ls`.  \r\n\r\nTo fix this issue, you may follow our guide to reinstall oneAPI: http",
          "created_at": "2024-06-24T03:21:40Z"
        }
      ]
    },
    {
      "issue_number": 11698,
      "title": "bark model on intel gpu takes 60 seconds",
      "body": "hello i am attempting to create text to speech with bark on intel a770 but it takes around 60 seconds to generate audio is that normal ? is there a way to make it faster like few seconds ? https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/GPU/PyTorch-Models/Model/bark\r\n\r\n`(phytia2) C:\\phytia\\Phytia>python ./synthesize_speech.py --text \"IPEX-LLM is a library for running large language model on Intel XPU with very low latency.\"\r\nC:\\Users\\SlyRebula\\miniconda3\\envs\\phytia2\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\SlyRebula\\miniconda3\\envs\\Phytia2\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n  warn(\r\n2024-07-31 13:47:18,476 - INFO - intel_extension_for_pytorch auto imported\r\nC:\\Users\\SlyRebula\\miniconda3\\envs\\phytia2\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\nC:\\Users\\SlyRebula\\miniconda3\\envs\\phytia2\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\r\n  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\r\n2024-07-31 13:47:22,731 - INFO - Converting the current model to sym_int4 format......\r\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\r\nSetting `pad_token_id` to `eos_token_id`:10000 for open-end generation.\r\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\r\nSetting `pad_token_id` to `eos_token_id`:10000 for open-end generation.\r\nInference time: 54.660537242889404 s`",
      "state": "open",
      "author": "SlyRebula",
      "author_type": "User",
      "created_at": "2024-07-31T10:51:46Z",
      "updated_at": "2024-08-04T02:44:19Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11698/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11698",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11698",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:44.396208",
      "comments": [
        {
          "author": "lzivan",
          "body": "Hi, we are trying to reproduce your issue.",
          "created_at": "2024-08-01T06:05:18Z"
        },
        {
          "author": "lzivan",
          "body": "Hi @SlyRebula , we tried several times but couldn't reproduce your problem. We got our inference times around 11s.\r\n```bash\r\n(bark) arda@arda-arc01:~/zijie/bark$ python ./synthesize_speech.py --repo-id-or-model-path /mnt/disk1/models/bark-small --text 'IPEX-LLM is a library for running large languag",
          "created_at": "2024-08-01T06:45:05Z"
        }
      ]
    },
    {
      "issue_number": 11607,
      "title": "Ollama not utilizing ARC A770 GPU on a Windows 11 Pro install ",
      "body": "I have the latest Windows ARC Drivers installed however Ollama utilizes my CPU rather than my Intel powered GPU (Asrock A770, 16GB)\r\n\r\nRegards\r\n",
      "state": "open",
      "author": "SG-Python1",
      "author_type": "User",
      "created_at": "2024-07-18T00:42:59Z",
      "updated_at": "2024-08-03T01:10:52Z",
      "closed_at": null,
      "labels": [
        "user issue"
      ],
      "label_count": 1,
      "has_labels": true,
      "comments_count": 2,
      "reactions": {
        "url": "https://api.github.com/repos/intel/ipex-llm/issues/11607/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "assignees": [],
      "milestone": null,
      "html_url": "https://github.com/intel/ipex-llm/issues/11607",
      "api_url": "https://api.github.com/repos/intel/ipex-llm/issues/11607",
      "repository": "intel/ipex-llm",
      "extraction_date": "2025-06-22T00:32:46.463024",
      "comments": [
        {
          "author": "sgwhat",
          "body": "Hi @SG-Python1 , \r\n1. Could you provide the output of `ollama serve` when doing model inference.\r\n2. You may run the ENV-Check script in https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/scripts and reply us with the results.",
          "created_at": "2024-07-18T01:54:28Z"
        },
        {
          "author": "KyleHagy",
          "body": "Hi @sgwhat,\r\n\r\nI'm experiencing the same issue. I have the latest Intel GPU driver for my A770 and the latest Windows update. \r\n\r\nMy output from `ollama serve`: \r\n```\r\n2024/08/02 16:34:04 routes.go:1028: INFO server config env=\"map[OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST: OLLAMA_",
          "created_at": "2024-08-03T00:56:49Z"
        }
      ]
    }
  ]
}